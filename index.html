<html><head><script src="resources/jquery.min.js"></script><link href="resources/bootstrap.min.css" rel="stylesheet"></link><link href="resources/bootstrap-theme.min.css" rel="stylesheet"></link><script src="resources/bootstrap.min.js"></script><script src="helper.js">function hideAll(){console.log("hideAll")}function showme(e){console.log("showme");var l,n=e.substring(7),o=document.getElementsByName("data");for(l=0;l&lt;o.length;l++)o[l].style.display="none";var t=document.getElementsByName("summary");for(l=0;l&lt;t.length;l++)t[l].style.display="none";document.getElementById(n).style.display="block"}</script><style>table, th, td { vertical-align:top; padding: 3px} table {table-layout:fixed} td {word-wrap:break-word} .bs-callout { padding: 5px; margin: 5px 0; border: 1px solid #eee; border-left-width: 5px; border-radius: 3px; font-weight:normal; }.bs-callout-info {border-left-color: #5bc0de;}</style></head><body><nav class="navbar navbar-light"><div style="background-color: #F0F8FF;" class="container-fluid"><ul class="nav nav-pills"><li role="presentation"><a style="font-weight:bold" href="#" id="anchor_ppcx86" onclick="showme(this.id);">FULL SUMMARY</a></li><li role="presentation"><a style="font-weight:bold" href="#" id="anchor_ubuntu16" onclick="showme(this.id);">UBUNTU16</a></li><li role="presentation"><a style="font-weight:bold" href="#" id="anchor_ubuntu18" onclick="showme(this.id);">UBUNTU18</a></li><li role="presentation"><a style="font-weight:bold" href="#" id="anchor_rhel72" onclick="showme(this.id);">RHEL72</a></li><li role="presentation"><a style="font-weight:bold" href="#" id="anchor_rhel75" onclick="showme(this.id);">RHEL75</a></li><li role="presentation"><a style="font-weight:bold" href="#" id="anchor_developers" onclick="showme(this.id);">DEVELOPERS</a></li><p style="float:right;color:grey;font-size:13;padding-top:5px" role="presentation">11-11-2018 01:30 UTC</p></ul><div style="float:right;color:grey;font-size:12">Notations:<img src="resources/red.png" style="width: 16px; height: 16px;">Build failed </img><img src="resources/blue.png" style="width: 16px; height: 16px;">Build success with no failure </img><img src="resources/yellow.png" style="width: 16px; height: 16px;">N (M) Build success with N test failures &amp; M unique failures </img></div></div></nav><div style="table-cell" class="col-sm-2 col-md-2 sidebar"><div class="list-group"><a href="#" class="list-group-item list-group-item-action active" onclick="showme(this.id);" id="anchor_ppcx86">Packages</a><a class="list-group-item list-group-item-action" href="#" id="anchor_accumulo" onclick="showme(this.id);" title="Owned by Prajyot">ACCUMULO</a><a class="list-group-item list-group-item-action" href="#" id="anchor_ambari" onclick="showme(this.id);" title="Owned by Prajyot">AMBARI</a><a class="list-group-item list-group-item-action" href="#" id="anchor_atlas" onclick="showme(this.id);" title="Owned by Yussuf">ATLAS</a><a class="list-group-item list-group-item-action" href="#" id="anchor_calcite" onclick="showme(this.id);" title="Owned by Pravin">CALCITE</a><a class="list-group-item list-group-item-action" href="#" id="anchor_datafu" onclick="showme(this.id);" title="Owned by N/A">DATAFU</a><a class="list-group-item list-group-item-action" href="#" id="anchor_druid" onclick="showme(this.id);" title="Owned by N/A">DRUID</a><a class="list-group-item list-group-item-action" href="#" id="anchor_falcon" onclick="showme(this.id);" title="Owned by Yussuf">FALCON</a><a class="list-group-item list-group-item-action" href="#" id="anchor_flume" onclick="showme(this.id);" title="Owned by Pravin">FLUME</a><a class="list-group-item list-group-item-action" href="#" id="anchor_hadoop" onclick="showme(this.id);" title="Owned by Pravin">HADOOP</a><a class="list-group-item list-group-item-action" href="#" id="anchor_hbase" onclick="showme(this.id);" title="Owned by Prajyot">HBASE</a><a class="list-group-item list-group-item-action" href="#" id="anchor_hive" onclick="showme(this.id);" title="Owned by Alisha">HIVE</a><a class="list-group-item list-group-item-action" href="#" id="anchor_kafka" onclick="showme(this.id);" title="Owned by Prajyot">KAFKA</a><a class="list-group-item list-group-item-action" href="#" id="anchor_knox" onclick="showme(this.id);" title="Owned by Yussuf">KNOX</a><a class="list-group-item list-group-item-action" href="#" id="anchor_metron" onclick="showme(this.id);" title="Owned by Pravin">METRON</a><a class="list-group-item list-group-item-action" href="#" id="anchor_oozie" onclick="showme(this.id);" title="Owned by Alisha">OOZIE</a><a class="list-group-item list-group-item-action" href="#" id="anchor_phoenix" onclick="showme(this.id);" title="Owned by Prajyot">PHOENIX</a><a class="list-group-item list-group-item-action" href="#" id="anchor_pig" onclick="showme(this.id);" title="Owned by Yussuf">PIG</a><a class="list-group-item list-group-item-action" href="#" id="anchor_ranger" onclick="showme(this.id);" title="Owned by Yussuf">RANGER</a><a class="list-group-item list-group-item-action" href="#" id="anchor_slider" onclick="showme(this.id);" title="Owned by Yussuf">SLIDER</a><a class="list-group-item list-group-item-action" href="#" id="anchor_spark" onclick="showme(this.id);" title="Owned by Prajyot">SPARK</a><a class="list-group-item list-group-item-action" href="#" id="anchor_sqoop" onclick="showme(this.id);" title="Owned by Yussuf">SQOOP</a><a class="list-group-item list-group-item-action" href="#" id="anchor_storm" onclick="showme(this.id);" title="Owned by Alisha">STORM</a><a class="list-group-item list-group-item-action" href="#" id="anchor_tez" onclick="showme(this.id);" title="Owned by Prajyot">TEZ</a><a class="list-group-item list-group-item-action" href="#" id="anchor_zeppelin" onclick="showme(this.id);" title="Owned by Alisha">ZEPPELIN</a><a class="list-group-item list-group-item-action" href="#" id="anchor_zookeeper" onclick="showme(this.id);" title="Owned by Pravin">ZOOKEEPER</a></div></div><div style="display: table-cell"><div id="developers" style="display:block;font-weight:bold;display:none;" class="panel panel-info" name="summary"><div class="panel-heading"><div class="panel-title">DEVELOPERS</div></div><div class="panel-body"><table style="font-size:15" id="summarytable" class="table table-striped"><tr><td style="width: 100px;font-weight:bold">ALISHA</td><td><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_storm">STORM </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_zeppelin">ZEPPELIN </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_oozie">OOZIE </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_hive">HIVE </button></td></tr><tr><td style="width: 100px;font-weight:bold">PRAVIN</td><td><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_metron">METRON </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_zookeeper">ZOOKEEPER </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_hadoop">HADOOP </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_calcite">CALCITE </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_flume">FLUME </button></td></tr><tr><td style="width: 100px;font-weight:bold">PRAJYOT</td><td><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_kafka">KAFKA </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_tez">TEZ </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_hbase">HBASE </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_accumulo">ACCUMULO </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_phoenix">PHOENIX </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_spark">SPARK </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_ambari">AMBARI </button></td></tr><tr><td style="width: 100px;font-weight:bold">YUSSUF</td><td><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_knox">KNOX </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_atlas">ATLAS </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_sqoop">SQOOP </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_slider">SLIDER </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_falcon">FALCON </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_pig">PIG </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_ranger">RANGER </button></td></tr></table></div></div></div><div style="display: table-cell"><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="accumulo"><div style="font-weight:bold;" class="panel-heading">ACCUMULO<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Prajyot)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>d83837c81a29f253e41bac313ae768963320e71a</div><div><b>Last Run: </b>30-10-2018 20:26 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC UBUNTU18</th><th>X86 UBUNTU18</th><th>PPC RHEL72</th><th>X86 RHEL72</th><th>PPC RHEL75</th><th>X86 RHEL75</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 1715</div><div>Failed Count : 0</div><div>Skipped Count : 6</div></td><td><div>Total Count : 1713</div><div>Failed Count : 0</div><div>Skipped Count : 6</div></td><td><div>Total Count : 1715</div><div>Failed Count : 0</div><div>Skipped Count : 6</div></td><td><div>Total Count : 1713</div><div>Failed Count : 0</div><div>Skipped Count : 6</div></td><td><div>Total Count : 1715</div><div>Failed Count : 0</div><div>Skipped Count : 6</div></td><td><div>Total Count : 1713</div><div>Failed Count : 2</div><div>Skipped Count : 6</div></td><td><div>Total Count : 1715</div><div>Failed Count : 0</div><div>Skipped Count : 6</div></td><td><div>Total Count : 1713</div><div>Failed Count : 0</div><div>Skipped Count : 6</div></td></tr><tr><td>Result</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.accumulo.minicluster.MiniAccumuloClusterTest.testPerTableClasspath</li></div><div><li>org.apache.accumulo.minicluster.MiniAccumuloClusterTest.test</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>test timed out after 60000 milliseconds</li></div><div><li>test timed out after 30000 milliseconds</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.accumulo.minicluster.MiniAccumuloClusterTest.testPerTableClasspath</div></li><li><div>org.apache.accumulo.minicluster.MiniAccumuloClusterTest.test</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="ambari"><div style="font-weight:bold;" class="panel-heading">AMBARI<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Prajyot)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> origin/trunk</div><div><b>Last Revision: </b>d7ce1ef3668d551061fa0539fda7befdb6cb1b43</div><div><b>Last Run: </b>31-10-2018 00:31 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC UBUNTU18</th><th>X86 UBUNTU18</th><th>PPC RHEL72</th><th>X86 RHEL72</th><th>PPC RHEL75</th><th>X86 RHEL75</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 0</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 5192</div><div>Failed Count : 1</div><div>Skipped Count : 72</div></td><td><div>Total Count : 0</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 5192</div><div>Failed Count : 1</div><div>Skipped Count : 72</div></td><td><div>Total Count : 5191</div><div>Failed Count : 1</div><div>Skipped Count : 72</div></td><td><div>Total Count : 5192</div><div>Failed Count : 0</div><div>Skipped Count : 72</div></td><td><div>Total Count : 5191</div><div>Failed Count : 1</div><div>Skipped Count : 72</div></td><td><div>Total Count : 5192</div><div>Failed Count : 2</div><div>Skipped Count : 72</div></td></tr><tr><td>Result</td><td>N/A</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td>N/A</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.ambari.server.security.encryption.CredentialStoreTest.testInMemoryCredentialStoreService_CredentialExpired</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.ambari.server.api.services.AmbariMetaInfoTest.testLatestVdf</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.ambari.server.controller.metrics.RestMetricsPropertyProviderTest.testRestMetricsPropertyProviderAsAdministrator</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.ambari.server.orm.dao.AlertDispatchDAOTest.testFindNoticeByUuid</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.ambari.server.controller.test.BufferedThreadPoolExecutorCompletionServiceTest.testScalingThreadPoolExecutor</li></div><div><li>org.apache.ambari.server.state.cluster.ClusterDeadlockTest.testDeadlockWhileRestartingComponents</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>java.lang.NullPointerException
	at org.apache.ambari.server.security.encryption.CredentialStoreTest.getExpiredCredentialTest(CredentialStoreTest.java:169)
	at org.apache.ambari.server.security.encryption.CredentialStoreTest.testInMemoryCredentialStoreService_CredentialExpired(CredentialStoreTest.java:90)
</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>java.lang.AssertionError
	at org.apache.ambari.server.api.services.AmbariMetaInfoTest.testLatestVdf(AmbariMetaInfoTest.java:1436)
</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;400&gt; but was:&lt;157&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>Host not found, hostname=c6401.ambari.apache.org</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;10&gt; but was:&lt;9&gt;</li></div><div><li>[Deadlocked Thread:
------------------
"Thread-22" Id=54 WAITING on java.util.concurrent.locks.ReentrantReadWriteLock$NonfairSync@dbd5c6 owned by "Thread-26" Id=58
 at sun.misc.Unsafe.park(Native Method)
 -  waiting on java.util.concurrent.locks.ReentrantReadWriteLock$NonfairSync@dbd5c6
 at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
 at java.util.concurrent.locks.AbstractQue</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.ambari.server.security.encryption.CredentialStoreTest.testInMemoryCredentialStoreService_CredentialExpired</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.ambari.server.api.services.AmbariMetaInfoTest.testLatestVdf</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.ambari.server.controller.metrics.RestMetricsPropertyProviderTest.testRestMetricsPropertyProviderAsAdministrator</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.ambari.server.orm.dao.AlertDispatchDAOTest.testFindNoticeByUuid</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.ambari.server.controller.test.BufferedThreadPoolExecutorCompletionServiceTest.testScalingThreadPoolExecutor</div></li><li><div>org.apache.ambari.server.state.cluster.ClusterDeadlockTest.testDeadlockWhileRestartingComponents</div></li></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="atlas"><div style="font-weight:bold;" class="panel-heading">ATLAS<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Yussuf)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>93bd535eb66562ea8aadf5219d09a7893fff724e</div><div><b>Last Run: </b>24-10-2018 15:22 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC UBUNTU18</th><th>X86 UBUNTU18</th><th>PPC RHEL72</th><th>X86 RHEL72</th><th>PPC RHEL75</th><th>X86 RHEL75</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 956</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 956</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 956</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 956</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 956</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 956</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 956</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 956</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td></tr><tr><td>Result</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="calcite"><div style="font-weight:bold;" class="panel-heading">CALCITE<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Pravin)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>6284d3c88d9c659e0a8ef3dd5cf4819133de7522</div><div><b>Last Run: </b>29-10-2018 01:44 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC UBUNTU18</th><th>X86 UBUNTU18</th><th>PPC RHEL72</th><th>X86 RHEL72</th><th>PPC RHEL75</th><th>X86 RHEL75</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 5884</div><div>Failed Count : 0</div><div>Skipped Count : 179</div></td><td><div>Total Count : 5895</div><div>Failed Count : 0</div><div>Skipped Count : 179</div></td><td><div>Total Count : 5884</div><div>Failed Count : 0</div><div>Skipped Count : 179</div></td><td><div>Total Count : 5895</div><div>Failed Count : 0</div><div>Skipped Count : 179</div></td><td><div>Total Count : 5884</div><div>Failed Count : 0</div><div>Skipped Count : 179</div></td><td><div>Total Count : 5895</div><div>Failed Count : 0</div><div>Skipped Count : 179</div></td><td><div>Total Count : 5884</div><div>Failed Count : 0</div><div>Skipped Count : 179</div></td><td><div>Total Count : 5895</div><div>Failed Count : 0</div><div>Skipped Count : 179</div></td></tr><tr><td>Result</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="datafu"><div style="font-weight:bold;" class="panel-heading">DATAFU<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(N/A)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>857cf164c30883d739c4895c9a9c758880526435</div><div><b>Last Run: </b>05-11-2018 01:11 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC UBUNTU18</th><th>X86 UBUNTU18</th><th>PPC RHEL72</th><th>X86 RHEL72</th><th>PPC RHEL75</th><th>X86 RHEL75</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 278</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 278</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 278</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 278</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 278</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 278</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 278</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 278</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td></tr><tr><td>Result</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="druid"><div style="font-weight:bold;" class="panel-heading">DRUID<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(N/A)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>c780aacc03e30b929ba14f70d7f278811fd8ba44</div><div><b>Last Run: </b>17-10-2018 06:25 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC UBUNTU18</th><th>X86 UBUNTU18</th><th>PPC RHEL72</th><th>X86 RHEL72</th><th>PPC RHEL75</th><th>X86 RHEL75</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 90903</div><div>Failed Count : 6</div><div>Skipped Count : 91</div></td><td><div>Total Count : 90903</div><div>Failed Count : 2</div><div>Skipped Count : 91</div></td><td><div>Total Count : 90903</div><div>Failed Count : 6</div><div>Skipped Count : 91</div></td><td><div>Total Count : 90903</div><div>Failed Count : 2</div><div>Skipped Count : 91</div></td><td><div>Total Count : 90903</div><div>Failed Count : 6</div><div>Skipped Count : 91</div></td><td><div>Total Count : 90903</div><div>Failed Count : 2</div><div>Skipped Count : 91</div></td><td><div>Total Count : 90903</div><div>Failed Count : 6</div><div>Skipped Count : 91</div></td><td><div>Total Count : 90903</div><div>Failed Count : 2</div><div>Skipped Count : 91</div></td></tr><tr><td>Result</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"><div><li>org.apache.druid.java.util.metrics.MonitorsTest.testDefaultFeed</li></div><div><li>org.apache.druid.java.util.metrics.MonitorsTest.testSetFeed</li></div><div><li>org.apache.druid.java.util.metrics.SigarLoadTest.testSigarLoad</li></div><div><li>org.apache.druid.java.util.metrics.SigarPidDiscovererTest.simpleTest</li></div><div><li>org.apache.druid.storage.s3.TestAWSCredentialsProvider.testWithFileSessionCredentials</li></div><div><li>org.apache.druid.storage.s3.TestAWSCredentialsProvider.testWithFixedAWSKeys</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.druid.storage.s3.TestAWSCredentialsProvider.testWithFileSessionCredentials</li></div><div><li>org.apache.druid.storage.s3.TestAWSCredentialsProvider.testWithFixedAWSKeys</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.druid.java.util.metrics.MonitorsTest.testDefaultFeed</li></div><div><li>org.apache.druid.java.util.metrics.MonitorsTest.testSetFeed</li></div><div><li>org.apache.druid.java.util.metrics.SigarLoadTest.testSigarLoad</li></div><div><li>org.apache.druid.java.util.metrics.SigarPidDiscovererTest.simpleTest</li></div><div><li>org.apache.druid.storage.s3.TestAWSCredentialsProvider.testWithFileSessionCredentials</li></div><div><li>org.apache.druid.storage.s3.TestAWSCredentialsProvider.testWithFixedAWSKeys</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.druid.storage.s3.TestAWSCredentialsProvider.testWithFileSessionCredentials</li></div><div><li>org.apache.druid.storage.s3.TestAWSCredentialsProvider.testWithFixedAWSKeys</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.druid.java.util.metrics.MonitorsTest.testDefaultFeed</li></div><div><li>org.apache.druid.java.util.metrics.MonitorsTest.testSetFeed</li></div><div><li>org.apache.druid.java.util.metrics.SigarLoadTest.testSigarLoad</li></div><div><li>org.apache.druid.java.util.metrics.SigarPidDiscovererTest.simpleTest</li></div><div><li>org.apache.druid.storage.s3.TestAWSCredentialsProvider.testWithFileSessionCredentials</li></div><div><li>org.apache.druid.storage.s3.TestAWSCredentialsProvider.testWithFixedAWSKeys</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.druid.storage.s3.TestAWSCredentialsProvider.testWithFileSessionCredentials</li></div><div><li>org.apache.druid.storage.s3.TestAWSCredentialsProvider.testWithFixedAWSKeys</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.druid.java.util.metrics.MonitorsTest.testDefaultFeed</li></div><div><li>org.apache.druid.java.util.metrics.MonitorsTest.testSetFeed</li></div><div><li>org.apache.druid.java.util.metrics.SigarLoadTest.testSigarLoad</li></div><div><li>org.apache.druid.java.util.metrics.SigarPidDiscovererTest.simpleTest</li></div><div><li>org.apache.druid.storage.s3.TestAWSCredentialsProvider.testWithFileSessionCredentials</li></div><div><li>org.apache.druid.storage.s3.TestAWSCredentialsProvider.testWithFixedAWSKeys</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.druid.storage.s3.TestAWSCredentialsProvider.testWithFileSessionCredentials</li></div><div><li>org.apache.druid.storage.s3.TestAWSCredentialsProvider.testWithFixedAWSKeys</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"><div><li>org.hyperic.sigar.Sigar.getPid()J</li></div><div><li>org.hyperic.sigar.Sigar.getPid()J</li></div><div><li>org.hyperic.sigar.Sigar.getPid()J</li></div><div><li>org.hyperic.sigar.Sigar.getPid()J</li></div><div><li>Unable to find a region via the region provider chain. Must provide an explicit region in the builder or setup environment to supply a region.</li></div><div><li>Unable to find a region via the region provider chain. Must provide an explicit region in the builder or setup environment to supply a region.</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Unable to find a region via the region provider chain. Must provide an explicit region in the builder or setup environment to supply a region.</li></div><div><li>Unable to find a region via the region provider chain. Must provide an explicit region in the builder or setup environment to supply a region.</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.hyperic.sigar.Sigar.getPid()J</li></div><div><li>org.hyperic.sigar.Sigar.getPid()J</li></div><div><li>org.hyperic.sigar.Sigar.getPid()J</li></div><div><li>org.hyperic.sigar.Sigar.getPid()J</li></div><div><li>Unable to find a region via the region provider chain. Must provide an explicit region in the builder or setup environment to supply a region.</li></div><div><li>Unable to find a region via the region provider chain. Must provide an explicit region in the builder or setup environment to supply a region.</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Unable to find a region via the region provider chain. Must provide an explicit region in the builder or setup environment to supply a region.</li></div><div><li>Unable to find a region via the region provider chain. Must provide an explicit region in the builder or setup environment to supply a region.</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.hyperic.sigar.Sigar.getPid()J</li></div><div><li>org.hyperic.sigar.Sigar.getPid()J</li></div><div><li>org.hyperic.sigar.Sigar.getPid()J</li></div><div><li>org.hyperic.sigar.Sigar.getPid()J</li></div><div><li>Unable to find a region via the region provider chain. Must provide an explicit region in the builder or setup environment to supply a region.</li></div><div><li>Unable to find a region via the region provider chain. Must provide an explicit region in the builder or setup environment to supply a region.</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Unable to find a region via the region provider chain. Must provide an explicit region in the builder or setup environment to supply a region.</li></div><div><li>Unable to find a region via the region provider chain. Must provide an explicit region in the builder or setup environment to supply a region.</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.hyperic.sigar.Sigar.getPid()J</li></div><div><li>org.hyperic.sigar.Sigar.getPid()J</li></div><div><li>org.hyperic.sigar.Sigar.getPid()J</li></div><div><li>org.hyperic.sigar.Sigar.getPid()J</li></div><div><li>Unable to find a region via the region provider chain. Must provide an explicit region in the builder or setup environment to supply a region.</li></div><div><li>Unable to find a region via the region provider chain. Must provide an explicit region in the builder or setup environment to supply a region.</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Unable to find a region via the region provider chain. Must provide an explicit region in the builder or setup environment to supply a region.</li></div><div><li>Unable to find a region via the region provider chain. Must provide an explicit region in the builder or setup environment to supply a region.</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.druid.java.util.metrics.MonitorsTest.testDefaultFeed</div></li><li><div>org.apache.druid.java.util.metrics.MonitorsTest.testSetFeed</div></li><li><div>org.apache.druid.java.util.metrics.SigarLoadTest.testSigarLoad</div></li><li><div>org.apache.druid.java.util.metrics.SigarPidDiscovererTest.simpleTest</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.druid.java.util.metrics.MonitorsTest.testDefaultFeed</div></li><li><div>org.apache.druid.java.util.metrics.MonitorsTest.testSetFeed</div></li><li><div>org.apache.druid.java.util.metrics.SigarLoadTest.testSigarLoad</div></li><li><div>org.apache.druid.java.util.metrics.SigarPidDiscovererTest.simpleTest</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.druid.java.util.metrics.MonitorsTest.testDefaultFeed</div></li><li><div>org.apache.druid.java.util.metrics.MonitorsTest.testSetFeed</div></li><li><div>org.apache.druid.java.util.metrics.SigarLoadTest.testSigarLoad</div></li><li><div>org.apache.druid.java.util.metrics.SigarPidDiscovererTest.simpleTest</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.druid.java.util.metrics.MonitorsTest.testDefaultFeed</div></li><li><div>org.apache.druid.java.util.metrics.MonitorsTest.testSetFeed</div></li><li><div>org.apache.druid.java.util.metrics.SigarLoadTest.testSigarLoad</div></li><li><div>org.apache.druid.java.util.metrics.SigarPidDiscovererTest.simpleTest</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="falcon"><div style="font-weight:bold;" class="panel-heading">FALCON<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Yussuf)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>470e5e9f5de9ba1b6149dec60e87d3a04270eda3</div><div><b>Last Run: </b>31-10-2018 03:27 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC UBUNTU18</th><th>X86 UBUNTU18</th><th>PPC RHEL72</th><th>X86 RHEL72</th><th>PPC RHEL75</th><th>X86 RHEL75</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 996</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 1003</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 1003</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 999</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 1001</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 1001</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 1003</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 997</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td></tr><tr><td>Result</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="flume"><div style="font-weight:bold;" class="panel-heading">FLUME<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Pravin)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/trunk</div><div><b>Last Revision: </b>493b53b6430c687f88c438e5883ef9120d7aedb2</div><div><b>Last Run: </b>06-11-2018 01:48 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC UBUNTU18</th><th>X86 UBUNTU18</th><th>PPC RHEL72</th><th>X86 RHEL72</th><th>PPC RHEL75</th><th>X86 RHEL75</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 1312</div><div>Failed Count : 0</div><div>Skipped Count : 7</div></td><td><div>Total Count : 1309</div><div>Failed Count : 0</div><div>Skipped Count : 7</div></td><td><div>Total Count : 1312</div><div>Failed Count : 0</div><div>Skipped Count : 7</div></td><td><div>Total Count : 1312</div><div>Failed Count : 0</div><div>Skipped Count : 7</div></td><td><div>Total Count : 1312</div><div>Failed Count : 0</div><div>Skipped Count : 7</div></td><td><div>Total Count : 1312</div><div>Failed Count : 0</div><div>Skipped Count : 7</div></td><td><div>Total Count : 1280</div><div>Failed Count : 5</div><div>Skipped Count : 7</div></td><td><div>Total Count : 1312</div><div>Failed Count : 0</div><div>Skipped Count : 7</div></td></tr><tr><td>Result</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.flume.channel.file.TestLog.testMinimumRequiredSpaceTooSmallForPut</li></div><div><li>org.apache.flume.sink.kafka.TestKafkaSink.testTopicAndKeyFromHeader</li></div><div><li>org.apache.flume.sink.kafka.TestKafkaSink.testTopicFromConfHeader</li></div><div><li>org.apache.flume.sink.kafka.TestKafkaSink.testAvroEvent</li></div><div><li>org.apache.flume.sink.kafka.TestKafkaSink.org.apache.flume.sink.kafka.TestKafkaSink</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>java.lang.AssertionError
	at org.apache.flume.channel.file.TestLog.doTestMinimumRequiredSpaceTooSmallForPut(TestLog.java:241)
	at org.apache.flume.channel.file.TestLog.testMinimumRequiredSpaceTooSmallForPut(TestLog.java:210)
</li></div><div><li>java.lang.NullPointerException
	at org.apache.flume.sink.kafka.TestKafkaSink.testTopicAndKeyFromHeader(TestKafkaSink.java:240)
</li></div><div><li>expected:&lt;test-topic-[from-config]-header&gt; but was:&lt;test-topic-[and-key-from]-header&gt;</li></div><div><li>java.lang.IndexOutOfBoundsException
	at org.apache.flume.sink.kafka.TestKafkaSink.testAvroEvent(TestKafkaSink.java:408)
</li></div><div><li>ZookeeperConsumerConnector can create message streams at most once</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.flume.channel.file.TestLog.testMinimumRequiredSpaceTooSmallForPut</div></li><li><div>org.apache.flume.sink.kafka.TestKafkaSink.testTopicAndKeyFromHeader</div></li><li><div>org.apache.flume.sink.kafka.TestKafkaSink.testTopicFromConfHeader</div></li><li><div>org.apache.flume.sink.kafka.TestKafkaSink.testAvroEvent</div></li><li><div>org.apache.flume.sink.kafka.TestKafkaSink.org.apache.flume.sink.kafka.TestKafkaSink</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="hadoop"><div style="font-weight:bold;" class="panel-heading">HADOOP<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Pravin)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/trunk</div><div><b>Last Revision: </b>52cb766ad0bbea378a529fcff3cf372cf0a33cde</div><div><b>Last Run: </b>22-10-2018 13:31 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC UBUNTU18</th><th>X86 UBUNTU18</th><th>PPC RHEL72</th><th>X86 RHEL72</th><th>PPC RHEL75</th><th>X86 RHEL75</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 20268</div><div>Failed Count : 67</div><div>Skipped Count : 1186</div></td><td><div>Total Count : 19903</div><div>Failed Count : 61</div><div>Skipped Count : 1186</div></td><td><div>Total Count : 19374</div><div>Failed Count : 69</div><div>Skipped Count : 1140</div></td><td><div>Total Count : 19029</div><div>Failed Count : 71</div><div>Skipped Count : 1138</div></td><td><div>Total Count : 19926</div><div>Failed Count : 212</div><div>Skipped Count : 1185</div></td><td><div>Total Count : 19904</div><div>Failed Count : 63</div><div>Skipped Count : 1185</div></td><td><div>Total Count : 20273</div><div>Failed Count : 60</div><div>Skipped Count : 1186</div></td><td><div>Total Count : 19904</div><div>Failed Count : 68</div><div>Skipped Count : 1185</div></td></tr><tr><td>Result</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider.testTokenServiceCreationWithLegacyFormat</li></div><div><li>org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider.testTokenServiceCreationWithUriFormat</li></div><div><li>org.apache.hadoop.util.TestShell.testEnvVarsWithInheritance</li></div><div><li>org.apache.hadoop.hdfs.server.federation.router.TestRouterClientRejectOverload.testOverloadControl</li></div><div><li>org.apache.hadoop.hdfs.TestLeaseRecovery2.testCloseWhileRecoverLease</li></div><div><li>org.apache.hadoop.hdfs.TestReadStripedFileWithMissingBlocks.testReadFileWithMissingBlocks</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.TestNNHandlesBlockReportPerStorage.blockReport_08</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestLazyPersistReplicaRecovery.testDnRestartWithSavedReplicas</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestSpaceReservation.testReservedSpaceForLeaseRecovery</li></div><div><li>org.apache.hadoop.hdfs.server.namenode.TestPersistentStoragePolicySatisfier.testSPSxAttrWhenSpsCalledForDir</li></div><div><li>org.apache.hadoop.mapreduce.v2.hs.server.TestHSAdminServer.testRefreshSuperUserGroups[0]</li></div><div><li>org.apache.hadoop.mapreduce.v2.hs.server.TestHSAdminServer.testRefreshSuperUserGroups[1]</li></div><div><li>org.apache.hadoop.tools.TestDistCh.testDistCh</li></div><div><li>org.apache.hadoop.mapred.gridmix.TestDistCacheEmulation.org.apache.hadoop.mapred.gridmix.TestDistCacheEmulation</li></div><div><li>org.apache.hadoop.mapred.gridmix.TestGridmixSubmission.org.apache.hadoop.mapred.gridmix.TestGridmixSubmission</li></div><div><li>org.apache.hadoop.mapred.gridmix.TestLoadJob.org.apache.hadoop.mapred.gridmix.TestLoadJob</li></div><div><li>org.apache.hadoop.mapred.gridmix.TestSleepJob.org.apache.hadoop.mapred.gridmix.TestSleepJob</li></div><div><li>org.apache.hadoop.streaming.TestFileArgs.testCommandLine</li></div><div><li>org.apache.hadoop.streaming.TestMultipleArchiveFiles.testCommandLine</li></div><div><li>org.apache.hadoop.streaming.TestMultipleCachefiles.testMultipleCachefiles</li></div><div><li>org.apache.hadoop.streaming.TestStreamingBadRecords.testNoOp</li></div><div><li>org.apache.hadoop.streaming.TestSymLink.testSymLink</li></div><div><li>org.apache.hadoop.yarn.service.monitor.probe.TestDefaultProbe.testDefaultProbe[2]</li></div><div><li>org.apache.hadoop.yarn.util.TestProcfsBasedProcessTree.testProcessTree</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.testContainerUpgradeSuccessExplicitRollback</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestLogAggregationService.testFixedSizeThreadPool</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.TestApplicationMasterServiceFair.testUpdateTrackingUrl</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService.testRefreshAclWithDaemonUser</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService.testRMInitialsWithFileSystemBasedConfigurationProvider</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService.testAdminAclsWithFileSystemBasedConfigurationProvider</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.reservation.TestCapacityOverTimePolicy.testAllocation[Duration 90,000,000, height 0.25, numSubmission 1, periodic 86400000)]</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestContainerAllocation.testUserLimitAllocationMultipleContainers</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestQueueManagementDynamicEditPolicy.testEditSchedule</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.security.TestDelegationTokenRenewer.testRMRestartWithExpiredToken</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesMetricFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowsNotPresent</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRunsMetricsToRetrieve</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlows</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetAppsMetricsRange</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesInfoFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testForFlowRunsPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntityWithoutFlowInfo</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesMetricsTimeRange</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowApps</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRuns</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetAppWithoutFlowInfo</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRunAppsNotPresent</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRun</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntityDataToRetrieve</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesEventFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testUIDQueryWithAndWithoutFlowContextInfo</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowAppsFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesByUID</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowAppsNotPresent</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowsForPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testForFlowAppsPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesConfigFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesDataToRetrieve</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testUIDNotProperlyEscaped</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRunNotPresent</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRunApps</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesRelationFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetApp</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesWithoutFlowInfo</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGenericEntitiesForPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testForFlowRunAppsPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetAppNotPresent</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider.testTokenServiceCreationWithLegacyFormat</li></div><div><li>org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider.testTokenServiceCreationWithUriFormat</li></div><div><li>org.apache.hadoop.util.TestShell.testEnvVarsWithInheritance</li></div><div><li>org.apache.hadoop.hdfs.TestLeaseRecovery2.testCloseWhileRecoverLease</li></div><div><li>org.apache.hadoop.hdfs.TestLeaseRecovery2.testHardLeaseRecoveryWithRenameAfterNameNodeRestart</li></div><div><li>org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFSStriped.testRead</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestLazyPersistReplicaRecovery.testDnRestartWithSavedReplicas</li></div><div><li>org.apache.hadoop.hdfs.server.namenode.ha.TestHAAppend.testMultipleAppendsDuringCatchupTailing</li></div><div><li>org.apache.hadoop.mapreduce.v2.hs.server.TestHSAdminServer.testRefreshSuperUserGroups[0]</li></div><div><li>org.apache.hadoop.mapreduce.v2.hs.server.TestHSAdminServer.testRefreshSuperUserGroups[1]</li></div><div><li>org.apache.hadoop.tools.TestDistCh.testDistCh</li></div><div><li>org.apache.hadoop.mapred.gridmix.TestDistCacheEmulation.org.apache.hadoop.mapred.gridmix.TestDistCacheEmulation</li></div><div><li>org.apache.hadoop.mapred.gridmix.TestGridmixSubmission.org.apache.hadoop.mapred.gridmix.TestGridmixSubmission</li></div><div><li>org.apache.hadoop.mapred.gridmix.TestLoadJob.org.apache.hadoop.mapred.gridmix.TestLoadJob</li></div><div><li>org.apache.hadoop.mapred.gridmix.TestSleepJob.org.apache.hadoop.mapred.gridmix.TestSleepJob</li></div><div><li>org.apache.hadoop.streaming.TestFileArgs.testCommandLine</li></div><div><li>org.apache.hadoop.streaming.TestMultipleArchiveFiles.testCommandLine</li></div><div><li>org.apache.hadoop.streaming.TestMultipleCachefiles.testMultipleCachefiles</li></div><div><li>org.apache.hadoop.streaming.TestStreamingBadRecords.testNoOp</li></div><div><li>org.apache.hadoop.streaming.TestSymLink.testSymLink</li></div><div><li>org.apache.hadoop.yarn.service.monitor.probe.TestDefaultProbe.testDefaultProbe[2]</li></div><div><li>org.apache.hadoop.yarn.util.TestProcfsBasedProcessTree.testProcessTree</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.testLocalingResourceWhileContainerRunning</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService.testRefreshAclWithDaemonUser</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService.testRMInitialsWithFileSystemBasedConfigurationProvider</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService.testAdminAclsWithFileSystemBasedConfigurationProvider</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestContainerAllocation.testUserLimitAllocationMultipleContainers</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestQueueManagementDynamicEditPolicy.testEditSchedule</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesMetricFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowsNotPresent</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRunsMetricsToRetrieve</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlows</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetAppsMetricsRange</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesInfoFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testForFlowRunsPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntityWithoutFlowInfo</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesMetricsTimeRange</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowApps</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRuns</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetAppWithoutFlowInfo</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRunAppsNotPresent</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRun</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntityDataToRetrieve</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesEventFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testUIDQueryWithAndWithoutFlowContextInfo</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowAppsFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesByUID</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowAppsNotPresent</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowsForPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testForFlowAppsPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesConfigFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesDataToRetrieve</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testUIDNotProperlyEscaped</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRunNotPresent</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRunApps</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesRelationFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetApp</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesWithoutFlowInfo</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGenericEntitiesForPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testForFlowRunAppsPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetAppNotPresent</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.io.compress.TestCompressorDecompressor.testCompressorDecompressor</li></div><div><li>org.apache.hadoop.io.compress.TestCompressorDecompressor.testCompressorDecompressorWithExeedBufferLimit</li></div><div><li>org.apache.hadoop.io.compress.snappy.TestSnappyCompressorDecompressor.testSnappyCompressDecompressInMultiThreads</li></div><div><li>org.apache.hadoop.io.compress.snappy.TestSnappyCompressorDecompressor.testSnappyCompressDecompress</li></div><div><li>org.apache.hadoop.ipc.TestRPCWaitForProxy.testInterruptedWaitForProxy</li></div><div><li>org.apache.hadoop.util.TestShell.testEnvVarsWithInheritance</li></div><div><li>org.apache.hadoop.hdfs.TestLeaseRecovery2.testCloseWhileRecoverLease</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.TestDataNodeUUID.testUUIDRegeneration</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testAuthUrlConnectTimeout[timeoutSource=ConnectionFactory]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testConnectTimeout[timeoutSource=ConnectionFactory]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testRedirectConnectTimeout[timeoutSource=ConnectionFactory]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testTwoStepWriteConnectTimeout[timeoutSource=ConnectionFactory]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testAuthUrlConnectTimeout[timeoutSource=Configuration]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testConnectTimeout[timeoutSource=Configuration]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testRedirectConnectTimeout[timeoutSource=Configuration]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testTwoStepWriteConnectTimeout[timeoutSource=Configuration]</li></div><div><li>org.apache.hadoop.mapreduce.v2.hs.server.TestHSAdminServer.testRefreshSuperUserGroups[0]</li></div><div><li>org.apache.hadoop.mapreduce.v2.hs.server.TestHSAdminServer.testRefreshSuperUserGroups[1]</li></div><div><li>org.apache.hadoop.tools.TestDistCh.testDistCh</li></div><div><li>org.apache.hadoop.mapred.gridmix.TestDistCacheEmulation.org.apache.hadoop.mapred.gridmix.TestDistCacheEmulation</li></div><div><li>org.apache.hadoop.mapred.gridmix.TestGridmixSubmission.org.apache.hadoop.mapred.gridmix.TestGridmixSubmission</li></div><div><li>org.apache.hadoop.mapred.gridmix.TestLoadJob.org.apache.hadoop.mapred.gridmix.TestLoadJob</li></div><div><li>org.apache.hadoop.mapred.gridmix.TestSleepJob.org.apache.hadoop.mapred.gridmix.TestSleepJob</li></div><div><li>org.apache.hadoop.streaming.TestFileArgs.testCommandLine</li></div><div><li>org.apache.hadoop.streaming.TestMultipleArchiveFiles.testCommandLine</li></div><div><li>org.apache.hadoop.streaming.TestMultipleCachefiles.testMultipleCachefiles</li></div><div><li>org.apache.hadoop.streaming.TestStreamingBadRecords.testNoOp</li></div><div><li>org.apache.hadoop.streaming.TestSymLink.testSymLink</li></div><div><li>org.apache.hadoop.yarn.service.monitor.probe.TestDefaultProbe.testDefaultProbe[2]</li></div><div><li>org.apache.hadoop.yarn.util.TestProcfsBasedProcessTree.testProcessTree</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService.testRefreshAclWithDaemonUser</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService.testRMInitialsWithFileSystemBasedConfigurationProvider</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService.testAdminAclsWithFileSystemBasedConfigurationProvider</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestContainerAllocation.testUserLimitAllocationMultipleContainers</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestQueueManagementDynamicEditPolicy.testEditSchedule</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.security.TestRMDelegationTokens.testRMDTMasterKeyStateOnRollingMasterKey</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesMetricFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowsNotPresent</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRunsMetricsToRetrieve</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlows</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetAppsMetricsRange</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesInfoFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testForFlowRunsPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntityWithoutFlowInfo</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesMetricsTimeRange</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowApps</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRuns</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetAppWithoutFlowInfo</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRunAppsNotPresent</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRun</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntityDataToRetrieve</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesEventFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testUIDQueryWithAndWithoutFlowContextInfo</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowAppsFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesByUID</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowAppsNotPresent</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowsForPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testForFlowAppsPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesConfigFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesDataToRetrieve</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testUIDNotProperlyEscaped</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRunNotPresent</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRunApps</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesRelationFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetApp</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesWithoutFlowInfo</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGenericEntitiesForPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testForFlowRunAppsPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetAppNotPresent</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider.testTokenServiceCreationWithLegacyFormat</li></div><div><li>org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider.testTokenServiceCreationWithUriFormat</li></div><div><li>org.apache.hadoop.io.compress.TestCompressorDecompressor.testCompressorDecompressor</li></div><div><li>org.apache.hadoop.io.compress.TestCompressorDecompressor.testCompressorDecompressorWithExeedBufferLimit</li></div><div><li>org.apache.hadoop.io.compress.snappy.TestSnappyCompressorDecompressor.testSnappyCompressDecompressInMultiThreads</li></div><div><li>org.apache.hadoop.io.compress.snappy.TestSnappyCompressorDecompressor.testSnappyCompressDecompress</li></div><div><li>org.apache.hadoop.util.TestShell.testEnvVarsWithInheritance</li></div><div><li>org.apache.hadoop.fs.contract.router.web.TestRouterWebHDFSContractAppend.testRenameFileBeingAppended</li></div><div><li>org.apache.hadoop.hdfs.server.federation.router.TestRouterAllResolver.testSpaceAll</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailure.testDataNodeShutdownAfterNumFailedVolumeExceedsTolerated</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestLazyPersistReplicaRecovery.testDnRestartWithSavedReplicas</li></div><div><li>org.apache.hadoop.hdfs.server.namenode.TestDecommissioningStatus.testDecommissionStatus</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testAuthUrlConnectTimeout[timeoutSource=ConnectionFactory]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testConnectTimeout[timeoutSource=ConnectionFactory]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testRedirectConnectTimeout[timeoutSource=ConnectionFactory]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testTwoStepWriteConnectTimeout[timeoutSource=ConnectionFactory]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testAuthUrlConnectTimeout[timeoutSource=Configuration]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testConnectTimeout[timeoutSource=Configuration]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testRedirectConnectTimeout[timeoutSource=Configuration]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testTwoStepWriteConnectTimeout[timeoutSource=Configuration]</li></div><div><li>org.apache.hadoop.mapreduce.v2.hs.server.TestHSAdminServer.testRefreshSuperUserGroups[0]</li></div><div><li>org.apache.hadoop.mapreduce.v2.hs.server.TestHSAdminServer.testRefreshSuperUserGroups[1]</li></div><div><li>org.apache.hadoop.tools.TestDistCh.testDistCh</li></div><div><li>org.apache.hadoop.mapred.gridmix.TestDistCacheEmulation.org.apache.hadoop.mapred.gridmix.TestDistCacheEmulation</li></div><div><li>org.apache.hadoop.mapred.gridmix.TestGridmixSubmission.org.apache.hadoop.mapred.gridmix.TestGridmixSubmission</li></div><div><li>org.apache.hadoop.mapred.gridmix.TestLoadJob.org.apache.hadoop.mapred.gridmix.TestLoadJob</li></div><div><li>org.apache.hadoop.mapred.gridmix.TestSleepJob.org.apache.hadoop.mapred.gridmix.TestSleepJob</li></div><div><li>org.apache.hadoop.streaming.TestFileArgs.testCommandLine</li></div><div><li>org.apache.hadoop.streaming.TestMultipleArchiveFiles.testCommandLine</li></div><div><li>org.apache.hadoop.streaming.TestMultipleCachefiles.testMultipleCachefiles</li></div><div><li>org.apache.hadoop.streaming.TestStreamingBadRecords.testNoOp</li></div><div><li>org.apache.hadoop.streaming.TestSymLink.testSymLink</li></div><div><li>org.apache.hadoop.yarn.util.TestProcfsBasedProcessTree.testProcessTree</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService.testRefreshAclWithDaemonUser</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService.testRMInitialsWithFileSystemBasedConfigurationProvider</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService.testAdminAclsWithFileSystemBasedConfigurationProvider</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.rmapp.TestRMAppTransitions.testAppSuccessPath[1]</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestContainerAllocation.testUserLimitAllocationMultipleContainers</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesMetricFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowsNotPresent</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRunsMetricsToRetrieve</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlows</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetAppsMetricsRange</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesInfoFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testForFlowRunsPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntityWithoutFlowInfo</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesMetricsTimeRange</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowApps</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRuns</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetAppWithoutFlowInfo</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRunAppsNotPresent</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRun</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntityDataToRetrieve</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesEventFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testUIDQueryWithAndWithoutFlowContextInfo</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowAppsFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesByUID</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowAppsNotPresent</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowsForPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testForFlowAppsPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesConfigFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesDataToRetrieve</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testUIDNotProperlyEscaped</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRunNotPresent</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRunApps</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesRelationFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetApp</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesWithoutFlowInfo</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGenericEntitiesForPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testForFlowRunAppsPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetAppNotPresent</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider.testCreation</li></div><div><li>org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider.testTokenServiceCreationWithLegacyFormat</li></div><div><li>org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider.testTokenServiceCreationWithUriFormat</li></div><div><li>org.apache.hadoop.util.TestShell.testEnvVarsWithInheritance</li></div><div><li>org.apache.hadoop.hdfs.TestDFSShell.testCopyCommandsWithPreserveOption</li></div><div><li>org.apache.hadoop.hdfs.TestLeaseRecovery2.testHardLeaseRecoveryAfterNameNodeRestart2</li></div><div><li>org.apache.hadoop.hdfs.TestReadStripedFileWithMissingBlocks.testReadFileWithMissingBlocks</li></div><div><li>org.apache.hadoop.hdfs.server.balancer.TestBalancerRPCDelay.testBalancerRPCDelay</li></div><div><li>org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.TestInMemoryLevelDBAliasMapClient.writeRead</li></div><div><li>org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.TestInMemoryLevelDBAliasMapClient.iterateThreeBatches</li></div><div><li>org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.TestInMemoryLevelDBAliasMapClient.multipleReads</li></div><div><li>org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.TestInMemoryLevelDBAliasMapClient.iterateSingleBatch</li></div><div><li>org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.TestInMemoryLevelDBAliasMapClient.testServerBindHost</li></div><div><li>org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.TestInMemoryLevelDBAliasMapClient.testNonExistentBlock</li></div><div><li>org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.TestLevelDBFileRegionAliasMap.testReadBack</li></div><div><li>org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.TestLevelDBFileRegionAliasMap.testIterate</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting.testHotSwapOutFailedVolumeAndReporting</li></div><div><li>org.apache.hadoop.hdfs.server.namenode.TestNameNodeMXBean.testDecommissioningNodes</li></div><div><li>org.apache.hadoop.hdfs.tools.offlineImageViewer.TestOfflineImageViewer.testPBDelimitedWriter</li></div><div><li>org.apache.hadoop.mapreduce.v2.hs.TestHistoryServerLeveldbStateStoreService.testTokenStore</li></div><div><li>org.apache.hadoop.mapreduce.v2.hs.TestHistoryServerLeveldbStateStoreService.testCheckVersion</li></div><div><li>org.apache.hadoop.mapreduce.v2.hs.server.TestHSAdminServer.testRefreshSuperUserGroups[0]</li></div><div><li>org.apache.hadoop.mapreduce.v2.hs.server.TestHSAdminServer.testRefreshSuperUserGroups[1]</li></div><div><li>org.apache.hadoop.mapred.TestClusterMRNotification.testMR</li></div><div><li>org.apache.hadoop.mapred.TestShuffleHandler.testRecoveryFromOtherVersions</li></div><div><li>org.apache.hadoop.mapred.TestShuffleHandler.testRecovery</li></div><div><li>org.apache.hadoop.tools.TestDistCh.testDistCh</li></div><div><li>org.apache.hadoop.mapred.gridmix.TestDistCacheEmulation.org.apache.hadoop.mapred.gridmix.TestDistCacheEmulation</li></div><div><li>org.apache.hadoop.mapred.gridmix.TestGridmixSubmission.org.apache.hadoop.mapred.gridmix.TestGridmixSubmission</li></div><div><li>org.apache.hadoop.mapred.gridmix.TestLoadJob.org.apache.hadoop.mapred.gridmix.TestLoadJob</li></div><div><li>org.apache.hadoop.mapred.gridmix.TestSleepJob.org.apache.hadoop.mapred.gridmix.TestSleepJob</li></div><div><li>org.apache.hadoop.streaming.TestFileArgs.testCommandLine</li></div><div><li>org.apache.hadoop.streaming.TestMultipleArchiveFiles.testCommandLine</li></div><div><li>org.apache.hadoop.streaming.TestMultipleCachefiles.testMultipleCachefiles</li></div><div><li>org.apache.hadoop.streaming.TestStreamingBadRecords.testNoOp</li></div><div><li>org.apache.hadoop.streaming.TestSymLink.testSymLink</li></div><div><li>org.apache.hadoop.yarn.applications.distributedshell.TestDistributedShell.testDSShellWithDomainV1_5</li></div><div><li>org.apache.hadoop.yarn.applications.distributedshell.TestDistributedShell.testDSShellWithoutDomainV1_5</li></div><div><li>org.apache.hadoop.yarn.service.monitor.probe.TestDefaultProbe.testDefaultProbe[2]</li></div><div><li>org.apache.hadoop.yarn.util.TestProcfsBasedProcessTree.testProcessTree</li></div><div><li>org.apache.hadoop.yarn.server.applicationhistoryservice.TestApplicationHistoryServer.testLaunchWithArguments</li></div><div><li>org.apache.hadoop.yarn.server.applicationhistoryservice.TestApplicationHistoryServer.testLaunch</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testGetDomains</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testRelatingToNonExistingEntity</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testValidateConfig</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testGetEntitiesWithPrimaryFilters</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testRelatingToOldEntityWithoutDomainId</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testRootDirPermission</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testGetSingleEntity</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testDeleteEntitiesPrimaryFilters</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testDeleteEntities</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testCacheSizes</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testGetEntities</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testGetEntityTypes</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testFromTsWithDeletion</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testLevelDbRepair</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testGetDomain</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testGetEvents</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testGetEntitiesWithSecondaryFilters</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testCheckVersion</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testGetEntitiesWithFromId</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testGetEntitiesWithFromTs</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDB.testInsertForPreviousPeriodAfterRollPeriodRollsDB</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDB.testInsertAfterRollPeriodRollsDB</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testGetDomains</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testGetDomains</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testRelatingToNonExistingEntity</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testRelatingToNonExistingEntity</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testValidateConfig</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testValidateConfig</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testGetEntitiesWithPrimaryFilters</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testGetEntitiesWithPrimaryFilters</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testRelatingToOldEntityWithoutDomainId</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testRelatingToOldEntityWithoutDomainId</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testRootDirPermission</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testRootDirPermission</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testGetSingleEntity</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testGetSingleEntity</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testCacheSizes</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testCacheSizes</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testGetEntities</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testGetEntities</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testRelatingToEntityInSamePut</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testRelatingToEntityInSamePut</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testGetDomain</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testGetDomain</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testGetEvents</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testGetEvents</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testGetEntitiesWithSecondaryFilters</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testGetEntitiesWithSecondaryFilters</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testCheckVersion</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testCheckVersion</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testGetEntitiesWithFromId</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testGetEntitiesWithFromId</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testGetEntitiesWithFromTs</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testGetEntitiesWithFromTs</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestTimelineDataManager.testGetOldEntitiesWithOutDomainId</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestTimelineDataManager.testGetEntitiesAclEnabled</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestTimelineDataManager.testGetOldEntityWithOutDomainId</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestTimelineDataManager.testUpdatingOldEntityWithoutDomainId</li></div><div><li>org.apache.hadoop.yarn.server.timeline.recovery.TestLeveldbTimelineStateStore.testTokenStore</li></div><div><li>org.apache.hadoop.yarn.server.timeline.recovery.TestLeveldbTimelineStateStore.testCheckVersion</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerShutdown.testStateStoreRemovalOnDecommission</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerShutdown.testStateStoreRemovalOnDecommission</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.testContainerUpgradeSuccessExplicitRollback</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestLogAggregationService.testFixedSizeThreadPool</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService.testAMRMProxyStorage</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService.testEmptyRestartTimes</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService.testEmptyState</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService.testStateStoreNodeHealth</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService.testContainerStorage</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService.testFinishResourceLocalization</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService.testNMTokenStorage</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService.testCompactionCycle</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService.testLogDeleterStorage</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService.testDeletionTaskStorage</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService.testContainerTokenStorage</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService.testLocalTrackerStateIterator</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService.testStateStoreForResourceMapping</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService.testStartResourceLocalization</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService.testApplicationStorage</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService.testCheckVersion</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService.testRemoveLocalizedResource</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService.testIsNewlyCreated</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService.testUnexpectedKeyDoesntThrowException</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService.testRefreshAclWithDaemonUser</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService.testRMInitialsWithFileSystemBasedConfigurationProvider</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService.testAdminAclsWithFileSystemBasedConfigurationProvider</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.metrics.TestCombinedSystemMetricsPublisher.testTimelineServiceEventPublishingNoService</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.metrics.TestCombinedSystemMetricsPublisher.testTimelineServiceEventPublishingV2Enabled</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.recovery.TestLeveldbRMStateStore.testRemoveAttempt</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.recovery.TestLeveldbRMStateStore.testBadKeyIteration</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.recovery.TestLeveldbRMStateStore.testDeleteStore</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.recovery.TestLeveldbRMStateStore.testEpoch</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.recovery.TestLeveldbRMStateStore.testApps</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.recovery.TestLeveldbRMStateStore.testProxyCA</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.recovery.TestLeveldbRMStateStore.testClientTokens</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.recovery.TestLeveldbRMStateStore.testVersion</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.recovery.TestLeveldbRMStateStore.testAMTokens</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.recovery.TestLeveldbRMStateStore.testAppDeletion</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.recovery.TestLeveldbRMStateStore.testRemoveApplication</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.recovery.TestLeveldbRMStateStore.testReservation</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestContainerAllocation.testUserLimitAllocationMultipleContainers</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.TestLeveldbConfigurationStore.testVersioning</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.TestLeveldbConfigurationStore.testPersistConfiguration</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.TestLeveldbConfigurationStore.testMaxLogs</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.TestLeveldbConfigurationStore.testRestartReadsFromUpdatedStore</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.TestLeveldbConfigurationStore.testPersistUpdatedConfiguration</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.TestLeveldbConfigurationStore.testConfigurationUpdate</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.TestLeveldbConfigurationStore.testNullConfigurationUpdate</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.TestPlacementConstraintsUtil.testInterAppConstraintsByAppID</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.security.TestRMDelegationTokens.testRMDTMasterKeyStateOnRollingMasterKey</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestEntityGroupFSTimelineStore.testSummaryRead</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestEntityGroupFSTimelineStore.testAppLogsScanLogs</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestEntityGroupFSTimelineStore.testPluginRead</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestEntityGroupFSTimelineStore.testCleanLogs</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestEntityGroupFSTimelineStore.testGetEntityPluginRead</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestEntityGroupFSTimelineStore.testScanActiveLogsAndMoveToDonePluginRead</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestEntityGroupFSTimelineStore.testParseSummaryLogs</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestEntityGroupFSTimelineStore.testMoveToDone</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestLevelDBCacheTimelineStore.testGetDomains</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestLevelDBCacheTimelineStore.testGetDomains</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestLevelDBCacheTimelineStore.testGetEntitiesWithPrimaryFilters</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestLevelDBCacheTimelineStore.testGetEntitiesWithPrimaryFilters</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestLevelDBCacheTimelineStore.testGetSingleEntity</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestLevelDBCacheTimelineStore.testGetSingleEntity</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestLevelDBCacheTimelineStore.testGetEntities</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestLevelDBCacheTimelineStore.testGetEntities</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestLevelDBCacheTimelineStore.testGetDomain</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestLevelDBCacheTimelineStore.testGetDomain</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestLevelDBCacheTimelineStore.testGetEvents</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestLevelDBCacheTimelineStore.testGetEvents</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestLevelDBCacheTimelineStore.testGetEntitiesWithSecondaryFilters</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestLevelDBCacheTimelineStore.testGetEntitiesWithSecondaryFilters</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestLevelDBCacheTimelineStore.testGetEntitiesWithFromId</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestLevelDBCacheTimelineStore.testGetEntitiesWithFromId</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestLevelDBCacheTimelineStore.testGetEntitiesWithFromTs</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestLevelDBCacheTimelineStore.testGetEntitiesWithFromTs</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestOverrideTimelineStoreYarnClient.testLifecycleAndOverride</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesMetricFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowsNotPresent</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRunsMetricsToRetrieve</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlows</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetAppsMetricsRange</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesInfoFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testForFlowRunsPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntityWithoutFlowInfo</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesMetricsTimeRange</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowApps</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRuns</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetAppWithoutFlowInfo</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRunAppsNotPresent</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRun</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntityDataToRetrieve</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesEventFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testUIDQueryWithAndWithoutFlowContextInfo</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowAppsFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesByUID</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowAppsNotPresent</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowsForPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testForFlowAppsPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesConfigFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesDataToRetrieve</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testUIDNotProperlyEscaped</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRunNotPresent</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRunApps</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesRelationFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetApp</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesWithoutFlowInfo</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGenericEntitiesForPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testForFlowRunAppsPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetAppNotPresent</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider.testTokenServiceCreationWithLegacyFormat</li></div><div><li>org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider.testTokenServiceCreationWithUriFormat</li></div><div><li>org.apache.hadoop.util.TestShell.testEnvVarsWithInheritance</li></div><div><li>org.apache.hadoop.hdfs.TestDistributedFileSystem.testDFSClientPeerWriteTimeout</li></div><div><li>org.apache.hadoop.hdfs.TestLeaseRecovery2.testCloseWhileRecoverLease</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestLazyPersistReplicaRecovery.testDnRestartWithSavedReplicas</li></div><div><li>org.apache.hadoop.mapreduce.v2.hs.server.TestHSAdminServer.testRefreshSuperUserGroups[0]</li></div><div><li>org.apache.hadoop.mapreduce.v2.hs.server.TestHSAdminServer.testRefreshSuperUserGroups[1]</li></div><div><li>org.apache.hadoop.tools.TestDistCh.testDistCh</li></div><div><li>org.apache.hadoop.mapred.gridmix.TestDistCacheEmulation.org.apache.hadoop.mapred.gridmix.TestDistCacheEmulation</li></div><div><li>org.apache.hadoop.mapred.gridmix.TestGridmixSubmission.org.apache.hadoop.mapred.gridmix.TestGridmixSubmission</li></div><div><li>org.apache.hadoop.mapred.gridmix.TestLoadJob.org.apache.hadoop.mapred.gridmix.TestLoadJob</li></div><div><li>org.apache.hadoop.mapred.gridmix.TestSleepJob.org.apache.hadoop.mapred.gridmix.TestSleepJob</li></div><div><li>org.apache.hadoop.streaming.TestFileArgs.testCommandLine</li></div><div><li>org.apache.hadoop.streaming.TestMultipleArchiveFiles.testCommandLine</li></div><div><li>org.apache.hadoop.streaming.TestMultipleCachefiles.testMultipleCachefiles</li></div><div><li>org.apache.hadoop.streaming.TestStreamingBadRecords.testNoOp</li></div><div><li>org.apache.hadoop.streaming.TestSymLink.testSymLink</li></div><div><li>org.apache.hadoop.yarn.service.monitor.probe.TestDefaultProbe.testDefaultProbe[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMProxy.testAMRMProxyTokenRenewal</li></div><div><li>org.apache.hadoop.yarn.util.TestProcfsBasedProcessTree.testProcessTree</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.TestContainerLaunch.testKillProcessGroup</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.TestContainersMonitor.testContainerKillOnMemoryOverflow</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.TestApplicationMasterServiceFair.testUpdateTrackingUrl</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService.testRefreshAclWithDaemonUser</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService.testRMInitialsWithFileSystemBasedConfigurationProvider</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService.testAdminAclsWithFileSystemBasedConfigurationProvider</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestContainerAllocation.testUserLimitAllocationMultipleContainers</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestIncreaseAllocationExpirer.testContainerIncreaseAllocationExpiration</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestQueueManagementDynamicEditPolicy.testEditSchedule</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesMetricFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowsNotPresent</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRunsMetricsToRetrieve</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlows</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetAppsMetricsRange</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesInfoFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testForFlowRunsPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntityWithoutFlowInfo</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesMetricsTimeRange</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowApps</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRuns</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetAppWithoutFlowInfo</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRunAppsNotPresent</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRun</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntityDataToRetrieve</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesEventFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testUIDQueryWithAndWithoutFlowContextInfo</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowAppsFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesByUID</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowAppsNotPresent</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowsForPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testForFlowAppsPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesConfigFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesDataToRetrieve</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testUIDNotProperlyEscaped</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRunNotPresent</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRunApps</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesRelationFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetApp</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesWithoutFlowInfo</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGenericEntitiesForPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testForFlowRunAppsPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetAppNotPresent</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.util.TestShell.testEnvVarsWithInheritance</li></div><div><li>org.apache.hadoop.hdfs.server.blockmanagement.TestUnderReplicatedBlocks.testSetRepIncWithUnderReplicatedBlocks</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.TestDataNodeErasureCodingMetrics.testReconstructionBytesPartialGroup2</li></div><div><li>org.apache.hadoop.mapreduce.v2.hs.server.TestHSAdminServer.testRefreshSuperUserGroups[0]</li></div><div><li>org.apache.hadoop.mapreduce.v2.hs.server.TestHSAdminServer.testRefreshSuperUserGroups[1]</li></div><div><li>org.apache.hadoop.mapred.TestMRTimelineEventHandling.testMapreduceJobTimelineServiceEnabled</li></div><div><li>org.apache.hadoop.mapreduce.v2.TestUberAM.testJobWithChangePriority</li></div><div><li>org.apache.hadoop.tools.TestDistCh.testDistCh</li></div><div><li>org.apache.hadoop.mapred.gridmix.TestDistCacheEmulation.org.apache.hadoop.mapred.gridmix.TestDistCacheEmulation</li></div><div><li>org.apache.hadoop.mapred.gridmix.TestGridmixSubmission.org.apache.hadoop.mapred.gridmix.TestGridmixSubmission</li></div><div><li>org.apache.hadoop.mapred.gridmix.TestLoadJob.org.apache.hadoop.mapred.gridmix.TestLoadJob</li></div><div><li>org.apache.hadoop.mapred.gridmix.TestSleepJob.org.apache.hadoop.mapred.gridmix.TestSleepJob</li></div><div><li>org.apache.hadoop.yarn.sls.TestSLSRunner.testSimulatorRunning[Testing with: SYNTH, org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler, (nodeFile null)]</li></div><div><li>org.apache.hadoop.streaming.TestFileArgs.testCommandLine</li></div><div><li>org.apache.hadoop.streaming.TestMultipleArchiveFiles.testCommandLine</li></div><div><li>org.apache.hadoop.streaming.TestMultipleCachefiles.testMultipleCachefiles</li></div><div><li>org.apache.hadoop.streaming.TestStreamingBadRecords.testNoOp</li></div><div><li>org.apache.hadoop.streaming.TestSymLink.testSymLink</li></div><div><li>org.apache.hadoop.yarn.service.monitor.probe.TestDefaultProbe.testDefaultProbe[2]</li></div><div><li>org.apache.hadoop.yarn.util.TestProcfsBasedProcessTree.testProcessTree</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.TestContainerLaunch.testKillProcessGroup</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.TestContainersMonitor.testContainerKillOnMemoryOverflow</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService.testRefreshAclWithDaemonUser</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService.testRMInitialsWithFileSystemBasedConfigurationProvider</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService.testAdminAclsWithFileSystemBasedConfigurationProvider</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestContainerAllocation.testUserLimitAllocationMultipleContainers</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestQueueManagementDynamicEditPolicy.testEditSchedule</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesMetricFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowsNotPresent</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRunsMetricsToRetrieve</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlows</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetAppsMetricsRange</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesInfoFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testForFlowRunsPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntityWithoutFlowInfo</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesMetricsTimeRange</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowApps</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRuns</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetAppWithoutFlowInfo</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRunAppsNotPresent</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRun</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntityDataToRetrieve</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesEventFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testUIDQueryWithAndWithoutFlowContextInfo</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowAppsFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesByUID</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowAppsNotPresent</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowsForPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testForFlowAppsPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesConfigFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesDataToRetrieve</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testUIDNotProperlyEscaped</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRunNotPresent</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRunApps</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesRelationFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetApp</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesWithoutFlowInfo</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGenericEntitiesForPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testForFlowRunAppsPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetAppNotPresent</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider.testTokenServiceCreationWithLegacyFormat</li></div><div><li>org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider.testTokenServiceCreationWithUriFormat</li></div><div><li>org.apache.hadoop.util.TestShell.testEnvVarsWithInheritance</li></div><div><li>org.apache.hadoop.hdfs.TestDistributedFileSystem.testDFSClientPeerWriteTimeout</li></div><div><li>org.apache.hadoop.hdfs.TestEncryptedTransfer.testEncryptedAppendRequiringBlockTransfer[1]</li></div><div><li>org.apache.hadoop.hdfs.server.blockmanagement.TestUnderReplicatedBlocks.testSetRepIncWithUnderReplicatedBlocks</li></div><div><li>org.apache.hadoop.hdfs.server.namenode.sps.TestBlockStorageMovementAttemptedItems.testNoBlockMovementAttemptFinishedReportAdded</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testAuthUrlConnectTimeout[timeoutSource=ConnectionFactory]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testConnectTimeout[timeoutSource=ConnectionFactory]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testRedirectConnectTimeout[timeoutSource=ConnectionFactory]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testTwoStepWriteConnectTimeout[timeoutSource=ConnectionFactory]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testAuthUrlConnectTimeout[timeoutSource=Configuration]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testConnectTimeout[timeoutSource=Configuration]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testRedirectConnectTimeout[timeoutSource=Configuration]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testTwoStepWriteConnectTimeout[timeoutSource=Configuration]</li></div><div><li>org.apache.hadoop.mapreduce.v2.hs.server.TestHSAdminServer.testRefreshSuperUserGroups[0]</li></div><div><li>org.apache.hadoop.mapreduce.v2.hs.server.TestHSAdminServer.testRefreshSuperUserGroups[1]</li></div><div><li>org.apache.hadoop.tools.TestDistCh.testDistCh</li></div><div><li>org.apache.hadoop.mapred.gridmix.TestDistCacheEmulation.org.apache.hadoop.mapred.gridmix.TestDistCacheEmulation</li></div><div><li>org.apache.hadoop.mapred.gridmix.TestGridmixSubmission.org.apache.hadoop.mapred.gridmix.TestGridmixSubmission</li></div><div><li>org.apache.hadoop.mapred.gridmix.TestLoadJob.org.apache.hadoop.mapred.gridmix.TestLoadJob</li></div><div><li>org.apache.hadoop.mapred.gridmix.TestSleepJob.org.apache.hadoop.mapred.gridmix.TestSleepJob</li></div><div><li>org.apache.hadoop.streaming.TestFileArgs.testCommandLine</li></div><div><li>org.apache.hadoop.streaming.TestMultipleArchiveFiles.testCommandLine</li></div><div><li>org.apache.hadoop.streaming.TestMultipleCachefiles.testMultipleCachefiles</li></div><div><li>org.apache.hadoop.streaming.TestStreamingBadRecords.testNoOp</li></div><div><li>org.apache.hadoop.streaming.TestSymLink.testSymLink</li></div><div><li>org.apache.hadoop.yarn.util.TestProcfsBasedProcessTree.testProcessTree</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.testLocalingResourceWhileContainerRunning</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService.testRefreshAclWithDaemonUser</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService.testRMInitialsWithFileSystemBasedConfigurationProvider</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService.testAdminAclsWithFileSystemBasedConfigurationProvider</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.rmapp.TestRMAppTransitions.testAppSuccessPath[1]</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacitySchedulerSchedulingRequestUpdate.testBasicPendingResourceUpdate</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestContainerAllocation.testUserLimitAllocationMultipleContainers</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesMetricFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowsNotPresent</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRunsMetricsToRetrieve</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlows</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetAppsMetricsRange</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesInfoFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testForFlowRunsPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntityWithoutFlowInfo</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesMetricsTimeRange</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowApps</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRuns</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetAppWithoutFlowInfo</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRunAppsNotPresent</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRun</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntityDataToRetrieve</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesEventFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testUIDQueryWithAndWithoutFlowContextInfo</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowAppsFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesByUID</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowAppsNotPresent</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowsForPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testForFlowAppsPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesConfigFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesDataToRetrieve</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testUIDNotProperlyEscaped</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRunNotPresent</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRunApps</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesRelationFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetApp</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesWithoutFlowInfo</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGenericEntitiesForPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testForFlowRunAppsPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetAppNotPresent</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;host1[]:9600&gt; but was:&lt;host1[.persistent.co.in.]:9600&gt;</li></div><div><li>expected:&lt;host1[]:9600&gt; but was:&lt;host1[.persistent.co.in.]:9600&gt;</li></div><div><li>String index out of range: -1</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div><div><li> Expected to find 'whereas it is under recovery' but got unexpected exception: java.io.IOException: Unable to close file because the last blockBP-1173887286-172.17.0.2-1540242846856:blk_1073741826_1002 does not have enough number of replicas.
 at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:964)
 at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.jav</li></div><div><li>test timed out after 300000 milliseconds</li></div><div><li>Was waiting too long for a replica to become TEMPORARY</li></div><div><li>Timed out waiting for condition. Thread diagnostics:
Timestamp: 2018-10-22 05:08:28,666

"org.eclipse.jetty.server.session.HashSessionManager@3fc86c08Timer" daemon prio=5 tid=33 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.Abs</li></div><div><li>2</li></div><div><li>Timed out waiting for condition. Thread diagnostics:
Timestamp: 2018-10-22 04:33:36,770

"qtp1055415357-2122" daemon prio=5 tid=2122 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.mapreduce.v2.hs.server.TestHSAdminServer.testRefreshSuperUserGroups(TestHSAdminServer.java:208)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeM</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.mapreduce.v2.hs.server.TestHSAdminServer.testRefreshSuperUserGroups(TestHSAdminServer.java:208)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeM</li></div><div><li>org/bouncycastle/jce/provider/BouncyCastleProvider</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/jce/provider/BouncyCastleProvider</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>Expected success for Probe Status, time="Tue Oct 23 04:07:49 UTC 2018", outcome="failure", message="Failure in Default probe: IP presence with DNS checking and DNS server address 8.8.8.8", exception="java.io.IOException: comp-0: DNS checking is enabled, but lookup for example.com is not available yet"</li></div><div><li>Child process owned by init escaped process tree.</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.doRestartTests(TestContainerManager.java:482)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.testContainerUpgradeSuc</li></div><div><li>expected:&lt;3&gt; but was:&lt;2&gt;</li></div><div><li>expected:&lt;[hadoop.apache.org]&gt; but was:&lt;[N/A]&gt;</li></div><div><li>expected:&lt;jenkins[xyz,jenkins]&gt; but was:&lt;jenkins[,jenkinsxyz]&gt;</li></div><div><li>expected:&lt;[jenkins,world:anyone:rwcda]&gt; but was:&lt;[world:anyone:rwcda,jenkins]&gt;</li></div><div><li>expected:&lt;[jenkins,world:anyone:rwcda]&gt; but was:&lt;[world:anyone:rwcda,jenkins]&gt;</li></div><div><li>junit.framework.AssertionFailedError
	at junit.framework.Assert.fail(Assert.java:55)
	at junit.framework.Assert.fail(Assert.java:64)
	at junit.framework.TestCase.fail(TestCase.java:235)
	at org.apache.hadoop.yarn.server.resourcemanager.reservation.BaseSharingPolicyTest.runTest(BaseSharingPolicyTest.java:146)
	at org.apache.hadoop.yarn.server.resourcemanager.reservation.TestCapacityOverTimePolicy.t</li></div><div><li>expected:&lt;101&gt; but was:&lt;71&gt;</li></div><div><li>expected:&lt;0.5&gt; but was:&lt;0.0&gt;</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertNotNull(Assert.java:621)
	at org.junit.Assert.assertNotNull(Assert.java:631)
	at org.apache.hadoop.yarn.server.resourcemanager.security.TestDelegationTokenRenewer.testRMRestartWithExpiredToken(TestDelegationTokenRenewer.java:1067)
	at sun.reflect.NativeMetho</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Response from server should have been Bad Request</li></div><div><li>Response from server should have been Not Found</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Response from server should have been Not Found</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;host1[]:9600&gt; but was:&lt;host1[.persistent.co.in.]:9600&gt;</li></div><div><li>expected:&lt;host1[]:9600&gt; but was:&lt;host1[.persistent.co.in.]:9600&gt;</li></div><div><li>String index out of range: -1</li></div><div><li> Expected to find 'whereas it is under recovery' but got unexpected exception: java.io.IOException: Unable to close file because the last blockBP-471152024-172.17.0.2-1540234579442:blk_1073741826_1002 does not have enough number of replicas.
 at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:964)
 at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java</li></div><div><li>lease holder should now be the NN</li></div><div><li>Problem binding to [localhost:36160] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException</li></div><div><li>Timed out waiting for condition. Thread diagnostics:
Timestamp: 2018-10-22 03:32:07,363

"org.eclipse.jetty.server.session.HashSessionManager@31cfc07cTimer" daemon prio=5 tid=160 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.Ab</li></div><div><li>inode should complete in ~60000 ms.
Expected: is &lt;true&gt;
     but: was &lt;false&gt;</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.mapreduce.v2.hs.server.TestHSAdminServer.testRefreshSuperUserGroups(TestHSAdminServer.java:208)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeM</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.mapreduce.v2.hs.server.TestHSAdminServer.testRefreshSuperUserGroups(TestHSAdminServer.java:208)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeM</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>Expected success for Probe Status, time="Tue Oct 23 01:05:07 UTC 2018", outcome="failure", message="Failure in Default probe: IP presence with DNS checking and DNS server address 8.8.8.8", exception="java.io.IOException: comp-0: DNS checking is enabled, but lookup for example.com is not available yet"</li></div><div><li>Child process owned by init escaped process tree.</li></div><div><li>Timed out waiting for condition. Thread diagnostics:
Timestamp: 2018-10-22 07:51:48,157

"DeletionService #2"  prio=5 tid=789 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.</li></div><div><li>expected:&lt;jenkins[xyz,jenkins]&gt; but was:&lt;jenkins[,jenkinsxyz]&gt;</li></div><div><li>expected:&lt;[jenkins,world:anyone:rwcda]&gt; but was:&lt;[world:anyone:rwcda,jenkins]&gt;</li></div><div><li>expected:&lt;[jenkins,world:anyone:rwcda]&gt; but was:&lt;[world:anyone:rwcda,jenkins]&gt;</li></div><div><li>expected:&lt;101&gt; but was:&lt;71&gt;</li></div><div><li>expected:&lt;0.5&gt; but was:&lt;0.0&gt;</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Response from server should have been Bad Request</li></div><div><li>Response from server should have been Not Found</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Response from server should have been Not Found</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li> Expected to find 'testCompressorDecompressor error !!!' but got unexpected exception: java.lang.NullPointerException
 at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:187)
 at com.google.common.base.Joiner.toString(Joiner.java:532)
 at com.google.common.base.Joiner.appendTo(Joiner.java:124)
 at com.google.common.base.Joiner.appendTo(Joiner.java:181)
 at com.google.common.ba</li></div><div><li> Expected to find 'testCompressorDecompressorWithExeedBufferLimit error !!!' but got unexpected exception: java.lang.NullPointerException
 at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:187)
 at com.google.common.base.Joiner.toString(Joiner.java:532)
 at com.google.common.base.Joiner.appendTo(Joiner.java:124)
 at com.google.common.base.Joiner.appendTo(Joiner.java:181)
 at </li></div><div><li>Deferred</li></div><div><li>Could not decompress data. Input is invalid.</li></div><div><li>java.net.ConnectException: Your endpoint configuration is wrong; For more details see:  http://wiki.apache.org/hadoop/UnsetHostnameOrPort</li></div><div><li>String index out of range: -1</li></div><div><li> Expected to find 'whereas it is under recovery' but got unexpected exception: java.io.IOException: Unable to close file because the last blockBP-1488314123-172.17.0.2-1540231197760:blk_1073741826_1002 does not have enough number of replicas.
 at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:964)
 at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.jav</li></div><div><li>test timed out after 10000 milliseconds</li></div><div><li> Expected to find 'localhost:34863: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:34863: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li> Expected to find 'localhost:38197: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:38197: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li> Expected to find 'localhost:42791: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:42791: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li>expected timeout</li></div><div><li> Expected to find 'localhost:35989: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:35989: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li> Expected to find 'localhost:42653: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:42653: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li> Expected to find 'localhost:46802: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:46802: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li>expected timeout</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.mapreduce.v2.hs.server.TestHSAdminServer.testRefreshSuperUserGroups(TestHSAdminServer.java:208)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeM</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.mapreduce.v2.hs.server.TestHSAdminServer.testRefreshSuperUserGroups(TestHSAdminServer.java:208)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeM</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>Expected success for Probe Status, time="Mon Oct 22 20:43:05 CDT 2018", outcome="failure", message="Failure in Default probe: IP presence with DNS checking and DNS server address 8.8.8.8", exception="java.io.IOException: comp-0: DNS checking is enabled, but lookup for example.com is not available yet"</li></div><div><li>Child process owned by init escaped process tree.</li></div><div><li>expected:&lt;jenkins[xyz,jenkins]&gt; but was:&lt;jenkins[,jenkinsxyz]&gt;</li></div><div><li>expected:&lt;[jenkins,world:anyone:rwcda]&gt; but was:&lt;[world:anyone:rwcda,jenkins]&gt;</li></div><div><li>expected:&lt;[jenkins,world:anyone:rwcda]&gt; but was:&lt;[world:anyone:rwcda,jenkins]&gt;</li></div><div><li>expected:&lt;101&gt; but was:&lt;71&gt;</li></div><div><li>expected:&lt;0.5&gt; but was:&lt;0.0&gt;</li></div><div><li>expected:&lt;[org.apache.hadoop.security.token.delegation.DelegationKey@c445c2f1, org.apache.hadoop.security.token.delegation.DelegationKey@270bb094]&gt; but was:&lt;[org.apache.hadoop.security.token.delegation.DelegationKey@c445c2f1, org.apache.hadoop.security.token.delegation.DelegationKey@2c290ff4, org.apache.hadoop.security.token.delegation.DelegationKey@270bb094]&gt;</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Response from server should have been Bad Request</li></div><div><li>Response from server should have been Not Found</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Response from server should have been Not Found</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;host1[]:9600&gt; but was:&lt;host1[.persistent.co.in.]:9600&gt;</li></div><div><li>expected:&lt;host1[]:9600&gt; but was:&lt;host1[.persistent.co.in.]:9600&gt;</li></div><div><li> Expected to find 'testCompressorDecompressor error !!!' but got unexpected exception: java.lang.NullPointerException
 at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:187)
 at com.google.common.base.Joiner.toString(Joiner.java:532)
 at com.google.common.base.Joiner.appendTo(Joiner.java:124)
 at com.google.common.base.Joiner.appendTo(Joiner.java:181)
 at com.google.common.ba</li></div><div><li> Expected to find 'testCompressorDecompressorWithExeedBufferLimit error !!!' but got unexpected exception: java.lang.NullPointerException
 at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:187)
 at com.google.common.base.Joiner.toString(Joiner.java:532)
 at com.google.common.base.Joiner.appendTo(Joiner.java:124)
 at com.google.common.base.Joiner.appendTo(Joiner.java:181)
 at </li></div><div><li>Deferred</li></div><div><li>Could not decompress data. Input is invalid.</li></div><div><li>String index out of range: -1</li></div><div><li>Failed to append to non-existent file /test/test/target for client 127.0.0.1
 at org.apache.hadoop.hdfs.server.namenode.FSDirAppendOp.appendFile(FSDirAppendOp.java:104)
 at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.appendFile(FSNamesystem.java:2705)
 at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.append(NameNodeRpcServer.java:806)
 at org.apache.hadoop.hdfs.protocolPB.Client</li></div><div><li>Files not distributed: [10, 0]</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertFalse(Assert.java:64)
	at org.junit.Assert.assertFalse(Assert.java:74)
	at org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailure.testDataNodeShutdownAfterNumFailedVolumeExceedsTolerated(TestDataNodeVolumeFailure.java:311)
	at sun.reflect.NativeMet</li></div><div><li>Timed out waiting for condition. Thread diagnostics:
Timestamp: 2018-10-22 11:06:05,864

"IPC Server handler 0 on 35065" daemon prio=5 tid=52 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObj</li></div><div><li>Unexpected num under-replicated blocks expected:&lt;3&gt; but was:&lt;4&gt;</li></div><div><li> Expected to find 'localhost:37420: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:37420: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li> Expected to find 'localhost:34023: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:34023: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li> Expected to find 'localhost:38776: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:38776: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li>expected timeout</li></div><div><li> Expected to find 'localhost:45936: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:45936: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li> Expected to find 'localhost:35823: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:35823: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li> Expected to find 'localhost:41443: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:41443: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li>expected timeout</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.mapreduce.v2.hs.server.TestHSAdminServer.testRefreshSuperUserGroups(TestHSAdminServer.java:208)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeM</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.mapreduce.v2.hs.server.TestHSAdminServer.testRefreshSuperUserGroups(TestHSAdminServer.java:208)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeM</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>Child process owned by init escaped process tree.</li></div><div><li>expected:&lt;jenkins[xyz,jenkins]&gt; but was:&lt;jenkins[,jenkinsxyz]&gt;</li></div><div><li>expected:&lt;[jenkins,world:anyone:rwcda]&gt; but was:&lt;[world:anyone:rwcda,jenkins]&gt;</li></div><div><li>expected:&lt;[jenkins,world:anyone:rwcda]&gt; but was:&lt;[world:anyone:rwcda,jenkins]&gt;</li></div><div><li>expected:&lt;1&gt; but was:&lt;0&gt;</li></div><div><li>expected:&lt;101&gt; but was:&lt;71&gt;</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Response from server should have been Bad Request</li></div><div><li>Response from server should have been Not Found</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Response from server should have been Not Found</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>test timed out after 30000 milliseconds</li></div><div><li>expected:&lt;host1[]:9600&gt; but was:&lt;host1[.persistent.co.in.]:9600&gt;</li></div><div><li>expected:&lt;host1[]:9600&gt; but was:&lt;host1[.persistent.co.in.]:9600&gt;</li></div><div><li>String index out of range: -1</li></div><div><li>expected:&lt;1540241685122&gt; but was:&lt;1540241686275&gt;</li></div><div><li>lease holder should now be the NN</li></div><div><li>test timed out after 300000 milliseconds</li></div><div><li>test timed out after 100000 milliseconds</li></div><div><li>Could not load library. Reasons: [no leveldbjni64-1.8 in java.library.path, no leveldbjni-1.8 in java.library.path, no leveldbjni in java.library.path, /tmp/libleveldbjni-64-1-6846183329515870239.8: /tmp/libleveldbjni-64-1-6846183329515870239.8: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64-bit platform)]</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not load library. Reasons: [no leveldbjni64-1.8 in java.library.path, no leveldbjni-1.8 in java.library.path, no leveldbjni in java.library.path, /tmp/libleveldbjni-64-1-554128541044744978.8: /tmp/libleveldbjni-64-1-554128541044744978.8: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64-bit platform)]</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Actual async detected volume failures should be greater or equal than [Ljava.lang.String;@2f5db674</li></div><div><li>expected:&lt;...0,"lastBlockReport":[0},"127.0.0.1:42858":{"infoAddr":"127.0.0.1:34790","infoSecureAddr":"127.0.0.1:0","xferaddr":"127.0.0.1:42858","lastContact":0,"usedSpace":8192,"adminState":"In Service","nonDfsUsedSpace":342604595200,"capacity":1051753185280,"numBlocks":0,"version":"3.3.0-SNAPSHOT","used":8192,"remaining":709148581888,"blockScheduled":0,"blockPoolUsed":8192,"blockPoolUsedPercent"</li></div><div><li>Could not load library. Reasons: [no leveldbjni64-1.8 in java.library.path, no leveldbjni-1.8 in java.library.path, no leveldbjni in java.library.path, /tmp/libleveldbjni-64-1-2259625289895769694.8: /tmp/libleveldbjni-64-1-2259625289895769694.8: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64-bit platform)]</li></div><div><li>Could not load library. Reasons: [no leveldbjni64-1.8 in java.library.path, no leveldbjni-1.8 in java.library.path, no leveldbjni in java.library.path, /tmp/libleveldbjni-64-1-523006923745447619.8: /tmp/libleveldbjni-64-1-523006923745447619.8: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64-bit platform)]</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.mapreduce.v2.hs.server.TestHSAdminServer.testRefreshSuperUserGroups(TestHSAdminServer.java:208)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeM</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.mapreduce.v2.hs.server.TestHSAdminServer.testRefreshSuperUserGroups(TestHSAdminServer.java:208)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeM</li></div><div><li>Job didn't finish in 30 seconds</li></div><div><li>Could not load library. Reasons: [no leveldbjni64-1.8 in java.library.path, no leveldbjni-1.8 in java.library.path, no leveldbjni in java.library.path, /tmp/libleveldbjni-64-1-3087808647152313191.8: /tmp/libleveldbjni-64-1-3087808647152313191.8: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64-bit platform)]</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>org/bouncycastle/jce/provider/BouncyCastleProvider</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/jce/provider/BouncyCastleProvider</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/jce/provider/BouncyCastleProvider</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/jce/provider/BouncyCastleProvider</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/jce/provider/BouncyCastleProvider</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>Could not load library. Reasons: [no leveldbjni64-1.8 in java.library.path, no leveldbjni-1.8 in java.library.path, no leveldbjni in java.library.path, /tmp/libleveldbjni-64-1-6894306488027051862.8: /tmp/libleveldbjni-64-1-6894306488027051862.8: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64-bit platform)]</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Expected success for Probe Status, time="Tue Oct 23 02:51:44 UTC 2018", outcome="failure", message="Failure in Default probe: IP presence with DNS checking and DNS server address 8.8.8.8", exception="java.io.IOException: comp-0: DNS checking is enabled, but lookup for example.com is not available yet"</li></div><div><li>Child process owned by init escaped process tree.</li></div><div><li>expected:&lt;0&gt; but was:&lt;-1&gt;</li></div><div><li>expected:&lt;0&gt; but was:&lt;-1&gt;</li></div><div><li>Could not load library. Reasons: [no leveldbjni64-1.8 in java.library.path, no leveldbjni-1.8 in java.library.path, no leveldbjni in java.library.path, /tmp/libleveldbjni-64-1-6106966317519162310.8: /tmp/libleveldbjni-64-1-6106966317519162310.8: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64-bit platform)]</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not load library. Reasons: [no leveldbjni64-1.8 in java.library.path, no leveldbjni-1.8 in java.library.path, no leveldbjni in java.library.path, /tmp/libleveldbjni-64-1-8074716728000596817.8: /tmp/libleveldbjni-64-1-8074716728000596817.8: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64-bit platform)]</li></div><div><li>Could not initialize class org.apache.hadoop.yarn.server.timeline.TestRollingLevelDB$MyRollingLevelDB</li></div><div><li>Could not load library. Reasons: [no leveldbjni64-1.8 in java.library.path, no leveldbjni-1.8 in java.library.path, no leveldbjni in java.library.path, /tmp/libleveldbjni-64-1-2749967554164131014.8: /tmp/libleveldbjni-64-1-2749967554164131014.8: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64-bit platform)]</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore.serviceStop(RollingLevelDBTimelineStore.java:374)
	at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:220)
	at org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.tearDown(TestRollingLevelDBTimelineStore.java:74)
	at sun.reflect.NativeMethodAccessorImpl.invok</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore.serviceStop(RollingLevelDBTimelineStore.java:374)
	at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:220)
	at org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.tearDown(TestRollingLevelDBTimelineStore.java:74)
	at sun.reflect.NativeMethodAccessorImpl.invok</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore.serviceStop(RollingLevelDBTimelineStore.java:374)
	at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:220)
	at org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.tearDown(TestRollingLevelDBTimelineStore.java:74)
	at sun.reflect.NativeMethodAccessorImpl.invok</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore.serviceStop(RollingLevelDBTimelineStore.java:374)
	at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:220)
	at org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.tearDown(TestRollingLevelDBTimelineStore.java:74)
	at sun.reflect.NativeMethodAccessorImpl.invok</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore.serviceStop(RollingLevelDBTimelineStore.java:374)
	at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:220)
	at org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.tearDown(TestRollingLevelDBTimelineStore.java:74)
	at sun.reflect.NativeMethodAccessorImpl.invok</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore.serviceStop(RollingLevelDBTimelineStore.java:374)
	at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:220)
	at org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.tearDown(TestRollingLevelDBTimelineStore.java:74)
	at sun.reflect.NativeMethodAccessorImpl.invok</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore.serviceStop(RollingLevelDBTimelineStore.java:374)
	at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:220)
	at org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.tearDown(TestRollingLevelDBTimelineStore.java:74)
	at sun.reflect.NativeMethodAccessorImpl.invok</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore.serviceStop(RollingLevelDBTimelineStore.java:374)
	at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:220)
	at org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.tearDown(TestRollingLevelDBTimelineStore.java:74)
	at sun.reflect.NativeMethodAccessorImpl.invok</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore.serviceStop(RollingLevelDBTimelineStore.java:374)
	at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:220)
	at org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.tearDown(TestRollingLevelDBTimelineStore.java:74)
	at sun.reflect.NativeMethodAccessorImpl.invok</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore.serviceStop(RollingLevelDBTimelineStore.java:374)
	at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:220)
	at org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.tearDown(TestRollingLevelDBTimelineStore.java:74)
	at sun.reflect.NativeMethodAccessorImpl.invok</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore.serviceStop(RollingLevelDBTimelineStore.java:374)
	at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:220)
	at org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.tearDown(TestRollingLevelDBTimelineStore.java:74)
	at sun.reflect.NativeMethodAccessorImpl.invok</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore.serviceStop(RollingLevelDBTimelineStore.java:374)
	at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:220)
	at org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.tearDown(TestRollingLevelDBTimelineStore.java:74)
	at sun.reflect.NativeMethodAccessorImpl.invok</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore.serviceStop(RollingLevelDBTimelineStore.java:374)
	at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:220)
	at org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.tearDown(TestRollingLevelDBTimelineStore.java:74)
	at sun.reflect.NativeMethodAccessorImpl.invok</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore.serviceStop(RollingLevelDBTimelineStore.java:374)
	at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:220)
	at org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.tearDown(TestRollingLevelDBTimelineStore.java:74)
	at sun.reflect.NativeMethodAccessorImpl.invok</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore.serviceStop(RollingLevelDBTimelineStore.java:374)
	at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:220)
	at org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.tearDown(TestRollingLevelDBTimelineStore.java:74)
	at sun.reflect.NativeMethodAccessorImpl.invok</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore.serviceStop(RollingLevelDBTimelineStore.java:374)
	at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:220)
	at org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.tearDown(TestRollingLevelDBTimelineStore.java:74)
	at sun.reflect.NativeMethodAccessorImpl.invok</li></div><div><li>Could not load library. Reasons: [no leveldbjni64-1.8 in java.library.path, no leveldbjni-1.8 in java.library.path, no leveldbjni in java.library.path, /tmp/libleveldbjni-64-1-2064119338140698141.8: /tmp/libleveldbjni-64-1-2064119338140698141.8: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64-bit platform)]</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not load library. Reasons: [no leveldbjni64-1.8 in java.library.path, no leveldbjni-1.8 in java.library.path, no leveldbjni in java.library.path, /tmp/libleveldbjni-64-1-3323024006714592726.8: /tmp/libleveldbjni-64-1-3323024006714592726.8: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64-bit platform)]</li></div><div><li>Could not initialize class org.apache.hadoop.yarn.server.timeline.recovery.LeveldbTimelineStateStore</li></div><div><li>Could not load library. Reasons: [no leveldbjni64-1.8 in java.library.path, no leveldbjni-1.8 in java.library.path, no leveldbjni in java.library.path, /tmp/libleveldbjni-64-1-7138150054086281878.8: /tmp/libleveldbjni-64-1-7138150054086281878.8: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64-bit platform)]</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceStop(NodeManager.java:529)
	at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:220)
	at org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerShutdown.tearDown(TestNodeManagerShutdown.java:107)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeM</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.doRestartTests(TestContainerManager.java:482)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.testContainerUpgradeSuc</li></div><div><li>expected:&lt;3&gt; but was:&lt;2&gt;</li></div><div><li>Could not load library. Reasons: [no leveldbjni64-1.8 in java.library.path, no leveldbjni-1.8 in java.library.path, no leveldbjni in java.library.path, /tmp/libleveldbjni-64-1-6774930248829059649.8: /tmp/libleveldbjni-64-1-6774930248829059649.8: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64-bit platform)]</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>expected:&lt;jenkins[xyz,jenkins]&gt; but was:&lt;jenkins[,jenkinsxyz]&gt;</li></div><div><li>expected:&lt;[jenkins,world:anyone:rwcda]&gt; but was:&lt;[world:anyone:rwcda,jenkins]&gt;</li></div><div><li>expected:&lt;[jenkins,world:anyone:rwcda]&gt; but was:&lt;[world:anyone:rwcda,jenkins]&gt;</li></div><div><li>Could not load library. Reasons: [no leveldbjni64-1.8 in java.library.path, no leveldbjni-1.8 in java.library.path, no leveldbjni in java.library.path, /tmp/libleveldbjni-64-1-5831130919690654925.8: /tmp/libleveldbjni-64-1-5831130919690654925.8: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64-bit platform)]</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not load library. Reasons: [no leveldbjni64-1.8 in java.library.path, no leveldbjni-1.8 in java.library.path, no leveldbjni in java.library.path, /tmp/libleveldbjni-64-1-8064991655020391824.8: /tmp/libleveldbjni-64-1-8064991655020391824.8: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64-bit platform)]</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>expected:&lt;101&gt; but was:&lt;71&gt;</li></div><div><li>Could not load library. Reasons: [no leveldbjni64-1.8 in java.library.path, no leveldbjni-1.8 in java.library.path, no leveldbjni in java.library.path, /tmp/libleveldbjni-64-1-3057714886751738676.8: /tmp/libleveldbjni-64-1-3057714886751738676.8: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64-bit platform)]</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertFalse(Assert.java:64)
	at org.junit.Assert.assertFalse(Assert.java:74)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.TestPlacementConstraintsUtil.testInterAppConstraintsByAppID(TestPlacementConstraintsUtil.java:965)
	at sun.reflect.N</li></div><div><li>expected:&lt;[org.apache.hadoop.security.token.delegation.DelegationKey@f4d60782, org.apache.hadoop.security.token.delegation.DelegationKey@69898573]&gt; but was:&lt;[org.apache.hadoop.security.token.delegation.DelegationKey@4f72a6d0, org.apache.hadoop.security.token.delegation.DelegationKey@f4d60782, org.apache.hadoop.security.token.delegation.DelegationKey@69898573]&gt;</li></div><div><li>Could not load library. Reasons: [no leveldbjni64-1.8 in java.library.path, no leveldbjni-1.8 in java.library.path, no leveldbjni in java.library.path, /tmp/libleveldbjni-64-1-6599754015035025982.8: /tmp/libleveldbjni-64-1-6599754015035025982.8: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64-bit platform)]</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not load library. Reasons: [no leveldbjni64-1.8 in java.library.path, no leveldbjni-1.8 in java.library.path, no leveldbjni in java.library.path, /tmp/libleveldbjni-64-1-5931867130731789331.8: /tmp/libleveldbjni-64-1-5931867130731789331.8: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64-bit platform)]</li></div><div><li>java.io.IOException: Couldn't delete data file for leveldb timeline store /var/lib/jenkins/workspace/hadoop/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timeline-pluginstorage/target/tmp/yarn/timeline/app1-timeline-cache.ldb</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>java.io.IOException: Couldn't delete data file for leveldb timeline store /var/lib/jenkins/workspace/hadoop/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timeline-pluginstorage/target/tmp/yarn/timeline/app1-timeline-cache.ldb</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>java.io.IOException: Couldn't delete data file for leveldb timeline store /var/lib/jenkins/workspace/hadoop/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timeline-pluginstorage/target/tmp/yarn/timeline/app1-timeline-cache.ldb</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>java.io.IOException: Couldn't delete data file for leveldb timeline store /var/lib/jenkins/workspace/hadoop/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timeline-pluginstorage/target/tmp/yarn/timeline/app1-timeline-cache.ldb</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>java.io.IOException: Couldn't delete data file for leveldb timeline store /var/lib/jenkins/workspace/hadoop/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timeline-pluginstorage/target/tmp/yarn/timeline/app1-timeline-cache.ldb</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>java.io.IOException: Couldn't delete data file for leveldb timeline store /var/lib/jenkins/workspace/hadoop/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timeline-pluginstorage/target/tmp/yarn/timeline/app1-timeline-cache.ldb</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>java.io.IOException: Couldn't delete data file for leveldb timeline store /var/lib/jenkins/workspace/hadoop/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timeline-pluginstorage/target/tmp/yarn/timeline/app1-timeline-cache.ldb</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>java.io.IOException: Couldn't delete data file for leveldb timeline store /var/lib/jenkins/workspace/hadoop/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timeline-pluginstorage/target/tmp/yarn/timeline/app1-timeline-cache.ldb</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>java.io.IOException: Couldn't delete data file for leveldb timeline store /var/lib/jenkins/workspace/hadoop/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timeline-pluginstorage/target/tmp/yarn/timeline/app1-timeline-cache.ldb</li></div><div><li>Could not load library. Reasons: [no leveldbjni64-1.8 in java.library.path, no leveldbjni-1.8 in java.library.path, no leveldbjni in java.library.path, /tmp/libleveldbjni-64-1-6757987075476754166.8: /tmp/libleveldbjni-64-1-6757987075476754166.8: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64-bit platform)]</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Response from server should have been Bad Request</li></div><div><li>Response from server should have been Not Found</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Response from server should have been Not Found</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;host1[]:9600&gt; but was:&lt;host1[.persistent.co.in.]:9600&gt;</li></div><div><li>expected:&lt;host1[]:9600&gt; but was:&lt;host1[.persistent.co.in.]:9600&gt;</li></div><div><li>String index out of range: -1</li></div><div><li>write timedout too late in 1228 ms.</li></div><div><li> Expected to find 'whereas it is under recovery' but got unexpected exception: java.io.IOException: Unable to close file because the last blockBP-1207188834-172.17.0.2-1540231270220:blk_1073741826_1002 does not have enough number of replicas.
 at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:964)
 at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.jav</li></div><div><li>Timed out waiting for condition. Thread diagnostics:
Timestamp: 2018-10-22 03:56:15,799

"IPC Server handler 1 on 43121" daemon prio=5 tid=53 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObj</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.mapreduce.v2.hs.server.TestHSAdminServer.testRefreshSuperUserGroups(TestHSAdminServer.java:208)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeM</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.mapreduce.v2.hs.server.TestHSAdminServer.testRefreshSuperUserGroups(TestHSAdminServer.java:208)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeM</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>Expected success for Probe Status, time="Tue Oct 23 01:08:32 UTC 2018", outcome="failure", message="Failure in Default probe: IP presence with DNS checking and DNS server address 8.8.8.8", exception="java.io.IOException: comp-0: DNS checking is enabled, but lookup for example.com is not available yet"</li></div><div><li>Application attempt appattempt_1540245924560_0001_000001 doesn't exist in ApplicationMasterService cache.
 at org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.allocate(ApplicationMasterService.java:398)
 at org.apache.hadoop.yarn.server.nodemanager.amrmproxy.DefaultRequestInterceptor$3.allocate(DefaultRequestInterceptor.java:224)
 at org.apache.hadoop.yarn.server.nodemanager.</li></div><div><li>ProcessTree shouldn't be alive</li></div><div><li>Process is still alive!</li></div><div><li>Process is still alive!</li></div><div><li>expected:&lt;[hadoop.apache.org]&gt; but was:&lt;[N/A]&gt;</li></div><div><li>expected:&lt;jenkins[xyz,jenkins]&gt; but was:&lt;jenkins[,jenkinsxyz]&gt;</li></div><div><li>expected:&lt;[jenkins,world:anyone:rwcda]&gt; but was:&lt;[world:anyone:rwcda,jenkins]&gt;</li></div><div><li>expected:&lt;[jenkins,world:anyone:rwcda]&gt; but was:&lt;[world:anyone:rwcda,jenkins]&gt;</li></div><div><li>expected:&lt;101&gt; but was:&lt;71&gt;</li></div><div><li>expected:&lt;2048&gt; but was:&lt;4096&gt;</li></div><div><li>expected:&lt;0.5&gt; but was:&lt;0.0&gt;</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Response from server should have been Bad Request</li></div><div><li>Response from server should have been Not Found</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Response from server should have been Not Found</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>String index out of range: -1</li></div><div><li>test timed out after 120000 milliseconds</li></div><div><li>Wrongly computed block reconstruction work</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.mapreduce.v2.hs.server.TestHSAdminServer.testRefreshSuperUserGroups(TestHSAdminServer.java:208)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeM</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.mapreduce.v2.hs.server.TestHSAdminServer.testRefreshSuperUserGroups(TestHSAdminServer.java:208)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeM</li></div><div><li>Job didn't finish in 30 seconds</li></div><div><li>expected:&lt;DEFAULT&gt; but was:&lt;HIGH&gt;</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/jce/provider/BouncyCastleProvider</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>TestSLSRunner catched exception from child thread (TaskRunner.TaskDefinition): [java.lang.reflect.UndeclaredThrowableException]</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>Expected success for Probe Status, time="Tue Oct 23 02:08:04 UTC 2018", outcome="failure", message="Failure in Default probe: IP presence with DNS checking and DNS server address 8.8.8.8", exception="java.io.IOException: comp-0: DNS checking is enabled, but lookup for example.com is not available yet"</li></div><div><li>ProcessTree shouldn't be alive</li></div><div><li>Process is still alive!</li></div><div><li>Process is still alive!</li></div><div><li>expected:&lt;jenkins[xyz,jenkins]&gt; but was:&lt;jenkins[,jenkinsxyz]&gt;</li></div><div><li>expected:&lt;[jenkins,world:anyone:rwcda]&gt; but was:&lt;[world:anyone:rwcda,jenkins]&gt;</li></div><div><li>expected:&lt;[jenkins,world:anyone:rwcda]&gt; but was:&lt;[world:anyone:rwcda,jenkins]&gt;</li></div><div><li>expected:&lt;101&gt; but was:&lt;71&gt;</li></div><div><li>expected:&lt;0.5&gt; but was:&lt;0.0&gt;</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Response from server should have been Bad Request</li></div><div><li>Response from server should have been Not Found</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Response from server should have been Not Found</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;host1[]:9600&gt; but was:&lt;host1[.persistent.co.in.]:9600&gt;</li></div><div><li>expected:&lt;host1[]:9600&gt; but was:&lt;host1[.persistent.co.in.]:9600&gt;</li></div><div><li>String index out of range: -1</li></div><div><li>write timedout too late in 1319 ms.</li></div><div><li>expected:&lt;3&gt; but was:&lt;2&gt;</li></div><div><li>test timed out after 120000 milliseconds</li></div><div><li>Item doesn't exist in the attempted list expected:&lt;1&gt; but was:&lt;0&gt;</li></div><div><li> Expected to find 'localhost:38366: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:38366: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li> Expected to find 'localhost:41502: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:41502: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li> Expected to find 'localhost:44340: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:44340: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li>expected timeout</li></div><div><li> Expected to find 'localhost:36677: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:36677: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li> Expected to find 'localhost:37084: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:37084: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li> Expected to find 'localhost:35003: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:35003: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li>expected timeout</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.mapreduce.v2.hs.server.TestHSAdminServer.testRefreshSuperUserGroups(TestHSAdminServer.java:208)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeM</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.mapreduce.v2.hs.server.TestHSAdminServer.testRefreshSuperUserGroups(TestHSAdminServer.java:208)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeM</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>Child process owned by init escaped process tree.</li></div><div><li>Timed out waiting for condition. Thread diagnostics:
Timestamp: 2018-10-23 07:41:20,688

"Finalizer" daemon prio=8 tid=3 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143)
        at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:164)
        at java.la</li></div><div><li>expected:&lt;jenkins[xyz,jenkins]&gt; but was:&lt;jenkins[,jenkinsxyz]&gt;</li></div><div><li>expected:&lt;[jenkins,world:anyone:rwcda]&gt; but was:&lt;[world:anyone:rwcda,jenkins]&gt;</li></div><div><li>expected:&lt;[jenkins,world:anyone:rwcda]&gt; but was:&lt;[world:anyone:rwcda,jenkins]&gt;</li></div><div><li>expected:&lt;1&gt; but was:&lt;0&gt;</li></div><div><li>expected:&lt;0&gt; but was:&lt;8192&gt;</li></div><div><li>expected:&lt;101&gt; but was:&lt;71&gt;</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Response from server should have been Bad Request</li></div><div><li>Response from server should have been Not Found</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Response from server should have been Not Found</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.hdfs.server.federation.router.TestRouterClientRejectOverload.testOverloadControl</div></li><li><div>org.apache.hadoop.hdfs.TestReadStripedFileWithMissingBlocks.testReadFileWithMissingBlocks</div></li><li><div>org.apache.hadoop.hdfs.server.datanode.TestNNHandlesBlockReportPerStorage.blockReport_08</div></li><li><div>org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestSpaceReservation.testReservedSpaceForLeaseRecovery</div></li><li><div>org.apache.hadoop.hdfs.server.namenode.TestPersistentStoragePolicySatisfier.testSPSxAttrWhenSpsCalledForDir</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.testContainerUpgradeSuccessExplicitRollback</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestLogAggregationService.testFixedSizeThreadPool</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.TestApplicationMasterServiceFair.testUpdateTrackingUrl</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.reservation.TestCapacityOverTimePolicy.testAllocation[Duration 90,000,000, height 0.25, numSubmission 1, periodic 86400000)]</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.security.TestDelegationTokenRenewer.testRMRestartWithExpiredToken</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.hdfs.TestLeaseRecovery2.testHardLeaseRecoveryWithRenameAfterNameNodeRestart</div></li><li><div>org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFSStriped.testRead</div></li><li><div>org.apache.hadoop.hdfs.server.namenode.ha.TestHAAppend.testMultipleAppendsDuringCatchupTailing</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.testLocalingResourceWhileContainerRunning</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.ipc.TestRPCWaitForProxy.testInterruptedWaitForProxy</div></li><li><div>org.apache.hadoop.hdfs.TestLeaseRecovery2.testCloseWhileRecoverLease</div></li><li><div>org.apache.hadoop.hdfs.server.datanode.TestDataNodeUUID.testUUIDRegeneration</div></li><li><div>org.apache.hadoop.yarn.service.monitor.probe.TestDefaultProbe.testDefaultProbe[2]</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestQueueManagementDynamicEditPolicy.testEditSchedule</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.security.TestRMDelegationTokens.testRMDTMasterKeyStateOnRollingMasterKey</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider.testTokenServiceCreationWithLegacyFormat</div></li><li><div>org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider.testTokenServiceCreationWithUriFormat</div></li><li><div>org.apache.hadoop.fs.contract.router.web.TestRouterWebHDFSContractAppend.testRenameFileBeingAppended</div></li><li><div>org.apache.hadoop.hdfs.server.federation.router.TestRouterAllResolver.testSpaceAll</div></li><li><div>org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailure.testDataNodeShutdownAfterNumFailedVolumeExceedsTolerated</div></li><li><div>org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestLazyPersistReplicaRecovery.testDnRestartWithSavedReplicas</div></li><li><div>org.apache.hadoop.hdfs.server.namenode.TestDecommissioningStatus.testDecommissionStatus</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.rmapp.TestRMAppTransitions.testAppSuccessPath[1]</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider.testCreation</div></li><li><div>org.apache.hadoop.hdfs.TestDFSShell.testCopyCommandsWithPreserveOption</div></li><li><div>org.apache.hadoop.hdfs.TestLeaseRecovery2.testHardLeaseRecoveryAfterNameNodeRestart2</div></li><li><div>org.apache.hadoop.hdfs.TestReadStripedFileWithMissingBlocks.testReadFileWithMissingBlocks</div></li><li><div>org.apache.hadoop.hdfs.server.balancer.TestBalancerRPCDelay.testBalancerRPCDelay</div></li><li><div>org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.TestInMemoryLevelDBAliasMapClient.writeRead</div></li><li><div>org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.TestInMemoryLevelDBAliasMapClient.iterateThreeBatches</div></li><li><div>org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.TestInMemoryLevelDBAliasMapClient.multipleReads</div></li><li><div>org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.TestInMemoryLevelDBAliasMapClient.iterateSingleBatch</div></li><li><div>org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.TestInMemoryLevelDBAliasMapClient.testServerBindHost</div></li><li><div>org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.TestInMemoryLevelDBAliasMapClient.testNonExistentBlock</div></li><li><div>org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.TestLevelDBFileRegionAliasMap.testReadBack</div></li><li><div>org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.TestLevelDBFileRegionAliasMap.testIterate</div></li><li><div>org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting.testHotSwapOutFailedVolumeAndReporting</div></li><li><div>org.apache.hadoop.hdfs.server.namenode.TestNameNodeMXBean.testDecommissioningNodes</div></li><li><div>org.apache.hadoop.hdfs.tools.offlineImageViewer.TestOfflineImageViewer.testPBDelimitedWriter</div></li><li><div>org.apache.hadoop.mapreduce.v2.hs.TestHistoryServerLeveldbStateStoreService.testTokenStore</div></li><li><div>org.apache.hadoop.mapreduce.v2.hs.TestHistoryServerLeveldbStateStoreService.testCheckVersion</div></li><li><div>org.apache.hadoop.mapred.TestClusterMRNotification.testMR</div></li><li><div>org.apache.hadoop.mapred.TestShuffleHandler.testRecoveryFromOtherVersions</div></li><li><div>org.apache.hadoop.mapred.TestShuffleHandler.testRecovery</div></li><li><div>org.apache.hadoop.yarn.applications.distributedshell.TestDistributedShell.testDSShellWithDomainV1_5</div></li><li><div>org.apache.hadoop.yarn.applications.distributedshell.TestDistributedShell.testDSShellWithoutDomainV1_5</div></li><li><div>org.apache.hadoop.yarn.server.applicationhistoryservice.TestApplicationHistoryServer.testLaunchWithArguments</div></li><li><div>org.apache.hadoop.yarn.server.applicationhistoryservice.TestApplicationHistoryServer.testLaunch</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testGetDomains</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testRelatingToNonExistingEntity</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testValidateConfig</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testGetEntitiesWithPrimaryFilters</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testRelatingToOldEntityWithoutDomainId</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testRootDirPermission</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testGetSingleEntity</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testDeleteEntitiesPrimaryFilters</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testDeleteEntities</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testCacheSizes</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testGetEntities</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testGetEntityTypes</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testFromTsWithDeletion</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testLevelDbRepair</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testGetDomain</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testGetEvents</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testGetEntitiesWithSecondaryFilters</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testCheckVersion</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testGetEntitiesWithFromId</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testGetEntitiesWithFromTs</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDB.testInsertForPreviousPeriodAfterRollPeriodRollsDB</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDB.testInsertAfterRollPeriodRollsDB</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testGetDomains</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testGetDomains</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testRelatingToNonExistingEntity</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testRelatingToNonExistingEntity</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testValidateConfig</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testValidateConfig</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testGetEntitiesWithPrimaryFilters</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testGetEntitiesWithPrimaryFilters</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testRelatingToOldEntityWithoutDomainId</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testRelatingToOldEntityWithoutDomainId</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testRootDirPermission</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testRootDirPermission</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testGetSingleEntity</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testGetSingleEntity</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testCacheSizes</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testCacheSizes</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testGetEntities</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testGetEntities</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testRelatingToEntityInSamePut</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testRelatingToEntityInSamePut</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testGetDomain</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testGetDomain</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testGetEvents</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testGetEvents</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testGetEntitiesWithSecondaryFilters</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testGetEntitiesWithSecondaryFilters</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testCheckVersion</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testCheckVersion</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testGetEntitiesWithFromId</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testGetEntitiesWithFromId</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testGetEntitiesWithFromTs</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testGetEntitiesWithFromTs</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestTimelineDataManager.testGetOldEntitiesWithOutDomainId</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestTimelineDataManager.testGetEntitiesAclEnabled</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestTimelineDataManager.testGetOldEntityWithOutDomainId</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestTimelineDataManager.testUpdatingOldEntityWithoutDomainId</div></li><li><div>org.apache.hadoop.yarn.server.timeline.recovery.TestLeveldbTimelineStateStore.testTokenStore</div></li><li><div>org.apache.hadoop.yarn.server.timeline.recovery.TestLeveldbTimelineStateStore.testCheckVersion</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerShutdown.testStateStoreRemovalOnDecommission</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerShutdown.testStateStoreRemovalOnDecommission</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.testContainerUpgradeSuccessExplicitRollback</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestLogAggregationService.testFixedSizeThreadPool</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService.testAMRMProxyStorage</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService.testEmptyRestartTimes</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService.testEmptyState</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService.testStateStoreNodeHealth</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService.testContainerStorage</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService.testFinishResourceLocalization</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService.testNMTokenStorage</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService.testCompactionCycle</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService.testLogDeleterStorage</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService.testDeletionTaskStorage</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService.testContainerTokenStorage</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService.testLocalTrackerStateIterator</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService.testStateStoreForResourceMapping</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService.testStartResourceLocalization</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService.testApplicationStorage</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService.testCheckVersion</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService.testRemoveLocalizedResource</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService.testIsNewlyCreated</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService.testUnexpectedKeyDoesntThrowException</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.metrics.TestCombinedSystemMetricsPublisher.testTimelineServiceEventPublishingNoService</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.metrics.TestCombinedSystemMetricsPublisher.testTimelineServiceEventPublishingV2Enabled</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.recovery.TestLeveldbRMStateStore.testRemoveAttempt</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.recovery.TestLeveldbRMStateStore.testBadKeyIteration</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.recovery.TestLeveldbRMStateStore.testDeleteStore</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.recovery.TestLeveldbRMStateStore.testEpoch</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.recovery.TestLeveldbRMStateStore.testApps</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.recovery.TestLeveldbRMStateStore.testProxyCA</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.recovery.TestLeveldbRMStateStore.testClientTokens</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.recovery.TestLeveldbRMStateStore.testVersion</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.recovery.TestLeveldbRMStateStore.testAMTokens</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.recovery.TestLeveldbRMStateStore.testAppDeletion</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.recovery.TestLeveldbRMStateStore.testRemoveApplication</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.recovery.TestLeveldbRMStateStore.testReservation</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.TestLeveldbConfigurationStore.testVersioning</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.TestLeveldbConfigurationStore.testPersistConfiguration</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.TestLeveldbConfigurationStore.testMaxLogs</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.TestLeveldbConfigurationStore.testRestartReadsFromUpdatedStore</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.TestLeveldbConfigurationStore.testPersistUpdatedConfiguration</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.TestLeveldbConfigurationStore.testConfigurationUpdate</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.TestLeveldbConfigurationStore.testNullConfigurationUpdate</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.TestPlacementConstraintsUtil.testInterAppConstraintsByAppID</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.security.TestRMDelegationTokens.testRMDTMasterKeyStateOnRollingMasterKey</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestEntityGroupFSTimelineStore.testSummaryRead</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestEntityGroupFSTimelineStore.testAppLogsScanLogs</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestEntityGroupFSTimelineStore.testPluginRead</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestEntityGroupFSTimelineStore.testCleanLogs</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestEntityGroupFSTimelineStore.testGetEntityPluginRead</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestEntityGroupFSTimelineStore.testScanActiveLogsAndMoveToDonePluginRead</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestEntityGroupFSTimelineStore.testParseSummaryLogs</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestEntityGroupFSTimelineStore.testMoveToDone</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestLevelDBCacheTimelineStore.testGetDomains</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestLevelDBCacheTimelineStore.testGetDomains</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestLevelDBCacheTimelineStore.testGetEntitiesWithPrimaryFilters</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestLevelDBCacheTimelineStore.testGetEntitiesWithPrimaryFilters</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestLevelDBCacheTimelineStore.testGetSingleEntity</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestLevelDBCacheTimelineStore.testGetSingleEntity</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestLevelDBCacheTimelineStore.testGetEntities</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestLevelDBCacheTimelineStore.testGetEntities</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestLevelDBCacheTimelineStore.testGetDomain</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestLevelDBCacheTimelineStore.testGetDomain</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestLevelDBCacheTimelineStore.testGetEvents</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestLevelDBCacheTimelineStore.testGetEvents</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestLevelDBCacheTimelineStore.testGetEntitiesWithSecondaryFilters</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestLevelDBCacheTimelineStore.testGetEntitiesWithSecondaryFilters</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestLevelDBCacheTimelineStore.testGetEntitiesWithFromId</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestLevelDBCacheTimelineStore.testGetEntitiesWithFromId</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestLevelDBCacheTimelineStore.testGetEntitiesWithFromTs</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestLevelDBCacheTimelineStore.testGetEntitiesWithFromTs</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestOverrideTimelineStoreYarnClient.testLifecycleAndOverride</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.hdfs.TestDistributedFileSystem.testDFSClientPeerWriteTimeout</div></li><li><div>org.apache.hadoop.hdfs.TestLeaseRecovery2.testCloseWhileRecoverLease</div></li><li><div>org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestLazyPersistReplicaRecovery.testDnRestartWithSavedReplicas</div></li><li><div>org.apache.hadoop.yarn.client.api.impl.TestAMRMProxy.testAMRMProxyTokenRenewal</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.TestContainerLaunch.testKillProcessGroup</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.TestContainersMonitor.testContainerKillOnMemoryOverflow</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.TestApplicationMasterServiceFair.testUpdateTrackingUrl</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestIncreaseAllocationExpirer.testContainerIncreaseAllocationExpiration</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestQueueManagementDynamicEditPolicy.testEditSchedule</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.hdfs.server.datanode.TestDataNodeErasureCodingMetrics.testReconstructionBytesPartialGroup2</div></li><li><div>org.apache.hadoop.mapred.TestMRTimelineEventHandling.testMapreduceJobTimelineServiceEnabled</div></li><li><div>org.apache.hadoop.mapreduce.v2.TestUberAM.testJobWithChangePriority</div></li><li><div>org.apache.hadoop.yarn.sls.TestSLSRunner.testSimulatorRunning[Testing with: SYNTH, org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler, (nodeFile null)]</div></li><li><div>org.apache.hadoop.yarn.service.monitor.probe.TestDefaultProbe.testDefaultProbe[2]</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.TestContainerLaunch.testKillProcessGroup</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.TestContainersMonitor.testContainerKillOnMemoryOverflow</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestQueueManagementDynamicEditPolicy.testEditSchedule</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider.testTokenServiceCreationWithLegacyFormat</div></li><li><div>org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider.testTokenServiceCreationWithUriFormat</div></li><li><div>org.apache.hadoop.hdfs.TestDistributedFileSystem.testDFSClientPeerWriteTimeout</div></li><li><div>org.apache.hadoop.hdfs.TestEncryptedTransfer.testEncryptedAppendRequiringBlockTransfer[1]</div></li><li><div>org.apache.hadoop.hdfs.server.namenode.sps.TestBlockStorageMovementAttemptedItems.testNoBlockMovementAttemptFinishedReportAdded</div></li><li><div>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testAuthUrlConnectTimeout[timeoutSource=ConnectionFactory]</div></li><li><div>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testConnectTimeout[timeoutSource=ConnectionFactory]</div></li><li><div>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testRedirectConnectTimeout[timeoutSource=ConnectionFactory]</div></li><li><div>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testTwoStepWriteConnectTimeout[timeoutSource=ConnectionFactory]</div></li><li><div>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testAuthUrlConnectTimeout[timeoutSource=Configuration]</div></li><li><div>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testConnectTimeout[timeoutSource=Configuration]</div></li><li><div>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testRedirectConnectTimeout[timeoutSource=Configuration]</div></li><li><div>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testTwoStepWriteConnectTimeout[timeoutSource=Configuration]</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.testLocalingResourceWhileContainerRunning</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.rmapp.TestRMAppTransitions.testAppSuccessPath[1]</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacitySchedulerSchedulingRequestUpdate.testBasicPendingResourceUpdate</div></li></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="hbase"><div style="font-weight:bold;" class="panel-heading">HBASE<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Prajyot)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>6bdaedd7ce806d3ec3d5a9a4540b544594898374</div><div><b>Last Run: </b>01-11-2018 14:31 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC UBUNTU18</th><th>X86 UBUNTU18</th><th>PPC RHEL72</th><th>X86 RHEL72</th><th>PPC RHEL75</th><th>X86 RHEL75</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 4814</div><div>Failed Count : 7</div><div>Skipped Count : 40</div></td><td><div>Total Count : 4699</div><div>Failed Count : 7</div><div>Skipped Count : 41</div></td><td><div>Total Count : 4763</div><div>Failed Count : 84</div><div>Skipped Count : 40</div></td><td><div>Total Count : 4699</div><div>Failed Count : 8</div><div>Skipped Count : 41</div></td><td><div>Total Count : 4815</div><div>Failed Count : 2</div><div>Skipped Count : 40</div></td><td><div>Total Count : 4749</div><div>Failed Count : 9</div><div>Skipped Count : 41</div></td><td><div>Total Count : 4814</div><div>Failed Count : 0</div><div>Skipped Count : 40</div></td><td><div>Total Count : 4745</div><div>Failed Count : 2</div><div>Skipped Count : 41</div></td></tr><tr><td>Result</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.testExportWithTargetName</li></div><div><li>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.testExportFileSystemState</li></div><div><li>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.testConsecutiveExports</li></div><div><li>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.testEmptyExportFileSystemState</li></div><div><li>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.testExportFileSystemStateWithSkipTmp</li></div><div><li>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.testExportRetry</li></div><div><li>org.apache.hadoop.hbase.regionserver.TestCompaction.testStopStartCompaction</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.testExportWithTargetName</li></div><div><li>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.testExportFileSystemState</li></div><div><li>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.testConsecutiveExports</li></div><div><li>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.testEmptyExportFileSystemState</li></div><div><li>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.testExportFileSystemStateWithSkipTmp</li></div><div><li>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.testExportRetry</li></div><div><li>org.apache.hadoop.hbase.master.balancer.TestRSGroupBasedLoadBalancerWithStochasticLoadBalancerAsInternal.testBalanceCluster</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.hbase.mapreduce.TestImportExport.org.apache.hadoop.hbase.mapreduce.TestImportExport</li></div><div><li>org.apache.hadoop.hbase.mapreduce.TestImportExport.org.apache.hadoop.hbase.mapreduce.TestImportExport</li></div><div><li>org.apache.hadoop.hbase.mapreduce.TestImportExport.testWithFilter</li></div><div><li>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.testExportWithTargetName</li></div><div><li>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.testExportFileSystemState</li></div><div><li>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory</li></div><div><li>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory</li></div><div><li>org.apache.hadoop.hbase.rest.TestGetAndPutResource.org.apache.hadoop.hbase.rest.TestGetAndPutResource</li></div><div><li>org.apache.hadoop.hbase.rest.TestGetAndPutResource.org.apache.hadoop.hbase.rest.TestGetAndPutResource</li></div><div><li>org.apache.hadoop.hbase.rsgroup.TestRSGroupsAdmin1.org.apache.hadoop.hbase.rsgroup.TestRSGroupsAdmin1</li></div><div><li>org.apache.hadoop.hbase.rsgroup.TestRSGroupsAdmin1.org.apache.hadoop.hbase.rsgroup.TestRSGroupsAdmin1</li></div><div><li>org.apache.hadoop.hbase.rsgroup.TestRSGroupsBasics.org.apache.hadoop.hbase.rsgroup.TestRSGroupsBasics</li></div><div><li>org.apache.hadoop.hbase.rsgroup.TestRSGroupsBasics.org.apache.hadoop.hbase.rsgroup.TestRSGroupsBasics</li></div><div><li>org.apache.hadoop.hbase.TestClientOperationTimeout.testPutTimeout</li></div><div><li>org.apache.hadoop.hbase.TestClientOperationTimeout.testGetTimeout</li></div><div><li>org.apache.hadoop.hbase.TestClientOperationTimeout.testScanTimeout</li></div><div><li>org.apache.hadoop.hbase.TestInfoServers.testInfoServersStatusPages</li></div><div><li>org.apache.hadoop.hbase.client.TestAsyncDecommissionAdminApi.testAsyncDecommissionRegionServers[0]</li></div><div><li>org.apache.hadoop.hbase.client.TestAsyncDecommissionAdminApi.testAsyncDecommissionRegionServers[1]</li></div><div><li>org.apache.hadoop.hbase.client.TestReplicaWithCluster.testReplicaAndReplication</li></div><div><li>org.apache.hadoop.hbase.client.TestReplicaWithCluster.org.apache.hadoop.hbase.client.TestReplicaWithCluster</li></div><div><li>org.apache.hadoop.hbase.client.TestReplicaWithCluster.org.apache.hadoop.hbase.client.TestReplicaWithCluster</li></div><div><li>org.apache.hadoop.hbase.client.TestScannersFromClientSide.testScanRawDeleteFamilyVersion</li></div><div><li>org.apache.hadoop.hbase.client.TestScannersFromClientSide.org.apache.hadoop.hbase.client.TestScannersFromClientSide</li></div><div><li>org.apache.hadoop.hbase.client.TestScannersFromClientSide.org.apache.hadoop.hbase.client.TestScannersFromClientSide</li></div><div><li>org.apache.hadoop.hbase.client.TestSeparateClientZKCluster.testMetaMoveDuringClientZkClusterRestart</li></div><div><li>org.apache.hadoop.hbase.client.TestTableFavoredNodes.testMergeTable</li></div><div><li>org.apache.hadoop.hbase.client.replication.TestReplicationAdmin.org.apache.hadoop.hbase.client.replication.TestReplicationAdmin</li></div><div><li>org.apache.hadoop.hbase.client.replication.TestReplicationAdminWithClusters.org.apache.hadoop.hbase.client.replication.TestReplicationAdminWithClusters</li></div><div><li>org.apache.hadoop.hbase.client.replication.TestReplicationAdminWithClusters.org.apache.hadoop.hbase.client.replication.TestReplicationAdminWithClusters</li></div><div><li>org.apache.hadoop.hbase.coprocessor.TestMasterObserver.org.apache.hadoop.hbase.coprocessor.TestMasterObserver</li></div><div><li>org.apache.hadoop.hbase.coprocessor.TestMasterObserver.org.apache.hadoop.hbase.coprocessor.TestMasterObserver</li></div><div><li>org.apache.hadoop.hbase.coprocessor.TestMasterObserver.testTableOperations</li></div><div><li>org.apache.hadoop.hbase.io.hfile.TestScannerSelectionUsingTTL.testScannerSelection[5]</li></div><div><li>org.apache.hadoop.hbase.master.TestAssignmentManagerMetrics.testRITAssignmentManagerMetrics</li></div><div><li>org.apache.hadoop.hbase.master.TestMasterMetricsWrapper.testOfflineRegion</li></div><div><li>org.apache.hadoop.hbase.master.TestMasterRepairMode.testNewCluster</li></div><div><li>org.apache.hadoop.hbase.master.assignment.TestCloseRegionWhileRSCrash.testRetryBackoff</li></div><div><li>org.apache.hadoop.hbase.master.assignment.TestMergeTableRegionsProcedure.org.apache.hadoop.hbase.master.assignment.TestMergeTableRegionsProcedure</li></div><div><li>org.apache.hadoop.hbase.master.assignment.TestMergeTableRegionsProcedure.org.apache.hadoop.hbase.master.assignment.TestMergeTableRegionsProcedure</li></div><div><li>org.apache.hadoop.hbase.master.assignment.TestSplitTableRegionProcedure.org.apache.hadoop.hbase.master.assignment.TestSplitTableRegionProcedure</li></div><div><li>org.apache.hadoop.hbase.master.assignment.TestSplitTableRegionProcedure.org.apache.hadoop.hbase.master.assignment.TestSplitTableRegionProcedure</li></div><div><li>org.apache.hadoop.hbase.master.cleaner.TestHFileLinkCleaner.org.apache.hadoop.hbase.master.cleaner.TestHFileLinkCleaner</li></div><div><li>org.apache.hadoop.hbase.master.locking.TestLockManager.org.apache.hadoop.hbase.master.locking.TestLockManager</li></div><div><li>org.apache.hadoop.hbase.master.locking.TestLockManager.org.apache.hadoop.hbase.master.locking.TestLockManager</li></div><div><li>org.apache.hadoop.hbase.master.procedure.TestCloneSnapshotProcedure.testRecoveryAndDoubleExecution</li></div><div><li>org.apache.hadoop.hbase.master.procedure.TestCloneSnapshotProcedure.org.apache.hadoop.hbase.master.procedure.TestCloneSnapshotProcedure</li></div><div><li>org.apache.hadoop.hbase.master.procedure.TestCloneSnapshotProcedure.org.apache.hadoop.hbase.master.procedure.TestCloneSnapshotProcedure</li></div><div><li>org.apache.hadoop.hbase.master.procedure.TestRestoreSnapshotProcedure.org.apache.hadoop.hbase.master.procedure.TestRestoreSnapshotProcedure</li></div><div><li>org.apache.hadoop.hbase.master.procedure.TestRestoreSnapshotProcedure.org.apache.hadoop.hbase.master.procedure.TestRestoreSnapshotProcedure</li></div><div><li>org.apache.hadoop.hbase.master.procedure.TestTruncateTableProcedure.org.apache.hadoop.hbase.master.procedure.TestTruncateTableProcedure</li></div><div><li>org.apache.hadoop.hbase.master.procedure.TestTruncateTableProcedure.org.apache.hadoop.hbase.master.procedure.TestTruncateTableProcedure</li></div><div><li>org.apache.hadoop.hbase.master.procedure.TestTruncateTableProcedure.testRecoveryAndDoubleExecutionPreserveSplits</li></div><div><li>org.apache.hadoop.hbase.namespace.TestNamespaceAuditor.org.apache.hadoop.hbase.namespace.TestNamespaceAuditor</li></div><div><li>org.apache.hadoop.hbase.namespace.TestNamespaceAuditor.org.apache.hadoop.hbase.namespace.TestNamespaceAuditor</li></div><div><li>org.apache.hadoop.hbase.regionserver.TestLogRoller.testRemoveClosedWAL</li></div><div><li>org.apache.hadoop.hbase.regionserver.TestRegionReplicasAreDistributed.testRegionReplicasCreatedAreDistributed</li></div><div><li>org.apache.hadoop.hbase.regionserver.TestRegionReplicasWithModifyTable.org.apache.hadoop.hbase.regionserver.TestRegionReplicasWithModifyTable</li></div><div><li>org.apache.hadoop.hbase.regionserver.TestRegionReplicasWithModifyTable.org.apache.hadoop.hbase.regionserver.TestRegionReplicasWithModifyTable</li></div><div><li>org.apache.hadoop.hbase.regionserver.throttle.TestFlushWithThroughputController.testFlushControlForStripedStore</li></div><div><li>org.apache.hadoop.hbase.regionserver.throttle.TestFlushWithThroughputController.testFlushControl</li></div><div><li>org.apache.hadoop.hbase.replication.TestAddToSerialReplicationPeer.org.apache.hadoop.hbase.replication.TestAddToSerialReplicationPeer</li></div><div><li>org.apache.hadoop.hbase.replication.TestAddToSerialReplicationPeer.org.apache.hadoop.hbase.replication.TestAddToSerialReplicationPeer</li></div><div><li>org.apache.hadoop.hbase.replication.TestNamespaceReplication.org.apache.hadoop.hbase.replication.TestNamespaceReplication</li></div><div><li>org.apache.hadoop.hbase.replication.TestNamespaceReplication.org.apache.hadoop.hbase.replication.TestNamespaceReplication</li></div><div><li>org.apache.hadoop.hbase.replication.TestNamespaceReplication.testNamespaceReplication[0: serialPeer=true]</li></div><div><li>org.apache.hadoop.hbase.replication.TestReplicationEndpoint.org.apache.hadoop.hbase.replication.TestReplicationEndpoint</li></div><div><li>org.apache.hadoop.hbase.replication.TestReplicationEndpoint.org.apache.hadoop.hbase.replication.TestReplicationEndpoint</li></div><div><li>org.apache.hadoop.hbase.replication.TestSerialReplication.org.apache.hadoop.hbase.replication.TestSerialReplication</li></div><div><li>org.apache.hadoop.hbase.replication.TestSerialReplication.org.apache.hadoop.hbase.replication.TestSerialReplication</li></div><div><li>org.apache.hadoop.hbase.replication.multiwal.TestReplicationEndpointWithMultipleAsyncWAL.org.apache.hadoop.hbase.replication.multiwal.TestReplicationEndpointWithMultipleAsyncWAL</li></div><div><li>org.apache.hadoop.hbase.replication.multiwal.TestReplicationEndpointWithMultipleAsyncWAL.org.apache.hadoop.hbase.replication.multiwal.TestReplicationEndpointWithMultipleAsyncWAL</li></div><div><li>org.apache.hadoop.hbase.replication.multiwal.TestReplicationEndpointWithMultipleAsyncWAL.testWALEntryFilterUpdateValidation</li></div><div><li>org.apache.hadoop.hbase.replication.multiwal.TestReplicationEndpointWithMultipleWAL.org.apache.hadoop.hbase.replication.multiwal.TestReplicationEndpointWithMultipleWAL</li></div><div><li>org.apache.hadoop.hbase.replication.multiwal.TestReplicationEndpointWithMultipleWAL.org.apache.hadoop.hbase.replication.multiwal.TestReplicationEndpointWithMultipleWAL</li></div><div><li>org.apache.hadoop.hbase.replication.regionserver.TestDrainReplicationQueuesForStandBy.test</li></div><div><li>org.apache.hadoop.hbase.replication.regionserver.TestDrainReplicationQueuesForStandBy.org.apache.hadoop.hbase.replication.regionserver.TestDrainReplicationQueuesForStandBy</li></div><div><li>org.apache.hadoop.hbase.replication.regionserver.TestDrainReplicationQueuesForStandBy.org.apache.hadoop.hbase.replication.regionserver.TestDrainReplicationQueuesForStandBy</li></div><div><li>org.apache.hadoop.hbase.replication.regionserver.TestReplicator.org.apache.hadoop.hbase.replication.regionserver.TestReplicator</li></div><div><li>org.apache.hadoop.hbase.replication.regionserver.TestReplicator.org.apache.hadoop.hbase.replication.regionserver.TestReplicator</li></div><div><li>org.apache.hadoop.hbase.tool.TestCanaryTool.testBasicCanaryWorks</li></div><div><li>org.apache.hadoop.hbase.util.TestRegionMover.testUnloadWithAck</li></div><div><li>org.apache.hadoop.hbase.util.TestRegionMover.org.apache.hadoop.hbase.util.TestRegionMover</li></div><div><li>org.apache.hadoop.hbase.util.TestRegionMover.org.apache.hadoop.hbase.util.TestRegionMover</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.testExportWithTargetName</li></div><div><li>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.testExportFileSystemState</li></div><div><li>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.testConsecutiveExports</li></div><div><li>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.testEmptyExportFileSystemState</li></div><div><li>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.testExportFileSystemStateWithSkipTmp</li></div><div><li>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.testExportRetry</li></div><div><li>org.apache.hadoop.hbase.master.balancer.TestRSGroupBasedLoadBalancerWithStochasticLoadBalancerAsInternal.testBalanceCluster</li></div><div><li>org.apache.hadoop.hbase.replication.TestAddToSerialReplicationPeer.testAddToSerialPeer</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.hbase.master.procedure.TestDisableTableProcedure.testRecoveryAndDoubleExecution</li></div><div><li>org.apache.hadoop.hbase.master.procedure.TestDisableTableProcedure.testRecoveryAndDoubleExecution</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.hbase.master.assignment.TestMergeTableRegionsProcedure.testRollbackAndDoubleExecution</li></div><div><li>org.apache.hadoop.hbase.master.assignment.TestMergeTableRegionsProcedure.testRollbackAndDoubleExecution</li></div><div><li>org.apache.hadoop.hbase.master.assignment.TestMergeTableRegionsProcedure.testMergeTwoRegions</li></div><div><li>org.apache.hadoop.hbase.master.assignment.TestMergeTableRegionsProcedure.testMergeTwoRegions</li></div><div><li>org.apache.hadoop.hbase.master.assignment.TestMergeTableRegionsProcedure.testMergeRegionsConcurrently</li></div><div><li>org.apache.hadoop.hbase.master.assignment.TestMergeTableRegionsProcedure.testMergeRegionsConcurrently</li></div><div><li>org.apache.hadoop.hbase.master.assignment.TestMergeTableRegionsProcedure.testRecoveryAndDoubleExecution</li></div><div><li>org.apache.hadoop.hbase.master.assignment.TestMergeTableRegionsProcedure.testRecoveryAndDoubleExecution</li></div><div><li>org.apache.hadoop.hbase.master.cleaner.TestSnapshotFromMaster.testSnapshotHFileArchiving</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.hbase.procedure2.TestProcedureSkipPersistence.test</li></div><div><li>org.apache.hadoop.hbase.regionserver.TestCompactingToCellFlatMapMemStore.testForceCopyOfBigCellIntoImmutableSegment[0]</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div><div><li>expected:&lt;1&gt; but was:&lt;2&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div><div><li>expected:&lt;2&gt; but was:&lt;0&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>test timed out after 780 seconds</li></div><div><li>Appears to be stuck in thread RS-EventLoopGroup-3-2</li></div><div><li>Interrupt while waiting on Operation: CREATE, Table Name: default:testWithFilterimport, procId: 39</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div><div><li>test timed out after 780 seconds</li></div><div><li>Appears to be stuck in thread RS-EventLoopGroup-5-2</li></div><div><li>test timed out after 780 seconds</li></div><div><li>Appears to be stuck in thread DataXceiver for client DFSClient_NONMAPREDUCE_882106373_13 at /127.0.0.1:50720 [Receiving block BP-1086041749-172.17.0.2-1541132739880:blk_1073741829_1005]</li></div><div><li>test timed out after 780 seconds</li></div><div><li>Appears to be stuck in thread DataXceiver for client DFSClient_NONMAPREDUCE_-326953751_11 at /127.0.0.1:35436 [Receiving block BP-1355957444-172.17.0.2-1541128191182:blk_1073741829_1005]</li></div><div><li>test timed out after 780 seconds</li></div><div><li>Appears to be stuck in thread Socket Reader #1 for port 41036</li></div><div><li>callTimeout=500, callDuration=617: Call to 5ec8def561b1/172.17.0.2:33899 failed on local exception: org.apache.hadoop.hbase.ipc.CallTimeoutException: Call id=2, waitTime=481, rpcTimeout=472 </li></div><div><li>callTimeout=500, callDuration=607: Call to 5ec8def561b1/172.17.0.2:33899 failed on local exception: org.apache.hadoop.hbase.ipc.CallTimeoutException: Call id=4, waitTime=498, rpcTimeout=497 </li></div><div><li>callTimeout=500, callDuration=604: Call to 5ec8def561b1/172.17.0.2:33899 failed on local exception: org.apache.hadoop.hbase.ipc.CallTimeoutException: Call id=6, waitTime=495, rpcTimeout=491 </li></div><div><li>Read timed out</li></div><div><li>Could not find region testAsyncDecommissionRegionServers,aaaaa,1541109052243.f2be2e92179979fbc1905c83ac67d2f4. on server 5ec8def561b1,41863,1541109005870</li></div><div><li>java.lang.AssertionError
	at org.apache.hadoop.hbase.client.TestAsyncDecommissionAdminApi.testAsyncDecommissionRegionServers(TestAsyncDecommissionAdminApi.java:50)
</li></div><div><li>Interrupt while waiting on Operation: CREATE, Table Name: default:testReplicaAndReplication, procId: 102</li></div><div><li>test timed out after 780 seconds</li></div><div><li>Appears to be stuck in thread Socket Reader #1 for port 36142</li></div><div><li>java.io.InterruptedIOException
	at org.apache.hadoop.hbase.client.TestScannersFromClientSide.testScanRawDeleteFamilyVersion(TestScannersFromClientSide.java:616)
Caused by: java.lang.InterruptedException
	at org.apache.hadoop.hbase.client.TestScannersFromClientSide.testScanRawDeleteFamilyVersion(TestScannersFromClientSide.java:616)
</li></div><div><li>test timed out after 780 seconds</li></div><div><li>Appears to be stuck in thread RS-EventLoopGroup-5-2</li></div><div><li>Failed after attempts=3, exceptions:
Thu Nov 01 16:30:50 CDT 2018, RpcRetryingCaller{globalStartTime=1541107848204, pause=100, maxAttempts=3}, org.apache.hadoop.hbase.client.RetriesExhaustedException: Failed after attempts=3, exceptions:
Thu Nov 01 16:30:50 CDT 2018, RpcRetryingCaller{globalStartTime=1541107850221, pause=100, maxAttempts=3}, java.io.IOException: Call to 5ec8def561b1/172.17.0.2:411</li></div><div><li>The procedure 16 is still running</li></div><div><li>Shutting down</li></div><div><li>test timed out after 780 seconds</li></div><div><li>Appears to be stuck in thread Socket Reader #1 for port 46692</li></div><div><li>test timed out after 780 seconds</li></div><div><li>Appears to be stuck in thread DataXceiver for client DFSClient_NONMAPREDUCE_286118030_13 at /127.0.0.1:57588 [Receiving block BP-1016530193-172.17.0.2-1541101891757:blk_1073741829_1005]</li></div><div><li>Interrupt while waiting on Operation: ENABLE, Table Name: default:testTableOperations, procId: 399</li></div><div><li>expected:&lt;8&gt; but was:&lt;0&gt;</li></div><div><li>java.util.concurrent.TimeoutException: The procedure 9 is still running</li></div><div><li>test timed out after 30000 milliseconds</li></div><div><li>Shutting down</li></div><div><li>Failed after attempts=16, exceptions:
Thu Nov 01 13:39:45 CDT 2018, RpcRetryingCaller{globalStartTime=1541097585807, pause=100, maxAttempts=16}, java.net.ConnectException: Call to 5ec8def561b1/172.17.0.2:43861 failed on connection exception: org.apache.hbase.thirdparty.io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: 5ec8def561b1/172.17.0.2:43861
Thu Nov 01 13:39:45 </li></div><div><li>test timed out after 780 seconds</li></div><div><li>Appears to be stuck in thread Default-IPC-NioEventLoopGroup-5-1</li></div><div><li>test timed out after 780 seconds</li></div><div><li>Appears to be stuck in thread Socket Reader #1 for port 42107</li></div><div><li>test timed out after 780 seconds</li></div><div><li>test timed out after 780 seconds</li></div><div><li>Appears to be stuck in thread RS-EventLoopGroup-3-2</li></div><div><li>Interrupt while waiting on Operation: DISABLE, Table Name: default:testRecoveryAndDoubleExecution, procId: 267</li></div><div><li>test timed out after 780 seconds</li></div><div><li>Appears to be stuck in thread RS-EventLoopGroup-3-2</li></div><div><li>test timed out after 780 seconds</li></div><div><li>Appears to be stuck in thread Socket Reader #1 for port 42180</li></div><div><li>test timed out after 780 seconds</li></div><div><li>Appears to be stuck in thread Socket Reader #1 for port 39030</li></div><div><li>Interrupt while waiting on Operation: DISABLE, Table Name: default:testRecoveryAndDoubleExecutionPreserveSplits, procId: 293</li></div><div><li>test timed out after 780 seconds</li></div><div><li>Appears to be stuck in thread DataXceiver for client DFSClient_NONMAPREDUCE_-758896921_13 at /127.0.0.1:37716 [Receiving block BP-614639906-172.17.0.2-1541122628810:blk_1073741829_1005]</li></div><div><li>expected:&lt;2&gt; but was:&lt;4&gt;</li></div><div><li>Region retainment not done </li></div><div><li>test timed out after 780 seconds</li></div><div><li>Appears to be stuck in thread DataXceiver for client DFSClient_NONMAPREDUCE_-865646040_13 at /127.0.0.1:44480 [Receiving block BP-1158679575-172.17.0.2-1541089009193:blk_1073741829_1005]</li></div><div><li>java.lang.AssertionError
	at org.apache.hadoop.hbase.regionserver.throttle.TestFlushWithThroughputController.testFlushWithThroughputLimit(TestFlushWithThroughputController.java:154)
	at org.apache.hadoop.hbase.regionserver.throttle.TestFlushWithThroughputController.testFlushControlForStripedStore(TestFlushWithThroughputController.java:227)
</li></div><div><li>java.lang.AssertionError
	at org.apache.hadoop.hbase.regionserver.throttle.TestFlushWithThroughputController.testFlushWithThroughputLimit(TestFlushWithThroughputController.java:154)
	at org.apache.hadoop.hbase.regionserver.throttle.TestFlushWithThroughputController.testFlushControl(TestFlushWithThroughputController.java:160)
</li></div><div><li>test timed out after 780 seconds</li></div><div><li>Appears to be stuck in thread RS-EventLoopGroup-5-3</li></div><div><li>test timed out after 780 seconds</li></div><div><li>Appears to be stuck in thread Socket Reader #1 for port 37227</li></div><div><li>Interrupt while waiting on Operation: REMOVE_REPLICATION_PEER, peerId: 2</li></div><div><li>test timed out after 780 seconds</li></div><div><li>Appears to be stuck in thread Socket Reader #1 for port 44605</li></div><div><li>test timed out after 780 seconds</li></div><div><li>Appears to be stuck in thread RS-EventLoopGroup-5-3</li></div><div><li>test timed out after 780 seconds</li></div><div><li>Appears to be stuck in thread Socket Reader #1 for port 44466</li></div><div><li>Interrupt while waiting on Operation: ADD_REPLICATION_PEER, peerId: 2</li></div><div><li>test timed out after 780 seconds</li></div><div><li>Appears to be stuck in thread Socket Reader #1 for port 37013</li></div><div><li>Interrupt while waiting on Operation: TRANSIT_REPLICATION_PEER_SYNCHRONOUS_REPLICATION_STATE, peerId: 1</li></div><div><li>test timed out after 780 seconds</li></div><div><li>Appears to be stuck in thread Socket Reader #1 for port 39842</li></div><div><li>test timed out after 780 seconds</li></div><div><li>Appears to be stuck in thread RS_REFRESH_PEER-regionserver/5ec8def561b1:0-0.replicationSource,2.replicationSource.wal-reader.5ec8def561b1%2C34760%2C1541116151525,2</li></div><div><li>expected:&lt;0&gt; but was:&lt;3&gt;</li></div><div><li>java.lang.InterruptedException
	at org.apache.hadoop.hbase.util.TestRegionMover.testUnloadWithAck(TestRegionMover.java:173)
</li></div><div><li>test timed out after 780 seconds</li></div><div><li>Appears to be stuck in thread DataXceiver for client DFSClient_NONMAPREDUCE_-1258351144_14 at /127.0.0.1:56926 [Receiving block BP-675674844-172.17.0.2-1541111175251:blk_1073741829_1005]</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div><div><li>expected:&lt;2&gt; but was:&lt;0&gt;</li></div><div><li>Sequence id go backwards from 122 to 24</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>run queue not empty</li></div><div><li>expected executor to be running</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>found 5 corrupted procedure(s) on replay</li></div><div><li>expected executor to be running</li></div><div><li>expected executor to be running</li></div><div><li>expected executor to be running</li></div><div><li>expected executor to be running</li></div><div><li>expected executor to be running</li></div><div><li>expected executor to be running</li></div><div><li>expected executor to be running</li></div><div><li>Archived hfiles [] and table hfiles [e65882baf715426cbc236cbbadfac383] is missing snapshot file:2f1cbf0665424c539d0e69fddd87e1d3</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>java.lang.AssertionError
	at org.apache.hadoop.hbase.procedure2.TestProcedureSkipPersistence.test(TestProcedureSkipPersistence.java:161)
</li></div><div><li>i=1 expected:&lt;8389924&gt; but was:&lt;8389992&gt;</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.hbase.regionserver.TestCompaction.testStopStartCompaction</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.hbase.master.balancer.TestRSGroupBasedLoadBalancerWithStochasticLoadBalancerAsInternal.testBalanceCluster</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.hbase.mapreduce.TestImportExport.org.apache.hadoop.hbase.mapreduce.TestImportExport</div></li><li><div>org.apache.hadoop.hbase.mapreduce.TestImportExport.org.apache.hadoop.hbase.mapreduce.TestImportExport</div></li><li><div>org.apache.hadoop.hbase.mapreduce.TestImportExport.testWithFilter</div></li><li><div>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory</div></li><li><div>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory</div></li><li><div>org.apache.hadoop.hbase.rest.TestGetAndPutResource.org.apache.hadoop.hbase.rest.TestGetAndPutResource</div></li><li><div>org.apache.hadoop.hbase.rest.TestGetAndPutResource.org.apache.hadoop.hbase.rest.TestGetAndPutResource</div></li><li><div>org.apache.hadoop.hbase.rsgroup.TestRSGroupsAdmin1.org.apache.hadoop.hbase.rsgroup.TestRSGroupsAdmin1</div></li><li><div>org.apache.hadoop.hbase.rsgroup.TestRSGroupsAdmin1.org.apache.hadoop.hbase.rsgroup.TestRSGroupsAdmin1</div></li><li><div>org.apache.hadoop.hbase.rsgroup.TestRSGroupsBasics.org.apache.hadoop.hbase.rsgroup.TestRSGroupsBasics</div></li><li><div>org.apache.hadoop.hbase.rsgroup.TestRSGroupsBasics.org.apache.hadoop.hbase.rsgroup.TestRSGroupsBasics</div></li><li><div>org.apache.hadoop.hbase.TestClientOperationTimeout.testPutTimeout</div></li><li><div>org.apache.hadoop.hbase.TestClientOperationTimeout.testGetTimeout</div></li><li><div>org.apache.hadoop.hbase.TestClientOperationTimeout.testScanTimeout</div></li><li><div>org.apache.hadoop.hbase.TestInfoServers.testInfoServersStatusPages</div></li><li><div>org.apache.hadoop.hbase.client.TestAsyncDecommissionAdminApi.testAsyncDecommissionRegionServers[0]</div></li><li><div>org.apache.hadoop.hbase.client.TestAsyncDecommissionAdminApi.testAsyncDecommissionRegionServers[1]</div></li><li><div>org.apache.hadoop.hbase.client.TestReplicaWithCluster.testReplicaAndReplication</div></li><li><div>org.apache.hadoop.hbase.client.TestReplicaWithCluster.org.apache.hadoop.hbase.client.TestReplicaWithCluster</div></li><li><div>org.apache.hadoop.hbase.client.TestReplicaWithCluster.org.apache.hadoop.hbase.client.TestReplicaWithCluster</div></li><li><div>org.apache.hadoop.hbase.client.TestScannersFromClientSide.testScanRawDeleteFamilyVersion</div></li><li><div>org.apache.hadoop.hbase.client.TestScannersFromClientSide.org.apache.hadoop.hbase.client.TestScannersFromClientSide</div></li><li><div>org.apache.hadoop.hbase.client.TestScannersFromClientSide.org.apache.hadoop.hbase.client.TestScannersFromClientSide</div></li><li><div>org.apache.hadoop.hbase.client.TestSeparateClientZKCluster.testMetaMoveDuringClientZkClusterRestart</div></li><li><div>org.apache.hadoop.hbase.client.TestTableFavoredNodes.testMergeTable</div></li><li><div>org.apache.hadoop.hbase.client.replication.TestReplicationAdmin.org.apache.hadoop.hbase.client.replication.TestReplicationAdmin</div></li><li><div>org.apache.hadoop.hbase.client.replication.TestReplicationAdminWithClusters.org.apache.hadoop.hbase.client.replication.TestReplicationAdminWithClusters</div></li><li><div>org.apache.hadoop.hbase.client.replication.TestReplicationAdminWithClusters.org.apache.hadoop.hbase.client.replication.TestReplicationAdminWithClusters</div></li><li><div>org.apache.hadoop.hbase.coprocessor.TestMasterObserver.org.apache.hadoop.hbase.coprocessor.TestMasterObserver</div></li><li><div>org.apache.hadoop.hbase.coprocessor.TestMasterObserver.org.apache.hadoop.hbase.coprocessor.TestMasterObserver</div></li><li><div>org.apache.hadoop.hbase.coprocessor.TestMasterObserver.testTableOperations</div></li><li><div>org.apache.hadoop.hbase.io.hfile.TestScannerSelectionUsingTTL.testScannerSelection[5]</div></li><li><div>org.apache.hadoop.hbase.master.TestAssignmentManagerMetrics.testRITAssignmentManagerMetrics</div></li><li><div>org.apache.hadoop.hbase.master.TestMasterMetricsWrapper.testOfflineRegion</div></li><li><div>org.apache.hadoop.hbase.master.TestMasterRepairMode.testNewCluster</div></li><li><div>org.apache.hadoop.hbase.master.assignment.TestCloseRegionWhileRSCrash.testRetryBackoff</div></li><li><div>org.apache.hadoop.hbase.master.assignment.TestMergeTableRegionsProcedure.org.apache.hadoop.hbase.master.assignment.TestMergeTableRegionsProcedure</div></li><li><div>org.apache.hadoop.hbase.master.assignment.TestMergeTableRegionsProcedure.org.apache.hadoop.hbase.master.assignment.TestMergeTableRegionsProcedure</div></li><li><div>org.apache.hadoop.hbase.master.assignment.TestSplitTableRegionProcedure.org.apache.hadoop.hbase.master.assignment.TestSplitTableRegionProcedure</div></li><li><div>org.apache.hadoop.hbase.master.assignment.TestSplitTableRegionProcedure.org.apache.hadoop.hbase.master.assignment.TestSplitTableRegionProcedure</div></li><li><div>org.apache.hadoop.hbase.master.cleaner.TestHFileLinkCleaner.org.apache.hadoop.hbase.master.cleaner.TestHFileLinkCleaner</div></li><li><div>org.apache.hadoop.hbase.master.locking.TestLockManager.org.apache.hadoop.hbase.master.locking.TestLockManager</div></li><li><div>org.apache.hadoop.hbase.master.locking.TestLockManager.org.apache.hadoop.hbase.master.locking.TestLockManager</div></li><li><div>org.apache.hadoop.hbase.master.procedure.TestCloneSnapshotProcedure.testRecoveryAndDoubleExecution</div></li><li><div>org.apache.hadoop.hbase.master.procedure.TestCloneSnapshotProcedure.org.apache.hadoop.hbase.master.procedure.TestCloneSnapshotProcedure</div></li><li><div>org.apache.hadoop.hbase.master.procedure.TestCloneSnapshotProcedure.org.apache.hadoop.hbase.master.procedure.TestCloneSnapshotProcedure</div></li><li><div>org.apache.hadoop.hbase.master.procedure.TestRestoreSnapshotProcedure.org.apache.hadoop.hbase.master.procedure.TestRestoreSnapshotProcedure</div></li><li><div>org.apache.hadoop.hbase.master.procedure.TestRestoreSnapshotProcedure.org.apache.hadoop.hbase.master.procedure.TestRestoreSnapshotProcedure</div></li><li><div>org.apache.hadoop.hbase.master.procedure.TestTruncateTableProcedure.org.apache.hadoop.hbase.master.procedure.TestTruncateTableProcedure</div></li><li><div>org.apache.hadoop.hbase.master.procedure.TestTruncateTableProcedure.org.apache.hadoop.hbase.master.procedure.TestTruncateTableProcedure</div></li><li><div>org.apache.hadoop.hbase.master.procedure.TestTruncateTableProcedure.testRecoveryAndDoubleExecutionPreserveSplits</div></li><li><div>org.apache.hadoop.hbase.namespace.TestNamespaceAuditor.org.apache.hadoop.hbase.namespace.TestNamespaceAuditor</div></li><li><div>org.apache.hadoop.hbase.namespace.TestNamespaceAuditor.org.apache.hadoop.hbase.namespace.TestNamespaceAuditor</div></li><li><div>org.apache.hadoop.hbase.regionserver.TestLogRoller.testRemoveClosedWAL</div></li><li><div>org.apache.hadoop.hbase.regionserver.TestRegionReplicasAreDistributed.testRegionReplicasCreatedAreDistributed</div></li><li><div>org.apache.hadoop.hbase.regionserver.TestRegionReplicasWithModifyTable.org.apache.hadoop.hbase.regionserver.TestRegionReplicasWithModifyTable</div></li><li><div>org.apache.hadoop.hbase.regionserver.TestRegionReplicasWithModifyTable.org.apache.hadoop.hbase.regionserver.TestRegionReplicasWithModifyTable</div></li><li><div>org.apache.hadoop.hbase.regionserver.throttle.TestFlushWithThroughputController.testFlushControlForStripedStore</div></li><li><div>org.apache.hadoop.hbase.regionserver.throttle.TestFlushWithThroughputController.testFlushControl</div></li><li><div>org.apache.hadoop.hbase.replication.TestAddToSerialReplicationPeer.org.apache.hadoop.hbase.replication.TestAddToSerialReplicationPeer</div></li><li><div>org.apache.hadoop.hbase.replication.TestAddToSerialReplicationPeer.org.apache.hadoop.hbase.replication.TestAddToSerialReplicationPeer</div></li><li><div>org.apache.hadoop.hbase.replication.TestNamespaceReplication.org.apache.hadoop.hbase.replication.TestNamespaceReplication</div></li><li><div>org.apache.hadoop.hbase.replication.TestNamespaceReplication.org.apache.hadoop.hbase.replication.TestNamespaceReplication</div></li><li><div>org.apache.hadoop.hbase.replication.TestNamespaceReplication.testNamespaceReplication[0: serialPeer=true]</div></li><li><div>org.apache.hadoop.hbase.replication.TestReplicationEndpoint.org.apache.hadoop.hbase.replication.TestReplicationEndpoint</div></li><li><div>org.apache.hadoop.hbase.replication.TestReplicationEndpoint.org.apache.hadoop.hbase.replication.TestReplicationEndpoint</div></li><li><div>org.apache.hadoop.hbase.replication.TestSerialReplication.org.apache.hadoop.hbase.replication.TestSerialReplication</div></li><li><div>org.apache.hadoop.hbase.replication.TestSerialReplication.org.apache.hadoop.hbase.replication.TestSerialReplication</div></li><li><div>org.apache.hadoop.hbase.replication.multiwal.TestReplicationEndpointWithMultipleAsyncWAL.org.apache.hadoop.hbase.replication.multiwal.TestReplicationEndpointWithMultipleAsyncWAL</div></li><li><div>org.apache.hadoop.hbase.replication.multiwal.TestReplicationEndpointWithMultipleAsyncWAL.org.apache.hadoop.hbase.replication.multiwal.TestReplicationEndpointWithMultipleAsyncWAL</div></li><li><div>org.apache.hadoop.hbase.replication.multiwal.TestReplicationEndpointWithMultipleAsyncWAL.testWALEntryFilterUpdateValidation</div></li><li><div>org.apache.hadoop.hbase.replication.multiwal.TestReplicationEndpointWithMultipleWAL.org.apache.hadoop.hbase.replication.multiwal.TestReplicationEndpointWithMultipleWAL</div></li><li><div>org.apache.hadoop.hbase.replication.multiwal.TestReplicationEndpointWithMultipleWAL.org.apache.hadoop.hbase.replication.multiwal.TestReplicationEndpointWithMultipleWAL</div></li><li><div>org.apache.hadoop.hbase.replication.regionserver.TestDrainReplicationQueuesForStandBy.test</div></li><li><div>org.apache.hadoop.hbase.replication.regionserver.TestDrainReplicationQueuesForStandBy.org.apache.hadoop.hbase.replication.regionserver.TestDrainReplicationQueuesForStandBy</div></li><li><div>org.apache.hadoop.hbase.replication.regionserver.TestDrainReplicationQueuesForStandBy.org.apache.hadoop.hbase.replication.regionserver.TestDrainReplicationQueuesForStandBy</div></li><li><div>org.apache.hadoop.hbase.replication.regionserver.TestReplicator.org.apache.hadoop.hbase.replication.regionserver.TestReplicator</div></li><li><div>org.apache.hadoop.hbase.replication.regionserver.TestReplicator.org.apache.hadoop.hbase.replication.regionserver.TestReplicator</div></li><li><div>org.apache.hadoop.hbase.tool.TestCanaryTool.testBasicCanaryWorks</div></li><li><div>org.apache.hadoop.hbase.util.TestRegionMover.testUnloadWithAck</div></li><li><div>org.apache.hadoop.hbase.util.TestRegionMover.org.apache.hadoop.hbase.util.TestRegionMover</div></li><li><div>org.apache.hadoop.hbase.util.TestRegionMover.org.apache.hadoop.hbase.util.TestRegionMover</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.testConsecutiveExports</div></li><li><div>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.testEmptyExportFileSystemState</div></li><li><div>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.testExportFileSystemStateWithSkipTmp</div></li><li><div>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.testExportRetry</div></li><li><div>org.apache.hadoop.hbase.master.balancer.TestRSGroupBasedLoadBalancerWithStochasticLoadBalancerAsInternal.testBalanceCluster</div></li><li><div>org.apache.hadoop.hbase.replication.TestAddToSerialReplicationPeer.testAddToSerialPeer</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.hbase.master.procedure.TestDisableTableProcedure.testRecoveryAndDoubleExecution</div></li><li><div>org.apache.hadoop.hbase.master.procedure.TestDisableTableProcedure.testRecoveryAndDoubleExecution</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.hbase.master.assignment.TestMergeTableRegionsProcedure.testRollbackAndDoubleExecution</div></li><li><div>org.apache.hadoop.hbase.master.assignment.TestMergeTableRegionsProcedure.testRollbackAndDoubleExecution</div></li><li><div>org.apache.hadoop.hbase.master.assignment.TestMergeTableRegionsProcedure.testMergeTwoRegions</div></li><li><div>org.apache.hadoop.hbase.master.assignment.TestMergeTableRegionsProcedure.testMergeTwoRegions</div></li><li><div>org.apache.hadoop.hbase.master.assignment.TestMergeTableRegionsProcedure.testMergeRegionsConcurrently</div></li><li><div>org.apache.hadoop.hbase.master.assignment.TestMergeTableRegionsProcedure.testMergeRegionsConcurrently</div></li><li><div>org.apache.hadoop.hbase.master.assignment.TestMergeTableRegionsProcedure.testRecoveryAndDoubleExecution</div></li><li><div>org.apache.hadoop.hbase.master.assignment.TestMergeTableRegionsProcedure.testRecoveryAndDoubleExecution</div></li><li><div>org.apache.hadoop.hbase.master.cleaner.TestSnapshotFromMaster.testSnapshotHFileArchiving</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.hbase.procedure2.TestProcedureSkipPersistence.test</div></li><li><div>org.apache.hadoop.hbase.regionserver.TestCompactingToCellFlatMapMemStore.testForceCopyOfBigCellIntoImmutableSegment[0]</div></li></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="hive"><div style="font-weight:bold;" class="panel-heading">HIVE<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Alisha)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>0d4d03fd1daeb3b75182b73f7b40de7a3b7d48ea</div><div><b>Last Run: </b>23-10-2018 15:00 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC UBUNTU18</th><th>X86 UBUNTU18</th><th>PPC RHEL72</th><th>X86 RHEL72</th><th>PPC RHEL75</th><th>X86 RHEL75</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 7655</div><div>Failed Count : 3</div><div>Skipped Count : 246</div></td><td><div>Total Count : 7659</div><div>Failed Count : 1</div><div>Skipped Count : 246</div></td><td><div>Total Count : 7662</div><div>Failed Count : 3</div><div>Skipped Count : 246</div></td><td><div>Total Count : 7662</div><div>Failed Count : 3</div><div>Skipped Count : 246</div></td><td><div>Total Count : 7662</div><div>Failed Count : 4</div><div>Skipped Count : 246</div></td><td><div>Total Count : 7662</div><div>Failed Count : 3</div><div>Skipped Count : 246</div></td><td><div>Total Count : 7662</div><div>Failed Count : 1</div><div>Skipped Count : 246</div></td><td><div>Total Count : 7662</div><div>Failed Count : 1</div><div>Skipped Count : 246</div></td></tr><tr><td>Result</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hive.hcatalog.pig.TestAvroHCatStorer.testWriteDecimalX</li></div><div><li>org.apache.hive.hcatalog.pig.TestAvroHCatStorer.testWriteDecimal</li></div><div><li>org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskCommunicator.testFinishableStateUpdateFailure</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskCommunicator.testFinishableStateUpdateFailure</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hive.hcatalog.pig.TestAvroHCatStorer.testWriteDecimalX</li></div><div><li>org.apache.hive.hcatalog.pig.TestAvroHCatStorer.testWriteDecimal</li></div><div><li>org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskCommunicator.testFinishableStateUpdateFailure</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hive.hcatalog.pig.TestAvroHCatStorer.testWriteDecimalX</li></div><div><li>org.apache.hive.hcatalog.pig.TestAvroHCatStorer.testWriteDecimal</li></div><div><li>org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskCommunicator.testFinishableStateUpdateFailure</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hive.hcatalog.pig.TestAvroHCatStorer.testWriteDecimalX</li></div><div><li>org.apache.hive.hcatalog.pig.TestAvroHCatStorer.testWriteDecimal</li></div><div><li>org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskCommunicator.testFinishableStateUpdateFailure</li></div><div><li>org.apache.hadoop.hive.ql.exec.tez.TestDynamicPartitionPruner.testSingleSourceMultipleFiltersOrdering1</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hive.hcatalog.pig.TestAvroHCatStorer.testWriteDecimalX</li></div><div><li>org.apache.hive.hcatalog.pig.TestAvroHCatStorer.testWriteDecimal</li></div><div><li>org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskCommunicator.testFinishableStateUpdateFailure</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskCommunicator.testFinishableStateUpdateFailure</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskCommunicator.testFinishableStateUpdateFailure</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"><div><li>Failed to execute "create table junitTypeTest1(f1 decimal(2)) stored as AVRO TBLPROPERTIES ('transactional'='false')". Driver returned 1 Error: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. java.lang.RuntimeException: MetaException(message:java.lang.NoSuchMethodError org.codehaus.jackson.JsonNode.asInt()I)</li></div><div><li>Failed to execute "create table junitTypeTest1(f1 decimal) stored as AVRO TBLPROPERTIES ('transactional'='false')". Driver returned 1 Error: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. java.lang.RuntimeException: MetaException(message:java.lang.NoSuchMethodError org.codehaus.jackson.JsonNode.asInt()I)</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos$SignableVertexSpec$Builder.setUser(LlapDaemonProtocolProtos.java:4899)
	at org.apache.hadoop.hive.llap.tez.Converters.constructSignableVertexSpec(Converters.java:135)
	at org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.constructSubmitWorkRequest(LlapTaskCommunicator.java:806)
	at org.apac</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>java.lang.NullPointerException
	at org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos$SignableVertexSpec$Builder.setUser(LlapDaemonProtocolProtos.java:4899)
	at org.apache.hadoop.hive.llap.tez.Converters.constructSignableVertexSpec(Converters.java:135)
	at org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.constructSubmitWorkRequest(LlapTaskCommunicator.java:806)
	at org.apac</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Failed to execute "create table junitTypeTest1(f1 decimal(2)) stored as AVRO TBLPROPERTIES ('transactional'='false')". Driver returned 1 Error: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. java.lang.RuntimeException: MetaException(message:java.lang.NoSuchMethodError org.codehaus.jackson.JsonNode.asInt()I)</li></div><div><li>Failed to execute "create table junitTypeTest1(f1 decimal) stored as AVRO TBLPROPERTIES ('transactional'='false')". Driver returned 1 Error: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. java.lang.RuntimeException: MetaException(message:java.lang.NoSuchMethodError org.codehaus.jackson.JsonNode.asInt()I)</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos$SignableVertexSpec$Builder.setUser(LlapDaemonProtocolProtos.java:4899)
	at org.apache.hadoop.hive.llap.tez.Converters.constructSignableVertexSpec(Converters.java:135)
	at org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.constructSubmitWorkRequest(LlapTaskCommunicator.java:806)
	at org.apac</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Failed to execute "create table junitTypeTest1(f1 decimal(2)) stored as AVRO TBLPROPERTIES ('transactional'='false')". Driver returned 1 Error: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. java.lang.RuntimeException: MetaException(message:java.lang.NoSuchMethodError org.codehaus.jackson.JsonNode.asInt()I)</li></div><div><li>Failed to execute "create table junitTypeTest1(f1 decimal) stored as AVRO TBLPROPERTIES ('transactional'='false')". Driver returned 1 Error: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. java.lang.RuntimeException: MetaException(message:java.lang.NoSuchMethodError org.codehaus.jackson.JsonNode.asInt()I)</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos$SignableVertexSpec$Builder.setUser(LlapDaemonProtocolProtos.java:4899)
	at org.apache.hadoop.hive.llap.tez.Converters.constructSignableVertexSpec(Converters.java:135)
	at org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.constructSubmitWorkRequest(LlapTaskCommunicator.java:806)
	at org.apac</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Failed to execute "create table junitTypeTest1(f1 decimal(2)) stored as AVRO TBLPROPERTIES ('transactional'='false')". Driver returned 1 Error: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. java.lang.RuntimeException: MetaException(message:java.lang.NoSuchMethodError org.codehaus.jackson.JsonNode.asInt()I)</li></div><div><li>Failed to execute "create table junitTypeTest1(f1 decimal) stored as AVRO TBLPROPERTIES ('transactional'='false')". Driver returned 1 Error: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. java.lang.RuntimeException: MetaException(message:java.lang.NoSuchMethodError org.codehaus.jackson.JsonNode.asInt()I)</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos$SignableVertexSpec$Builder.setUser(LlapDaemonProtocolProtos.java:4899)
	at org.apache.hadoop.hive.llap.tez.Converters.constructSignableVertexSpec(Converters.java:135)
	at org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.constructSubmitWorkRequest(LlapTaskCommunicator.java:806)
	at org.apac</li></div><div><li>test timed out after 5000 milliseconds</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Failed to execute "create table junitTypeTest1(f1 decimal(2)) stored as AVRO TBLPROPERTIES ('transactional'='false')". Driver returned 1 Error: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. java.lang.RuntimeException: MetaException(message:java.lang.NoSuchMethodError org.codehaus.jackson.JsonNode.asInt()I)</li></div><div><li>Failed to execute "create table junitTypeTest1(f1 decimal) stored as AVRO TBLPROPERTIES ('transactional'='false')". Driver returned 1 Error: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. java.lang.RuntimeException: MetaException(message:java.lang.NoSuchMethodError org.codehaus.jackson.JsonNode.asInt()I)</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos$SignableVertexSpec$Builder.setUser(LlapDaemonProtocolProtos.java:4899)
	at org.apache.hadoop.hive.llap.tez.Converters.constructSignableVertexSpec(Converters.java:135)
	at org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.constructSubmitWorkRequest(LlapTaskCommunicator.java:806)
	at org.apac</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>java.lang.NullPointerException
	at org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos$SignableVertexSpec$Builder.setUser(LlapDaemonProtocolProtos.java:4899)
	at org.apache.hadoop.hive.llap.tez.Converters.constructSignableVertexSpec(Converters.java:135)
	at org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.constructSubmitWorkRequest(LlapTaskCommunicator.java:806)
	at org.apac</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>java.lang.NullPointerException
	at org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos$SignableVertexSpec$Builder.setUser(LlapDaemonProtocolProtos.java:4899)
	at org.apache.hadoop.hive.llap.tez.Converters.constructSignableVertexSpec(Converters.java:135)
	at org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.constructSubmitWorkRequest(LlapTaskCommunicator.java:806)
	at org.apac</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hive.hcatalog.pig.TestAvroHCatStorer.testWriteDecimalX</div></li><li><div>org.apache.hive.hcatalog.pig.TestAvroHCatStorer.testWriteDecimal</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.hive.ql.exec.tez.TestDynamicPartitionPruner.testSingleSourceMultipleFiltersOrdering1</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="kafka"><div style="font-weight:bold;" class="panel-heading">KAFKA<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Prajyot)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/trunk</div><div><b>Last Revision: </b>262bb006d58f96e3e24cab552506561310ebfb5d</div><div><b>Last Run: </b>25-10-2018 19:00 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC UBUNTU18</th><th>X86 UBUNTU18</th><th>PPC RHEL72</th><th>X86 RHEL72</th><th>PPC RHEL75</th><th>X86 RHEL75</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 10202</div><div>Failed Count : 1</div><div>Skipped Count : 5</div></td><td><div>Total Count : 10202</div><div>Failed Count : 1</div><div>Skipped Count : 5</div></td><td><div>Total Count : 10202</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 10202</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 10202</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 10202</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 10202</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 10202</div><div>Failed Count : 1</div><div>Skipped Count : 5</div></td></tr><tr><td>Result</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"><div><li>kafka.admin.DeleteTopicTest.testAddPartitionDuringDeleteTopic</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.kafka.clients.admin.KafkaAdminClientTest.testCreateTopicsRetryBackoff</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.kafka.connect.runtime.WorkerTest.testCleanupTasksOnStop</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"><div><li>kafka.admin.AdminOperationException: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /brokers/topics/test</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.junit.runners.model.TestTimedOutException: test timed out after 120000 milliseconds</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>java.lang.AssertionError: 
  Expectation failure on verify:
    WorkerSourceTask.run(): expected: 1, actual: 0</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>kafka.admin.DeleteTopicTest.testAddPartitionDuringDeleteTopic</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.kafka.clients.admin.KafkaAdminClientTest.testCreateTopicsRetryBackoff</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.kafka.connect.runtime.WorkerTest.testCleanupTasksOnStop</div></li></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="knox"><div style="font-weight:bold;" class="panel-heading">KNOX<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Yussuf)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>87d7291bd3bdfe498d6f692430b3d02fe47c21ab</div><div><b>Last Run: </b>23-10-2018 01:04 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC UBUNTU18</th><th>X86 UBUNTU18</th><th>PPC RHEL72</th><th>X86 RHEL72</th><th>PPC RHEL75</th><th>X86 RHEL75</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 1094</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1094</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1094</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1094</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1094</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1094</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1094</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1094</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td></tr><tr><td>Result</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="metron"><div style="font-weight:bold;" class="panel-heading">METRON<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Pravin)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>9b6260fd4b9ecf355864b77c8889d27539623381</div><div><b>Last Run: </b>04-11-2018 12:39 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC UBUNTU18</th><th>X86 UBUNTU18</th><th>PPC RHEL72</th><th>X86 RHEL72</th><th>PPC RHEL75</th><th>X86 RHEL75</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 2011</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 2030</div><div>Failed Count : 13</div><div>Skipped Count : 2</div></td><td><div>Total Count : 2011</div><div>Failed Count : 1</div><div>Skipped Count : 2</div></td><td><div>Total Count : 2030</div><div>Failed Count : 13</div><div>Skipped Count : 2</div></td><td><div>Total Count : 2011</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 2030</div><div>Failed Count : 13</div><div>Skipped Count : 2</div></td><td><div>Total Count : 2011</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 2030</div><div>Failed Count : 13</div><div>Skipped Count : 2</div></td></tr><tr><td>Result</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testBytesMatch[0]</li></div><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testStringMatch[0]</li></div><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testBytesMatch[1]</li></div><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testStringMatch[1]</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathStringReturned</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathFormatVariable</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathFormatConstant</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testSingleFileIfNoStreamClosed</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testHandleAttemptsRotateIfStreamClosed</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathMultipleFunctions</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFile</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteMultipleFiles</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFileWithNull</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.metron.stellar.dsl.functions.DateFunctionsTest.testDateFormatDefault</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testBytesMatch[0]</li></div><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testStringMatch[0]</li></div><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testBytesMatch[1]</li></div><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testStringMatch[1]</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathStringReturned</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathFormatVariable</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathFormatConstant</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testSingleFileIfNoStreamClosed</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testHandleAttemptsRotateIfStreamClosed</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathMultipleFunctions</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFile</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteMultipleFiles</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFileWithNull</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testBytesMatch[0]</li></div><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testStringMatch[0]</li></div><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testBytesMatch[1]</li></div><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testStringMatch[1]</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathStringReturned</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathFormatVariable</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathFormatConstant</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testSingleFileIfNoStreamClosed</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testHandleAttemptsRotateIfStreamClosed</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathMultipleFunctions</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFile</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteMultipleFiles</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFileWithNull</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testBytesMatch[0]</li></div><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testStringMatch[0]</li></div><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testBytesMatch[1]</li></div><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testStringMatch[1]</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathStringReturned</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathFormatVariable</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathFormatConstant</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testSingleFileIfNoStreamClosed</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testHandleAttemptsRotateIfStreamClosed</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathMultipleFunctions</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFile</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteMultipleFiles</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFileWithNull</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>Unable to parse BYTEARRAY_MATCHER(pattern, data): Unable to parse: BYTEARRAY_MATCHER(pattern, data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing,pattern=missing</li></div><div><li>Unable to parse BYTEARRAY_MATCHER(pattern, data): Unable to parse: BYTEARRAY_MATCHER(pattern, data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing,pattern=missing</li></div><div><li>Unable to parse BYTEARRAY_MATCHER('2f56abd814bc56420489ca38e7faf8cec3d4', data): Unable to parse: BYTEARRAY_MATCHER('2f56abd814bc56420489ca38e7faf8cec3d4', data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing</li></div><div><li>Unable to parse BYTEARRAY_MATCHER('`metron`', data): Unable to parse: BYTEARRAY_MATCHER('`metron`', data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing</li></div><div><li>Unable to parse: TO_UPPER(FORMAT(MAP_GET('key', {'key': 'AbC%s'}), test.key)) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables test.key=test.value</li></div><div><li>Unable to parse: FORMAT('%s/%s/%s', test.key, test.key.2, test.key.3) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables test.key.2=test.value.2,test.key=test.value,test.key.3=test.value.3</li></div><div><li>Unable to parse: FORMAT('/test/folder/') due to: org/apache/http/conn/HttpClientConnectionManager</li></div><div><li>java.lang.NullPointerException
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testSingleFileIfNoStreamClosed(HdfsWriterTest.java:447)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method</li></div><div><li>expected:&lt;1&gt; but was:&lt;0&gt;</li></div><div><li>Unable to parse: FORMAT('%s', test.key) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables test.key=test.value</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFile(HdfsWriterTest.java:313)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:6</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteMultipleFiles(HdfsWriterTest.java:356)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.jav</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFileWithNull(HdfsWriterTest.java:408)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImp</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.metron.stellar.dsl.functions.DateFunctionsTest.testDateFormatDefault(DateFunctionsTest.java:239)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodA</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Unable to parse BYTEARRAY_MATCHER(pattern, data): Unable to parse: BYTEARRAY_MATCHER(pattern, data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing,pattern=missing</li></div><div><li>Unable to parse BYTEARRAY_MATCHER(pattern, data): Unable to parse: BYTEARRAY_MATCHER(pattern, data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing,pattern=missing</li></div><div><li>Unable to parse BYTEARRAY_MATCHER('2f56abd814bc56420489ca38e7faf8cec3d4', data): Unable to parse: BYTEARRAY_MATCHER('2f56abd814bc56420489ca38e7faf8cec3d4', data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing</li></div><div><li>Unable to parse BYTEARRAY_MATCHER('`metron`', data): Unable to parse: BYTEARRAY_MATCHER('`metron`', data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing</li></div><div><li>Unable to parse: TO_UPPER(FORMAT(MAP_GET('key', {'key': 'AbC%s'}), test.key)) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables test.key=test.value</li></div><div><li>Unable to parse: FORMAT('%s/%s/%s', test.key, test.key.2, test.key.3) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables test.key.2=test.value.2,test.key=test.value,test.key.3=test.value.3</li></div><div><li>Unable to parse: FORMAT('/test/folder/') due to: org/apache/http/conn/HttpClientConnectionManager</li></div><div><li>java.lang.NullPointerException
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testSingleFileIfNoStreamClosed(HdfsWriterTest.java:447)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method</li></div><div><li>expected:&lt;1&gt; but was:&lt;0&gt;</li></div><div><li>Unable to parse: FORMAT('%s', test.key) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables test.key=test.value</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFile(HdfsWriterTest.java:313)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:6</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteMultipleFiles(HdfsWriterTest.java:356)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.jav</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFileWithNull(HdfsWriterTest.java:408)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImp</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>Unable to parse BYTEARRAY_MATCHER(pattern, data): Unable to parse: BYTEARRAY_MATCHER(pattern, data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing,pattern=missing</li></div><div><li>Unable to parse BYTEARRAY_MATCHER(pattern, data): Unable to parse: BYTEARRAY_MATCHER(pattern, data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing,pattern=missing</li></div><div><li>Unable to parse BYTEARRAY_MATCHER('2f56abd814bc56420489ca38e7faf8cec3d4', data): Unable to parse: BYTEARRAY_MATCHER('2f56abd814bc56420489ca38e7faf8cec3d4', data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing</li></div><div><li>Unable to parse BYTEARRAY_MATCHER('`metron`', data): Unable to parse: BYTEARRAY_MATCHER('`metron`', data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing</li></div><div><li>Unable to parse: TO_UPPER(FORMAT(MAP_GET('key', {'key': 'AbC%s'}), test.key)) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables test.key=test.value</li></div><div><li>Unable to parse: FORMAT('%s/%s/%s', test.key, test.key.2, test.key.3) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables test.key.2=test.value.2,test.key=test.value,test.key.3=test.value.3</li></div><div><li>Unable to parse: FORMAT('/test/folder/') due to: org/apache/http/conn/HttpClientConnectionManager</li></div><div><li>java.lang.NullPointerException
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testSingleFileIfNoStreamClosed(HdfsWriterTest.java:447)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method</li></div><div><li>expected:&lt;1&gt; but was:&lt;0&gt;</li></div><div><li>Unable to parse: FORMAT('%s', test.key) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables test.key=test.value</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFile(HdfsWriterTest.java:313)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:6</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteMultipleFiles(HdfsWriterTest.java:356)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.jav</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFileWithNull(HdfsWriterTest.java:408)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImp</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>Unable to parse BYTEARRAY_MATCHER(pattern, data): Unable to parse: BYTEARRAY_MATCHER(pattern, data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing,pattern=missing</li></div><div><li>Unable to parse BYTEARRAY_MATCHER(pattern, data): Unable to parse: BYTEARRAY_MATCHER(pattern, data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing,pattern=missing</li></div><div><li>Unable to parse BYTEARRAY_MATCHER('2f56abd814bc56420489ca38e7faf8cec3d4', data): Unable to parse: BYTEARRAY_MATCHER('2f56abd814bc56420489ca38e7faf8cec3d4', data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing</li></div><div><li>Unable to parse BYTEARRAY_MATCHER('`metron`', data): Unable to parse: BYTEARRAY_MATCHER('`metron`', data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing</li></div><div><li>Unable to parse: TO_UPPER(FORMAT(MAP_GET('key', {'key': 'AbC%s'}), test.key)) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables test.key=test.value</li></div><div><li>Unable to parse: FORMAT('%s/%s/%s', test.key, test.key.2, test.key.3) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables test.key.2=test.value.2,test.key=test.value,test.key.3=test.value.3</li></div><div><li>Unable to parse: FORMAT('/test/folder/') due to: org/apache/http/conn/HttpClientConnectionManager</li></div><div><li>java.lang.NullPointerException
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testSingleFileIfNoStreamClosed(HdfsWriterTest.java:447)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method</li></div><div><li>expected:&lt;1&gt; but was:&lt;0&gt;</li></div><div><li>Unable to parse: FORMAT('%s', test.key) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables test.key=test.value</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFile(HdfsWriterTest.java:313)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:6</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteMultipleFiles(HdfsWriterTest.java:356)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.jav</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFileWithNull(HdfsWriterTest.java:408)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImp</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testBytesMatch[0]</div></li><li><div>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testStringMatch[0]</div></li><li><div>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testBytesMatch[1]</div></li><li><div>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testStringMatch[1]</div></li><li><div>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathStringReturned</div></li><li><div>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathFormatVariable</div></li><li><div>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathFormatConstant</div></li><li><div>org.apache.metron.writer.hdfs.HdfsWriterTest.testSingleFileIfNoStreamClosed</div></li><li><div>org.apache.metron.writer.hdfs.HdfsWriterTest.testHandleAttemptsRotateIfStreamClosed</div></li><li><div>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathMultipleFunctions</div></li><li><div>org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFile</div></li><li><div>org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteMultipleFiles</div></li><li><div>org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFileWithNull</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.metron.stellar.dsl.functions.DateFunctionsTest.testDateFormatDefault</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testBytesMatch[0]</div></li><li><div>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testStringMatch[0]</div></li><li><div>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testBytesMatch[1]</div></li><li><div>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testStringMatch[1]</div></li><li><div>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathStringReturned</div></li><li><div>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathFormatVariable</div></li><li><div>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathFormatConstant</div></li><li><div>org.apache.metron.writer.hdfs.HdfsWriterTest.testSingleFileIfNoStreamClosed</div></li><li><div>org.apache.metron.writer.hdfs.HdfsWriterTest.testHandleAttemptsRotateIfStreamClosed</div></li><li><div>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathMultipleFunctions</div></li><li><div>org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFile</div></li><li><div>org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteMultipleFiles</div></li><li><div>org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFileWithNull</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testBytesMatch[0]</div></li><li><div>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testStringMatch[0]</div></li><li><div>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testBytesMatch[1]</div></li><li><div>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testStringMatch[1]</div></li><li><div>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathStringReturned</div></li><li><div>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathFormatVariable</div></li><li><div>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathFormatConstant</div></li><li><div>org.apache.metron.writer.hdfs.HdfsWriterTest.testSingleFileIfNoStreamClosed</div></li><li><div>org.apache.metron.writer.hdfs.HdfsWriterTest.testHandleAttemptsRotateIfStreamClosed</div></li><li><div>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathMultipleFunctions</div></li><li><div>org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFile</div></li><li><div>org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteMultipleFiles</div></li><li><div>org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFileWithNull</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testBytesMatch[0]</div></li><li><div>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testStringMatch[0]</div></li><li><div>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testBytesMatch[1]</div></li><li><div>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testStringMatch[1]</div></li><li><div>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathStringReturned</div></li><li><div>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathFormatVariable</div></li><li><div>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathFormatConstant</div></li><li><div>org.apache.metron.writer.hdfs.HdfsWriterTest.testSingleFileIfNoStreamClosed</div></li><li><div>org.apache.metron.writer.hdfs.HdfsWriterTest.testHandleAttemptsRotateIfStreamClosed</div></li><li><div>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathMultipleFunctions</div></li><li><div>org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFile</div></li><li><div>org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteMultipleFiles</div></li><li><div>org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFileWithNull</div></li></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="oozie"><div style="font-weight:bold;" class="panel-heading">OOZIE<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Alisha)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>65936460e263f9076bb552190be85396c2cc6d33</div><div><b>Last Run: </b>01-11-2018 21:58 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC UBUNTU18</th><th>X86 UBUNTU18</th><th>PPC RHEL72</th><th>X86 RHEL72</th><th>PPC RHEL75</th><th>X86 RHEL75</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 3062</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 3063</div><div>Failed Count : 2</div><div>Skipped Count : 2</div></td><td><div>Total Count : 3062</div><div>Failed Count : 4</div><div>Skipped Count : 2</div></td><td><div>Total Count : 3063</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 3062</div><div>Failed Count : 1</div><div>Skipped Count : 2</div></td><td><div>Total Count : 3063</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 3062</div><div>Failed Count : 2</div><div>Skipped Count : 2</div></td><td><div>Total Count : 3063</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td></tr><tr><td>Result</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.oozie.command.coord.TestCoordActionInputCheckXCommand.testNone</li></div><div><li>org.apache.oozie.service.TestCallableQueueService.testQueueSizeWithDelayedElements</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.oozie.command.coord.TestCoordActionInputCheckXCommandNonUTC.testCoordActionInputCheckXCommandUniqueness</li></div><div><li>org.apache.oozie.command.coord.TestCoordActionsKillXCommand.testActionKillCommandDate</li></div><div><li>org.apache.oozie.command.wf.TestWorkflowActionRetryInfoXCommand.testRetryConsoleUrlForked</li></div><div><li>org.apache.oozie.util.TestMetricsInstrumentation.testJMXInstrumentation</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.oozie.command.coord.TestCoordActionInputCheckXCommandNonUTC.testCoordActionInputCheckXCommandUniqueness</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.oozie.command.coord.TestCoordActionsKillXCommand.testActionKillCommandDate</li></div><div><li>org.apache.oozie.tools.TestDBLoadDump.testImportTablesOverflowBatchSize</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;SKIPPED&gt; but was:&lt;WAITING&gt;</li></div><div><li>Queue size after execution expected:&lt;0&gt; but was:&lt;1&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>junit.framework.AssertionFailedError
	at junit.framework.Assert.fail(Assert.java:55)
	at junit.framework.Assert.assertTrue(Assert.java:22)
	at junit.framework.Assert.assertTrue(Assert.java:31)
	at junit.framework.TestCase.assertTrue(TestCase.java:201)
	at org.apache.oozie.command.coord.TestCoordActionInputCheckXCommand.testCoordActionInputCheckXCommandUniqueness(TestCoordActionInputCheckXCommand.j</li></div><div><li>expected:&lt;RUNNING&gt; but was:&lt;KILLED&gt;</li></div><div><li>expected:&lt;2&gt; but was:&lt;1&gt;</li></div><div><li>Could not find own virtual machine</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>junit.framework.AssertionFailedError
	at junit.framework.Assert.fail(Assert.java:55)
	at junit.framework.Assert.assertTrue(Assert.java:22)
	at junit.framework.Assert.assertTrue(Assert.java:31)
	at junit.framework.TestCase.assertTrue(TestCase.java:201)
	at org.apache.oozie.command.coord.TestCoordActionInputCheckXCommand.testCoordActionInputCheckXCommandUniqueness(TestCoordActionInputCheckXCommand.j</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;RUNNING&gt; but was:&lt;KILLED&gt;</li></div><div><li>org.apache.oozie.tools.TestDBLoadDump$ExitException
	at org.apache.oozie.tools.TestDBLoadDump$NoExitSecurityManager.checkExit(TestDBLoadDump.java:284)
	at java.lang.Runtime.exit(Runtime.java:107)
	at java.lang.System.exit(System.java:971)
	at org.apache.oozie.tools.OozieDBImportCLI.main(OozieDBImportCLI.java:158)
	at org.apache.oozie.tools.TestDBLoadDump.importToDB(TestDBLoadDump.java:218)
	at org</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.oozie.command.coord.TestCoordActionInputCheckXCommand.testNone</div></li><li><div>org.apache.oozie.service.TestCallableQueueService.testQueueSizeWithDelayedElements</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.oozie.command.coord.TestCoordActionInputCheckXCommandNonUTC.testCoordActionInputCheckXCommandUniqueness</div></li><li><div>org.apache.oozie.command.coord.TestCoordActionsKillXCommand.testActionKillCommandDate</div></li><li><div>org.apache.oozie.command.wf.TestWorkflowActionRetryInfoXCommand.testRetryConsoleUrlForked</div></li><li><div>org.apache.oozie.util.TestMetricsInstrumentation.testJMXInstrumentation</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.oozie.command.coord.TestCoordActionInputCheckXCommandNonUTC.testCoordActionInputCheckXCommandUniqueness</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.oozie.command.coord.TestCoordActionsKillXCommand.testActionKillCommandDate</div></li><li><div>org.apache.oozie.tools.TestDBLoadDump.testImportTablesOverflowBatchSize</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="phoenix"><div style="font-weight:bold;" class="panel-heading">PHOENIX<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Prajyot)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>59a7dd138c661c743d58421776cb5e731e1cda99</div><div><b>Last Run: </b>01-11-2018 23:45 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC UBUNTU18</th><th>X86 UBUNTU18</th><th>PPC RHEL72</th><th>X86 RHEL72</th><th>PPC RHEL75</th><th>X86 RHEL75</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 1726</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 1726</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 1726</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 1726</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 1726</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 1726</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 1726</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 1726</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td></tr><tr><td>Result</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="pig"><div style="font-weight:bold;" class="panel-heading">PIG<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Yussuf)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/trunk</div><div><b>Last Revision: </b>c25a84233900f499757bd0cbde79c7b77b5b1caf</div><div><b>Last Run: </b>01-11-2018 02:58 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC UBUNTU18</th><th>X86 UBUNTU18</th><th>PPC RHEL72</th><th>X86 RHEL72</th><th>PPC RHEL75</th><th>X86 RHEL75</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 895</div><div>Failed Count : 9</div><div>Skipped Count : 0</div></td><td><div>Total Count : 895</div><div>Failed Count : 9</div><div>Skipped Count : 0</div></td><td><div>Total Count : 895</div><div>Failed Count : 9</div><div>Skipped Count : 0</div></td><td><div>Total Count : 895</div><div>Failed Count : 9</div><div>Skipped Count : 0</div></td><td><div>Total Count : 895</div><div>Failed Count : 9</div><div>Skipped Count : 0</div></td><td><div>Total Count : 895</div><div>Failed Count : 9</div><div>Skipped Count : 0</div></td><td><div>Total Count : 895</div><div>Failed Count : 9</div><div>Skipped Count : 0</div></td><td><div>Total Count : 895</div><div>Failed Count : 9</div><div>Skipped Count : 0</div></td></tr><tr><td>Result</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileDeclareCombo</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testSameParamInMultipleFiles</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileComboDuplicate</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileCombo</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testMultipleParamsinSingleLine</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileDeclareDefaultComboDuplicates</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testFileParamsFromMultipleFiles</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testMultipleDeclareScope</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileDeclareComboDuplicates</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileDeclareCombo</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testSameParamInMultipleFiles</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileComboDuplicate</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileCombo</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testMultipleParamsinSingleLine</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileDeclareDefaultComboDuplicates</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testFileParamsFromMultipleFiles</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testMultipleDeclareScope</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileDeclareComboDuplicates</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileDeclareCombo</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testSameParamInMultipleFiles</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileComboDuplicate</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileCombo</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testMultipleParamsinSingleLine</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileDeclareDefaultComboDuplicates</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testFileParamsFromMultipleFiles</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testMultipleDeclareScope</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileDeclareComboDuplicates</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileDeclareCombo</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testSameParamInMultipleFiles</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileComboDuplicate</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileCombo</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testMultipleParamsinSingleLine</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileDeclareDefaultComboDuplicates</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testFileParamsFromMultipleFiles</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testMultipleDeclareScope</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileDeclareComboDuplicates</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileDeclareCombo</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testSameParamInMultipleFiles</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileComboDuplicate</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileCombo</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testMultipleParamsinSingleLine</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileDeclareDefaultComboDuplicates</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testFileParamsFromMultipleFiles</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testMultipleDeclareScope</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileDeclareComboDuplicates</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileDeclareCombo</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testSameParamInMultipleFiles</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileComboDuplicate</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileCombo</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testMultipleParamsinSingleLine</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileDeclareDefaultComboDuplicates</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testFileParamsFromMultipleFiles</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testMultipleDeclareScope</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileDeclareComboDuplicates</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileDeclareCombo</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testSameParamInMultipleFiles</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileComboDuplicate</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileCombo</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testMultipleParamsinSingleLine</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileDeclareDefaultComboDuplicates</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testFileParamsFromMultipleFiles</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testMultipleDeclareScope</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileDeclareComboDuplicates</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileDeclareCombo</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testSameParamInMultipleFiles</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileComboDuplicate</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileCombo</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testMultipleParamsinSingleLine</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileDeclareDefaultComboDuplicates</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testFileParamsFromMultipleFiles</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testMultipleDeclareScope</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileDeclareComboDuplicates</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 27 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 27 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 27 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 27 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 27 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 27 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 27 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 27 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="ranger"><div style="font-weight:bold;" class="panel-heading">RANGER<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Yussuf)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>71ecc5a19a2c58a0875113e53061d30822eb1c94</div><div><b>Last Run: </b>01-11-2018 01:58 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC UBUNTU18</th><th>X86 UBUNTU18</th><th>PPC RHEL72</th><th>X86 RHEL72</th><th>PPC RHEL75</th><th>X86 RHEL75</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 1314</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1314</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1314</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1314</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1314</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1314</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1314</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1314</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td></tr><tr><td>Result</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="slider"><div style="font-weight:bold;" class="panel-heading">SLIDER<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Yussuf)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/develop</div><div><b>Last Revision: </b>1d4f519d763210f46e327338be72efa99e65cb5d</div><div><b>Last Run: </b>01-11-2018 20:54 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC UBUNTU18</th><th>X86 UBUNTU18</th><th>PPC RHEL72</th><th>X86 RHEL72</th><th>PPC RHEL75</th><th>X86 RHEL75</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 607</div><div>Failed Count : 0</div><div>Skipped Count : 13</div></td><td><div>Total Count : 607</div><div>Failed Count : 0</div><div>Skipped Count : 13</div></td><td><div>Total Count : 607</div><div>Failed Count : 0</div><div>Skipped Count : 13</div></td><td><div>Total Count : 607</div><div>Failed Count : 0</div><div>Skipped Count : 13</div></td><td><div>Total Count : 607</div><div>Failed Count : 0</div><div>Skipped Count : 13</div></td><td><div>Total Count : 607</div><div>Failed Count : 0</div><div>Skipped Count : 13</div></td><td><div>Total Count : 607</div><div>Failed Count : 0</div><div>Skipped Count : 13</div></td><td><div>Total Count : 607</div><div>Failed Count : 0</div><div>Skipped Count : 13</div></td></tr><tr><td>Result</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="spark"><div style="font-weight:bold;" class="panel-heading">SPARK<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Prajyot)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>f83fedc9f20869ab4c62bb07bac50113d921207f</div><div><b>Last Run: </b>24-10-2018 18:59 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC UBUNTU18</th><th>X86 UBUNTU18</th><th>PPC RHEL72</th><th>X86 RHEL72</th><th>PPC RHEL75</th><th>X86 RHEL75</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 15141</div><div>Failed Count : 0</div><div>Skipped Count : 651</div></td><td><div>Total Count : 16914</div><div>Failed Count : 0</div><div>Skipped Count : 658</div></td><td><div>Total Count : 12219</div><div>Failed Count : 48</div><div>Skipped Count : 636</div></td><td><div>Total Count : 2795</div><div>Failed Count : 831</div><div>Skipped Count : 20</div></td><td><div>Total Count : 15141</div><div>Failed Count : 0</div><div>Skipped Count : 651</div></td><td><div>Total Count : 2795</div><div>Failed Count : 827</div><div>Skipped Count : 20</div></td><td><div>Total Count : 14806</div><div>Failed Count : 46</div><div>Skipped Count : 650</div></td><td><div>Total Count : 2790</div><div>Failed Count : 823</div><div>Skipped Count : 20</div></td></tr><tr><td>Result</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexDescendingWithStart</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexWithStart</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndexDescendingWithStart</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexDescending</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexWithStart</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexWithLast</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexWithSkip</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexWithMax</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexDescending</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndexDescendingWithLast</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexDescending</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexDescendingWithLast</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndex</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndexWithLast</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexWithStart</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexDescendingWithStart</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexWithLast</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexWithSkip</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndexDescending</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.testRefWithIntNaturalKey</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexDescending</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexDescendingWithStart</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexWithMax</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndex</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexWithLast</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexWithSkip</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexWithMax</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexDescendingWithLast</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexDescendingWithLast</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexDescendingWithStart</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndex</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexWithLast</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexWithSkip</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexWithStart</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndex</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexDescendingWithLast</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndexWithStart</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndex</li></div><div><li>org.apache.spark.util.kvstore.LevelDBSuite.testMultipleTypesWriteReadDelete</li></div><div><li>org.apache.spark.util.kvstore.LevelDBSuite.testObjectWriteReadDelete</li></div><div><li>org.apache.spark.util.kvstore.LevelDBSuite.testSkip</li></div><div><li>org.apache.spark.util.kvstore.LevelDBSuite.testMultipleObjectWriteReadDelete</li></div><div><li>org.apache.spark.util.kvstore.LevelDBSuite.testReopenAndVersionCheckDb</li></div><div><li>org.apache.spark.util.kvstore.LevelDBSuite.testMetadata</li></div><div><li>org.apache.spark.util.kvstore.LevelDBSuite.testUpdate</li></div><div><li>org.apache.spark.util.kvstore.LevelDBSuite.testNegativeIndexValues</li></div><div><li>org.apache.spark.sql.kafka010.KafkaSourceStressSuite.stress test with multiple topics and partitions</li></div><div><li>org.apache.spark.sql.hive.HiveSparkSubmitSuite.SPARK-8020: set sql conf in spark conf</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.spark.network.ChunkFetchIntegrationSuite.fetchFileChunk</li></div><div><li>org.apache.spark.JavaJdbcRDDSuite.testJavaJdbcRDD</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.leftOuterJoin</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.leftOuterJoin</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.foldReduce</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.foldReduce</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.mapsFromPairsToPairs</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.mapsFromPairsToPairs</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.flatMap</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.flatMap</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.foreach</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.foreach</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.map</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.map</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.zip</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.zip</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.keyBy</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.keyBy</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.groupBy</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.groupBy</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.mapPartitions</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.mapPartitions</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.foldByKey</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.foldByKey</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.mapOnPairRDD</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.mapOnPairRDD</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.sequenceFile</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.sequenceFile</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.collectPartitions</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.collectPartitions</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.reduceByKey</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.reduceByKey</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.foreachWithAnonymousClass</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.foreachWithAnonymousClass</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.collectAsMapWithIntArrayValues</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.collectAsMapWithIntArrayValues</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.zipPartitions</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.zipPartitions</li></div><div><li>test.org.apache.spark.JavaAPISuite.groupByOnPairRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.groupByOnPairRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.binaryFilesCaching</li></div><div><li>test.org.apache.spark.JavaAPISuite.binaryFilesCaching</li></div><div><li>test.org.apache.spark.JavaAPISuite.sparkContextUnion</li></div><div><li>test.org.apache.spark.JavaAPISuite.sparkContextUnion</li></div><div><li>test.org.apache.spark.JavaAPISuite.checkpointAndComputation</li></div><div><li>test.org.apache.spark.JavaAPISuite.checkpointAndComputation</li></div><div><li>test.org.apache.spark.JavaAPISuite.leftOuterJoin</li></div><div><li>test.org.apache.spark.JavaAPISuite.keyByOnPairRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.wholeTextFiles</li></div><div><li>test.org.apache.spark.JavaAPISuite.wholeTextFiles</li></div><div><li>test.org.apache.spark.JavaAPISuite.binaryFiles</li></div><div><li>test.org.apache.spark.JavaAPISuite.binaryFiles</li></div><div><li>test.org.apache.spark.JavaAPISuite.foldReduce</li></div><div><li>test.org.apache.spark.JavaAPISuite.foldReduce</li></div><div><li>test.org.apache.spark.JavaAPISuite.writeWithNewAPIHadoopFile</li></div><div><li>test.org.apache.spark.JavaAPISuite.writeWithNewAPIHadoopFile</li></div><div><li>test.org.apache.spark.JavaAPISuite.hadoopFile</li></div><div><li>test.org.apache.spark.JavaAPISuite.hadoopFile</li></div><div><li>test.org.apache.spark.JavaAPISuite.lookup</li></div><div><li>test.org.apache.spark.JavaAPISuite.lookup</li></div><div><li>test.org.apache.spark.JavaAPISuite.countAsync</li></div><div><li>test.org.apache.spark.JavaAPISuite.countAsync</li></div><div><li>test.org.apache.spark.JavaAPISuite.textFiles</li></div><div><li>test.org.apache.spark.JavaAPISuite.binaryRecords</li></div><div><li>test.org.apache.spark.JavaAPISuite.toLocalIterator</li></div><div><li>test.org.apache.spark.JavaAPISuite.toLocalIterator</li></div><div><li>test.org.apache.spark.JavaAPISuite.repartitionAndSortWithinPartitions</li></div><div><li>test.org.apache.spark.JavaAPISuite.repartitionAndSortWithinPartitions</li></div><div><li>test.org.apache.spark.JavaAPISuite.reduce</li></div><div><li>test.org.apache.spark.JavaAPISuite.reduce</li></div><div><li>test.org.apache.spark.JavaAPISuite.sample</li></div><div><li>test.org.apache.spark.JavaAPISuite.sample</li></div><div><li>test.org.apache.spark.JavaAPISuite.sortBy</li></div><div><li>test.org.apache.spark.JavaAPISuite.sortBy</li></div><div><li>test.org.apache.spark.JavaAPISuite.mapsFromPairsToPairs</li></div><div><li>test.org.apache.spark.JavaAPISuite.mapsFromPairsToPairs</li></div><div><li>test.org.apache.spark.JavaAPISuite.flatMap</li></div><div><li>test.org.apache.spark.JavaAPISuite.flatMap</li></div><div><li>test.org.apache.spark.JavaAPISuite.cogroup3</li></div><div><li>test.org.apache.spark.JavaAPISuite.cogroup4</li></div><div><li>test.org.apache.spark.JavaAPISuite.cogroup4</li></div><div><li>test.org.apache.spark.JavaAPISuite.randomSplit</li></div><div><li>test.org.apache.spark.JavaAPISuite.randomSplit</li></div><div><li>test.org.apache.spark.JavaAPISuite.persist</li></div><div><li>test.org.apache.spark.JavaAPISuite.persist</li></div><div><li>test.org.apache.spark.JavaAPISuite.foreach</li></div><div><li>test.org.apache.spark.JavaAPISuite.foreach</li></div><div><li>test.org.apache.spark.JavaAPISuite.hadoopFileCompressed</li></div><div><li>test.org.apache.spark.JavaAPISuite.hadoopFileCompressed</li></div><div><li>test.org.apache.spark.JavaAPISuite.textFilesCompressed</li></div><div><li>test.org.apache.spark.JavaAPISuite.textFilesCompressed</li></div><div><li>test.org.apache.spark.JavaAPISuite.testAsyncActionCancellation</li></div><div><li>test.org.apache.spark.JavaAPISuite.testAsyncActionCancellation</li></div><div><li>test.org.apache.spark.JavaAPISuite.checkpointAndRestore</li></div><div><li>test.org.apache.spark.JavaAPISuite.checkpointAndRestore</li></div><div><li>test.org.apache.spark.JavaAPISuite.sortByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.sortByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.aggregateByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.aggregateByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.map</li></div><div><li>test.org.apache.spark.JavaAPISuite.map</li></div><div><li>test.org.apache.spark.JavaAPISuite.max</li></div><div><li>test.org.apache.spark.JavaAPISuite.max</li></div><div><li>test.org.apache.spark.JavaAPISuite.min</li></div><div><li>test.org.apache.spark.JavaAPISuite.min</li></div><div><li>test.org.apache.spark.JavaAPISuite.top</li></div><div><li>test.org.apache.spark.JavaAPISuite.top</li></div><div><li>test.org.apache.spark.JavaAPISuite.zip</li></div><div><li>test.org.apache.spark.JavaAPISuite.zip</li></div><div><li>test.org.apache.spark.JavaAPISuite.fold</li></div><div><li>test.org.apache.spark.JavaAPISuite.fold</li></div><div><li>test.org.apache.spark.JavaAPISuite.glom</li></div><div><li>test.org.apache.spark.JavaAPISuite.glom</li></div><div><li>test.org.apache.spark.JavaAPISuite.take</li></div><div><li>test.org.apache.spark.JavaAPISuite.take</li></div><div><li>test.org.apache.spark.JavaAPISuite.javaDoubleRDDHistoGram</li></div><div><li>test.org.apache.spark.JavaAPISuite.javaDoubleRDDHistoGram</li></div><div><li>test.org.apache.spark.JavaAPISuite.collectUnderlyingScalaRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.collectUnderlyingScalaRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.keyBy</li></div><div><li>test.org.apache.spark.JavaAPISuite.keyBy</li></div><div><li>test.org.apache.spark.JavaAPISuite.mapPartitionsWithIndex</li></div><div><li>test.org.apache.spark.JavaAPISuite.mapPartitionsWithIndex</li></div><div><li>test.org.apache.spark.JavaAPISuite.sampleByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.sampleByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.intersection</li></div><div><li>test.org.apache.spark.JavaAPISuite.intersection</li></div><div><li>test.org.apache.spark.JavaAPISuite.aggregate</li></div><div><li>test.org.apache.spark.JavaAPISuite.aggregate</li></div><div><li>test.org.apache.spark.JavaAPISuite.cartesian</li></div><div><li>test.org.apache.spark.JavaAPISuite.cartesian</li></div><div><li>test.org.apache.spark.JavaAPISuite.countApproxDistinctByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.countApproxDistinctByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.readWithNewAPIHadoopFile</li></div><div><li>test.org.apache.spark.JavaAPISuite.readWithNewAPIHadoopFile</li></div><div><li>test.org.apache.spark.JavaAPISuite.testRegisterKryoClasses</li></div><div><li>test.org.apache.spark.JavaAPISuite.testRegisterKryoClasses</li></div><div><li>test.org.apache.spark.JavaAPISuite.groupBy</li></div><div><li>test.org.apache.spark.JavaAPISuite.groupBy</li></div><div><li>test.org.apache.spark.JavaAPISuite.sampleByKeyExact</li></div><div><li>test.org.apache.spark.JavaAPISuite.sampleByKeyExact</li></div><div><li>test.org.apache.spark.JavaAPISuite.mapPartitions</li></div><div><li>test.org.apache.spark.JavaAPISuite.takeOrdered</li></div><div><li>test.org.apache.spark.JavaAPISuite.foldByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.objectFilesOfInts</li></div><div><li>test.org.apache.spark.JavaAPISuite.treeAggregate</li></div><div><li>test.org.apache.spark.JavaAPISuite.testGetPersistentRDDs</li></div><div><li>test.org.apache.spark.JavaAPISuite.testGetPersistentRDDs</li></div><div><li>test.org.apache.spark.JavaAPISuite.approximateResults</li></div><div><li>test.org.apache.spark.JavaAPISuite.approximateResults</li></div><div><li>test.org.apache.spark.JavaAPISuite.treeReduce</li></div><div><li>test.org.apache.spark.JavaAPISuite.treeReduce</li></div><div><li>test.org.apache.spark.JavaAPISuite.collectAsMapAndSerialize</li></div><div><li>test.org.apache.spark.JavaAPISuite.collectAsMapAndSerialize</li></div><div><li>test.org.apache.spark.JavaAPISuite.countApproxDistinct</li></div><div><li>test.org.apache.spark.JavaAPISuite.countApproxDistinct</li></div><div><li>test.org.apache.spark.JavaAPISuite.javaDoubleRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.javaDoubleRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.mapOnPairRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.mapOnPairRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.testAsyncActionErrorWrapping</li></div><div><li>test.org.apache.spark.JavaAPISuite.testAsyncActionErrorWrapping</li></div><div><li>test.org.apache.spark.JavaAPISuite.naturalMax</li></div><div><li>test.org.apache.spark.JavaAPISuite.naturalMax</li></div><div><li>test.org.apache.spark.JavaAPISuite.naturalMin</li></div><div><li>test.org.apache.spark.JavaAPISuite.naturalMin</li></div><div><li>test.org.apache.spark.JavaAPISuite.sequenceFile</li></div><div><li>test.org.apache.spark.JavaAPISuite.sequenceFile</li></div><div><li>test.org.apache.spark.JavaAPISuite.collectPartitions</li></div><div><li>test.org.apache.spark.JavaAPISuite.collectPartitions</li></div><div><li>test.org.apache.spark.JavaAPISuite.cogroup</li></div><div><li>test.org.apache.spark.JavaAPISuite.cogroup</li></div><div><li>test.org.apache.spark.JavaAPISuite.reduceByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.reduceByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.repartition</li></div><div><li>test.org.apache.spark.JavaAPISuite.iterator</li></div><div><li>test.org.apache.spark.JavaAPISuite.iterator</li></div><div><li>test.org.apache.spark.JavaAPISuite.emptyRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.emptyRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.zipWithIndex</li></div><div><li>test.org.apache.spark.JavaAPISuite.zipWithIndex</li></div><div><li>test.org.apache.spark.JavaAPISuite.foreachPartition</li></div><div><li>test.org.apache.spark.JavaAPISuite.foreachPartition</li></div><div><li>test.org.apache.spark.JavaAPISuite.combineByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.combineByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.takeAsync</li></div><div><li>test.org.apache.spark.JavaAPISuite.takeAsync</li></div><div><li>test.org.apache.spark.JavaAPISuite.collectAsMapWithIntArrayValues</li></div><div><li>test.org.apache.spark.JavaAPISuite.collectAsMapWithIntArrayValues</li></div><div><li>test.org.apache.spark.JavaAPISuite.objectFilesOfComplexTypes</li></div><div><li>test.org.apache.spark.JavaAPISuite.objectFilesOfComplexTypes</li></div><div><li>test.org.apache.spark.JavaAPISuite.zipWithUniqueId</li></div><div><li>test.org.apache.spark.JavaAPISuite.zipWithUniqueId</li></div><div><li>test.org.apache.spark.JavaAPISuite.collectAsync</li></div><div><li>test.org.apache.spark.JavaAPISuite.collectAsync</li></div><div><li>test.org.apache.spark.JavaAPISuite.foreachAsync</li></div><div><li>test.org.apache.spark.JavaAPISuite.foreachAsync</li></div><div><li>test.org.apache.spark.JavaAPISuite.zipPartitions</li></div><div><li>test.org.apache.spark.JavaAPISuite.reduceOnJavaDoubleRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.reduceOnJavaDoubleRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.isEmpty</li></div><div><li>test.org.apache.spark.JavaAPISuite.isEmpty</li></div><div><li>test.org.apache.spark.JavaSparkContextSuite.javaSparkContext</li></div><div><li>test.org.apache.spark.JavaSparkContextSuite.scalaSparkContext</li></div><div><li>org.apache.spark.sql.avro.AvroFunctionsSuite.roundtrip in to_avro and from_avro - int and string</li></div><div><li>org.apache.spark.sql.avro.AvroFunctionsSuite.roundtrip in to_avro and from_avro - struct</li></div><div><li>org.apache.spark.sql.avro.AvroFunctionsSuite.roundtrip in to_avro and from_avro - array with null</li></div><div><li>org.apache.spark.streaming.kafka010.JavaDirectKafkaStreamSuite.testKafkaStream</li></div><div><li>org.apache.spark.streaming.kafka010.JavaKafkaRDDSuite.testKafkaRDD</li></div><div><li>org.apache.spark.graphx.EdgeRDDSuite.checkpointing</li></div><div><li>org.apache.spark.graphx.GraphLoaderSuite.GraphLoader.edgeListFile</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.joinVertices</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.collectNeighborIds</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.removeSelfEdges</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.filter</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.convertToCanonicalEdges</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.collectEdgesCycleDirectionOut</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.collectEdgesCycleDirectionIn</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.collectEdgesCycleDirectionEither</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.collectEdgesChainDirectionOut</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.collectEdgesChainDirectionIn</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.collectEdgesChainDirectionEither</li></div><div><li>org.apache.spark.graphx.GraphSuite.Graph.fromEdgeTuples</li></div><div><li>org.apache.spark.graphx.GraphSuite.Graph.fromEdges</li></div><div><li>org.apache.spark.graphx.GraphSuite.Graph.apply</li></div><div><li>org.apache.spark.graphx.GraphSuite.triplets</li></div><div><li>org.apache.spark.graphx.GraphSuite.partitionBy</li></div><div><li>org.apache.spark.graphx.GraphSuite.mapVertices</li></div><div><li>org.apache.spark.graphx.GraphSuite.mapVertices changing type with same erased type</li></div><div><li>org.apache.spark.graphx.GraphSuite.mapEdges</li></div><div><li>org.apache.spark.graphx.GraphSuite.mapTriplets</li></div><div><li>org.apache.spark.graphx.GraphSuite.reverse</li></div><div><li>org.apache.spark.graphx.GraphSuite.reverse with join elimination</li></div><div><li>org.apache.spark.graphx.GraphSuite.subgraph</li></div><div><li>org.apache.spark.graphx.GraphSuite.mask</li></div><div><li>org.apache.spark.graphx.GraphSuite.groupEdges</li></div><div><li>org.apache.spark.graphx.GraphSuite.aggregateMessages</li></div><div><li>org.apache.spark.graphx.GraphSuite.outerJoinVertices</li></div><div><li>org.apache.spark.graphx.GraphSuite.more edge partitions than vertex partitions</li></div><div><li>org.apache.spark.graphx.GraphSuite.checkpoint</li></div><div><li>org.apache.spark.graphx.GraphSuite.cache, getStorageLevel</li></div><div><li>org.apache.spark.graphx.GraphSuite.non-default number of edge partitions</li></div><div><li>org.apache.spark.graphx.GraphSuite.unpersist graph RDD</li></div><div><li>org.apache.spark.graphx.GraphSuite.SPARK-14219: pickRandomVertex</li></div><div><li>org.apache.spark.graphx.PregelSuite.1 iteration</li></div><div><li>org.apache.spark.graphx.PregelSuite.chain propagation</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.filter</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.mapValues</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.minus</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.minus with RDD[(VertexId, VD)]</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.minus with non-equal number of partitions</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.diff</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.diff with RDD[(VertexId, VD)]</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.diff vertices with non-equal number of partitions</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.leftJoin</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.leftJoin vertices with non-equal number of partitions</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.innerJoin</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.innerJoin vertices with the non-equal number of partitions</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.aggregateUsingIndex</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.mergeFunc</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.cache, getStorageLevel</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.checkpoint</li></div><div><li>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Grid Connected Components</li></div><div><li>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Reverse Grid Connected Components</li></div><div><li>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Chain Connected Components</li></div><div><li>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Reverse Chain Connected Components</li></div><div><li>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Connected Components on a Toy Connected Graph</li></div><div><li>org.apache.spark.graphx.lib.LabelPropagationSuite.Label Propagation</li></div><div><li>org.apache.spark.graphx.lib.PageRankSuite.Star PageRank</li></div><div><li>org.apache.spark.graphx.lib.PageRankSuite.Star PersonalPageRank</li></div><div><li>org.apache.spark.graphx.lib.PageRankSuite.Grid PageRank</li></div><div><li>org.apache.spark.graphx.lib.PageRankSuite.Chain PageRank</li></div><div><li>org.apache.spark.graphx.lib.PageRankSuite.Chain PersonalizedPageRank</li></div><div><li>org.apache.spark.graphx.lib.PageRankSuite.Loop with source PageRank</li></div><div><li>org.apache.spark.graphx.lib.PageRankSuite.Loop with sink PageRank</li></div><div><li>org.apache.spark.graphx.lib.SVDPlusPlusSuite.Test SVD++ with mean square error on training set</li></div><div><li>org.apache.spark.graphx.lib.ShortestPathsSuite.Shortest Path Computations</li></div><div><li>org.apache.spark.graphx.lib.StronglyConnectedComponentsSuite.Island Strongly Connected Components</li></div><div><li>org.apache.spark.graphx.lib.StronglyConnectedComponentsSuite.Cycle Strongly Connected Components</li></div><div><li>org.apache.spark.graphx.lib.StronglyConnectedComponentsSuite.2 Cycle Strongly Connected Components</li></div><div><li>org.apache.spark.graphx.lib.TriangleCountSuite.Count a single triangle</li></div><div><li>org.apache.spark.graphx.lib.TriangleCountSuite.Count two triangles</li></div><div><li>org.apache.spark.graphx.lib.TriangleCountSuite.Count two triangles with bi-directed edges</li></div><div><li>org.apache.spark.graphx.lib.TriangleCountSuite.Count a single triangle with duplicate edges</li></div><div><li>org.apache.spark.graphx.util.GraphGeneratorsSuite.GraphGenerators.logNormalGraph</li></div><div><li>org.apache.spark.graphx.util.GraphGeneratorsSuite.SPARK-5064 GraphGenerators.rmatGraph numEdges upper bound</li></div><div><li>org.apache.spark.graphx.util.PeriodicGraphCheckpointerSuite.Persisting</li></div><div><li>org.apache.spark.graphx.util.PeriodicGraphCheckpointerSuite.Checkpointing</li></div><div><li>org.apache.spark.ml.JavaPipelineSuite.pipeline</li></div><div><li>org.apache.spark.ml.JavaPipelineSuite.pipeline</li></div><div><li>org.apache.spark.ml.classification.JavaDecisionTreeClassifierSuite.runDT</li></div><div><li>org.apache.spark.ml.classification.JavaDecisionTreeClassifierSuite.runDT</li></div><div><li>org.apache.spark.ml.classification.JavaGBTClassifierSuite.runDT</li></div><div><li>org.apache.spark.ml.classification.JavaGBTClassifierSuite.runDT</li></div><div><li>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionWithSetters</li></div><div><li>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionWithSetters</li></div><div><li>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionTrainingSummary</li></div><div><li>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionTrainingSummary</li></div><div><li>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionPredictorClassifierMethods</li></div><div><li>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionPredictorClassifierMethods</li></div><div><li>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionDefaultParams</li></div><div><li>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionDefaultParams</li></div><div><li>org.apache.spark.ml.classification.JavaMultilayerPerceptronClassifierSuite.testMLPC</li></div><div><li>org.apache.spark.ml.classification.JavaMultilayerPerceptronClassifierSuite.testMLPC</li></div><div><li>org.apache.spark.ml.classification.JavaNaiveBayesSuite.testNaiveBayes</li></div><div><li>org.apache.spark.ml.classification.JavaNaiveBayesSuite.naiveBayesDefaultParams</li></div><div><li>org.apache.spark.ml.classification.JavaNaiveBayesSuite.naiveBayesDefaultParams</li></div><div><li>org.apache.spark.ml.classification.JavaOneVsRestSuite.oneVsRestDefaultParams</li></div><div><li>org.apache.spark.ml.classification.JavaOneVsRestSuite.oneVsRestDefaultParams</li></div><div><li>org.apache.spark.ml.classification.JavaRandomForestClassifierSuite.runDT</li></div><div><li>org.apache.spark.ml.clustering.JavaKMeansSuite.fitAndTransform</li></div><div><li>org.apache.spark.ml.clustering.JavaKMeansSuite.fitAndTransform</li></div><div><li>org.apache.spark.ml.feature.JavaBucketizerSuite.bucketizerTest</li></div><div><li>org.apache.spark.ml.feature.JavaBucketizerSuite.bucketizerMultipleColumnsTest</li></div><div><li>org.apache.spark.ml.feature.JavaBucketizerSuite.bucketizerMultipleColumnsTest</li></div><div><li>org.apache.spark.ml.feature.JavaDCTSuite.javaCompatibilityTest</li></div><div><li>org.apache.spark.ml.feature.JavaDCTSuite.javaCompatibilityTest</li></div><div><li>org.apache.spark.ml.feature.JavaHashingTFSuite.hashingTF</li></div><div><li>org.apache.spark.ml.feature.JavaNormalizerSuite.normalizer</li></div><div><li>org.apache.spark.ml.feature.JavaNormalizerSuite.normalizer</li></div><div><li>org.apache.spark.ml.feature.JavaPCASuite.testPCA</li></div><div><li>org.apache.spark.ml.feature.JavaPCASuite.testPCA</li></div><div><li>org.apache.spark.ml.feature.JavaPolynomialExpansionSuite.polynomialExpansionTest</li></div><div><li>org.apache.spark.ml.feature.JavaPolynomialExpansionSuite.polynomialExpansionTest</li></div><div><li>org.apache.spark.ml.feature.JavaStandardScalerSuite.standardScaler</li></div><div><li>org.apache.spark.ml.feature.JavaStandardScalerSuite.standardScaler</li></div><div><li>org.apache.spark.ml.feature.JavaStopWordsRemoverSuite.javaCompatibilityTest</li></div><div><li>org.apache.spark.ml.feature.JavaStringIndexerSuite.testStringIndexer</li></div><div><li>org.apache.spark.ml.feature.JavaStringIndexerSuite.testStringIndexer</li></div><div><li>org.apache.spark.ml.feature.JavaTokenizerSuite.regexTokenizer</li></div><div><li>org.apache.spark.ml.feature.JavaTokenizerSuite.regexTokenizer</li></div><div><li>org.apache.spark.ml.feature.JavaVectorAssemblerSuite.testVectorAssembler</li></div><div><li>org.apache.spark.ml.feature.JavaVectorAssemblerSuite.testVectorAssembler</li></div><div><li>org.apache.spark.ml.feature.JavaVectorIndexerSuite.vectorIndexerAPI</li></div><div><li>org.apache.spark.ml.feature.JavaVectorIndexerSuite.vectorIndexerAPI</li></div><div><li>org.apache.spark.ml.feature.JavaVectorSlicerSuite.vectorSlice</li></div><div><li>org.apache.spark.ml.feature.JavaVectorSlicerSuite.vectorSlice</li></div><div><li>org.apache.spark.ml.feature.JavaWord2VecSuite.testJavaWord2Vec</li></div><div><li>org.apache.spark.ml.regression.JavaDecisionTreeRegressorSuite.runDT</li></div><div><li>org.apache.spark.ml.regression.JavaDecisionTreeRegressorSuite.runDT</li></div><div><li>org.apache.spark.ml.regression.JavaGBTRegressorSuite.runDT</li></div><div><li>org.apache.spark.ml.regression.JavaGBTRegressorSuite.runDT</li></div><div><li>org.apache.spark.ml.regression.JavaLinearRegressionSuite.linearRegressionDefaultParams</li></div><div><li>org.apache.spark.ml.regression.JavaLinearRegressionSuite.linearRegressionDefaultParams</li></div><div><li>org.apache.spark.ml.regression.JavaLinearRegressionSuite.linearRegressionWithSetters</li></div><div><li>org.apache.spark.ml.regression.JavaLinearRegressionSuite.linearRegressionWithSetters</li></div><div><li>org.apache.spark.ml.regression.JavaRandomForestRegressorSuite.runDT</li></div><div><li>org.apache.spark.ml.regression.JavaRandomForestRegressorSuite.runDT</li></div><div><li>org.apache.spark.ml.source.libsvm.JavaLibSVMRelationSuite.verifyLibSVMDF</li></div><div><li>org.apache.spark.ml.stat.JavaKolmogorovSmirnovTestSuite.testKSTestNamedDistribution</li></div><div><li>org.apache.spark.ml.stat.JavaKolmogorovSmirnovTestSuite.testKSTestNamedDistribution</li></div><div><li>org.apache.spark.ml.stat.JavaKolmogorovSmirnovTestSuite.testKSTestCDF</li></div><div><li>org.apache.spark.ml.stat.JavaKolmogorovSmirnovTestSuite.testKSTestCDF</li></div><div><li>org.apache.spark.ml.stat.JavaSummarizerSuite.testSummarizer</li></div><div><li>org.apache.spark.ml.stat.JavaSummarizerSuite.testSummarizer</li></div><div><li>org.apache.spark.ml.tuning.JavaCrossValidatorSuite.crossValidationWithLogisticRegression</li></div><div><li>org.apache.spark.ml.tuning.JavaCrossValidatorSuite.crossValidationWithLogisticRegression</li></div><div><li>org.apache.spark.ml.util.JavaDefaultReadWriteSuite.testDefaultReadWrite</li></div><div><li>org.apache.spark.ml.util.JavaDefaultReadWriteSuite.testDefaultReadWrite</li></div><div><li>org.apache.spark.mllib.classification.JavaLogisticRegressionSuite.runLRUsingConstructor</li></div><div><li>org.apache.spark.mllib.classification.JavaLogisticRegressionSuite.runLRUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.classification.JavaLogisticRegressionSuite.runLRUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.testPredictJavaRDD</li></div><div><li>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.testPredictJavaRDD</li></div><div><li>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.runUsingConstructor</li></div><div><li>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.runUsingConstructor</li></div><div><li>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.runUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.runUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.testModelTypeSetters</li></div><div><li>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.testModelTypeSetters</li></div><div><li>org.apache.spark.mllib.classification.JavaSVMSuite.runSVMUsingConstructor</li></div><div><li>org.apache.spark.mllib.classification.JavaSVMSuite.runSVMUsingConstructor</li></div><div><li>org.apache.spark.mllib.classification.JavaSVMSuite.runSVMUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.classification.JavaSVMSuite.runSVMUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.classification.JavaStreamingLogisticRegressionSuite.javaAPI</li></div><div><li>org.apache.spark.mllib.classification.JavaStreamingLogisticRegressionSuite.javaAPI</li></div><div><li>org.apache.spark.mllib.clustering.JavaBisectingKMeansSuite.twoDimensionalData</li></div><div><li>org.apache.spark.mllib.clustering.JavaBisectingKMeansSuite.twoDimensionalData</li></div><div><li>org.apache.spark.mllib.clustering.JavaGaussianMixtureSuite.runGaussianMixture</li></div><div><li>org.apache.spark.mllib.clustering.JavaGaussianMixtureSuite.runGaussianMixture</li></div><div><li>org.apache.spark.mllib.clustering.JavaKMeansSuite.testPredictJavaRDD</li></div><div><li>org.apache.spark.mllib.clustering.JavaKMeansSuite.testPredictJavaRDD</li></div><div><li>org.apache.spark.mllib.clustering.JavaKMeansSuite.runKMeansUsingConstructor</li></div><div><li>org.apache.spark.mllib.clustering.JavaKMeansSuite.runKMeansUsingConstructor</li></div><div><li>org.apache.spark.mllib.clustering.JavaKMeansSuite.runKMeansUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.clustering.JavaKMeansSuite.runKMeansUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.clustering.JavaLDASuite.onlineOptimizerCompatibility</li></div><div><li>org.apache.spark.mllib.clustering.JavaLDASuite.distributedLDAModel</li></div><div><li>org.apache.spark.mllib.clustering.JavaLDASuite.distributedLDAModel</li></div><div><li>org.apache.spark.mllib.clustering.JavaLDASuite.localLDAModel</li></div><div><li>org.apache.spark.mllib.clustering.JavaLDASuite.localLDAModel</li></div><div><li>org.apache.spark.mllib.clustering.JavaLDASuite.localLdaMethods</li></div><div><li>org.apache.spark.mllib.clustering.JavaLDASuite.localLdaMethods</li></div><div><li>org.apache.spark.mllib.clustering.JavaStreamingKMeansSuite.javaAPI</li></div><div><li>org.apache.spark.mllib.clustering.JavaStreamingKMeansSuite.javaAPI</li></div><div><li>org.apache.spark.mllib.evaluation.JavaRankingMetricsSuite.rankingMetrics</li></div><div><li>org.apache.spark.mllib.evaluation.JavaRankingMetricsSuite.rankingMetrics</li></div><div><li>org.apache.spark.mllib.feature.JavaTfIdfSuite.tfIdfMinimumDocumentFrequency</li></div><div><li>org.apache.spark.mllib.feature.JavaTfIdfSuite.tfIdfMinimumDocumentFrequency</li></div><div><li>org.apache.spark.mllib.feature.JavaTfIdfSuite.tfIdf</li></div><div><li>org.apache.spark.mllib.feature.JavaTfIdfSuite.tfIdf</li></div><div><li>org.apache.spark.mllib.feature.JavaWord2VecSuite.word2Vec</li></div><div><li>org.apache.spark.mllib.feature.JavaWord2VecSuite.word2Vec</li></div><div><li>org.apache.spark.mllib.fpm.JavaAssociationRulesSuite.runAssociationRules</li></div><div><li>org.apache.spark.mllib.fpm.JavaAssociationRulesSuite.runAssociationRules</li></div><div><li>org.apache.spark.mllib.fpm.JavaFPGrowthSuite.runFPGrowthSaveLoad</li></div><div><li>org.apache.spark.mllib.fpm.JavaFPGrowthSuite.runFPGrowthSaveLoad</li></div><div><li>org.apache.spark.mllib.fpm.JavaFPGrowthSuite.runFPGrowth</li></div><div><li>org.apache.spark.mllib.fpm.JavaFPGrowthSuite.runFPGrowth</li></div><div><li>org.apache.spark.mllib.fpm.JavaPrefixSpanSuite.runPrefixSpan</li></div><div><li>org.apache.spark.mllib.fpm.JavaPrefixSpanSuite.runPrefixSpan</li></div><div><li>org.apache.spark.mllib.fpm.JavaPrefixSpanSuite.runPrefixSpanSaveLoad</li></div><div><li>org.apache.spark.mllib.fpm.JavaPrefixSpanSuite.runPrefixSpanSaveLoad</li></div><div><li>org.apache.spark.mllib.linalg.distributed.JavaRowMatrixSuite.rowMatrixQRDecomposition</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testNormalVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testNormalVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testArbitrary</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testArbitrary</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testLogNormalVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testLogNormalVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testExponentialVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testExponentialVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testUniformRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testUniformRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testRandomVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testRandomVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testGammaRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testGammaRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testUniformVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testUniformVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testPoissonRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testPoissonRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testNormalRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testNormalRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testPoissonVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testPoissonVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testGammaVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testGammaVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testExponentialRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testExponentialRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testLNormalRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testLNormalRDD</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runALSUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runALSUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSUsingConstructor</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSUsingConstructor</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runRecommend</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runRecommend</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSWithNegativeWeight</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSWithNegativeWeight</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runALSUsingConstructor</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runALSUsingConstructor</li></div><div><li>org.apache.spark.mllib.regression.JavaIsotonicRegressionSuite.testIsotonicRegressionJavaRDD</li></div><div><li>org.apache.spark.mllib.regression.JavaIsotonicRegressionSuite.testIsotonicRegressionJavaRDD</li></div><div><li>org.apache.spark.mllib.regression.JavaIsotonicRegressionSuite.testIsotonicRegressionPredictionsJavaRDD</li></div><div><li>org.apache.spark.mllib.regression.JavaIsotonicRegressionSuite.testIsotonicRegressionPredictionsJavaRDD</li></div><div><li>org.apache.spark.mllib.regression.JavaLassoSuite.runLassoUsingConstructor</li></div><div><li>org.apache.spark.mllib.regression.JavaLassoSuite.runLassoUsingConstructor</li></div><div><li>org.apache.spark.mllib.regression.JavaLassoSuite.runLassoUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.regression.JavaLassoSuite.runLassoUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.testPredictJavaRDD</li></div><div><li>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.testPredictJavaRDD</li></div><div><li>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.runLinearRegressionUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.runLinearRegressionUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.runLinearRegressionUsingConstructor</li></div><div><li>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.runLinearRegressionUsingConstructor</li></div><div><li>org.apache.spark.mllib.regression.JavaRidgeRegressionSuite.runRidgeRegressionUsingConstructor</li></div><div><li>org.apache.spark.mllib.regression.JavaRidgeRegressionSuite.runRidgeRegressionUsingConstructor</li></div><div><li>org.apache.spark.mllib.regression.JavaRidgeRegressionSuite.runRidgeRegressionUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.regression.JavaRidgeRegressionSuite.runRidgeRegressionUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.regression.JavaStreamingLinearRegressionSuite.javaAPI</li></div><div><li>org.apache.spark.mllib.regression.JavaStreamingLinearRegressionSuite.javaAPI</li></div><div><li>org.apache.spark.mllib.stat.JavaStatisticsSuite.testCorr</li></div><div><li>org.apache.spark.mllib.stat.JavaStatisticsSuite.testCorr</li></div><div><li>org.apache.spark.mllib.stat.JavaStatisticsSuite.chiSqTest</li></div><div><li>org.apache.spark.mllib.stat.JavaStatisticsSuite.chiSqTest</li></div><div><li>org.apache.spark.mllib.stat.JavaStatisticsSuite.streamingTest</li></div><div><li>org.apache.spark.mllib.stat.JavaStatisticsSuite.streamingTest</li></div><div><li>org.apache.spark.mllib.stat.JavaStatisticsSuite.kolmogorovSmirnovTest</li></div><div><li>org.apache.spark.mllib.tree.JavaDecisionTreeSuite.runDTUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.tree.JavaDecisionTreeSuite.runDTUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.tree.JavaDecisionTreeSuite.runDTUsingConstructor</li></div><div><li>org.apache.spark.mllib.tree.JavaDecisionTreeSuite.runDTUsingConstructor</li></div><div><li>org.apache.spark.mllib.util.JavaMLUtilsSuite.testConvertMatrixColumnsToAndFromML</li></div><div><li>org.apache.spark.mllib.util.JavaMLUtilsSuite.testConvertMatrixColumnsToAndFromML</li></div><div><li>org.apache.spark.mllib.util.JavaMLUtilsSuite.testConvertVectorColumnsToAndFromML</li></div><div><li>org.apache.spark.mllib.util.JavaMLUtilsSuite.testConvertVectorColumnsToAndFromML</li></div><div><li>org.apache.spark.repl.ReplSuite.SPARK-15236: use Hive catalog</li></div><div><li>org.apache.spark.repl.ReplSuite.SPARK-15236: use in-memory catalog</li></div><div><li>org.apache.spark.repl.ReplSuite.broadcast vars</li></div><div><li>org.apache.spark.repl.ReplSuite.line wrapper only initialized once when used as encoder outer scope</li></div><div><li>org.apache.spark.repl.ReplSuite.define case class and create Dataset together with paste mode</li></div><div><li>org.apache.spark.repl.ReplSuite.:replay should work correctly</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.simple foreach with accumulator</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.external vars</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.external classes</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.external functions</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.external functions that access vars</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.broadcast vars</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.interacting with files</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.local-cluster mode</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.SPARK-2576 importing implicits</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.Datasets and encoders</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.SPARK-2632 importing a method from non serializable class and not using it.</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.collecting objects of class defined in repl</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.collecting objects of class defined in repl - shuffling</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.replicating blocks of object with class defined in repl</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.newProductSeqEncoder with REPL defined class</li></div><div><li>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationCount</li></div><div><li>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationCount</li></div><div><li>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationSumDouble</li></div><div><li>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationSumDouble</li></div><div><li>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationSumLong</li></div><div><li>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationSumLong</li></div><div><li>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationAverage</li></div><div><li>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationAverage</li></div><div><li>test.org.apache.spark.sql.JavaApplySchemaSuite.applySchema</li></div><div><li>test.org.apache.spark.sql.JavaApplySchemaSuite.applySchema</li></div><div><li>test.org.apache.spark.sql.JavaApplySchemaSuite.dataFrameRDDOperations</li></div><div><li>test.org.apache.spark.sql.JavaApplySchemaSuite.dataFrameRDDOperations</li></div><div><li>test.org.apache.spark.sql.JavaApplySchemaSuite.applySchemaToJSON</li></div><div><li>test.org.apache.spark.sql.JavaApplySchemaSuite.applySchemaToJSON</li></div><div><li>test.org.apache.spark.sql.JavaBeanDeserializationSuite.testBeanWithArrayFieldDeserialization</li></div><div><li>test.org.apache.spark.sql.JavaBeanDeserializationSuite.testBeanWithArrayFieldDeserialization</li></div><div><li>test.org.apache.spark.sql.JavaBeanDeserializationSuite.testBeanWithMapFieldsDeserialization</li></div><div><li>test.org.apache.spark.sql.JavaBeanDeserializationSuite.testBeanWithMapFieldsDeserialization</li></div><div><li>test.org.apache.spark.sql.JavaColumnExpressionSuite.isInCollectionCheckExceptionMessage</li></div><div><li>test.org.apache.spark.sql.JavaColumnExpressionSuite.isInCollectionCheckExceptionMessage</li></div><div><li>test.org.apache.spark.sql.JavaColumnExpressionSuite.isInCollectionWorksCorrectlyOnJava</li></div><div><li>test.org.apache.spark.sql.JavaColumnExpressionSuite.isInCollectionWorksCorrectlyOnJava</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testFormatAPI</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testTextAPI</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testJsonAPI</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testLoadAPI</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testOptionsAPI</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testSaveModeAPI</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testCsvAPI</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testParquetAPI</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testTextFileAPI</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCollectAndTake</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCollectAndTake</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testJsonRDDToDataFrame</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testJsonRDDToDataFrame</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testVarargMethods</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testVarargMethods</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testBeanWithoutGetter</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testBeanWithoutGetter</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateStructTypeFromList</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateStructTypeFromList</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testSampleBy</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testSampleBy</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCrosstab</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCrosstab</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testUDF</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testUDF</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFromFromList</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFromFromList</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCircularReferenceBean</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCircularReferenceBean</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testFrequentItems</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testFrequentItems</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testSampleByColumn</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testSampleByColumn</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testExecution</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testExecution</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testTextLoad</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testTextLoad</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.pivot</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.pivot</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testGenericLoad</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testGenericLoad</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCountMinSketch</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCountMinSketch</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.pivotColumnValues</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.pivotColumnValues</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFrameFromJavaBeans</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFrameFromJavaBeans</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCorrelation</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCorrelation</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testBloomFilter</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testBloomFilter</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCovariance</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCovariance</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFrameFromLocalJavaBeans</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFrameFromLocalJavaBeans</li></div><div><li>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationCount</li></div><div><li>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationCount</li></div><div><li>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationSumDouble</li></div><div><li>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationSumLong</li></div><div><li>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationAnonClass</li></div><div><li>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationAverage</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testRuntimeNullabilityCheck</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean1</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean1</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean2</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean2</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean3</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean3</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testSerializeNull</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testSerializeNull</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testRandomSplit</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testRandomSplit</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testTypedFilterPreservingSchema</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testTypedFilterPreservingSchema</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJoin</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJoin</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testTake</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testTake</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testToLocalIterator</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testToLocalIterator</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testSpecificLists</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testSpecificLists</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testForeach</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJavaEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJavaEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testPrimitiveEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testPrimitiveEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testEmptyBean</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testEmptyBean</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testCommonOperation</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testCommonOperation</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testNullInTopLevelBean</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testNullInTopLevelBean</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testGroupBy</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testGroupBy</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testSetOperation</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testSetOperation</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testBeanWithEnum</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testBeanWithEnum</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testKryoEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.test</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.test</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJavaBeanEncoder2</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJavaBeanEncoder2</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testCollect</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testCollect</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testKryoEncoderErrorMessageForPrivateClass</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testKryoEncoderErrorMessageForPrivateClass</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJavaBeanEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJavaBeanEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testTupleEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testTupleEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testNestedTupleEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testNestedTupleEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testTupleEncoderSchema</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testTupleEncoderSchema</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testReduce</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testReduce</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testSelect</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testSelect</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJavaEncoderErrorMessageForPrivateClass</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJavaEncoderErrorMessageForPrivateClass</li></div><div><li>test.org.apache.spark.sql.JavaSaveLoadSuite.saveAndLoadWithSchema</li></div><div><li>test.org.apache.spark.sql.JavaSaveLoadSuite.saveAndLoadWithSchema</li></div><div><li>test.org.apache.spark.sql.JavaSaveLoadSuite.saveAndLoad</li></div><div><li>test.org.apache.spark.sql.JavaSaveLoadSuite.saveAndLoad</li></div><div><li>test.org.apache.spark.sql.JavaUDAFSuite.udf1Test</li></div><div><li>test.org.apache.spark.sql.JavaUDAFSuite.udf1Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf1Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf1Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf2Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf2Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf3Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf3Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf4Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf4Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf5Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf6Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf6Test</li></div><div><li>test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.testForeachBatchAPI</li></div><div><li>test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.testForeachBatchAPI</li></div><div><li>test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.testForeachAPI</li></div><div><li>test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.testForeachAPI</li></div><div><li>org.apache.spark.sql.hive.HiveMetastoreLazyInitializationSuite.lazily initialize Hive client</li></div><div><li>org.apache.spark.sql.hive.JavaDataFrameSuite.testUDAF</li></div><div><li>org.apache.spark.sql.hive.JavaDataFrameSuite.saveTableAndQueryIt</li></div><div><li>org.apache.spark.sql.hive.JavaMetastoreDataSourcesSuite.saveExternalTableAndQueryIt</li></div><div><li>org.apache.spark.sql.hive.JavaMetastoreDataSourcesSuite.saveTableAndQueryIt</li></div><div><li>org.apache.spark.sql.hive.JavaMetastoreDataSourcesSuite.saveExternalTableWithSchemaAndQueryIt</li></div><div><li>org.apache.spark.streaming.JavaMapWithStateSuite.testBasicFunction</li></div><div><li>org.apache.spark.streaming.JavaMapWithStateSuite.testBasicFunction</li></div><div><li>org.apache.spark.streaming.JavaReceiverAPISuite.testReceiver</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testStreamingContextTransform</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testFlatMapValues</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testFlatMapValues</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testMapPartitions</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testMapPartitions</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairFilter</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairFilter</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testCombineByKey</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testCombineByKey</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testMap</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testMap</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairToNormalRDDTransform</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairToNormalRDDTransform</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairReduceByKey</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairReduceByKey</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairMap</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairMap</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testFlatMap</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testFlatMap</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testReduceByKeyAndWindowWithInverse</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testReduceByKeyAndWindowWithInverse</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testReduceByWindow</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testReduceByWindow</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairFlatMap</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairFlatMap</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairToPairFlatMapWithChangingTypes</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairToPairFlatMapWithChangingTypes</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairMapPartitions</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairMapPartitions</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testVariousTransform</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testVariousTransform</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testTransformWith</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testTransformWith</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testVariousTransformWith</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testVariousTransformWith</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testReduceByKeyAndWindow</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testReduceByKeyAndWindow</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairTransform</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairTransform</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testFilter</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testFilter</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairMap2</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairMap2</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testMapValues</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testMapValues</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testReduce</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testReduce</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testUpdateStateByKey</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testUpdateStateByKey</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testTransform</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testTransform</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testStreamingContextTransform</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testFlatMapValues</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testReduceByWindowWithInverse</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testMapPartitions</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairFilter</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairFilter</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testRepartitionFewerPartitions</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testRepartitionFewerPartitions</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCombineByKey</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCombineByKey</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testContextGetOrCreate</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testContextGetOrCreate</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testWindowWithSlideDuration</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testWindowWithSlideDuration</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testQueueStream</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testQueueStream</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCountByValue</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCountByValue</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testMap</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testMap</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairToNormalRDDTransform</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairToNormalRDDTransform</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairReduceByKey</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairReduceByKey</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCount</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCount</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCheckpointMasterRecovery</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCheckpointMasterRecovery</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairMap</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairMap</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testUnion</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testUnion</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testFlatMap</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testFlatMap</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testReduceByKeyAndWindowWithInverse</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testReduceByKeyAndWindowWithInverse</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testGlom</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testGlom</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testJoin</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testJoin</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairFlatMap</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairToPairFlatMapWithChangingTypes</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairMapPartitions</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairMapPartitions</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testRepartitionMorePartitions</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testRepartitionMorePartitions</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testReduceByWindowWithoutInverse</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testReduceByWindowWithoutInverse</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testLeftOuterJoin</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testLeftOuterJoin</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testVariousTransform</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testVariousTransform</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testTransformWith</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testTransformWith</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testVariousTransformWith</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testVariousTransformWith</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testTextFileStream</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testTextFileStream</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairGroupByKey</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairGroupByKey</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCoGroup</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCoGroup</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testInitialization</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testInitialization</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testSocketString</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testSocketString</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testGroupByKeyAndWindow</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testGroupByKeyAndWindow</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testReduceByKeyAndWindow</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testReduceByKeyAndWindow</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testForeachRDD</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testForeachRDD</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testFileStream</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testFileStream</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairTransform</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairTransform</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testFilter</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testFilter</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairMap2</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairMap2</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testMapValues</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testMapValues</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testReduce</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testUpdateStateByKey</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testTransform</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testWindow</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCountByValueAndWindow</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCountByValueAndWindow</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testRawSocketStream</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testRawSocketStream</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testSocketTextStream</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testSocketTextStream</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testUpdateStateByKeyWithInitial</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testUpdateStateByKeyWithInitial</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testContextState</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testContextState</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.spark.network.ChunkFetchIntegrationSuite.fetchFileChunk</li></div><div><li>org.apache.spark.JavaJdbcRDDSuite.testJavaJdbcRDD</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.leftOuterJoin</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.foldReduce</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.mapsFromPairsToPairs</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.flatMap</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.foreach</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.map</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.map</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.zip</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.zip</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.keyBy</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.keyBy</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.groupBy</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.groupBy</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.mapPartitions</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.mapPartitions</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.foldByKey</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.foldByKey</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.mapOnPairRDD</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.sequenceFile</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.sequenceFile</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.collectPartitions</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.collectPartitions</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.reduceByKey</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.reduceByKey</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.foreachWithAnonymousClass</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.foreachWithAnonymousClass</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.collectAsMapWithIntArrayValues</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.collectAsMapWithIntArrayValues</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.zipPartitions</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.zipPartitions</li></div><div><li>test.org.apache.spark.JavaAPISuite.groupByOnPairRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.groupByOnPairRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.binaryFilesCaching</li></div><div><li>test.org.apache.spark.JavaAPISuite.binaryFilesCaching</li></div><div><li>test.org.apache.spark.JavaAPISuite.sparkContextUnion</li></div><div><li>test.org.apache.spark.JavaAPISuite.sparkContextUnion</li></div><div><li>test.org.apache.spark.JavaAPISuite.checkpointAndComputation</li></div><div><li>test.org.apache.spark.JavaAPISuite.checkpointAndComputation</li></div><div><li>test.org.apache.spark.JavaAPISuite.leftOuterJoin</li></div><div><li>test.org.apache.spark.JavaAPISuite.leftOuterJoin</li></div><div><li>test.org.apache.spark.JavaAPISuite.keyByOnPairRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.keyByOnPairRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.getNumPartitions</li></div><div><li>test.org.apache.spark.JavaAPISuite.getNumPartitions</li></div><div><li>test.org.apache.spark.JavaAPISuite.wholeTextFiles</li></div><div><li>test.org.apache.spark.JavaAPISuite.wholeTextFiles</li></div><div><li>test.org.apache.spark.JavaAPISuite.binaryFiles</li></div><div><li>test.org.apache.spark.JavaAPISuite.binaryFiles</li></div><div><li>test.org.apache.spark.JavaAPISuite.foldReduce</li></div><div><li>test.org.apache.spark.JavaAPISuite.writeWithNewAPIHadoopFile</li></div><div><li>test.org.apache.spark.JavaAPISuite.writeWithNewAPIHadoopFile</li></div><div><li>test.org.apache.spark.JavaAPISuite.hadoopFile</li></div><div><li>test.org.apache.spark.JavaAPISuite.hadoopFile</li></div><div><li>test.org.apache.spark.JavaAPISuite.lookup</li></div><div><li>test.org.apache.spark.JavaAPISuite.lookup</li></div><div><li>test.org.apache.spark.JavaAPISuite.countAsync</li></div><div><li>test.org.apache.spark.JavaAPISuite.countAsync</li></div><div><li>test.org.apache.spark.JavaAPISuite.textFiles</li></div><div><li>test.org.apache.spark.JavaAPISuite.textFiles</li></div><div><li>test.org.apache.spark.JavaAPISuite.binaryRecords</li></div><div><li>test.org.apache.spark.JavaAPISuite.binaryRecords</li></div><div><li>test.org.apache.spark.JavaAPISuite.toLocalIterator</li></div><div><li>test.org.apache.spark.JavaAPISuite.toLocalIterator</li></div><div><li>test.org.apache.spark.JavaAPISuite.repartitionAndSortWithinPartitions</li></div><div><li>test.org.apache.spark.JavaAPISuite.repartitionAndSortWithinPartitions</li></div><div><li>test.org.apache.spark.JavaAPISuite.reduce</li></div><div><li>test.org.apache.spark.JavaAPISuite.reduce</li></div><div><li>test.org.apache.spark.JavaAPISuite.sample</li></div><div><li>test.org.apache.spark.JavaAPISuite.sample</li></div><div><li>test.org.apache.spark.JavaAPISuite.sortBy</li></div><div><li>test.org.apache.spark.JavaAPISuite.sortBy</li></div><div><li>test.org.apache.spark.JavaAPISuite.mapsFromPairsToPairs</li></div><div><li>test.org.apache.spark.JavaAPISuite.mapsFromPairsToPairs</li></div><div><li>test.org.apache.spark.JavaAPISuite.flatMap</li></div><div><li>test.org.apache.spark.JavaAPISuite.flatMap</li></div><div><li>test.org.apache.spark.JavaAPISuite.cogroup3</li></div><div><li>test.org.apache.spark.JavaAPISuite.cogroup3</li></div><div><li>test.org.apache.spark.JavaAPISuite.cogroup4</li></div><div><li>test.org.apache.spark.JavaAPISuite.cogroup4</li></div><div><li>test.org.apache.spark.JavaAPISuite.randomSplit</li></div><div><li>test.org.apache.spark.JavaAPISuite.randomSplit</li></div><div><li>test.org.apache.spark.JavaAPISuite.persist</li></div><div><li>test.org.apache.spark.JavaAPISuite.persist</li></div><div><li>test.org.apache.spark.JavaAPISuite.foreach</li></div><div><li>test.org.apache.spark.JavaAPISuite.foreach</li></div><div><li>test.org.apache.spark.JavaAPISuite.hadoopFileCompressed</li></div><div><li>test.org.apache.spark.JavaAPISuite.hadoopFileCompressed</li></div><div><li>test.org.apache.spark.JavaAPISuite.textFilesCompressed</li></div><div><li>test.org.apache.spark.JavaAPISuite.textFilesCompressed</li></div><div><li>test.org.apache.spark.JavaAPISuite.testAsyncActionCancellation</li></div><div><li>test.org.apache.spark.JavaAPISuite.checkpointAndRestore</li></div><div><li>test.org.apache.spark.JavaAPISuite.checkpointAndRestore</li></div><div><li>test.org.apache.spark.JavaAPISuite.sortByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.sortByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.aggregateByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.aggregateByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.map</li></div><div><li>test.org.apache.spark.JavaAPISuite.map</li></div><div><li>test.org.apache.spark.JavaAPISuite.max</li></div><div><li>test.org.apache.spark.JavaAPISuite.max</li></div><div><li>test.org.apache.spark.JavaAPISuite.min</li></div><div><li>test.org.apache.spark.JavaAPISuite.min</li></div><div><li>test.org.apache.spark.JavaAPISuite.top</li></div><div><li>test.org.apache.spark.JavaAPISuite.top</li></div><div><li>test.org.apache.spark.JavaAPISuite.zip</li></div><div><li>test.org.apache.spark.JavaAPISuite.zip</li></div><div><li>test.org.apache.spark.JavaAPISuite.fold</li></div><div><li>test.org.apache.spark.JavaAPISuite.fold</li></div><div><li>test.org.apache.spark.JavaAPISuite.glom</li></div><div><li>test.org.apache.spark.JavaAPISuite.glom</li></div><div><li>test.org.apache.spark.JavaAPISuite.take</li></div><div><li>test.org.apache.spark.JavaAPISuite.take</li></div><div><li>test.org.apache.spark.JavaAPISuite.javaDoubleRDDHistoGram</li></div><div><li>test.org.apache.spark.JavaAPISuite.javaDoubleRDDHistoGram</li></div><div><li>test.org.apache.spark.JavaAPISuite.collectUnderlyingScalaRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.collectUnderlyingScalaRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.keyBy</li></div><div><li>test.org.apache.spark.JavaAPISuite.keyBy</li></div><div><li>test.org.apache.spark.JavaAPISuite.mapPartitionsWithIndex</li></div><div><li>test.org.apache.spark.JavaAPISuite.sampleByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.intersection</li></div><div><li>test.org.apache.spark.JavaAPISuite.aggregate</li></div><div><li>test.org.apache.spark.JavaAPISuite.cartesian</li></div><div><li>test.org.apache.spark.JavaAPISuite.cartesian</li></div><div><li>test.org.apache.spark.JavaAPISuite.countApproxDistinctByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.countApproxDistinctByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.readWithNewAPIHadoopFile</li></div><div><li>test.org.apache.spark.JavaAPISuite.readWithNewAPIHadoopFile</li></div><div><li>test.org.apache.spark.JavaAPISuite.groupBy</li></div><div><li>test.org.apache.spark.JavaAPISuite.groupBy</li></div><div><li>test.org.apache.spark.JavaAPISuite.sampleByKeyExact</li></div><div><li>test.org.apache.spark.JavaAPISuite.sampleByKeyExact</li></div><div><li>test.org.apache.spark.JavaAPISuite.mapPartitions</li></div><div><li>test.org.apache.spark.JavaAPISuite.takeOrdered</li></div><div><li>test.org.apache.spark.JavaAPISuite.takeOrdered</li></div><div><li>test.org.apache.spark.JavaAPISuite.foldByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.foldByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.objectFilesOfInts</li></div><div><li>test.org.apache.spark.JavaAPISuite.objectFilesOfInts</li></div><div><li>test.org.apache.spark.JavaAPISuite.treeAggregate</li></div><div><li>test.org.apache.spark.JavaAPISuite.treeAggregate</li></div><div><li>test.org.apache.spark.JavaAPISuite.testGetPersistentRDDs</li></div><div><li>test.org.apache.spark.JavaAPISuite.testGetPersistentRDDs</li></div><div><li>test.org.apache.spark.JavaAPISuite.approximateResults</li></div><div><li>test.org.apache.spark.JavaAPISuite.approximateResults</li></div><div><li>test.org.apache.spark.JavaAPISuite.treeReduce</li></div><div><li>test.org.apache.spark.JavaAPISuite.treeReduce</li></div><div><li>test.org.apache.spark.JavaAPISuite.collectAsMapAndSerialize</li></div><div><li>test.org.apache.spark.JavaAPISuite.collectAsMapAndSerialize</li></div><div><li>test.org.apache.spark.JavaAPISuite.countApproxDistinct</li></div><div><li>test.org.apache.spark.JavaAPISuite.countApproxDistinct</li></div><div><li>test.org.apache.spark.JavaAPISuite.javaDoubleRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.javaDoubleRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.mapOnPairRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.mapOnPairRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.testAsyncActionErrorWrapping</li></div><div><li>test.org.apache.spark.JavaAPISuite.testAsyncActionErrorWrapping</li></div><div><li>test.org.apache.spark.JavaAPISuite.naturalMax</li></div><div><li>test.org.apache.spark.JavaAPISuite.naturalMax</li></div><div><li>test.org.apache.spark.JavaAPISuite.naturalMin</li></div><div><li>test.org.apache.spark.JavaAPISuite.naturalMin</li></div><div><li>test.org.apache.spark.JavaAPISuite.sequenceFile</li></div><div><li>test.org.apache.spark.JavaAPISuite.sequenceFile</li></div><div><li>test.org.apache.spark.JavaAPISuite.collectPartitions</li></div><div><li>test.org.apache.spark.JavaAPISuite.collectPartitions</li></div><div><li>test.org.apache.spark.JavaAPISuite.cogroup</li></div><div><li>test.org.apache.spark.JavaAPISuite.cogroup</li></div><div><li>test.org.apache.spark.JavaAPISuite.reduceByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.reduceByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.repartition</li></div><div><li>test.org.apache.spark.JavaAPISuite.repartition</li></div><div><li>test.org.apache.spark.JavaAPISuite.iterator</li></div><div><li>test.org.apache.spark.JavaAPISuite.iterator</li></div><div><li>test.org.apache.spark.JavaAPISuite.emptyRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.emptyRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.zipWithIndex</li></div><div><li>test.org.apache.spark.JavaAPISuite.zipWithIndex</li></div><div><li>test.org.apache.spark.JavaAPISuite.foreachPartition</li></div><div><li>test.org.apache.spark.JavaAPISuite.foreachPartition</li></div><div><li>test.org.apache.spark.JavaAPISuite.combineByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.combineByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.takeAsync</li></div><div><li>test.org.apache.spark.JavaAPISuite.takeAsync</li></div><div><li>test.org.apache.spark.JavaAPISuite.collectAsMapWithIntArrayValues</li></div><div><li>test.org.apache.spark.JavaAPISuite.collectAsMapWithIntArrayValues</li></div><div><li>test.org.apache.spark.JavaAPISuite.objectFilesOfComplexTypes</li></div><div><li>test.org.apache.spark.JavaAPISuite.objectFilesOfComplexTypes</li></div><div><li>test.org.apache.spark.JavaAPISuite.zipWithUniqueId</li></div><div><li>test.org.apache.spark.JavaAPISuite.collectAsync</li></div><div><li>test.org.apache.spark.JavaAPISuite.foreachAsync</li></div><div><li>test.org.apache.spark.JavaAPISuite.foreachAsync</li></div><div><li>test.org.apache.spark.JavaAPISuite.zipPartitions</li></div><div><li>test.org.apache.spark.JavaAPISuite.zipPartitions</li></div><div><li>test.org.apache.spark.JavaAPISuite.reduceOnJavaDoubleRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.reduceOnJavaDoubleRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.isEmpty</li></div><div><li>test.org.apache.spark.JavaAPISuite.isEmpty</li></div><div><li>org.apache.spark.sql.avro.AvroFunctionsSuite.roundtrip in to_avro and from_avro - int and string</li></div><div><li>org.apache.spark.sql.avro.AvroFunctionsSuite.roundtrip in to_avro and from_avro - struct</li></div><div><li>org.apache.spark.sql.avro.AvroFunctionsSuite.roundtrip in to_avro and from_avro - array with null</li></div><div><li>org.apache.spark.streaming.kafka010.JavaDirectKafkaStreamSuite.testKafkaStream</li></div><div><li>org.apache.spark.streaming.kafka010.JavaKafkaRDDSuite.testKafkaRDD</li></div><div><li>org.apache.spark.graphx.EdgeRDDSuite.cache, getStorageLevel</li></div><div><li>org.apache.spark.graphx.EdgeRDDSuite.checkpointing</li></div><div><li>org.apache.spark.graphx.GraphLoaderSuite.GraphLoader.edgeListFile</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.joinVertices</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.collectNeighborIds</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.removeSelfEdges</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.filter</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.convertToCanonicalEdges</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.collectEdgesCycleDirectionOut</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.collectEdgesCycleDirectionIn</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.collectEdgesCycleDirectionEither</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.collectEdgesChainDirectionOut</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.collectEdgesChainDirectionIn</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.collectEdgesChainDirectionEither</li></div><div><li>org.apache.spark.graphx.GraphSuite.Graph.fromEdgeTuples</li></div><div><li>org.apache.spark.graphx.GraphSuite.Graph.fromEdges</li></div><div><li>org.apache.spark.graphx.GraphSuite.Graph.apply</li></div><div><li>org.apache.spark.graphx.GraphSuite.triplets</li></div><div><li>org.apache.spark.graphx.GraphSuite.partitionBy</li></div><div><li>org.apache.spark.graphx.GraphSuite.mapVertices</li></div><div><li>org.apache.spark.graphx.GraphSuite.mapVertices changing type with same erased type</li></div><div><li>org.apache.spark.graphx.GraphSuite.mapEdges</li></div><div><li>org.apache.spark.graphx.GraphSuite.mapTriplets</li></div><div><li>org.apache.spark.graphx.GraphSuite.reverse</li></div><div><li>org.apache.spark.graphx.GraphSuite.reverse with join elimination</li></div><div><li>org.apache.spark.graphx.GraphSuite.subgraph</li></div><div><li>org.apache.spark.graphx.GraphSuite.mask</li></div><div><li>org.apache.spark.graphx.GraphSuite.groupEdges</li></div><div><li>org.apache.spark.graphx.GraphSuite.aggregateMessages</li></div><div><li>org.apache.spark.graphx.GraphSuite.outerJoinVertices</li></div><div><li>org.apache.spark.graphx.GraphSuite.more edge partitions than vertex partitions</li></div><div><li>org.apache.spark.graphx.GraphSuite.checkpoint</li></div><div><li>org.apache.spark.graphx.GraphSuite.non-default number of edge partitions</li></div><div><li>org.apache.spark.graphx.GraphSuite.unpersist graph RDD</li></div><div><li>org.apache.spark.graphx.GraphSuite.SPARK-14219: pickRandomVertex</li></div><div><li>org.apache.spark.graphx.PregelSuite.1 iteration</li></div><div><li>org.apache.spark.graphx.PregelSuite.chain propagation</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.filter</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.mapValues</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.minus</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.minus with RDD[(VertexId, VD)]</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.minus with non-equal number of partitions</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.diff</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.diff with RDD[(VertexId, VD)]</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.diff vertices with non-equal number of partitions</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.leftJoin</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.leftJoin vertices with non-equal number of partitions</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.innerJoin</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.innerJoin vertices with the non-equal number of partitions</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.aggregateUsingIndex</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.mergeFunc</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.cache, getStorageLevel</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.checkpoint</li></div><div><li>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Grid Connected Components</li></div><div><li>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Reverse Grid Connected Components</li></div><div><li>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Chain Connected Components</li></div><div><li>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Reverse Chain Connected Components</li></div><div><li>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Connected Components on a Toy Connected Graph</li></div><div><li>org.apache.spark.graphx.lib.LabelPropagationSuite.Label Propagation</li></div><div><li>org.apache.spark.graphx.lib.PageRankSuite.Star PageRank</li></div><div><li>org.apache.spark.graphx.lib.PageRankSuite.Star PersonalPageRank</li></div><div><li>org.apache.spark.graphx.lib.PageRankSuite.Grid PageRank</li></div><div><li>org.apache.spark.graphx.lib.PageRankSuite.Chain PageRank</li></div><div><li>org.apache.spark.graphx.lib.PageRankSuite.Chain PersonalizedPageRank</li></div><div><li>org.apache.spark.graphx.lib.PageRankSuite.Loop with source PageRank</li></div><div><li>org.apache.spark.graphx.lib.PageRankSuite.Loop with sink PageRank</li></div><div><li>org.apache.spark.graphx.lib.SVDPlusPlusSuite.Test SVD++ with mean square error on training set</li></div><div><li>org.apache.spark.graphx.lib.ShortestPathsSuite.Shortest Path Computations</li></div><div><li>org.apache.spark.graphx.lib.StronglyConnectedComponentsSuite.Island Strongly Connected Components</li></div><div><li>org.apache.spark.graphx.lib.StronglyConnectedComponentsSuite.Cycle Strongly Connected Components</li></div><div><li>org.apache.spark.graphx.lib.StronglyConnectedComponentsSuite.2 Cycle Strongly Connected Components</li></div><div><li>org.apache.spark.graphx.lib.TriangleCountSuite.Count a single triangle</li></div><div><li>org.apache.spark.graphx.lib.TriangleCountSuite.Count two triangles</li></div><div><li>org.apache.spark.graphx.lib.TriangleCountSuite.Count two triangles with bi-directed edges</li></div><div><li>org.apache.spark.graphx.lib.TriangleCountSuite.Count a single triangle with duplicate edges</li></div><div><li>org.apache.spark.graphx.util.GraphGeneratorsSuite.GraphGenerators.logNormalGraph</li></div><div><li>org.apache.spark.graphx.util.GraphGeneratorsSuite.SPARK-5064 GraphGenerators.rmatGraph numEdges upper bound</li></div><div><li>org.apache.spark.graphx.util.PeriodicGraphCheckpointerSuite.Persisting</li></div><div><li>org.apache.spark.graphx.util.PeriodicGraphCheckpointerSuite.Checkpointing</li></div><div><li>org.apache.spark.ml.JavaPipelineSuite.pipeline</li></div><div><li>org.apache.spark.ml.JavaPipelineSuite.pipeline</li></div><div><li>org.apache.spark.ml.classification.JavaDecisionTreeClassifierSuite.runDT</li></div><div><li>org.apache.spark.ml.classification.JavaDecisionTreeClassifierSuite.runDT</li></div><div><li>org.apache.spark.ml.classification.JavaGBTClassifierSuite.runDT</li></div><div><li>org.apache.spark.ml.classification.JavaGBTClassifierSuite.runDT</li></div><div><li>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionWithSetters</li></div><div><li>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionWithSetters</li></div><div><li>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionTrainingSummary</li></div><div><li>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionTrainingSummary</li></div><div><li>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionPredictorClassifierMethods</li></div><div><li>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionPredictorClassifierMethods</li></div><div><li>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionDefaultParams</li></div><div><li>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionDefaultParams</li></div><div><li>org.apache.spark.ml.classification.JavaMultilayerPerceptronClassifierSuite.testMLPC</li></div><div><li>org.apache.spark.ml.classification.JavaMultilayerPerceptronClassifierSuite.testMLPC</li></div><div><li>org.apache.spark.ml.classification.JavaNaiveBayesSuite.testNaiveBayes</li></div><div><li>org.apache.spark.ml.classification.JavaNaiveBayesSuite.naiveBayesDefaultParams</li></div><div><li>org.apache.spark.ml.classification.JavaNaiveBayesSuite.naiveBayesDefaultParams</li></div><div><li>org.apache.spark.ml.classification.JavaOneVsRestSuite.oneVsRestDefaultParams</li></div><div><li>org.apache.spark.ml.classification.JavaOneVsRestSuite.oneVsRestDefaultParams</li></div><div><li>org.apache.spark.ml.classification.JavaRandomForestClassifierSuite.runDT</li></div><div><li>org.apache.spark.ml.classification.JavaRandomForestClassifierSuite.runDT</li></div><div><li>org.apache.spark.ml.clustering.JavaKMeansSuite.fitAndTransform</li></div><div><li>org.apache.spark.ml.clustering.JavaKMeansSuite.fitAndTransform</li></div><div><li>org.apache.spark.ml.feature.JavaBucketizerSuite.bucketizerTest</li></div><div><li>org.apache.spark.ml.feature.JavaBucketizerSuite.bucketizerMultipleColumnsTest</li></div><div><li>org.apache.spark.ml.feature.JavaBucketizerSuite.bucketizerMultipleColumnsTest</li></div><div><li>org.apache.spark.ml.feature.JavaDCTSuite.javaCompatibilityTest</li></div><div><li>org.apache.spark.ml.feature.JavaDCTSuite.javaCompatibilityTest</li></div><div><li>org.apache.spark.ml.feature.JavaHashingTFSuite.hashingTF</li></div><div><li>org.apache.spark.ml.feature.JavaHashingTFSuite.hashingTF</li></div><div><li>org.apache.spark.ml.feature.JavaNormalizerSuite.normalizer</li></div><div><li>org.apache.spark.ml.feature.JavaPCASuite.testPCA</li></div><div><li>org.apache.spark.ml.feature.JavaPCASuite.testPCA</li></div><div><li>org.apache.spark.ml.feature.JavaPolynomialExpansionSuite.polynomialExpansionTest</li></div><div><li>org.apache.spark.ml.feature.JavaStandardScalerSuite.standardScaler</li></div><div><li>org.apache.spark.ml.feature.JavaStandardScalerSuite.standardScaler</li></div><div><li>org.apache.spark.ml.feature.JavaStopWordsRemoverSuite.javaCompatibilityTest</li></div><div><li>org.apache.spark.ml.feature.JavaStopWordsRemoverSuite.javaCompatibilityTest</li></div><div><li>org.apache.spark.ml.feature.JavaStringIndexerSuite.testStringIndexer</li></div><div><li>org.apache.spark.ml.feature.JavaStringIndexerSuite.testStringIndexer</li></div><div><li>org.apache.spark.ml.feature.JavaTokenizerSuite.regexTokenizer</li></div><div><li>org.apache.spark.ml.feature.JavaTokenizerSuite.regexTokenizer</li></div><div><li>org.apache.spark.ml.feature.JavaVectorAssemblerSuite.testVectorAssembler</li></div><div><li>org.apache.spark.ml.feature.JavaVectorAssemblerSuite.testVectorAssembler</li></div><div><li>org.apache.spark.ml.feature.JavaVectorIndexerSuite.vectorIndexerAPI</li></div><div><li>org.apache.spark.ml.feature.JavaVectorIndexerSuite.vectorIndexerAPI</li></div><div><li>org.apache.spark.ml.feature.JavaVectorSlicerSuite.vectorSlice</li></div><div><li>org.apache.spark.ml.feature.JavaVectorSlicerSuite.vectorSlice</li></div><div><li>org.apache.spark.ml.feature.JavaWord2VecSuite.testJavaWord2Vec</li></div><div><li>org.apache.spark.ml.regression.JavaDecisionTreeRegressorSuite.runDT</li></div><div><li>org.apache.spark.ml.regression.JavaDecisionTreeRegressorSuite.runDT</li></div><div><li>org.apache.spark.ml.regression.JavaGBTRegressorSuite.runDT</li></div><div><li>org.apache.spark.ml.regression.JavaGBTRegressorSuite.runDT</li></div><div><li>org.apache.spark.ml.regression.JavaLinearRegressionSuite.linearRegressionDefaultParams</li></div><div><li>org.apache.spark.ml.regression.JavaLinearRegressionSuite.linearRegressionDefaultParams</li></div><div><li>org.apache.spark.ml.regression.JavaLinearRegressionSuite.linearRegressionWithSetters</li></div><div><li>org.apache.spark.ml.regression.JavaLinearRegressionSuite.linearRegressionWithSetters</li></div><div><li>org.apache.spark.ml.regression.JavaRandomForestRegressorSuite.runDT</li></div><div><li>org.apache.spark.ml.regression.JavaRandomForestRegressorSuite.runDT</li></div><div><li>org.apache.spark.ml.source.libsvm.JavaLibSVMRelationSuite.verifyLibSVMDF</li></div><div><li>org.apache.spark.ml.stat.JavaKolmogorovSmirnovTestSuite.testKSTestNamedDistribution</li></div><div><li>org.apache.spark.ml.stat.JavaKolmogorovSmirnovTestSuite.testKSTestNamedDistribution</li></div><div><li>org.apache.spark.ml.stat.JavaKolmogorovSmirnovTestSuite.testKSTestCDF</li></div><div><li>org.apache.spark.ml.stat.JavaKolmogorovSmirnovTestSuite.testKSTestCDF</li></div><div><li>org.apache.spark.ml.stat.JavaSummarizerSuite.testSummarizer</li></div><div><li>org.apache.spark.ml.stat.JavaSummarizerSuite.testSummarizer</li></div><div><li>org.apache.spark.ml.tuning.JavaCrossValidatorSuite.crossValidationWithLogisticRegression</li></div><div><li>org.apache.spark.ml.tuning.JavaCrossValidatorSuite.crossValidationWithLogisticRegression</li></div><div><li>org.apache.spark.ml.util.JavaDefaultReadWriteSuite.testDefaultReadWrite</li></div><div><li>org.apache.spark.ml.util.JavaDefaultReadWriteSuite.testDefaultReadWrite</li></div><div><li>org.apache.spark.mllib.classification.JavaLogisticRegressionSuite.runLRUsingConstructor</li></div><div><li>org.apache.spark.mllib.classification.JavaLogisticRegressionSuite.runLRUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.classification.JavaLogisticRegressionSuite.runLRUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.testPredictJavaRDD</li></div><div><li>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.testPredictJavaRDD</li></div><div><li>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.runUsingConstructor</li></div><div><li>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.runUsingConstructor</li></div><div><li>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.runUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.runUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.testModelTypeSetters</li></div><div><li>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.testModelTypeSetters</li></div><div><li>org.apache.spark.mllib.classification.JavaSVMSuite.runSVMUsingConstructor</li></div><div><li>org.apache.spark.mllib.classification.JavaSVMSuite.runSVMUsingConstructor</li></div><div><li>org.apache.spark.mllib.classification.JavaSVMSuite.runSVMUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.classification.JavaSVMSuite.runSVMUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.classification.JavaStreamingLogisticRegressionSuite.javaAPI</li></div><div><li>org.apache.spark.mllib.classification.JavaStreamingLogisticRegressionSuite.javaAPI</li></div><div><li>org.apache.spark.mllib.clustering.JavaBisectingKMeansSuite.twoDimensionalData</li></div><div><li>org.apache.spark.mllib.clustering.JavaBisectingKMeansSuite.twoDimensionalData</li></div><div><li>org.apache.spark.mllib.clustering.JavaGaussianMixtureSuite.runGaussianMixture</li></div><div><li>org.apache.spark.mllib.clustering.JavaGaussianMixtureSuite.runGaussianMixture</li></div><div><li>org.apache.spark.mllib.clustering.JavaKMeansSuite.testPredictJavaRDD</li></div><div><li>org.apache.spark.mllib.clustering.JavaKMeansSuite.runKMeansUsingConstructor</li></div><div><li>org.apache.spark.mllib.clustering.JavaKMeansSuite.runKMeansUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.clustering.JavaKMeansSuite.runKMeansUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.clustering.JavaLDASuite.onlineOptimizerCompatibility</li></div><div><li>org.apache.spark.mllib.clustering.JavaLDASuite.onlineOptimizerCompatibility</li></div><div><li>org.apache.spark.mllib.clustering.JavaLDASuite.distributedLDAModel</li></div><div><li>org.apache.spark.mllib.clustering.JavaLDASuite.distributedLDAModel</li></div><div><li>org.apache.spark.mllib.clustering.JavaLDASuite.localLDAModel</li></div><div><li>org.apache.spark.mllib.clustering.JavaLDASuite.localLDAModel</li></div><div><li>org.apache.spark.mllib.clustering.JavaLDASuite.localLdaMethods</li></div><div><li>org.apache.spark.mllib.clustering.JavaLDASuite.localLdaMethods</li></div><div><li>org.apache.spark.mllib.clustering.JavaStreamingKMeansSuite.javaAPI</li></div><div><li>org.apache.spark.mllib.evaluation.JavaRankingMetricsSuite.rankingMetrics</li></div><div><li>org.apache.spark.mllib.evaluation.JavaRankingMetricsSuite.rankingMetrics</li></div><div><li>org.apache.spark.mllib.feature.JavaTfIdfSuite.tfIdfMinimumDocumentFrequency</li></div><div><li>org.apache.spark.mllib.feature.JavaTfIdfSuite.tfIdfMinimumDocumentFrequency</li></div><div><li>org.apache.spark.mllib.feature.JavaTfIdfSuite.tfIdf</li></div><div><li>org.apache.spark.mllib.feature.JavaTfIdfSuite.tfIdf</li></div><div><li>org.apache.spark.mllib.feature.JavaWord2VecSuite.word2Vec</li></div><div><li>org.apache.spark.mllib.feature.JavaWord2VecSuite.word2Vec</li></div><div><li>org.apache.spark.mllib.fpm.JavaAssociationRulesSuite.runAssociationRules</li></div><div><li>org.apache.spark.mllib.fpm.JavaAssociationRulesSuite.runAssociationRules</li></div><div><li>org.apache.spark.mllib.fpm.JavaFPGrowthSuite.runFPGrowthSaveLoad</li></div><div><li>org.apache.spark.mllib.fpm.JavaFPGrowthSuite.runFPGrowthSaveLoad</li></div><div><li>org.apache.spark.mllib.fpm.JavaFPGrowthSuite.runFPGrowth</li></div><div><li>org.apache.spark.mllib.fpm.JavaPrefixSpanSuite.runPrefixSpan</li></div><div><li>org.apache.spark.mllib.fpm.JavaPrefixSpanSuite.runPrefixSpan</li></div><div><li>org.apache.spark.mllib.fpm.JavaPrefixSpanSuite.runPrefixSpanSaveLoad</li></div><div><li>org.apache.spark.mllib.fpm.JavaPrefixSpanSuite.runPrefixSpanSaveLoad</li></div><div><li>org.apache.spark.mllib.linalg.distributed.JavaRowMatrixSuite.rowMatrixQRDecomposition</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testNormalVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testNormalVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testArbitrary</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testArbitrary</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testLogNormalVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testLogNormalVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testExponentialVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testExponentialVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testUniformRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testUniformRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testRandomVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testRandomVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testGammaRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testGammaRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testUniformVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testUniformVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testPoissonRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testPoissonRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testNormalRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testNormalRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testPoissonVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testPoissonVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testGammaVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testGammaVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testExponentialRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testExponentialRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testLNormalRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testLNormalRDD</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runALSUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runALSUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSUsingConstructor</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSUsingConstructor</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runRecommend</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runRecommend</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSWithNegativeWeight</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSWithNegativeWeight</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runALSUsingConstructor</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runALSUsingConstructor</li></div><div><li>org.apache.spark.mllib.regression.JavaIsotonicRegressionSuite.testIsotonicRegressionJavaRDD</li></div><div><li>org.apache.spark.mllib.regression.JavaIsotonicRegressionSuite.testIsotonicRegressionJavaRDD</li></div><div><li>org.apache.spark.mllib.regression.JavaIsotonicRegressionSuite.testIsotonicRegressionPredictionsJavaRDD</li></div><div><li>org.apache.spark.mllib.regression.JavaIsotonicRegressionSuite.testIsotonicRegressionPredictionsJavaRDD</li></div><div><li>org.apache.spark.mllib.regression.JavaLassoSuite.runLassoUsingConstructor</li></div><div><li>org.apache.spark.mllib.regression.JavaLassoSuite.runLassoUsingConstructor</li></div><div><li>org.apache.spark.mllib.regression.JavaLassoSuite.runLassoUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.testPredictJavaRDD</li></div><div><li>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.runLinearRegressionUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.runLinearRegressionUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.runLinearRegressionUsingConstructor</li></div><div><li>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.runLinearRegressionUsingConstructor</li></div><div><li>org.apache.spark.mllib.regression.JavaRidgeRegressionSuite.runRidgeRegressionUsingConstructor</li></div><div><li>org.apache.spark.mllib.regression.JavaRidgeRegressionSuite.runRidgeRegressionUsingConstructor</li></div><div><li>org.apache.spark.mllib.regression.JavaRidgeRegressionSuite.runRidgeRegressionUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.regression.JavaRidgeRegressionSuite.runRidgeRegressionUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.regression.JavaStreamingLinearRegressionSuite.javaAPI</li></div><div><li>org.apache.spark.mllib.regression.JavaStreamingLinearRegressionSuite.javaAPI</li></div><div><li>org.apache.spark.mllib.stat.JavaStatisticsSuite.testCorr</li></div><div><li>org.apache.spark.mllib.stat.JavaStatisticsSuite.testCorr</li></div><div><li>org.apache.spark.mllib.stat.JavaStatisticsSuite.chiSqTest</li></div><div><li>org.apache.spark.mllib.stat.JavaStatisticsSuite.chiSqTest</li></div><div><li>org.apache.spark.mllib.stat.JavaStatisticsSuite.streamingTest</li></div><div><li>org.apache.spark.mllib.stat.JavaStatisticsSuite.streamingTest</li></div><div><li>org.apache.spark.mllib.stat.JavaStatisticsSuite.kolmogorovSmirnovTest</li></div><div><li>org.apache.spark.mllib.stat.JavaStatisticsSuite.kolmogorovSmirnovTest</li></div><div><li>org.apache.spark.mllib.tree.JavaDecisionTreeSuite.runDTUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.tree.JavaDecisionTreeSuite.runDTUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.tree.JavaDecisionTreeSuite.runDTUsingConstructor</li></div><div><li>org.apache.spark.mllib.tree.JavaDecisionTreeSuite.runDTUsingConstructor</li></div><div><li>org.apache.spark.mllib.util.JavaMLUtilsSuite.testConvertMatrixColumnsToAndFromML</li></div><div><li>org.apache.spark.mllib.util.JavaMLUtilsSuite.testConvertMatrixColumnsToAndFromML</li></div><div><li>org.apache.spark.mllib.util.JavaMLUtilsSuite.testConvertVectorColumnsToAndFromML</li></div><div><li>org.apache.spark.mllib.util.JavaMLUtilsSuite.testConvertVectorColumnsToAndFromML</li></div><div><li>org.apache.spark.repl.ReplSuite.SPARK-15236: use Hive catalog</li></div><div><li>org.apache.spark.repl.ReplSuite.SPARK-15236: use in-memory catalog</li></div><div><li>org.apache.spark.repl.ReplSuite.broadcast vars</li></div><div><li>org.apache.spark.repl.ReplSuite.line wrapper only initialized once when used as encoder outer scope</li></div><div><li>org.apache.spark.repl.ReplSuite.define case class and create Dataset together with paste mode</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.simple foreach with accumulator</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.external vars</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.external classes</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.external functions</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.external functions that access vars</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.broadcast vars</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.interacting with files</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.local-cluster mode</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.SPARK-2576 importing implicits</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.Datasets and encoders</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.SPARK-2632 importing a method from non serializable class and not using it.</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.collecting objects of class defined in repl</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.collecting objects of class defined in repl - shuffling</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.replicating blocks of object with class defined in repl</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.newProductSeqEncoder with REPL defined class</li></div><div><li>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationCount</li></div><div><li>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationCount</li></div><div><li>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationSumDouble</li></div><div><li>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationSumDouble</li></div><div><li>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationSumLong</li></div><div><li>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationSumLong</li></div><div><li>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationAverage</li></div><div><li>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationAverage</li></div><div><li>test.org.apache.spark.sql.JavaApplySchemaSuite.applySchema</li></div><div><li>test.org.apache.spark.sql.JavaApplySchemaSuite.applySchema</li></div><div><li>test.org.apache.spark.sql.JavaApplySchemaSuite.dataFrameRDDOperations</li></div><div><li>test.org.apache.spark.sql.JavaApplySchemaSuite.dataFrameRDDOperations</li></div><div><li>test.org.apache.spark.sql.JavaApplySchemaSuite.applySchemaToJSON</li></div><div><li>test.org.apache.spark.sql.JavaApplySchemaSuite.applySchemaToJSON</li></div><div><li>test.org.apache.spark.sql.JavaBeanDeserializationSuite.testBeanWithArrayFieldDeserialization</li></div><div><li>test.org.apache.spark.sql.JavaBeanDeserializationSuite.testBeanWithMapFieldsDeserialization</li></div><div><li>test.org.apache.spark.sql.JavaColumnExpressionSuite.isInCollectionCheckExceptionMessage</li></div><div><li>test.org.apache.spark.sql.JavaColumnExpressionSuite.isInCollectionCheckExceptionMessage</li></div><div><li>test.org.apache.spark.sql.JavaColumnExpressionSuite.isInCollectionWorksCorrectlyOnJava</li></div><div><li>test.org.apache.spark.sql.JavaColumnExpressionSuite.isInCollectionWorksCorrectlyOnJava</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testFormatAPI</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testTextAPI</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testJsonAPI</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testLoadAPI</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testOptionsAPI</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testSaveModeAPI</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testCsvAPI</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testParquetAPI</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testTextFileAPI</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCollectAndTake</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCollectAndTake</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testJsonRDDToDataFrame</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testJsonRDDToDataFrame</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testVarargMethods</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testVarargMethods</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testBeanWithoutGetter</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testBeanWithoutGetter</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateStructTypeFromList</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateStructTypeFromList</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testSampleBy</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testSampleBy</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCrosstab</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCrosstab</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testUDF</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testUDF</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFromFromList</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFromFromList</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCircularReferenceBean</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCircularReferenceBean</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testFrequentItems</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testFrequentItems</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testSampleByColumn</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testSampleByColumn</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testExecution</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testExecution</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testTextLoad</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testTextLoad</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.pivot</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.pivot</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testGenericLoad</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testGenericLoad</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCountMinSketch</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCountMinSketch</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.pivotColumnValues</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.pivotColumnValues</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFrameFromJavaBeans</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFrameFromJavaBeans</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCorrelation</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCorrelation</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testBloomFilter</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testBloomFilter</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCovariance</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCovariance</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFrameFromLocalJavaBeans</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFrameFromLocalJavaBeans</li></div><div><li>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationCount</li></div><div><li>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationCount</li></div><div><li>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationSumDouble</li></div><div><li>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationSumDouble</li></div><div><li>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationSumLong</li></div><div><li>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationSumLong</li></div><div><li>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationAnonClass</li></div><div><li>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationAnonClass</li></div><div><li>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationAverage</li></div><div><li>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationAverage</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testRuntimeNullabilityCheck</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean1</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean1</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean2</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean2</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean3</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean3</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testSerializeNull</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testSerializeNull</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testRandomSplit</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testRandomSplit</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testTypedFilterPreservingSchema</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testTypedFilterPreservingSchema</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJoin</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJoin</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testTake</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testTake</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testToLocalIterator</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testToLocalIterator</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testSpecificLists</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testSpecificLists</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testForeach</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testForeach</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJavaEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJavaEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testPrimitiveEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testPrimitiveEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testEmptyBean</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testEmptyBean</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testCommonOperation</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testCommonOperation</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testNullInTopLevelBean</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testNullInTopLevelBean</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testGroupBy</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testGroupBy</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testSetOperation</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testBeanWithEnum</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testKryoEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.test</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.test</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJavaBeanEncoder2</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJavaBeanEncoder2</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testCollect</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testCollect</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testKryoEncoderErrorMessageForPrivateClass</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testKryoEncoderErrorMessageForPrivateClass</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJavaBeanEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJavaBeanEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testTupleEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testTupleEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testNestedTupleEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testNestedTupleEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testTupleEncoderSchema</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testTupleEncoderSchema</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testReduce</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testReduce</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testSelect</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testSelect</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJavaEncoderErrorMessageForPrivateClass</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJavaEncoderErrorMessageForPrivateClass</li></div><div><li>test.org.apache.spark.sql.JavaSaveLoadSuite.saveAndLoadWithSchema</li></div><div><li>test.org.apache.spark.sql.JavaSaveLoadSuite.saveAndLoadWithSchema</li></div><div><li>test.org.apache.spark.sql.JavaSaveLoadSuite.saveAndLoad</li></div><div><li>test.org.apache.spark.sql.JavaSaveLoadSuite.saveAndLoad</li></div><div><li>test.org.apache.spark.sql.JavaUDAFSuite.udf1Test</li></div><div><li>test.org.apache.spark.sql.JavaUDAFSuite.udf1Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf1Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf1Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf2Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf2Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf3Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf3Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf4Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf4Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf5Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf5Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf6Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf6Test</li></div><div><li>test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.testForeachBatchAPI</li></div><div><li>test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.testForeachBatchAPI</li></div><div><li>test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.testForeachAPI</li></div><div><li>test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.testForeachAPI</li></div><div><li>org.apache.spark.sql.hive.HiveMetastoreLazyInitializationSuite.lazily initialize Hive client</li></div><div><li>org.apache.spark.sql.hive.JavaDataFrameSuite.testUDAF</li></div><div><li>org.apache.spark.sql.hive.JavaDataFrameSuite.saveTableAndQueryIt</li></div><div><li>org.apache.spark.sql.hive.JavaMetastoreDataSourcesSuite.saveExternalTableAndQueryIt</li></div><div><li>org.apache.spark.sql.hive.JavaMetastoreDataSourcesSuite.saveTableAndQueryIt</li></div><div><li>org.apache.spark.sql.hive.JavaMetastoreDataSourcesSuite.saveExternalTableWithSchemaAndQueryIt</li></div><div><li>org.apache.spark.streaming.JavaMapWithStateSuite.testBasicFunction</li></div><div><li>org.apache.spark.streaming.JavaMapWithStateSuite.testBasicFunction</li></div><div><li>org.apache.spark.streaming.JavaReceiverAPISuite.testReceiver</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testStreamingContextTransform</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testStreamingContextTransform</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testFlatMapValues</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testFlatMapValues</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testMapPartitions</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testMapPartitions</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairFilter</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairFilter</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testCombineByKey</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testCombineByKey</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testMap</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testMap</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairToNormalRDDTransform</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairToNormalRDDTransform</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairReduceByKey</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairReduceByKey</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairMap</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairMap</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testFlatMap</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testFlatMap</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testReduceByKeyAndWindowWithInverse</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testReduceByKeyAndWindowWithInverse</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testReduceByWindow</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testReduceByWindow</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairFlatMap</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairFlatMap</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairToPairFlatMapWithChangingTypes</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairToPairFlatMapWithChangingTypes</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairMapPartitions</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairMapPartitions</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testVariousTransform</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testVariousTransform</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testTransformWith</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testTransformWith</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testVariousTransformWith</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testVariousTransformWith</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testReduceByKeyAndWindow</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testReduceByKeyAndWindow</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairTransform</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairTransform</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testFilter</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testFilter</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairMap2</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairMap2</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testMapValues</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testMapValues</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testReduce</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testReduce</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testUpdateStateByKey</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testUpdateStateByKey</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testTransform</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testTransform</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testStreamingContextTransform</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testFlatMapValues</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testReduceByWindowWithInverse</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testMapPartitions</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairFilter</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairFilter</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testRepartitionFewerPartitions</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testRepartitionFewerPartitions</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCombineByKey</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCombineByKey</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testContextGetOrCreate</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testContextGetOrCreate</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testWindowWithSlideDuration</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testWindowWithSlideDuration</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testQueueStream</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testQueueStream</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCountByValue</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCountByValue</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testMap</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testMap</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairToNormalRDDTransform</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairToNormalRDDTransform</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairReduceByKey</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairReduceByKey</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCount</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCount</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCheckpointMasterRecovery</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCheckpointMasterRecovery</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairMap</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testUnion</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testUnion</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testFlatMap</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testFlatMap</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testReduceByKeyAndWindowWithInverse</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testReduceByKeyAndWindowWithInverse</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testGlom</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testJoin</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testJoin</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairFlatMap</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairFlatMap</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairToPairFlatMapWithChangingTypes</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairToPairFlatMapWithChangingTypes</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairMapPartitions</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairMapPartitions</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testRepartitionMorePartitions</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testRepartitionMorePartitions</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testReduceByWindowWithoutInverse</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testReduceByWindowWithoutInverse</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testLeftOuterJoin</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testLeftOuterJoin</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testVariousTransform</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testVariousTransform</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testTransformWith</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testTransformWith</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testVariousTransformWith</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testVariousTransformWith</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testTextFileStream</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testTextFileStream</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairGroupByKey</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairGroupByKey</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCoGroup</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCoGroup</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testInitialization</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testInitialization</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testSocketString</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testSocketString</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testGroupByKeyAndWindow</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testGroupByKeyAndWindow</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testReduceByKeyAndWindow</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testReduceByKeyAndWindow</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testForeachRDD</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testForeachRDD</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testFileStream</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testFileStream</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairTransform</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairTransform</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testFilter</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testFilter</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairMap2</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairMap2</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testMapValues</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testMapValues</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testReduce</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testReduce</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testUpdateStateByKey</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testUpdateStateByKey</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testTransform</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testTransform</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testWindow</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testWindow</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCountByValueAndWindow</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCountByValueAndWindow</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testSocketTextStream</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testSocketTextStream</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testUpdateStateByKeyWithInitial</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testUpdateStateByKeyWithInitial</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testContextState</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testContextState</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexDescendingWithStart</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexWithStart</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndexDescendingWithStart</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexDescending</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexWithStart</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexWithLast</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexWithSkip</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexWithMax</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexDescending</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndexDescendingWithLast</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexDescending</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexDescendingWithLast</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndex</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndexWithLast</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexWithStart</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexDescendingWithStart</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexWithLast</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexWithSkip</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndexDescending</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.testRefWithIntNaturalKey</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexDescending</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexDescendingWithStart</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexWithMax</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndex</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexWithLast</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexWithSkip</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexWithMax</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexDescendingWithLast</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexDescendingWithLast</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexDescendingWithStart</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndex</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexWithLast</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexWithSkip</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexWithStart</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndex</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexDescendingWithLast</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndexWithStart</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndex</li></div><div><li>org.apache.spark.util.kvstore.LevelDBSuite.testMultipleTypesWriteReadDelete</li></div><div><li>org.apache.spark.util.kvstore.LevelDBSuite.testObjectWriteReadDelete</li></div><div><li>org.apache.spark.util.kvstore.LevelDBSuite.testSkip</li></div><div><li>org.apache.spark.util.kvstore.LevelDBSuite.testMultipleObjectWriteReadDelete</li></div><div><li>org.apache.spark.util.kvstore.LevelDBSuite.testReopenAndVersionCheckDb</li></div><div><li>org.apache.spark.util.kvstore.LevelDBSuite.testMetadata</li></div><div><li>org.apache.spark.util.kvstore.LevelDBSuite.testUpdate</li></div><div><li>org.apache.spark.util.kvstore.LevelDBSuite.testNegativeIndexValues</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.spark.network.ChunkFetchIntegrationSuite.fetchFileChunk</li></div><div><li>org.apache.spark.JavaJdbcRDDSuite.testJavaJdbcRDD</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.leftOuterJoin</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.foldReduce</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.mapsFromPairsToPairs</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.flatMap</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.foreach</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.map</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.map</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.zip</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.zip</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.keyBy</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.keyBy</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.groupBy</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.groupBy</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.mapPartitions</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.mapPartitions</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.foldByKey</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.foldByKey</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.mapOnPairRDD</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.mapOnPairRDD</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.sequenceFile</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.sequenceFile</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.collectPartitions</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.collectPartitions</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.reduceByKey</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.reduceByKey</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.foreachWithAnonymousClass</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.foreachWithAnonymousClass</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.collectAsMapWithIntArrayValues</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.collectAsMapWithIntArrayValues</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.zipPartitions</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.zipPartitions</li></div><div><li>test.org.apache.spark.JavaAPISuite.groupByOnPairRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.binaryFilesCaching</li></div><div><li>test.org.apache.spark.JavaAPISuite.sparkContextUnion</li></div><div><li>test.org.apache.spark.JavaAPISuite.checkpointAndComputation</li></div><div><li>test.org.apache.spark.JavaAPISuite.checkpointAndComputation</li></div><div><li>test.org.apache.spark.JavaAPISuite.leftOuterJoin</li></div><div><li>test.org.apache.spark.JavaAPISuite.leftOuterJoin</li></div><div><li>test.org.apache.spark.JavaAPISuite.keyByOnPairRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.keyByOnPairRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.getNumPartitions</li></div><div><li>test.org.apache.spark.JavaAPISuite.getNumPartitions</li></div><div><li>test.org.apache.spark.JavaAPISuite.wholeTextFiles</li></div><div><li>test.org.apache.spark.JavaAPISuite.wholeTextFiles</li></div><div><li>test.org.apache.spark.JavaAPISuite.binaryFiles</li></div><div><li>test.org.apache.spark.JavaAPISuite.binaryFiles</li></div><div><li>test.org.apache.spark.JavaAPISuite.foldReduce</li></div><div><li>test.org.apache.spark.JavaAPISuite.foldReduce</li></div><div><li>test.org.apache.spark.JavaAPISuite.writeWithNewAPIHadoopFile</li></div><div><li>test.org.apache.spark.JavaAPISuite.writeWithNewAPIHadoopFile</li></div><div><li>test.org.apache.spark.JavaAPISuite.hadoopFile</li></div><div><li>test.org.apache.spark.JavaAPISuite.hadoopFile</li></div><div><li>test.org.apache.spark.JavaAPISuite.lookup</li></div><div><li>test.org.apache.spark.JavaAPISuite.lookup</li></div><div><li>test.org.apache.spark.JavaAPISuite.countAsync</li></div><div><li>test.org.apache.spark.JavaAPISuite.countAsync</li></div><div><li>test.org.apache.spark.JavaAPISuite.textFiles</li></div><div><li>test.org.apache.spark.JavaAPISuite.textFiles</li></div><div><li>test.org.apache.spark.JavaAPISuite.binaryRecords</li></div><div><li>test.org.apache.spark.JavaAPISuite.binaryRecords</li></div><div><li>test.org.apache.spark.JavaAPISuite.toLocalIterator</li></div><div><li>test.org.apache.spark.JavaAPISuite.toLocalIterator</li></div><div><li>test.org.apache.spark.JavaAPISuite.repartitionAndSortWithinPartitions</li></div><div><li>test.org.apache.spark.JavaAPISuite.repartitionAndSortWithinPartitions</li></div><div><li>test.org.apache.spark.JavaAPISuite.reduce</li></div><div><li>test.org.apache.spark.JavaAPISuite.reduce</li></div><div><li>test.org.apache.spark.JavaAPISuite.sample</li></div><div><li>test.org.apache.spark.JavaAPISuite.sample</li></div><div><li>test.org.apache.spark.JavaAPISuite.sortBy</li></div><div><li>test.org.apache.spark.JavaAPISuite.sortBy</li></div><div><li>test.org.apache.spark.JavaAPISuite.mapsFromPairsToPairs</li></div><div><li>test.org.apache.spark.JavaAPISuite.mapsFromPairsToPairs</li></div><div><li>test.org.apache.spark.JavaAPISuite.flatMap</li></div><div><li>test.org.apache.spark.JavaAPISuite.flatMap</li></div><div><li>test.org.apache.spark.JavaAPISuite.cogroup3</li></div><div><li>test.org.apache.spark.JavaAPISuite.cogroup3</li></div><div><li>test.org.apache.spark.JavaAPISuite.cogroup4</li></div><div><li>test.org.apache.spark.JavaAPISuite.cogroup4</li></div><div><li>test.org.apache.spark.JavaAPISuite.randomSplit</li></div><div><li>test.org.apache.spark.JavaAPISuite.randomSplit</li></div><div><li>test.org.apache.spark.JavaAPISuite.persist</li></div><div><li>test.org.apache.spark.JavaAPISuite.persist</li></div><div><li>test.org.apache.spark.JavaAPISuite.foreach</li></div><div><li>test.org.apache.spark.JavaAPISuite.foreach</li></div><div><li>test.org.apache.spark.JavaAPISuite.hadoopFileCompressed</li></div><div><li>test.org.apache.spark.JavaAPISuite.hadoopFileCompressed</li></div><div><li>test.org.apache.spark.JavaAPISuite.textFilesCompressed</li></div><div><li>test.org.apache.spark.JavaAPISuite.checkpointAndRestore</li></div><div><li>test.org.apache.spark.JavaAPISuite.checkpointAndRestore</li></div><div><li>test.org.apache.spark.JavaAPISuite.sortByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.sortByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.aggregateByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.aggregateByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.map</li></div><div><li>test.org.apache.spark.JavaAPISuite.map</li></div><div><li>test.org.apache.spark.JavaAPISuite.max</li></div><div><li>test.org.apache.spark.JavaAPISuite.max</li></div><div><li>test.org.apache.spark.JavaAPISuite.min</li></div><div><li>test.org.apache.spark.JavaAPISuite.min</li></div><div><li>test.org.apache.spark.JavaAPISuite.top</li></div><div><li>test.org.apache.spark.JavaAPISuite.top</li></div><div><li>test.org.apache.spark.JavaAPISuite.zip</li></div><div><li>test.org.apache.spark.JavaAPISuite.zip</li></div><div><li>test.org.apache.spark.JavaAPISuite.fold</li></div><div><li>test.org.apache.spark.JavaAPISuite.fold</li></div><div><li>test.org.apache.spark.JavaAPISuite.glom</li></div><div><li>test.org.apache.spark.JavaAPISuite.glom</li></div><div><li>test.org.apache.spark.JavaAPISuite.take</li></div><div><li>test.org.apache.spark.JavaAPISuite.take</li></div><div><li>test.org.apache.spark.JavaAPISuite.javaDoubleRDDHistoGram</li></div><div><li>test.org.apache.spark.JavaAPISuite.collectUnderlyingScalaRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.collectUnderlyingScalaRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.keyBy</li></div><div><li>test.org.apache.spark.JavaAPISuite.keyBy</li></div><div><li>test.org.apache.spark.JavaAPISuite.mapPartitionsWithIndex</li></div><div><li>test.org.apache.spark.JavaAPISuite.mapPartitionsWithIndex</li></div><div><li>test.org.apache.spark.JavaAPISuite.sampleByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.sampleByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.intersection</li></div><div><li>test.org.apache.spark.JavaAPISuite.intersection</li></div><div><li>test.org.apache.spark.JavaAPISuite.aggregate</li></div><div><li>test.org.apache.spark.JavaAPISuite.aggregate</li></div><div><li>test.org.apache.spark.JavaAPISuite.cartesian</li></div><div><li>test.org.apache.spark.JavaAPISuite.cartesian</li></div><div><li>test.org.apache.spark.JavaAPISuite.countApproxDistinctByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.countApproxDistinctByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.readWithNewAPIHadoopFile</li></div><div><li>test.org.apache.spark.JavaAPISuite.readWithNewAPIHadoopFile</li></div><div><li>test.org.apache.spark.JavaAPISuite.testRegisterKryoClasses</li></div><div><li>test.org.apache.spark.JavaAPISuite.testRegisterKryoClasses</li></div><div><li>test.org.apache.spark.JavaAPISuite.groupBy</li></div><div><li>test.org.apache.spark.JavaAPISuite.groupBy</li></div><div><li>test.org.apache.spark.JavaAPISuite.sampleByKeyExact</li></div><div><li>test.org.apache.spark.JavaAPISuite.mapPartitions</li></div><div><li>test.org.apache.spark.JavaAPISuite.takeOrdered</li></div><div><li>test.org.apache.spark.JavaAPISuite.takeOrdered</li></div><div><li>test.org.apache.spark.JavaAPISuite.foldByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.foldByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.objectFilesOfInts</li></div><div><li>test.org.apache.spark.JavaAPISuite.objectFilesOfInts</li></div><div><li>test.org.apache.spark.JavaAPISuite.treeAggregate</li></div><div><li>test.org.apache.spark.JavaAPISuite.treeAggregate</li></div><div><li>test.org.apache.spark.JavaAPISuite.testGetPersistentRDDs</li></div><div><li>test.org.apache.spark.JavaAPISuite.testGetPersistentRDDs</li></div><div><li>test.org.apache.spark.JavaAPISuite.approximateResults</li></div><div><li>test.org.apache.spark.JavaAPISuite.approximateResults</li></div><div><li>test.org.apache.spark.JavaAPISuite.treeReduce</li></div><div><li>test.org.apache.spark.JavaAPISuite.treeReduce</li></div><div><li>test.org.apache.spark.JavaAPISuite.collectAsMapAndSerialize</li></div><div><li>test.org.apache.spark.JavaAPISuite.collectAsMapAndSerialize</li></div><div><li>test.org.apache.spark.JavaAPISuite.countApproxDistinct</li></div><div><li>test.org.apache.spark.JavaAPISuite.countApproxDistinct</li></div><div><li>test.org.apache.spark.JavaAPISuite.javaDoubleRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.javaDoubleRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.mapOnPairRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.mapOnPairRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.testAsyncActionErrorWrapping</li></div><div><li>test.org.apache.spark.JavaAPISuite.testAsyncActionErrorWrapping</li></div><div><li>test.org.apache.spark.JavaAPISuite.naturalMax</li></div><div><li>test.org.apache.spark.JavaAPISuite.naturalMax</li></div><div><li>test.org.apache.spark.JavaAPISuite.naturalMin</li></div><div><li>test.org.apache.spark.JavaAPISuite.naturalMin</li></div><div><li>test.org.apache.spark.JavaAPISuite.sequenceFile</li></div><div><li>test.org.apache.spark.JavaAPISuite.sequenceFile</li></div><div><li>test.org.apache.spark.JavaAPISuite.collectPartitions</li></div><div><li>test.org.apache.spark.JavaAPISuite.collectPartitions</li></div><div><li>test.org.apache.spark.JavaAPISuite.cogroup</li></div><div><li>test.org.apache.spark.JavaAPISuite.cogroup</li></div><div><li>test.org.apache.spark.JavaAPISuite.reduceByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.reduceByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.repartition</li></div><div><li>test.org.apache.spark.JavaAPISuite.repartition</li></div><div><li>test.org.apache.spark.JavaAPISuite.iterator</li></div><div><li>test.org.apache.spark.JavaAPISuite.iterator</li></div><div><li>test.org.apache.spark.JavaAPISuite.emptyRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.emptyRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.zipWithIndex</li></div><div><li>test.org.apache.spark.JavaAPISuite.zipWithIndex</li></div><div><li>test.org.apache.spark.JavaAPISuite.foreachPartition</li></div><div><li>test.org.apache.spark.JavaAPISuite.foreachPartition</li></div><div><li>test.org.apache.spark.JavaAPISuite.combineByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.combineByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.takeAsync</li></div><div><li>test.org.apache.spark.JavaAPISuite.takeAsync</li></div><div><li>test.org.apache.spark.JavaAPISuite.collectAsMapWithIntArrayValues</li></div><div><li>test.org.apache.spark.JavaAPISuite.collectAsMapWithIntArrayValues</li></div><div><li>test.org.apache.spark.JavaAPISuite.objectFilesOfComplexTypes</li></div><div><li>test.org.apache.spark.JavaAPISuite.objectFilesOfComplexTypes</li></div><div><li>test.org.apache.spark.JavaAPISuite.zipWithUniqueId</li></div><div><li>test.org.apache.spark.JavaAPISuite.zipWithUniqueId</li></div><div><li>test.org.apache.spark.JavaAPISuite.collectAsync</li></div><div><li>test.org.apache.spark.JavaAPISuite.collectAsync</li></div><div><li>test.org.apache.spark.JavaAPISuite.foreachAsync</li></div><div><li>test.org.apache.spark.JavaAPISuite.foreachAsync</li></div><div><li>test.org.apache.spark.JavaAPISuite.zipPartitions</li></div><div><li>test.org.apache.spark.JavaAPISuite.zipPartitions</li></div><div><li>test.org.apache.spark.JavaAPISuite.reduceOnJavaDoubleRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.reduceOnJavaDoubleRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.isEmpty</li></div><div><li>test.org.apache.spark.JavaAPISuite.isEmpty</li></div><div><li>test.org.apache.spark.JavaSparkContextSuite.scalaSparkContext</li></div><div><li>org.apache.spark.sql.avro.AvroFunctionsSuite.roundtrip in to_avro and from_avro - int and string</li></div><div><li>org.apache.spark.sql.avro.AvroFunctionsSuite.roundtrip in to_avro and from_avro - struct</li></div><div><li>org.apache.spark.sql.avro.AvroFunctionsSuite.roundtrip in to_avro and from_avro - array with null</li></div><div><li>org.apache.spark.streaming.kafka010.JavaDirectKafkaStreamSuite.testKafkaStream</li></div><div><li>org.apache.spark.streaming.kafka010.JavaKafkaRDDSuite.testKafkaRDD</li></div><div><li>org.apache.spark.graphx.EdgeRDDSuite.cache, getStorageLevel</li></div><div><li>org.apache.spark.graphx.EdgeRDDSuite.checkpointing</li></div><div><li>org.apache.spark.graphx.GraphLoaderSuite.GraphLoader.edgeListFile</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.joinVertices</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.collectNeighborIds</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.removeSelfEdges</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.filter</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.convertToCanonicalEdges</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.collectEdgesCycleDirectionOut</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.collectEdgesCycleDirectionIn</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.collectEdgesCycleDirectionEither</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.collectEdgesChainDirectionOut</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.collectEdgesChainDirectionIn</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.collectEdgesChainDirectionEither</li></div><div><li>org.apache.spark.graphx.GraphSuite.Graph.fromEdgeTuples</li></div><div><li>org.apache.spark.graphx.GraphSuite.Graph.fromEdges</li></div><div><li>org.apache.spark.graphx.GraphSuite.Graph.apply</li></div><div><li>org.apache.spark.graphx.GraphSuite.triplets</li></div><div><li>org.apache.spark.graphx.GraphSuite.partitionBy</li></div><div><li>org.apache.spark.graphx.GraphSuite.mapVertices</li></div><div><li>org.apache.spark.graphx.GraphSuite.mapVertices changing type with same erased type</li></div><div><li>org.apache.spark.graphx.GraphSuite.mapEdges</li></div><div><li>org.apache.spark.graphx.GraphSuite.mapTriplets</li></div><div><li>org.apache.spark.graphx.GraphSuite.reverse</li></div><div><li>org.apache.spark.graphx.GraphSuite.reverse with join elimination</li></div><div><li>org.apache.spark.graphx.GraphSuite.subgraph</li></div><div><li>org.apache.spark.graphx.GraphSuite.mask</li></div><div><li>org.apache.spark.graphx.GraphSuite.groupEdges</li></div><div><li>org.apache.spark.graphx.GraphSuite.aggregateMessages</li></div><div><li>org.apache.spark.graphx.GraphSuite.outerJoinVertices</li></div><div><li>org.apache.spark.graphx.GraphSuite.more edge partitions than vertex partitions</li></div><div><li>org.apache.spark.graphx.GraphSuite.checkpoint</li></div><div><li>org.apache.spark.graphx.GraphSuite.cache, getStorageLevel</li></div><div><li>org.apache.spark.graphx.GraphSuite.non-default number of edge partitions</li></div><div><li>org.apache.spark.graphx.GraphSuite.unpersist graph RDD</li></div><div><li>org.apache.spark.graphx.GraphSuite.SPARK-14219: pickRandomVertex</li></div><div><li>org.apache.spark.graphx.PregelSuite.1 iteration</li></div><div><li>org.apache.spark.graphx.PregelSuite.chain propagation</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.filter</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.mapValues</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.minus</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.minus with RDD[(VertexId, VD)]</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.minus with non-equal number of partitions</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.diff</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.diff with RDD[(VertexId, VD)]</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.diff vertices with non-equal number of partitions</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.leftJoin</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.leftJoin vertices with non-equal number of partitions</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.innerJoin</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.innerJoin vertices with the non-equal number of partitions</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.aggregateUsingIndex</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.mergeFunc</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.cache, getStorageLevel</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.checkpoint</li></div><div><li>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Grid Connected Components</li></div><div><li>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Reverse Grid Connected Components</li></div><div><li>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Chain Connected Components</li></div><div><li>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Reverse Chain Connected Components</li></div><div><li>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Connected Components on a Toy Connected Graph</li></div><div><li>org.apache.spark.graphx.lib.LabelPropagationSuite.Label Propagation</li></div><div><li>org.apache.spark.graphx.lib.PageRankSuite.Star PageRank</li></div><div><li>org.apache.spark.graphx.lib.PageRankSuite.Star PersonalPageRank</li></div><div><li>org.apache.spark.graphx.lib.PageRankSuite.Grid PageRank</li></div><div><li>org.apache.spark.graphx.lib.PageRankSuite.Chain PageRank</li></div><div><li>org.apache.spark.graphx.lib.PageRankSuite.Chain PersonalizedPageRank</li></div><div><li>org.apache.spark.graphx.lib.PageRankSuite.Loop with source PageRank</li></div><div><li>org.apache.spark.graphx.lib.PageRankSuite.Loop with sink PageRank</li></div><div><li>org.apache.spark.graphx.lib.SVDPlusPlusSuite.Test SVD++ with mean square error on training set</li></div><div><li>org.apache.spark.graphx.lib.ShortestPathsSuite.Shortest Path Computations</li></div><div><li>org.apache.spark.graphx.lib.StronglyConnectedComponentsSuite.Island Strongly Connected Components</li></div><div><li>org.apache.spark.graphx.lib.StronglyConnectedComponentsSuite.Cycle Strongly Connected Components</li></div><div><li>org.apache.spark.graphx.lib.StronglyConnectedComponentsSuite.2 Cycle Strongly Connected Components</li></div><div><li>org.apache.spark.graphx.lib.TriangleCountSuite.Count a single triangle</li></div><div><li>org.apache.spark.graphx.lib.TriangleCountSuite.Count two triangles</li></div><div><li>org.apache.spark.graphx.lib.TriangleCountSuite.Count two triangles with bi-directed edges</li></div><div><li>org.apache.spark.graphx.lib.TriangleCountSuite.Count a single triangle with duplicate edges</li></div><div><li>org.apache.spark.graphx.util.GraphGeneratorsSuite.GraphGenerators.logNormalGraph</li></div><div><li>org.apache.spark.graphx.util.GraphGeneratorsSuite.SPARK-5064 GraphGenerators.rmatGraph numEdges upper bound</li></div><div><li>org.apache.spark.graphx.util.PeriodicGraphCheckpointerSuite.Persisting</li></div><div><li>org.apache.spark.graphx.util.PeriodicGraphCheckpointerSuite.Checkpointing</li></div><div><li>org.apache.spark.ml.JavaPipelineSuite.pipeline</li></div><div><li>org.apache.spark.ml.JavaPipelineSuite.pipeline</li></div><div><li>org.apache.spark.ml.classification.JavaDecisionTreeClassifierSuite.runDT</li></div><div><li>org.apache.spark.ml.classification.JavaDecisionTreeClassifierSuite.runDT</li></div><div><li>org.apache.spark.ml.classification.JavaGBTClassifierSuite.runDT</li></div><div><li>org.apache.spark.ml.classification.JavaGBTClassifierSuite.runDT</li></div><div><li>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionWithSetters</li></div><div><li>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionWithSetters</li></div><div><li>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionTrainingSummary</li></div><div><li>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionTrainingSummary</li></div><div><li>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionPredictorClassifierMethods</li></div><div><li>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionPredictorClassifierMethods</li></div><div><li>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionDefaultParams</li></div><div><li>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionDefaultParams</li></div><div><li>org.apache.spark.ml.classification.JavaMultilayerPerceptronClassifierSuite.testMLPC</li></div><div><li>org.apache.spark.ml.classification.JavaMultilayerPerceptronClassifierSuite.testMLPC</li></div><div><li>org.apache.spark.ml.classification.JavaNaiveBayesSuite.testNaiveBayes</li></div><div><li>org.apache.spark.ml.classification.JavaNaiveBayesSuite.testNaiveBayes</li></div><div><li>org.apache.spark.ml.classification.JavaNaiveBayesSuite.naiveBayesDefaultParams</li></div><div><li>org.apache.spark.ml.classification.JavaNaiveBayesSuite.naiveBayesDefaultParams</li></div><div><li>org.apache.spark.ml.classification.JavaOneVsRestSuite.oneVsRestDefaultParams</li></div><div><li>org.apache.spark.ml.classification.JavaOneVsRestSuite.oneVsRestDefaultParams</li></div><div><li>org.apache.spark.ml.classification.JavaRandomForestClassifierSuite.runDT</li></div><div><li>org.apache.spark.ml.classification.JavaRandomForestClassifierSuite.runDT</li></div><div><li>org.apache.spark.ml.clustering.JavaKMeansSuite.fitAndTransform</li></div><div><li>org.apache.spark.ml.clustering.JavaKMeansSuite.fitAndTransform</li></div><div><li>org.apache.spark.ml.feature.JavaBucketizerSuite.bucketizerTest</li></div><div><li>org.apache.spark.ml.feature.JavaBucketizerSuite.bucketizerTest</li></div><div><li>org.apache.spark.ml.feature.JavaBucketizerSuite.bucketizerMultipleColumnsTest</li></div><div><li>org.apache.spark.ml.feature.JavaBucketizerSuite.bucketizerMultipleColumnsTest</li></div><div><li>org.apache.spark.ml.feature.JavaDCTSuite.javaCompatibilityTest</li></div><div><li>org.apache.spark.ml.feature.JavaDCTSuite.javaCompatibilityTest</li></div><div><li>org.apache.spark.ml.feature.JavaHashingTFSuite.hashingTF</li></div><div><li>org.apache.spark.ml.feature.JavaHashingTFSuite.hashingTF</li></div><div><li>org.apache.spark.ml.feature.JavaNormalizerSuite.normalizer</li></div><div><li>org.apache.spark.ml.feature.JavaNormalizerSuite.normalizer</li></div><div><li>org.apache.spark.ml.feature.JavaPCASuite.testPCA</li></div><div><li>org.apache.spark.ml.feature.JavaPCASuite.testPCA</li></div><div><li>org.apache.spark.ml.feature.JavaPolynomialExpansionSuite.polynomialExpansionTest</li></div><div><li>org.apache.spark.ml.feature.JavaPolynomialExpansionSuite.polynomialExpansionTest</li></div><div><li>org.apache.spark.ml.feature.JavaStandardScalerSuite.standardScaler</li></div><div><li>org.apache.spark.ml.feature.JavaStandardScalerSuite.standardScaler</li></div><div><li>org.apache.spark.ml.feature.JavaStopWordsRemoverSuite.javaCompatibilityTest</li></div><div><li>org.apache.spark.ml.feature.JavaStopWordsRemoverSuite.javaCompatibilityTest</li></div><div><li>org.apache.spark.ml.feature.JavaStringIndexerSuite.testStringIndexer</li></div><div><li>org.apache.spark.ml.feature.JavaStringIndexerSuite.testStringIndexer</li></div><div><li>org.apache.spark.ml.feature.JavaTokenizerSuite.regexTokenizer</li></div><div><li>org.apache.spark.ml.feature.JavaTokenizerSuite.regexTokenizer</li></div><div><li>org.apache.spark.ml.feature.JavaVectorAssemblerSuite.testVectorAssembler</li></div><div><li>org.apache.spark.ml.feature.JavaVectorAssemblerSuite.testVectorAssembler</li></div><div><li>org.apache.spark.ml.feature.JavaVectorIndexerSuite.vectorIndexerAPI</li></div><div><li>org.apache.spark.ml.feature.JavaVectorIndexerSuite.vectorIndexerAPI</li></div><div><li>org.apache.spark.ml.feature.JavaVectorSlicerSuite.vectorSlice</li></div><div><li>org.apache.spark.ml.feature.JavaVectorSlicerSuite.vectorSlice</li></div><div><li>org.apache.spark.ml.feature.JavaWord2VecSuite.testJavaWord2Vec</li></div><div><li>org.apache.spark.ml.feature.JavaWord2VecSuite.testJavaWord2Vec</li></div><div><li>org.apache.spark.ml.regression.JavaDecisionTreeRegressorSuite.runDT</li></div><div><li>org.apache.spark.ml.regression.JavaDecisionTreeRegressorSuite.runDT</li></div><div><li>org.apache.spark.ml.regression.JavaGBTRegressorSuite.runDT</li></div><div><li>org.apache.spark.ml.regression.JavaGBTRegressorSuite.runDT</li></div><div><li>org.apache.spark.ml.regression.JavaLinearRegressionSuite.linearRegressionDefaultParams</li></div><div><li>org.apache.spark.ml.regression.JavaLinearRegressionSuite.linearRegressionDefaultParams</li></div><div><li>org.apache.spark.ml.regression.JavaLinearRegressionSuite.linearRegressionWithSetters</li></div><div><li>org.apache.spark.ml.regression.JavaLinearRegressionSuite.linearRegressionWithSetters</li></div><div><li>org.apache.spark.ml.regression.JavaRandomForestRegressorSuite.runDT</li></div><div><li>org.apache.spark.ml.source.libsvm.JavaLibSVMRelationSuite.verifyLibSVMDF</li></div><div><li>org.apache.spark.ml.source.libsvm.JavaLibSVMRelationSuite.verifyLibSVMDF</li></div><div><li>org.apache.spark.ml.stat.JavaKolmogorovSmirnovTestSuite.testKSTestNamedDistribution</li></div><div><li>org.apache.spark.ml.stat.JavaKolmogorovSmirnovTestSuite.testKSTestCDF</li></div><div><li>org.apache.spark.ml.stat.JavaSummarizerSuite.testSummarizer</li></div><div><li>org.apache.spark.ml.stat.JavaSummarizerSuite.testSummarizer</li></div><div><li>org.apache.spark.ml.tuning.JavaCrossValidatorSuite.crossValidationWithLogisticRegression</li></div><div><li>org.apache.spark.ml.tuning.JavaCrossValidatorSuite.crossValidationWithLogisticRegression</li></div><div><li>org.apache.spark.ml.util.JavaDefaultReadWriteSuite.testDefaultReadWrite</li></div><div><li>org.apache.spark.ml.util.JavaDefaultReadWriteSuite.testDefaultReadWrite</li></div><div><li>org.apache.spark.mllib.classification.JavaLogisticRegressionSuite.runLRUsingConstructor</li></div><div><li>org.apache.spark.mllib.classification.JavaLogisticRegressionSuite.runLRUsingConstructor</li></div><div><li>org.apache.spark.mllib.classification.JavaLogisticRegressionSuite.runLRUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.classification.JavaLogisticRegressionSuite.runLRUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.testPredictJavaRDD</li></div><div><li>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.testPredictJavaRDD</li></div><div><li>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.runUsingConstructor</li></div><div><li>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.runUsingConstructor</li></div><div><li>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.runUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.runUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.testModelTypeSetters</li></div><div><li>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.testModelTypeSetters</li></div><div><li>org.apache.spark.mllib.classification.JavaSVMSuite.runSVMUsingConstructor</li></div><div><li>org.apache.spark.mllib.classification.JavaSVMSuite.runSVMUsingConstructor</li></div><div><li>org.apache.spark.mllib.classification.JavaSVMSuite.runSVMUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.classification.JavaSVMSuite.runSVMUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.classification.JavaStreamingLogisticRegressionSuite.javaAPI</li></div><div><li>org.apache.spark.mllib.classification.JavaStreamingLogisticRegressionSuite.javaAPI</li></div><div><li>org.apache.spark.mllib.clustering.JavaBisectingKMeansSuite.twoDimensionalData</li></div><div><li>org.apache.spark.mllib.clustering.JavaBisectingKMeansSuite.twoDimensionalData</li></div><div><li>org.apache.spark.mllib.clustering.JavaGaussianMixtureSuite.runGaussianMixture</li></div><div><li>org.apache.spark.mllib.clustering.JavaGaussianMixtureSuite.runGaussianMixture</li></div><div><li>org.apache.spark.mllib.clustering.JavaKMeansSuite.testPredictJavaRDD</li></div><div><li>org.apache.spark.mllib.clustering.JavaKMeansSuite.runKMeansUsingConstructor</li></div><div><li>org.apache.spark.mllib.clustering.JavaKMeansSuite.runKMeansUsingConstructor</li></div><div><li>org.apache.spark.mllib.clustering.JavaKMeansSuite.runKMeansUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.clustering.JavaKMeansSuite.runKMeansUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.clustering.JavaLDASuite.onlineOptimizerCompatibility</li></div><div><li>org.apache.spark.mllib.clustering.JavaLDASuite.onlineOptimizerCompatibility</li></div><div><li>org.apache.spark.mllib.clustering.JavaLDASuite.distributedLDAModel</li></div><div><li>org.apache.spark.mllib.clustering.JavaLDASuite.distributedLDAModel</li></div><div><li>org.apache.spark.mllib.clustering.JavaLDASuite.localLDAModel</li></div><div><li>org.apache.spark.mllib.clustering.JavaLDASuite.localLDAModel</li></div><div><li>org.apache.spark.mllib.clustering.JavaLDASuite.localLdaMethods</li></div><div><li>org.apache.spark.mllib.clustering.JavaLDASuite.localLdaMethods</li></div><div><li>org.apache.spark.mllib.clustering.JavaStreamingKMeansSuite.javaAPI</li></div><div><li>org.apache.spark.mllib.evaluation.JavaRankingMetricsSuite.rankingMetrics</li></div><div><li>org.apache.spark.mllib.evaluation.JavaRankingMetricsSuite.rankingMetrics</li></div><div><li>org.apache.spark.mllib.feature.JavaTfIdfSuite.tfIdfMinimumDocumentFrequency</li></div><div><li>org.apache.spark.mllib.feature.JavaTfIdfSuite.tfIdfMinimumDocumentFrequency</li></div><div><li>org.apache.spark.mllib.feature.JavaTfIdfSuite.tfIdf</li></div><div><li>org.apache.spark.mllib.feature.JavaTfIdfSuite.tfIdf</li></div><div><li>org.apache.spark.mllib.feature.JavaWord2VecSuite.word2Vec</li></div><div><li>org.apache.spark.mllib.feature.JavaWord2VecSuite.word2Vec</li></div><div><li>org.apache.spark.mllib.fpm.JavaAssociationRulesSuite.runAssociationRules</li></div><div><li>org.apache.spark.mllib.fpm.JavaAssociationRulesSuite.runAssociationRules</li></div><div><li>org.apache.spark.mllib.fpm.JavaFPGrowthSuite.runFPGrowthSaveLoad</li></div><div><li>org.apache.spark.mllib.fpm.JavaFPGrowthSuite.runFPGrowthSaveLoad</li></div><div><li>org.apache.spark.mllib.fpm.JavaFPGrowthSuite.runFPGrowth</li></div><div><li>org.apache.spark.mllib.fpm.JavaFPGrowthSuite.runFPGrowth</li></div><div><li>org.apache.spark.mllib.fpm.JavaPrefixSpanSuite.runPrefixSpan</li></div><div><li>org.apache.spark.mllib.fpm.JavaPrefixSpanSuite.runPrefixSpan</li></div><div><li>org.apache.spark.mllib.fpm.JavaPrefixSpanSuite.runPrefixSpanSaveLoad</li></div><div><li>org.apache.spark.mllib.fpm.JavaPrefixSpanSuite.runPrefixSpanSaveLoad</li></div><div><li>org.apache.spark.mllib.linalg.distributed.JavaRowMatrixSuite.rowMatrixQRDecomposition</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testNormalVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testArbitrary</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testArbitrary</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testLogNormalVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testLogNormalVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testExponentialVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testExponentialVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testUniformRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testUniformRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testRandomVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testRandomVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testGammaRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testGammaRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testUniformVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testUniformVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testPoissonRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testPoissonRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testNormalRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testNormalRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testPoissonVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testPoissonVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testGammaVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testGammaVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testExponentialRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testExponentialRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testLNormalRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testLNormalRDD</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runALSUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runALSUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSUsingConstructor</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSUsingConstructor</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runRecommend</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runRecommend</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSWithNegativeWeight</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSWithNegativeWeight</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runALSUsingConstructor</li></div><div><li>org.apache.spark.mllib.regression.JavaIsotonicRegressionSuite.testIsotonicRegressionJavaRDD</li></div><div><li>org.apache.spark.mllib.regression.JavaIsotonicRegressionSuite.testIsotonicRegressionPredictionsJavaRDD</li></div><div><li>org.apache.spark.mllib.regression.JavaLassoSuite.runLassoUsingConstructor</li></div><div><li>org.apache.spark.mllib.regression.JavaLassoSuite.runLassoUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.testPredictJavaRDD</li></div><div><li>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.testPredictJavaRDD</li></div><div><li>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.runLinearRegressionUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.runLinearRegressionUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.runLinearRegressionUsingConstructor</li></div><div><li>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.runLinearRegressionUsingConstructor</li></div><div><li>org.apache.spark.mllib.regression.JavaRidgeRegressionSuite.runRidgeRegressionUsingConstructor</li></div><div><li>org.apache.spark.mllib.regression.JavaRidgeRegressionSuite.runRidgeRegressionUsingConstructor</li></div><div><li>org.apache.spark.mllib.regression.JavaRidgeRegressionSuite.runRidgeRegressionUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.regression.JavaRidgeRegressionSuite.runRidgeRegressionUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.regression.JavaStreamingLinearRegressionSuite.javaAPI</li></div><div><li>org.apache.spark.mllib.stat.JavaStatisticsSuite.testCorr</li></div><div><li>org.apache.spark.mllib.stat.JavaStatisticsSuite.testCorr</li></div><div><li>org.apache.spark.mllib.stat.JavaStatisticsSuite.chiSqTest</li></div><div><li>org.apache.spark.mllib.stat.JavaStatisticsSuite.chiSqTest</li></div><div><li>org.apache.spark.mllib.stat.JavaStatisticsSuite.streamingTest</li></div><div><li>org.apache.spark.mllib.stat.JavaStatisticsSuite.streamingTest</li></div><div><li>org.apache.spark.mllib.stat.JavaStatisticsSuite.kolmogorovSmirnovTest</li></div><div><li>org.apache.spark.mllib.stat.JavaStatisticsSuite.kolmogorovSmirnovTest</li></div><div><li>org.apache.spark.mllib.tree.JavaDecisionTreeSuite.runDTUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.tree.JavaDecisionTreeSuite.runDTUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.tree.JavaDecisionTreeSuite.runDTUsingConstructor</li></div><div><li>org.apache.spark.mllib.tree.JavaDecisionTreeSuite.runDTUsingConstructor</li></div><div><li>org.apache.spark.mllib.util.JavaMLUtilsSuite.testConvertMatrixColumnsToAndFromML</li></div><div><li>org.apache.spark.mllib.util.JavaMLUtilsSuite.testConvertVectorColumnsToAndFromML</li></div><div><li>org.apache.spark.mllib.util.JavaMLUtilsSuite.testConvertVectorColumnsToAndFromML</li></div><div><li>org.apache.spark.repl.ReplSuite.SPARK-15236: use Hive catalog</li></div><div><li>org.apache.spark.repl.ReplSuite.SPARK-15236: use in-memory catalog</li></div><div><li>org.apache.spark.repl.ReplSuite.broadcast vars</li></div><div><li>org.apache.spark.repl.ReplSuite.line wrapper only initialized once when used as encoder outer scope</li></div><div><li>org.apache.spark.repl.ReplSuite.define case class and create Dataset together with paste mode</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.simple foreach with accumulator</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.external vars</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.external classes</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.external functions</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.external functions that access vars</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.broadcast vars</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.interacting with files</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.local-cluster mode</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.SPARK-2576 importing implicits</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.Datasets and encoders</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.SPARK-2632 importing a method from non serializable class and not using it.</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.collecting objects of class defined in repl</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.collecting objects of class defined in repl - shuffling</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.replicating blocks of object with class defined in repl</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.newProductSeqEncoder with REPL defined class</li></div><div><li>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationCount</li></div><div><li>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationCount</li></div><div><li>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationSumDouble</li></div><div><li>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationSumDouble</li></div><div><li>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationSumLong</li></div><div><li>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationSumLong</li></div><div><li>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationAverage</li></div><div><li>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationAverage</li></div><div><li>test.org.apache.spark.sql.JavaApplySchemaSuite.applySchema</li></div><div><li>test.org.apache.spark.sql.JavaApplySchemaSuite.applySchema</li></div><div><li>test.org.apache.spark.sql.JavaApplySchemaSuite.dataFrameRDDOperations</li></div><div><li>test.org.apache.spark.sql.JavaApplySchemaSuite.dataFrameRDDOperations</li></div><div><li>test.org.apache.spark.sql.JavaApplySchemaSuite.applySchemaToJSON</li></div><div><li>test.org.apache.spark.sql.JavaApplySchemaSuite.applySchemaToJSON</li></div><div><li>test.org.apache.spark.sql.JavaBeanDeserializationSuite.testBeanWithArrayFieldDeserialization</li></div><div><li>test.org.apache.spark.sql.JavaBeanDeserializationSuite.testBeanWithArrayFieldDeserialization</li></div><div><li>test.org.apache.spark.sql.JavaBeanDeserializationSuite.testBeanWithMapFieldsDeserialization</li></div><div><li>test.org.apache.spark.sql.JavaBeanDeserializationSuite.testBeanWithMapFieldsDeserialization</li></div><div><li>test.org.apache.spark.sql.JavaColumnExpressionSuite.isInCollectionCheckExceptionMessage</li></div><div><li>test.org.apache.spark.sql.JavaColumnExpressionSuite.isInCollectionWorksCorrectlyOnJava</li></div><div><li>test.org.apache.spark.sql.JavaColumnExpressionSuite.isInCollectionWorksCorrectlyOnJava</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testFormatAPI</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testTextAPI</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testJsonAPI</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testLoadAPI</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testOptionsAPI</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testSaveModeAPI</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testCsvAPI</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testParquetAPI</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testTextFileAPI</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCollectAndTake</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCollectAndTake</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testJsonRDDToDataFrame</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testJsonRDDToDataFrame</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testVarargMethods</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testVarargMethods</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testBeanWithoutGetter</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testBeanWithoutGetter</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateStructTypeFromList</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateStructTypeFromList</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testSampleBy</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testSampleBy</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCrosstab</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCrosstab</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testUDF</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testUDF</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFromFromList</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFromFromList</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCircularReferenceBean</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCircularReferenceBean</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testFrequentItems</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testFrequentItems</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testSampleByColumn</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testSampleByColumn</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testExecution</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testExecution</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testTextLoad</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testTextLoad</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.pivot</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.pivot</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testGenericLoad</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testGenericLoad</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCountMinSketch</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCountMinSketch</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.pivotColumnValues</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.pivotColumnValues</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFrameFromJavaBeans</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFrameFromJavaBeans</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCorrelation</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCorrelation</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testBloomFilter</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCovariance</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCovariance</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFrameFromLocalJavaBeans</li></div><div><li>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationCount</li></div><div><li>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationSumDouble</li></div><div><li>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationSumLong</li></div><div><li>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationSumLong</li></div><div><li>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationAnonClass</li></div><div><li>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationAnonClass</li></div><div><li>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationAverage</li></div><div><li>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationAverage</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testRuntimeNullabilityCheck</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean1</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean1</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean2</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean2</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean3</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean3</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testSerializeNull</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testSerializeNull</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testRandomSplit</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testRandomSplit</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testTypedFilterPreservingSchema</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testTypedFilterPreservingSchema</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJoin</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJoin</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testTake</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testTake</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testToLocalIterator</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testToLocalIterator</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testSpecificLists</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testSpecificLists</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testForeach</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testForeach</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJavaEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJavaEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testPrimitiveEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testPrimitiveEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testEmptyBean</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testEmptyBean</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testCommonOperation</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testCommonOperation</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testNullInTopLevelBean</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testNullInTopLevelBean</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testGroupBy</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testGroupBy</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testSetOperation</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testSetOperation</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testBeanWithEnum</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testBeanWithEnum</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testKryoEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testKryoEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.test</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.test</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJavaBeanEncoder2</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJavaBeanEncoder2</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testCollect</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testCollect</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testKryoEncoderErrorMessageForPrivateClass</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testKryoEncoderErrorMessageForPrivateClass</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJavaBeanEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJavaBeanEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testTupleEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testTupleEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testNestedTupleEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testNestedTupleEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testTupleEncoderSchema</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testTupleEncoderSchema</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testReduce</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testReduce</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testSelect</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testSelect</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJavaEncoderErrorMessageForPrivateClass</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJavaEncoderErrorMessageForPrivateClass</li></div><div><li>test.org.apache.spark.sql.JavaSaveLoadSuite.saveAndLoadWithSchema</li></div><div><li>test.org.apache.spark.sql.JavaSaveLoadSuite.saveAndLoadWithSchema</li></div><div><li>test.org.apache.spark.sql.JavaSaveLoadSuite.saveAndLoad</li></div><div><li>test.org.apache.spark.sql.JavaSaveLoadSuite.saveAndLoad</li></div><div><li>test.org.apache.spark.sql.JavaUDAFSuite.udf1Test</li></div><div><li>test.org.apache.spark.sql.JavaUDAFSuite.udf1Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf1Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf1Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf2Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf2Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf3Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf3Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf4Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf4Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf5Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf5Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf6Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf6Test</li></div><div><li>test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.testForeachBatchAPI</li></div><div><li>test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.testForeachBatchAPI</li></div><div><li>test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.testForeachAPI</li></div><div><li>test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.testForeachAPI</li></div><div><li>org.apache.spark.sql.hive.HiveMetastoreLazyInitializationSuite.lazily initialize Hive client</li></div><div><li>org.apache.spark.sql.hive.JavaDataFrameSuite.testUDAF</li></div><div><li>org.apache.spark.sql.hive.JavaDataFrameSuite.saveTableAndQueryIt</li></div><div><li>org.apache.spark.sql.hive.JavaMetastoreDataSourcesSuite.saveExternalTableAndQueryIt</li></div><div><li>org.apache.spark.sql.hive.JavaMetastoreDataSourcesSuite.saveTableAndQueryIt</li></div><div><li>org.apache.spark.sql.hive.JavaMetastoreDataSourcesSuite.saveExternalTableWithSchemaAndQueryIt</li></div><div><li>org.apache.spark.streaming.JavaMapWithStateSuite.testBasicFunction</li></div><div><li>org.apache.spark.streaming.JavaMapWithStateSuite.testBasicFunction</li></div><div><li>org.apache.spark.streaming.JavaReceiverAPISuite.testReceiver</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testStreamingContextTransform</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testFlatMapValues</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testFlatMapValues</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testMapPartitions</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testMapPartitions</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairFilter</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairFilter</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testCombineByKey</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testCombineByKey</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testMap</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testMap</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairToNormalRDDTransform</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairToNormalRDDTransform</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairReduceByKey</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairReduceByKey</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairMap</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairMap</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testFlatMap</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testFlatMap</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testReduceByKeyAndWindowWithInverse</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testReduceByKeyAndWindowWithInverse</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testReduceByWindow</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testReduceByWindow</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairFlatMap</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairFlatMap</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairToPairFlatMapWithChangingTypes</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairMapPartitions</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testVariousTransform</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testVariousTransform</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testTransformWith</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testTransformWith</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testVariousTransformWith</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testVariousTransformWith</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testReduceByKeyAndWindow</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testReduceByKeyAndWindow</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairTransform</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairTransform</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testFilter</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testFilter</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairMap2</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairMap2</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testMapValues</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testReduce</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testUpdateStateByKey</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testTransform</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testStreamingContextTransform</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testFlatMapValues</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testReduceByWindowWithInverse</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testMapPartitions</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairFilter</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairFilter</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testRepartitionFewerPartitions</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testRepartitionFewerPartitions</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCombineByKey</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCombineByKey</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testContextGetOrCreate</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testContextGetOrCreate</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testWindowWithSlideDuration</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testWindowWithSlideDuration</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testQueueStream</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testQueueStream</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCountByValue</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCountByValue</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testMap</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testMap</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairToNormalRDDTransform</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairToNormalRDDTransform</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairReduceByKey</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairReduceByKey</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCount</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCount</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCheckpointMasterRecovery</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCheckpointMasterRecovery</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairMap</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairMap</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testUnion</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testUnion</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testFlatMap</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testFlatMap</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testReduceByKeyAndWindowWithInverse</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testReduceByKeyAndWindowWithInverse</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testGlom</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testJoin</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testJoin</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairFlatMap</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairFlatMap</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairToPairFlatMapWithChangingTypes</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairToPairFlatMapWithChangingTypes</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairMapPartitions</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairMapPartitions</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testRepartitionMorePartitions</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testRepartitionMorePartitions</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testReduceByWindowWithoutInverse</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testReduceByWindowWithoutInverse</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testLeftOuterJoin</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testLeftOuterJoin</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testVariousTransform</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testVariousTransform</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testTransformWith</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testTransformWith</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testVariousTransformWith</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testVariousTransformWith</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testTextFileStream</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testTextFileStream</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairGroupByKey</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairGroupByKey</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCoGroup</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCoGroup</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testSocketString</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testSocketString</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testGroupByKeyAndWindow</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testGroupByKeyAndWindow</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testReduceByKeyAndWindow</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testReduceByKeyAndWindow</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testForeachRDD</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testForeachRDD</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testFileStream</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testFileStream</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairTransform</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairTransform</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testFilter</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testFilter</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairMap2</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairMap2</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testMapValues</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testMapValues</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testReduce</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testReduce</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testUpdateStateByKey</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testUpdateStateByKey</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testTransform</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testTransform</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testWindow</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testWindow</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCountByValueAndWindow</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCountByValueAndWindow</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testRawSocketStream</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testRawSocketStream</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testUpdateStateByKeyWithInitial</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testUpdateStateByKeyWithInitial</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testContextState</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testContextState</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>Could not load library. Reasons: [no leveldbjni64-1.8 in java.library.path, no leveldbjni-1.8 in java.library.path, no leveldbjni in java.library.path, /var/lib/jenkins/workspace/spark/common/kvstore/target/tmp/libleveldbjni-64-1-7739308900962398917.8: /var/lib/jenkins/workspace/spark/common/kvstore/target/tmp/libleveldbjni-64-1-7739308900962398917.8: cannot open shared object file: No such file o</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>&amp;#010;Error adding data: The code passed to eventually never returned normally. Attempted 3949 times over 1.0002407833166667 minutes. Last failure message: assertion failed: Partition [stress4, 5] metadata not propagated after timeout.&amp;#010;org.scalatest.concurrent.Eventually$class.tryTryAgain$1(Eventually.scala:421)&amp;#010; org.scalatest.concurrent.Eventually$class.eventually(Eventually.scala:439)&amp;</li></div><div><li>Timeout of './bin/spark-submit' '--class' 'org.apache.spark.sql.hive.SparkSQLConfTest' '--name' 'SparkSQLConfTest' '--master' 'local-cluster[2,1,1024]' '--conf' 'spark.ui.enabled=false' '--conf' 'spark.master.rest.enabled=false' '--conf' 'spark.sql.hive.metastore.version=0.12' '--conf' 'spark.sql.hive.metastore.jars=maven' '--driver-java-options' '-Dderby.system.durability=test' 'file:/var/lib/jen</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Timeout getting response from the server</li></div><div><li>SparkContext has been shutdown</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:91)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:46)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:91)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:46)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:91)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:46)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:91)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:46)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:91)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:46)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:91)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:46)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:91)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:46)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:91)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:46)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:91)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:46)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:91)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:46)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:91)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:46)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:91)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:46)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:91)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:46)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:91)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:46)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:91)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:46)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:91)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:46)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:91)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:46)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:91)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:46)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:91)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:46)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:91)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:46)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:91)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:46)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:91)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:46)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAcce</li></div><div><li>SparkContext has been shutdown</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.N</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.N</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.N</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.N</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.N</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.N</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.N</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Job aborted.</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAcce</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.N</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.N</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.N</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.N</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.N</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.N</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.N</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>SparkContext has been shutdown</li></div><div><li>SparkContext has been shutdown</li></div><div><li>SparkContext has been shutdown</li></div><div><li>Job aborted.</li></div><div><li>SparkContext has been shutdown</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.repartition(JavaAPISuite.java:858)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:91)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:46)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:91)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:46)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.sql.avro.AvroFunctionsSuite.beforeAll(AvroFunctionsSuite.scala:26)&amp;#010;org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:212)&amp;#010;org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:210)&amp;#010;org.apache.spark.SparkFunSuite.run(SparkFunS</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>SparkContext has been shutdown</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.EdgeRDDSuite.withSpark(Edge</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.VertexRDDSuite.withSpark(Ve</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphOpsSuite.withSpark(GraphOpsSuite.scala:23)&amp;#010;org.apache.spark.graphx.GraphOpsSuite$$anonfun$9.apply(GraphO</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphOpsSuite.withSpark(Gra</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphOpsSuite.withSpark(Gra</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphOpsSuite.withSpark(Gra</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphOpsSuite.withSpark(Gra</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphOpsSuite.withSpark(Gra</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphOpsSuite.withSpark(Gra</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphOpsSuite.withSpark(Gra</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphOpsSuite.withSpark(Gra</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphOpsSuite.withSpark(Gra</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphOpsSuite.withSpark(Gra</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.LabelPropagationSuite.w</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.LabelPropagationSuite.w</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.LabelPropagationSuite.w</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.LabelPropagationSuite.w</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.LabelPropagationSuite.w</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.LabelPropagationSuite.w</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.LabelPropagationSuite.w</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphSuite.withSpark(GraphSuite.scala:27)&amp;#010;org.apache.spark.graphx.GraphSuite$$anonfun$39.apply(GraphSuite.sca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphSuite.withSpark(GraphS</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphSuite.withSpark(GraphS</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphSuite.withSpark(GraphS</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphSuite.withSpark(GraphS</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphSuite.withSpark(GraphS</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphSuite.withSpark(GraphS</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphSuite.withSpark(GraphS</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphSuite.withSpark(GraphS</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphSuite.withSpark(GraphS</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphSuite.withSpark(GraphS</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphSuite.withSpark(GraphS</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphSuite.withSpark(GraphS</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphSuite.withSpark(GraphS</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphSuite.withSpark(GraphS</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.PregelSuite.withSpark(PregelSuite.scala:22)&amp;#010;org.apache.spark.graphx.PregelSuite$$anonfun$4.apply(PregelSuite.</li></div><div><li>SparkContext has been shutdown</li></div><div><li>SparkContext has been shutdown</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.VertexRDDSuite.withSpark(Ve</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.VertexRDDSuite.withSpark(Ve</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.VertexRDDSuite.withSpark(Ve</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.VertexRDDSuite.withSpark(Ve</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.VertexRDDSuite.withSpark(Ve</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.VertexRDDSuite.withSpark(Ve</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.VertexRDDSuite.withSpark(Ve</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.VertexRDDSuite.withSpark(Ve</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.VertexRDDSuite.withSpark(Ve</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.VertexRDDSuite.withSpark(Ve</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.VertexRDDSuite.withSpark(Ve</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.VertexRDDSuite.withSpark(Ve</li></div><div><li>SparkContext has been shutdown</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.VertexRDDSuite.withSpark(Ve</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.VertexRDDSuite.withSpark(Ve</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.LabelPropagationSuite.withSpark(LabelPropagationSuite.scala:23)&amp;#010;org.apache.spark.graphx.lib.LabelPropagat</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.util.PeriodicGraphCheckpoin</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.PageRankSuite.withSpark(PageRankSuite.scala:60)&amp;#010;org.apache.spark.graphx.lib.PageRankSuite$$anonfun$19.app</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.LabelPropagationSuite.w</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.VertexRDDSuite.withSpark(Ve</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.VertexRDDSuite.withSpark(Ve</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.VertexRDDSuite.withSpark(Ve</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.VertexRDDSuite.withSpark(Ve</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.VertexRDDSuite.withSpark(Ve</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.VertexRDDSuite.withSpark(Ve</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.VertexRDDSuite.withSpark(Ve</li></div><div><li>SparkContext has been shutdown</li></div><div><li>SparkContext has been shutdown</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.util.PeriodicGraphCheckpointerSuite.withSpark(PeriodicGraphCheckpointerSuite.scala:28)&amp;#010;org.apache.spark.graph</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.util.PeriodicGraphCheckpoin</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Error while instantiating 'org.apache.spark.sql.internal.SessionStateBuilder':</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.tuning.JavaCrossValidatorSuite.setUp(JavaCrossValidatorSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Error while instantiating 'org.apache.spark.sql.internal.SessionStateBuilder':</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Error while instantiating 'org.apache.spark.sql.internal.SessionStateBuilder':</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Error while instantiating 'org.apache.spark.sql.internal.SessionStateBuilder':</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Error while instantiating 'org.apache.spark.sql.internal.SessionStateBuilder':</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Error while instantiating 'org.apache.spark.sql.internal.SessionStateBuilder':</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.tuning.JavaCrossValidatorSuite.setUp(JavaCrossValidatorSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.tuning.JavaCrossValidatorSuite.setUp(JavaCrossValidatorSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at org.apache.spark.ml.util.JavaDefaultReadWriteSuite.tearDown(JavaDefaultReadWriteSuite.java:41)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:4</li></div><div><li>java.lang.NullPointerException
	at org.apache.spark.mllib.classification.JavaStreamingLogisticRegressionSuite.tearDown(JavaStreamingLogisticRegressionSuite.java:55)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.mllib.clustering.JavaLDASuite.setUp(JavaLDASuite.java:41)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.M</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.mllib.clustering.JavaLDASuite.setUp(JavaLDASuite.java:41)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.M</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.mllib.clustering.JavaLDASuite.setUp(JavaLDASuite.java:41)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.M</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.mllib.clustering.JavaLDASuite.setUp(JavaLDASuite.java:41)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.M</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.mllib.clustering.JavaLDASuite.setUp(JavaLDASuite.java:41)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.M</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:4</li></div><div><li>java.lang.NullPointerException
	at org.apache.spark.mllib.clustering.JavaStreamingKMeansSuite.tearDown(JavaStreamingKMeansSuite.java:54)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.mllib.clustering.JavaLDASuite.setUp(JavaLDASuite.java:41)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.M</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.mllib.clustering.JavaLDASuite.setUp(JavaLDASuite.java:41)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.M</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.mllib.clustering.JavaLDASuite.setUp(JavaLDASuite.java:41)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.M</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.mllib.clustering.JavaLDASuite.setUp(JavaLDASuite.java:41)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.M</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.mllib.clustering.JavaLDASuite.setUp(JavaLDASuite.java:41)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.M</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.mllib.clustering.JavaLDASuite.setUp(JavaLDASuite.java:41)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.M</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.mllib.clustering.JavaLDASuite.setUp(JavaLDASuite.java:41)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.M</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.mllib.clustering.JavaLDASuite.setUp(JavaLDASuite.java:41)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.M</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.mllib.clustering.JavaLDASuite.setUp(JavaLDASuite.java:41)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.M</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:4</li></div><div><li>java.lang.NullPointerException
	at org.apache.spark.mllib.regression.JavaStreamingLinearRegressionSuite.tearDown(JavaStreamingLinearRegressionSuite.java:54)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
	at org.apache.spark.mllib.stat.JavaStatisticsSuite.tearDown(JavaStatisticsSuite.java:65)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
	at org.apache.spark.mllib.stat.JavaStatisticsSuite.tearDown(JavaStatisticsSuite.java:65)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
	at org.apache.spark.mllib.stat.JavaStatisticsSuite.tearDown(JavaStatisticsSuite.java:65)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
org.apache.spark.mllib.stat.JavaStatisticsSuite.setUp(JavaStatisticsSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.mllib.clustering.JavaLDASuite.setUp(JavaLDASuite.java:41)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.M</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.mllib.clustering.JavaLDASuite.setUp(JavaLDASuite.java:41)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.M</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.mllib.clustering.JavaLDASuite.setUp(JavaLDASuite.java:41)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.M</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.mllib.clustering.JavaLDASuite.setUp(JavaLDASuite.java:41)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.M</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)&amp;#010;org.apache.spark.repl.Main$.createSparkSession(Main.scala:112)&amp;#010;&lt;init&gt;(&lt;console&gt;:15)&amp;#010;&lt;init&gt;(&lt;console&gt;:43)&amp;#</li></div><div><li>isContain was true Interpreter output contained 'Exception':&amp;#010;Welcome to&amp;#010;      ____              __&amp;#010;     / __/__  ___ _____/ /__&amp;#010;    _\ \/ _ \/ _ `/ __/  '_/&amp;#010;   /___/ .__/\_,_/_/ /_/\_\   version 3.0.0-SNAPSHOT&amp;#010;      /_/&amp;#010;         &amp;#010;Using Scala version 2.11.12 (OpenJDK 64-Bit Server VM, Java 1.8.0_171)&amp;#010;Type in expressions to have them evaluated.&amp;#010;Type </li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)&amp;#010;org.apache.spark.repl.Main$.createSparkSession(Main.scala:118)&amp;#010;&lt;init&gt;(&lt;console&gt;:15)&amp;#010;&lt;init&gt;(&lt;console&gt;:43)&amp;#</li></div><div><li>isContain was true Interpreter output contained 'Exception':&amp;#010;Welcome to&amp;#010;      ____              __&amp;#010;     / __/__  ___ _____/ /__&amp;#010;    _\ \/ _ \/ _ `/ __/  '_/&amp;#010;   /___/ .__/\_,_/_/ /_/\_\   version 3.0.0-SNAPSHOT&amp;#010;      /_/&amp;#010;         &amp;#010;Using Scala version 2.11.12 (OpenJDK 64-Bit Server VM, Java 1.8.0_171)&amp;#010;Type in expressions to have them evaluated.&amp;#010;Type </li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;java.lang.NoClassDefFoundError: org/spark_project/guava/cache/Weigher&amp;#010;  at org.apache.spark.network.shuffle.ExternalShuffleBlockHandler.&lt;init&gt;(ExternalShuffleBlockHandler.java:64)&amp;#010;  at org.apache.spark.deploy.ExternalShuffleService.newShuffleBlockHandler(ExternalShuffleService.scala:63)&amp;#010;  at org.apache.spark.deploy.Exter</li></div><div><li>isContain was true Interpreter output contained 'error: not found: value sc':&amp;#010;java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)&amp;#010;org.apache.spark.repl.Main$.createSparkSession(Main.scala:118)&amp;#010;&lt;init&gt;(&lt;console&gt;:15)&amp;#010;&lt;i</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:17: error: not found: value sc&amp;#010;       val accum = sc.longAccumulator&amp;#010;                   ^&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:18: error: not found: value sc&amp;#010;       sc.parallelize(1 to 10).foreach(x =&gt; accum.add(x))&amp;#010;       ^&amp;#010;&lt;console&gt;:18: error: not found: value accum&amp;#010;       sc.parallelize(1 </li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; v: Int = 7&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:19: error: not found: value sc&amp;#010;       val res1 = sc.parallelize(1 to 10).map(x =&gt; v).collect().reduceLeft(_+_)&amp;#010;                  ^&amp;#010;&amp;#010;scala&gt; v: Int = 10&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:19: error: not found: value sc&amp;#010;       val res2 = sc.parallelize(1 to 10).map(x =&gt;</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt;      |      | defined class C&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:18: error: not found: value sc&amp;#010;       val res = sc.parallelize(1 to 10).map(x =&gt; (new C).foo).collect().reduceLeft(_+_)&amp;#010;                 ^&amp;#010;&amp;#010;scala&gt;      | _result_1540439933979: Int = 1&amp;#010;&amp;#010;scala&gt; </li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; double: (x: Int)Int&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:19: error: not found: value sc&amp;#010;       val res = sc.parallelize(1 to 10).map(x =&gt; double(x)).collect().reduceLeft(_+_)&amp;#010;                 ^&amp;#010;&amp;#010;scala&gt;      | _result_1540439934434: Int = 1&amp;#010;&amp;#010;scala&gt; </li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; v: Int = 7&amp;#010;&amp;#010;scala&gt; getV: ()Int&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:19: error: not found: value sc&amp;#010;       val res1 = sc.parallelize(1 to 10).map(x =&gt; getV()).collect().reduceLeft(_+_)&amp;#010;                  ^&amp;#010;&amp;#010;scala&gt; v: Int = 10&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:19: error: not found: value sc&amp;#010;       val res2</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; array: Array[Int] = Array(0, 0, 0, 0, 0)&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:19: error: not found: value sc&amp;#010;       val broadcastArray = sc.broadcast(array)&amp;#010;                            ^&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:17: error: not found: value sc&amp;#010;       val res1 = sc.parallelize(0 to 4).map(x =&gt; broadcastArray.value(x</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:17: error: not found: value sc&amp;#010;       var file = sc.textFile("/var/lib/jenkins/workspace/spark/repl/target/tmp/spark-ec8968cf-e83d-42dd-8f1b-104808636dd7/input").cache()&amp;#010;                  ^&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:17: error: not found: value file&amp;#010;       val res1 = file.count()&amp;#010;            </li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; v: Int = 7&amp;#010;&amp;#010;scala&gt; getV: ()Int&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:19: error: not found: value sc&amp;#010;       val res1 = sc.parallelize(1 to 10).map(x =&gt; getV()).collect().reduceLeft(_+_)&amp;#010;                  ^&amp;#010;&amp;#010;scala&gt; v: Int = 10&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:19: error: not found: value sc&amp;#010;       val res2</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:17: error: not found: value spark&amp;#010;       import spark.implicits._&amp;#010;              ^&amp;#010;&amp;#010;scala&gt; defined class TestCaseClass&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:22: error: not found: value sc&amp;#010;       sc.parallelize(1 to 10).map(x =&gt; TestCaseClass(x)).toDF().collect()&amp;#010;       ^&amp;#010;&amp;#010;scala&gt; &amp;#010</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; import org.apache.spark.sql.functions._&amp;#010;&amp;#010;scala&gt; import org.apache.spark.sql.{Encoder, Encoders}&amp;#010;&amp;#010;scala&gt; import org.apache.spark.sql.expressions.Aggregator&amp;#010;&amp;#010;scala&gt; import org.apache.spark.sql.TypedColumn&amp;#010;&amp;#010;scala&gt;      |      |      |      |      |      |      | simpleSum: org.apache.sp</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; defined class TestClass&amp;#010;&amp;#010;scala&gt; t: TestClass = TestClass@7b4ee26b&amp;#010;&amp;#010;scala&gt; import t.testMethod&amp;#010;&amp;#010;scala&gt; defined class TestCaseClass&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:31: error: not found: value sc&amp;#010;       sc.parallelize(1 to 10).map(x =&gt; TestCaseClass(x)).collect()&amp;#010;       ^&amp;#010;&amp;#010;scala&gt; </li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; defined class Foo&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:28: error: not found: value sc&amp;#010;       val res = sc.parallelize((1 to 100).map(Foo), 10).collect()&amp;#010;                 ^&amp;#010;&amp;#010;scala&gt;      | _result_1540439946375: Int = 1&amp;#010;&amp;#010;scala&gt; </li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; defined class Foo&amp;#010;&amp;#010;scala&gt; list: List[(Int, Foo)] = List((1,Foo(1)), (1,Foo(2)))&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:28: error: not found: value sc&amp;#010;       val res = sc.parallelize(list).groupByKey().collect()&amp;#010;                 ^&amp;#010;&amp;#010;scala&gt;      | _result_1540439946891: Int = 1&amp;#010;&amp;#010;scala&gt; </li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; timeout: Int = 60000&amp;#010;&amp;#010;scala&gt; start: Long = 1540439948283&amp;#010;&amp;#010;scala&gt;      |      |      | &lt;console&gt;:31: error: not found: value sc&amp;#010;       while(sc.statusTracker.getExecutorInfos.size != 3 &amp;&amp;&amp;#010;             ^&amp;#010;&amp;#010;scala&gt;      |      | &amp;#010;scala&gt; import org.apache.spark.storage.StorageLevel._&amp;</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; defined class Click&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:33: error: not found: value spark&amp;#010;       spark.implicits.newProductSeqEncoder[Click]&amp;#010;       ^&amp;#010;&amp;#010;scala&gt;      | _result_1540439950433: Int = 1&amp;#010;&amp;#010;scala&gt; </li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetAggregatorSuiteBase.setUp(JavaDatasetAggregatorSuiteBase.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.jav</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaApplySchemaSuite.tearDown(JavaApplySchemaSuite.java:60)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetAggregatorSuiteBase.setUp(JavaDatasetAggregatorSuiteBase.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.jav</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaApplySchemaSuite.tearDown(JavaApplySchemaSuite.java:60)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetAggregatorSuiteBase.setUp(JavaDatasetAggregatorSuiteBase.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.jav</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaApplySchemaSuite.tearDown(JavaApplySchemaSuite.java:60)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaBeanDeserializationSuite.tearDown(JavaBeanDeserializationSuite.java:41)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaBeanDeserializationSuite.tearDown(JavaBeanDeserializationSuite.java:41)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaColumnExpressionSuite.tearDown(JavaColumnExpressionSuite.java:46)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaColumnExpressionSuite.tearDown(JavaColumnExpressionSuite.java:46)
</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetAggregatorSuiteBase.setUp(JavaDatasetAggregatorSuiteBase.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.jav</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetAggregatorSuiteBase.setUp(JavaDatasetAggregatorSuiteBase.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.jav</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.Delegating</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeC</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaSaveLoadSuite.tearDown(JavaSaveLoadSuite.java:76)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeC</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaSaveLoadSuite.tearDown(JavaSaveLoadSuite.java:76)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeC</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaUDAFSuite.tearDown(JavaUDAFSuite.java:42)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMetho</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaUDFSuite.tearDown(JavaUDFSuite.java:50)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMetho</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaUDFSuite.tearDown(JavaUDFSuite.java:50)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMetho</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaUDFSuite.tearDown(JavaUDFSuite.java:50)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMetho</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaUDFSuite.tearDown(JavaUDFSuite.java:50)
</li></div><div><li>Unexpected exception, expected&lt;org.apache.spark.sql.AnalysisException&gt; but was&lt;java.lang.IllegalArgumentException&gt;</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMetho</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaUDFSuite.tearDown(JavaUDFSuite.java:50)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.tearDown(JavaDataStreamReaderWriterSuite.java:49)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.tearDown(JavaDataStreamReaderWriterSuite.java:49)
</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.sql.hive.HiveMetastoreLazyInitializationSuite$$anonfun$1.apply(HiveMetastoreLazyInitializationSuite.scala:32)&amp;#010;org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)&amp;#010;org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)&amp;#010;org.scalatest.Transformer.apply(Trans</li></div><div><li>java.lang.ExceptionInInitializerError
	at org.apache.spark.sql.hive.JavaDataFrameSuite.setUp(JavaDataFrameSuite.java:50)
Caused by: java.lang.IllegalStateException: 
Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.hive.JavaDataFrameSuite.setUp(JavaDataFrameSuite.java:50)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>Could not initialize class org.apache.spark.sql.hive.test.TestHive$</li></div><div><li>Could not initialize class org.apache.spark.sql.hive.test.TestHive$</li></div><div><li>Could not initialize class org.apache.spark.sql.hive.test.TestHive$</li></div><div><li>Could not initialize class org.apache.spark.sql.hive.test.TestHive$</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:850)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(StreamingContext.scala:85)
org.apache.spark.streaming.api.java.JavaStreamingContext.&lt;init&gt;(JavaStreaming</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>Timeout getting response from the server</li></div><div><li>SparkContext has been shutdown</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMe</li></div><div><li>SparkContext has been shutdown</li></div><div><li>SparkContext has been shutdown</li></div><div><li>SparkContext has been shutdown</li></div><div><li>SparkContext has been shutdown</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>SparkContext has been shutdown</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAcce</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.N</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.N</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.N</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.N</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.N</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.N</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.N</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.N</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.N</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.N</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.N</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.N</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.N</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.N</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.N</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.N</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.N</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.N</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>SparkContext has been shutdown</li></div><div><li>SparkContext has been shutdown</li></div><div><li>SparkContext has been shutdown</li></div><div><li>SparkContext has been shutdown</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>SparkContext has been shutdown</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.sql.avro.AvroFunctionsSuite.beforeAll(AvroFunctionsSuite.scala:26)&amp;#010;org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:212)&amp;#010;org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:210)&amp;#010;org.apache.spark.SparkFunSuite.run(SparkFunS</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>SparkContext has been shutdown</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.EdgeRDDSuite.withSpark(EdgeRDDSuite.scala:24)&amp;#010;org.apache.spark.graphx.EdgeRDDSuite$$anonfun$1.apply(EdgeRDDSu</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.EdgeRDDSuite.withSpark(Edge</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.VertexRDDSuite.withSpark(Ve</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphOpsSuite.withSpark(GraphOpsSuite.scala:23)&amp;#010;org.apache.spark.graphx.GraphOpsSuite$$anonfun$9.apply(GraphO</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphOpsSuite.withSpark(Gra</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphOpsSuite.withSpark(Gra</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphOpsSuite.withSpark(Gra</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphOpsSuite.withSpark(Gra</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphOpsSuite.withSpark(Gra</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphOpsSuite.withSpark(Gra</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphOpsSuite.withSpark(Gra</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphOpsSuite.withSpark(Gra</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphOpsSuite.withSpark(Gra</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphOpsSuite.withSpark(Gra</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.LabelPropagationSuite.w</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.LabelPropagationSuite.w</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.LabelPropagationSuite.w</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.LabelPropagationSuite.w</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.LabelPropagationSuite.w</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.LabelPropagationSuite.w</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.LabelPropagationSuite.w</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.LabelPropagationSuite.w</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.LabelPropagationSuite.w</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.LabelPropagationSuite.w</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.LabelPropagationSuite.w</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.LabelPropagationSuite.w</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.LabelPropagationSuite.w</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.LabelPropagationSuite.w</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphSuite.withSpark(GraphSuite.scala:27)&amp;#010;org.apache.spark.graphx.GraphSuite$$anonfun$76.apply(GraphSuite.sca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphSuite.withSpark(GraphS</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphSuite.withSpark(GraphS</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphSuite.withSpark(GraphS</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.PregelSuite.withSpark(PregelSuite.scala:22)&amp;#010;org.apache.spark.graphx.PregelSuite$$anonfun$4.apply(PregelSuite.</li></div><div><li>SparkContext has been shutdown</li></div><div><li>SparkContext has been shutdown</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.VertexRDDSuite.withSpark(Ve</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.VertexRDDSuite.withSpark(Ve</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.VertexRDDSuite.withSpark(Ve</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.VertexRDDSuite.withSpark(Ve</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.VertexRDDSuite.withSpark(Ve</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.VertexRDDSuite.withSpark(Ve</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.VertexRDDSuite.withSpark(Ve</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.VertexRDDSuite.withSpark(Ve</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.VertexRDDSuite.withSpark(Ve</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.VertexRDDSuite.withSpark(Ve</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.VertexRDDSuite.withSpark(Ve</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.VertexRDDSuite.withSpark(Ve</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.VertexRDDSuite.withSpark(Ve</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.VertexRDDSuite.withSpark(Ve</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.VertexRDDSuite.withSpark(Ve</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.LabelPropagationSuite.withSpark(LabelPropagationSuite.scala:23)&amp;#010;org.apache.spark.graphx.lib.LabelPropagat</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.util.PeriodicGraphCheckpoin</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.util.PeriodicGraphCheckpoin</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.util.PeriodicGraphCheckpoin</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.util.PeriodicGraphCheckpoin</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.PageRankSuite.withSpark(PageRankSuite.scala:60)&amp;#010;org.apache.spark.graphx.lib.PageRankSuite$$anonfun$19.app</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.LabelPropagationSuite.w</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.StronglyConnectedComponentsSuite.withSpark(StronglyConnectedComponentsSuite.scala:24)&amp;#010;org.apache.spark.gr</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.StronglyConnectedCompon</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.StronglyConnectedCompon</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.StronglyConnectedCompon</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>SparkContext has been shutdown</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.util.PeriodicGraphCheckpointerSuite.withSpark(PeriodicGraphCheckpointerSuite.scala:28)&amp;#010;org.apache.spark.graph</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.util.PeriodicGraphCheckpoin</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Error while instantiating 'org.apache.spark.sql.internal.SessionStateBuilder':</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.tuning.JavaCrossValidatorSuite.setUp(JavaCrossValidatorSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Error while instantiating 'org.apache.spark.sql.internal.SessionStateBuilder':</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Error while instantiating 'org.apache.spark.sql.internal.SessionStateBuilder':</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Error while instantiating 'org.apache.spark.sql.internal.SessionStateBuilder':</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Error while instantiating 'org.apache.spark.sql.internal.SessionStateBuilder':</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.tuning.JavaCrossValidatorSuite.setUp(JavaCrossValidatorSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.tuning.JavaCrossValidatorSuite.setUp(JavaCrossValidatorSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at org.apache.spark.ml.util.JavaDefaultReadWriteSuite.tearDown(JavaDefaultReadWriteSuite.java:41)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:4</li></div><div><li>java.lang.NullPointerException
	at org.apache.spark.mllib.classification.JavaStreamingLogisticRegressionSuite.tearDown(JavaStreamingLogisticRegressionSuite.java:55)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>SparkContext has been shutdown</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(StreamingContext.scala:85)
org.apache.spark.streaming.api.java.JavaStreamingContext.&lt;init&gt;(JavaStreaming</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>SparkContext has been shutdown</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>SparkContext has been shutdown</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:4</li></div><div><li>java.lang.NullPointerException
	at org.apache.spark.mllib.regression.JavaStreamingLinearRegressionSuite.tearDown(JavaStreamingLinearRegressionSuite.java:54)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
	at org.apache.spark.mllib.stat.JavaStatisticsSuite.tearDown(JavaStatisticsSuite.java:65)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
	at org.apache.spark.mllib.stat.JavaStatisticsSuite.tearDown(JavaStatisticsSuite.java:65)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
	at org.apache.spark.mllib.stat.JavaStatisticsSuite.tearDown(JavaStatisticsSuite.java:65)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
	at org.apache.spark.mllib.stat.JavaStatisticsSuite.tearDown(JavaStatisticsSuite.java:65)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>isContain was true Interpreter output contained 'Exception':&amp;#010;Welcome to&amp;#010;      ____              __&amp;#010;     / __/__  ___ _____/ /__&amp;#010;    _\ \/ _ \/ _ `/ __/  '_/&amp;#010;   /___/ .__/\_,_/_/ /_/\_\   version 3.0.0-SNAPSHOT&amp;#010;      /_/&amp;#010;         &amp;#010;Using Scala version 2.11.12 (OpenJDK 64-Bit Server VM, Java 1.8.0_161)&amp;#010;Type in expressions to have them evaluated.&amp;#010;Type </li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)&amp;#010;org.apache.spark.repl.Main$.createSparkSession(Main.scala:118)&amp;#010;&lt;init&gt;(&lt;console&gt;:15)&amp;#010;&lt;init&gt;(&lt;console&gt;:43)&amp;#</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;Welcome to&amp;#010;      ____              __&amp;#010;     / __/__  ___ _____/ /__&amp;#010;    _\ \/ _ \/ _ `/ __/  '_/&amp;#010;   /___/ .__/\_,_/_/ /_/\_\   version 3.0.0-SNAPSHOT&amp;#010;      /_/&amp;#010;         &amp;#010;Using Scala version 2.11.12 (OpenJDK 64-Bit Server VM, Java 1.8.0_161)&amp;#010;Type in expressions to have them evaluated.&amp;#010;Type :he</li></div><div><li>isContain was true Interpreter output contained 'Exception':&amp;#010;Welcome to&amp;#010;      ____              __&amp;#010;     / __/__  ___ _____/ /__&amp;#010;    _\ \/ _ \/ _ `/ __/  '_/&amp;#010;   /___/ .__/\_,_/_/ /_/\_\   version 3.0.0-SNAPSHOT&amp;#010;      /_/&amp;#010;         &amp;#010;Using Scala version 2.11.12 (OpenJDK 64-Bit Server VM, Java 1.8.0_161)&amp;#010;Type in expressions to have them evaluated.&amp;#010;Type </li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;java.lang.NoClassDefFoundError: org/spark_project/guava/cache/Weigher&amp;#010;  at org.apache.spark.network.shuffle.ExternalShuffleBlockHandler.&lt;init&gt;(ExternalShuffleBlockHandler.java:64)&amp;#010;  at org.apache.spark.deploy.ExternalShuffleService.newShuffleBlockHandler(ExternalShuffleService.scala:63)&amp;#010;  at org.apache.spark.deploy.Exter</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:17: error: not found: value sc&amp;#010;       val accum = sc.longAccumulator&amp;#010;                   ^&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:18: error: not found: value sc&amp;#010;       sc.parallelize(1 to 10).foreach(x =&gt; accum.add(x))&amp;#010;       ^&amp;#010;&lt;console&gt;:18: error: not found: value accum&amp;#010;       sc.parallelize(1 </li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; v: Int = 7&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:19: error: not found: value sc&amp;#010;       val res1 = sc.parallelize(1 to 10).map(x =&gt; v).collect().reduceLeft(_+_)&amp;#010;                  ^&amp;#010;&amp;#010;scala&gt; v: Int = 10&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:19: error: not found: value sc&amp;#010;       val res2 = sc.parallelize(1 to 10).map(x =&gt;</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt;      |      | defined class C&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:18: error: not found: value sc&amp;#010;       val res = sc.parallelize(1 to 10).map(x =&gt; (new C).foo).collect().reduceLeft(_+_)&amp;#010;                 ^&amp;#010;&amp;#010;scala&gt;      | _result_1540445291147: Int = 1&amp;#010;&amp;#010;scala&gt; </li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; double: (x: Int)Int&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:19: error: not found: value sc&amp;#010;       val res = sc.parallelize(1 to 10).map(x =&gt; double(x)).collect().reduceLeft(_+_)&amp;#010;                 ^&amp;#010;&amp;#010;scala&gt;      | _result_1540445291555: Int = 1&amp;#010;&amp;#010;scala&gt; </li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; v: Int = 7&amp;#010;&amp;#010;scala&gt; getV: ()Int&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:19: error: not found: value sc&amp;#010;       val res1 = sc.parallelize(1 to 10).map(x =&gt; getV()).collect().reduceLeft(_+_)&amp;#010;                  ^&amp;#010;&amp;#010;scala&gt; v: Int = 10&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:19: error: not found: value sc&amp;#010;       val res2</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; array: Array[Int] = Array(0, 0, 0, 0, 0)&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:19: error: not found: value sc&amp;#010;       val broadcastArray = sc.broadcast(array)&amp;#010;                            ^&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:17: error: not found: value sc&amp;#010;       val res1 = sc.parallelize(0 to 4).map(x =&gt; broadcastArray.value(x</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:17: error: not found: value sc&amp;#010;       var file = sc.textFile("/var/lib/jenkins/workspace/spark/repl/target/tmp/spark-044765b6-a90e-423d-9e26-e9cfa86acd63/input").cache()&amp;#010;                  ^&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:17: error: not found: value file&amp;#010;       val res1 = file.count()&amp;#010;            </li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; v: Int = 7&amp;#010;&amp;#010;scala&gt; getV: ()Int&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:19: error: not found: value sc&amp;#010;       val res1 = sc.parallelize(1 to 10).map(x =&gt; getV()).collect().reduceLeft(_+_)&amp;#010;                  ^&amp;#010;&amp;#010;scala&gt; v: Int = 10&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:19: error: not found: value sc&amp;#010;       val res2</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:17: error: not found: value spark&amp;#010;       import spark.implicits._&amp;#010;              ^&amp;#010;&amp;#010;scala&gt; defined class TestCaseClass&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:22: error: not found: value sc&amp;#010;       sc.parallelize(1 to 10).map(x =&gt; TestCaseClass(x)).toDF().collect()&amp;#010;       ^&amp;#010;&amp;#010;scala&gt; &amp;#010</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; import org.apache.spark.sql.functions._&amp;#010;&amp;#010;scala&gt; import org.apache.spark.sql.{Encoder, Encoders}&amp;#010;&amp;#010;scala&gt; import org.apache.spark.sql.expressions.Aggregator&amp;#010;&amp;#010;scala&gt; import org.apache.spark.sql.TypedColumn&amp;#010;&amp;#010;scala&gt;      |      |      |      |      |      |      | simpleSum: org.apache.sp</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; defined class TestClass&amp;#010;&amp;#010;scala&gt; t: TestClass = TestClass@1dfab697&amp;#010;&amp;#010;scala&gt; import t.testMethod&amp;#010;&amp;#010;scala&gt; defined class TestCaseClass&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:31: error: not found: value sc&amp;#010;       sc.parallelize(1 to 10).map(x =&gt; TestCaseClass(x)).collect()&amp;#010;       ^&amp;#010;&amp;#010;scala&gt; </li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; defined class Foo&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:28: error: not found: value sc&amp;#010;       val res = sc.parallelize((1 to 100).map(Foo), 10).collect()&amp;#010;                 ^&amp;#010;&amp;#010;scala&gt;      | _result_1540445302313: Int = 1&amp;#010;&amp;#010;scala&gt; </li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; defined class Foo&amp;#010;&amp;#010;scala&gt; list: List[(Int, Foo)] = List((1,Foo(1)), (1,Foo(2)))&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:28: error: not found: value sc&amp;#010;       val res = sc.parallelize(list).groupByKey().collect()&amp;#010;                 ^&amp;#010;&amp;#010;scala&gt;      | _result_1540445302668: Int = 1&amp;#010;&amp;#010;scala&gt; </li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; timeout: Int = 60000&amp;#010;&amp;#010;scala&gt; start: Long = 1540445304014&amp;#010;&amp;#010;scala&gt;      |      |      | &lt;console&gt;:31: error: not found: value sc&amp;#010;       while(sc.statusTracker.getExecutorInfos.size != 3 &amp;&amp;&amp;#010;             ^&amp;#010;&amp;#010;scala&gt;      |      | &amp;#010;scala&gt; import org.apache.spark.storage.StorageLevel._&amp;</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; defined class Click&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:33: error: not found: value spark&amp;#010;       spark.implicits.newProductSeqEncoder[Click]&amp;#010;       ^&amp;#010;&amp;#010;scala&gt;      | _result_1540445306205: Int = 1&amp;#010;&amp;#010;scala&gt; </li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMetho</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaApplySchemaSuite.tearDown(JavaApplySchemaSuite.java:60)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMetho</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaApplySchemaSuite.tearDown(JavaApplySchemaSuite.java:60)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMetho</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaApplySchemaSuite.tearDown(JavaApplySchemaSuite.java:60)
</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaBeanDeserializationSuite.testBeanWithMapFieldsDeserialization(JavaBeanDeserializationSuite.java:104)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaColumnExpressionSuite.tearDown(JavaColumnExpressionSuite.java:46)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaColumnExpressionSuite.tearDown(JavaColumnExpressionSuite.java:46)
</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.Delegating</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.refl</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeC</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaSaveLoadSuite.tearDown(JavaSaveLoadSuite.java:76)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeC</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaSaveLoadSuite.tearDown(JavaSaveLoadSuite.java:76)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeC</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaUDAFSuite.tearDown(JavaUDAFSuite.java:42)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMetho</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaUDFSuite.tearDown(JavaUDFSuite.java:50)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMetho</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaUDFSuite.tearDown(JavaUDFSuite.java:50)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMetho</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaUDFSuite.tearDown(JavaUDFSuite.java:50)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMetho</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaUDFSuite.tearDown(JavaUDFSuite.java:50)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMetho</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaUDFSuite.tearDown(JavaUDFSuite.java:50)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMetho</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaUDFSuite.tearDown(JavaUDFSuite.java:50)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.tearDown(JavaDataStreamReaderWriterSuite.java:49)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.tearDown(JavaDataStreamReaderWriterSuite.java:49)
</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.sql.hive.HiveMetastoreLazyInitializationSuite$$anonfun$1.apply(HiveMetastoreLazyInitializationSuite.scala:32)&amp;#010;org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)&amp;#010;org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)&amp;#010;org.scalatest.Transformer.apply(Trans</li></div><div><li>java.lang.ExceptionInInitializerError
	at org.apache.spark.sql.hive.JavaDataFrameSuite.setUp(JavaDataFrameSuite.java:50)
Caused by: java.lang.IllegalStateException: 
Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.hive.JavaDataFrameSuite.setUp(JavaDataFrameSuite.java:50)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>Could not initialize class org.apache.spark.sql.hive.test.TestHive$</li></div><div><li>Could not initialize class org.apache.spark.sql.hive.test.TestHive$</li></div><div><li>Could not initialize class org.apache.spark.sql.hive.test.TestHive$</li></div><div><li>Could not initialize class org.apache.spark.sql.hive.test.TestHive$</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:850)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Could not load library. Reasons: [no leveldbjni64-1.8 in java.library.path, no leveldbjni-1.8 in java.library.path, no leveldbjni in java.library.path, /var/lib/jenkins/workspace/spark/common/kvstore/target/tmp/libleveldbjni-64-1-8027722621579526045.8: /var/lib/jenkins/workspace/spark/common/kvstore/target/tmp/libleveldbjni-64-1-8027722621579526045.8: cannot open shared object file: No such file o</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Timeout getting response from the server</li></div><div><li>SparkContext has been shutdown</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMe</li></div><div><li>SparkContext has been shutdown</li></div><div><li>SparkContext has been shutdown</li></div><div><li>SparkContext has been shutdown</li></div><div><li>SparkContext has been shutdown</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>SparkContext has been shutdown</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAcce</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAcce</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.N</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.N</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.N</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.N</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.N</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.N</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.N</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.N</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.N</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.N</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.N</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.N</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.N</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.N</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.N</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.N</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.N</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.N</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.N</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.N</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.N</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.N</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.N</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.N</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.N</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.N</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Job aborted.</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>SparkContext has been shutdown</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>SparkContext has been shutdown</li></div><div><li>SparkContext has been shutdown</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaSparkContextSuite.scalaSparkContext(JavaSparkContextSuite.java:56)
</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.sql.avro.AvroFunctionsSuite.beforeAll(AvroFunctionsSuite.scala:26)&amp;#010;org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:212)&amp;#010;org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:210)&amp;#010;org.apache.spark.SparkFunSuite.run(SparkFunS</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>SparkContext has been shutdown</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.PageRankSuite.withSpark</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.PageRankSuite.withSpark</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphLoaderSuite.withSpark(GraphLoaderSuite.scala:28)&amp;#010;org.apache.spark.graphx.GraphLoaderSuite$$anonfun$2.app</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphOpsSuite.withSpark(GraphOpsSuite.scala:23)&amp;#010;org.apache.spark.graphx.GraphOpsSuite$$anonfun$9.apply(GraphO</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphOpsSuite.withSpark(GraphOpsSuite.scala:23)&amp;#010;org.apache.spark.graphx.GraphOpsSuite$$anonfun$20.apply(Graph</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphOpsSuite.withSpark(Gra</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphOpsSuite.withSpark(Gra</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphOpsSuite.withSpark(Gra</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphOpsSuite.withSpark(Gra</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphOpsSuite.withSpark(Gra</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphOpsSuite.withSpark(Gra</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.LabelPropagationSuite.w</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.LabelPropagationSuite.w</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.LabelPropagationSuite.w</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.LabelPropagationSuite.w</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.LabelPropagationSuite.w</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.LabelPropagationSuite.w</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.LabelPropagationSuite.w</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.LabelPropagationSuite.w</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.LabelPropagationSuite.w</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.LabelPropagationSuite.w</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.LabelPropagationSuite.w</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphSuite.withSpark(GraphSuite.scala:27)&amp;#010;org.apache.spark.graphx.GraphSuite$$anonfun$60.apply(GraphSuite.sca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphSuite.withSpark(GraphS</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphSuite.withSpark(GraphS</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphSuite.withSpark(GraphS</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphSuite.withSpark(GraphS</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphSuite.withSpark(GraphS</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphSuite.withSpark(GraphS</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphSuite.withSpark(GraphS</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphSuite.withSpark(GraphS</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.PregelSuite.withSpark(PregelSuite.scala:22)&amp;#010;org.apache.spark.graphx.PregelSuite$$anonfun$4.apply(PregelSuite.</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.PregelSuite.withSpark(PregelSuite.scala:22)&amp;#010;org.apache.spark.graphx.PregelSuite$$anonfun$9.apply(PregelSuite.</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.util.GraphGeneratorsSuite.w</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.util.GraphGeneratorsSuite.w</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.util.GraphGeneratorsSuite.w</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.util.GraphGeneratorsSuite.w</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.util.GraphGeneratorsSuite.w</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.util.GraphGeneratorsSuite.w</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.util.GraphGeneratorsSuite.w</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.util.GraphGeneratorsSuite.w</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.util.GraphGeneratorsSuite.w</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.util.GraphGeneratorsSuite.w</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.util.GraphGeneratorsSuite.w</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.util.GraphGeneratorsSuite.w</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.util.GraphGeneratorsSuite.w</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.util.GraphGeneratorsSuite.w</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.util.GraphGeneratorsSuite.w</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.util.GraphGeneratorsSuite.w</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.ConnectedComponentsSuite.withSpark(ConnectedComponentsSuite.scala:26)&amp;#010;org.apache.spark.graphx.lib.Connect</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.ConnectedComponentsSuit</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.ConnectedComponentsSuit</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.ConnectedComponentsSuit</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.ConnectedComponentsSuit</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.LabelPropagationSuite.withSpark(LabelPropagationSuite.scala:23)&amp;#010;org.apache.spark.graphx.lib.LabelPropagat</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.ConnectedComponentsSuit</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.ConnectedComponentsSuit</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.ConnectedComponentsSuit</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.ConnectedComponentsSuit</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.PageRankSuite.withSpark(PageRankSuite.scala:60)&amp;#010;org.apache.spark.graphx.lib.PageRankSuite$$anonfun$19.app</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.PageRankSuite.withSpark(PageRankSuite.scala:60)&amp;#010;org.apache.spark.graphx.lib.PageRankSuite$$anonfun$23.app</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.PageRankSuite.withSpark</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.LabelPropagationSuite.w</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.ConnectedComponentsSuit</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphLoaderSuite.withSpark(</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphLoaderSuite.withSpark(</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphLoaderSuite.withSpark(</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphLoaderSuite.withSpark(</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphLoaderSuite.withSpark(</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphLoaderSuite.withSpark(</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphLoaderSuite.withSpark(</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.util.GraphGeneratorsSuite.withSpark(GraphGeneratorsSuite.scala:23)&amp;#010;org.apache.spark.graphx.util.GraphGenerato</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.ConnectedComponentsSuit</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:126)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.ConnectedComponentsSuit</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.source.libsvm.JavaLibSVMRelationSuite.setUp(JavaLibSVMRelationSuite.java:47)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.r</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.tuning.JavaCrossValidatorSuite.setUp(JavaCrossValidatorSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.source.libsvm.JavaLibSVMRelationSuite.setUp(JavaLibSVMRelationSuite.java:47)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.r</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.source.libsvm.JavaLibSVMRelationSuite.setUp(JavaLibSVMRelationSuite.java:47)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.r</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.source.libsvm.JavaLibSVMRelationSuite.setUp(JavaLibSVMRelationSuite.java:47)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.r</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.source.libsvm.JavaLibSVMRelationSuite.setUp(JavaLibSVMRelationSuite.java:47)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.r</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.source.libsvm.JavaLibSVMRelationSuite.setUp(JavaLibSVMRelationSuite.java:47)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.r</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.source.libsvm.JavaLibSVMRelationSuite.setUp(JavaLibSVMRelationSuite.java:47)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.r</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.source.libsvm.JavaLibSVMRelationSuite.setUp(JavaLibSVMRelationSuite.java:47)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.r</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.source.libsvm.JavaLibSVMRelationSuite.setUp(JavaLibSVMRelationSuite.java:47)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.r</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.source.libsvm.JavaLibSVMRelationSuite.setUp(JavaLibSVMRelationSuite.java:47)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.r</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.source.libsvm.JavaLibSVMRelationSuite.setUp(JavaLibSVMRelationSuite.java:47)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.r</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.source.libsvm.JavaLibSVMRelationSuite.setUp(JavaLibSVMRelationSuite.java:47)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.r</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.source.libsvm.JavaLibSVMRelationSuite.setUp(JavaLibSVMRelationSuite.java:47)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.r</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.source.libsvm.JavaLibSVMRelationSuite.setUp(JavaLibSVMRelationSuite.java:47)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.r</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.source.libsvm.JavaLibSVMRelationSuite.setUp(JavaLibSVMRelationSuite.java:47)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.r</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.source.libsvm.JavaLibSVMRelationSuite.setUp(JavaLibSVMRelationSuite.java:47)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.r</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>java.lang.NullPointerException
	at org.apache.spark.ml.source.libsvm.JavaLibSVMRelationSuite.tearDown(JavaLibSVMRelationSuite.java:57)
</li></div><div><li>Error while instantiating 'org.apache.spark.sql.internal.SessionStateBuilder':</li></div><div><li>Error while encoding: java.lang.NullPointerException
input[0, java.lang.Double, true].doubleValue AS value#2</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.stat.JavaKolmogorovSmirnovTestSuite.setUp(JavaKolmogorovSmirnovTestSuite.java:40)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.j</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.tuning.JavaCrossValidatorSuite.setUp(JavaCrossValidatorSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.tuning.JavaCrossValidatorSuite.setUp(JavaCrossValidatorSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at org.apache.spark.ml.util.JavaDefaultReadWriteSuite.tearDown(JavaDefaultReadWriteSuite.java:41)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:4</li></div><div><li>java.lang.NullPointerException
	at org.apache.spark.mllib.classification.JavaStreamingLogisticRegressionSuite.tearDown(JavaStreamingLogisticRegressionSuite.java:55)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(StreamingContext.scala:85)
org.apache.spark.streaming.api.java.JavaStreamingContext.&lt;init&gt;(JavaStreaming</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>SparkContext has been shutdown</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.stat.JavaKolmogorovSmirnovTestSuite.setUp(JavaKolmogorovSmirnovTestSuite.java:40)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.j</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.stat.JavaKolmogorovSmirnovTestSuite.setUp(JavaKolmogorovSmirnovTestSuite.java:40)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.j</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.stat.JavaKolmogorovSmirnovTestSuite.setUp(JavaKolmogorovSmirnovTestSuite.java:40)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.j</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.stat.JavaKolmogorovSmirnovTestSuite.setUp(JavaKolmogorovSmirnovTestSuite.java:40)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.j</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>SparkContext has been shutdown</li></div><div><li>SparkContext has been shutdown</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(StreamingContext.scala:85)
org.apache.spark.streaming.api.java.JavaStreamingContext.&lt;init&gt;(JavaStreaming</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
	at org.apache.spark.mllib.stat.JavaStatisticsSuite.tearDown(JavaStatisticsSuite.java:65)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
	at org.apache.spark.mllib.stat.JavaStatisticsSuite.tearDown(JavaStatisticsSuite.java:65)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
	at org.apache.spark.mllib.stat.JavaStatisticsSuite.tearDown(JavaStatisticsSuite.java:65)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
	at org.apache.spark.mllib.stat.JavaStatisticsSuite.tearDown(JavaStatisticsSuite.java:65)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Error while instantiating 'org.apache.spark.sql.internal.SessionStateBuilder':</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)&amp;#010;org.apache.spark.repl.Main$.createSparkSession(Main.scala:112)&amp;#010;&lt;init&gt;(&lt;console&gt;:15)&amp;#010;&lt;init&gt;(&lt;console&gt;:43)&amp;#</li></div><div><li>isContain was true Interpreter output contained 'Exception':&amp;#010;Welcome to&amp;#010;      ____              __&amp;#010;     / __/__  ___ _____/ /__&amp;#010;    _\ \/ _ \/ _ `/ __/  '_/&amp;#010;   /___/ .__/\_,_/_/ /_/\_\   version 3.0.0-SNAPSHOT&amp;#010;      /_/&amp;#010;         &amp;#010;Using Scala version 2.11.12 (OpenJDK 64-Bit Server VM, Java 1.8.0_171)&amp;#010;Type in expressions to have them evaluated.&amp;#010;Type </li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;Welcome to&amp;#010;      ____              __&amp;#010;     / __/__  ___ _____/ /__&amp;#010;    _\ \/ _ \/ _ `/ __/  '_/&amp;#010;   /___/ .__/\_,_/_/ /_/\_\   version 3.0.0-SNAPSHOT&amp;#010;      /_/&amp;#010;         &amp;#010;Using Scala version 2.11.12 (OpenJDK 64-Bit Server VM, Java 1.8.0_171)&amp;#010;Type in expressions to have them evaluated.&amp;#010;Type :he</li></div><div><li>isContain was true Interpreter output contained 'Exception':&amp;#010;Welcome to&amp;#010;      ____              __&amp;#010;     / __/__  ___ _____/ /__&amp;#010;    _\ \/ _ \/ _ `/ __/  '_/&amp;#010;   /___/ .__/\_,_/_/ /_/\_\   version 3.0.0-SNAPSHOT&amp;#010;      /_/&amp;#010;         &amp;#010;Using Scala version 2.11.12 (OpenJDK 64-Bit Server VM, Java 1.8.0_171)&amp;#010;Type in expressions to have them evaluated.&amp;#010;Type </li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;java.lang.NoClassDefFoundError: org/spark_project/guava/cache/Weigher&amp;#010;  at org.apache.spark.network.shuffle.ExternalShuffleBlockHandler.&lt;init&gt;(ExternalShuffleBlockHandler.java:64)&amp;#010;  at org.apache.spark.deploy.ExternalShuffleService.newShuffleBlockHandler(ExternalShuffleService.scala:63)&amp;#010;  at org.apache.spark.deploy.Exter</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:17: error: not found: value sc&amp;#010;       val accum = sc.longAccumulator&amp;#010;                   ^&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:18: error: not found: value sc&amp;#010;       sc.parallelize(1 to 10).foreach(x =&gt; accum.add(x))&amp;#010;       ^&amp;#010;&lt;console&gt;:18: error: not found: value accum&amp;#010;       sc.parallelize(1 </li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; v: Int = 7&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:19: error: not found: value sc&amp;#010;       val res1 = sc.parallelize(1 to 10).map(x =&gt; v).collect().reduceLeft(_+_)&amp;#010;                  ^&amp;#010;&amp;#010;scala&gt; v: Int = 10&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:19: error: not found: value sc&amp;#010;       val res2 = sc.parallelize(1 to 10).map(x =&gt;</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt;      |      | defined class C&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:18: error: not found: value sc&amp;#010;       val res = sc.parallelize(1 to 10).map(x =&gt; (new C).foo).collect().reduceLeft(_+_)&amp;#010;                 ^&amp;#010;&amp;#010;scala&gt;      | _result_1540450693401: Int = 1&amp;#010;</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; &amp;#010;scala&gt; double: (x: Int)Int&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:19: error: not found: value sc&amp;#010;       val res = sc.parallelize(1 to 10).map(x =&gt; double(x)).collect().reduceLeft(_+_)&amp;#010;                 ^&amp;#010;&amp;#010;scala&gt;      | _result_1540450693856: Int = 1&amp;#010;&amp;#010;scala&gt; </li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; v: Int = 7&amp;#010;&amp;#010;scala&gt; getV: ()Int&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:19: error: not found: value sc&amp;#010;       val res1 = sc.parallelize(1 to 10).map(x =&gt; getV()).collect().reduceLeft(_+_)&amp;#010;                  ^&amp;#010;&amp;#010;scala&gt; v: Int = 10&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:19: error: not found: value sc&amp;#010;       val res2</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; array: Array[Int] = Array(0, 0, 0, 0, 0)&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:19: error: not found: value sc&amp;#010;       val broadcastArray = sc.broadcast(array)&amp;#010;                            ^&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:17: error: not found: value sc&amp;#010;       val res1 = sc.parallelize(0 to 4).map(x =&gt; broadcastArray.value(x</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:17: error: not found: value sc&amp;#010;       var file = sc.textFile("/var/lib/jenkins/workspace/spark/repl/target/tmp/spark-6ece3709-ac5c-44e3-95f6-a0affc657468/input").cache()&amp;#010;                  ^&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:17: error: not found: value file&amp;#010;       val res1 = file.count()&amp;#010;            </li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; v: Int = 7&amp;#010;&amp;#010;scala&gt; getV: ()Int&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:19: error: not found: value sc&amp;#010;       val res1 = sc.parallelize(1 to 10).map(x =&gt; getV()).collect().reduceLeft(_+_)&amp;#010;                  ^&amp;#010;&amp;#010;scala&gt; v: Int = 10&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:19: error: not found: value sc&amp;#010;       val res2</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:17: error: not found: value spark&amp;#010;       import spark.implicits._&amp;#010;              ^&amp;#010;&amp;#010;scala&gt; defined class TestCaseClass&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:22: error: not found: value sc&amp;#010;       sc.parallelize(1 to 10).map(x =&gt; TestCaseClass(x)).toDF().collect()&amp;#010;       ^&amp;#010;&amp;#010;scala&gt; &amp;#010</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; import org.apache.spark.sql.functions._&amp;#010;&amp;#010;scala&gt; import org.apache.spark.sql.{Encoder, Encoders}&amp;#010;&amp;#010;scala&gt; import org.apache.spark.sql.expressions.Aggregator&amp;#010;&amp;#010;scala&gt; import org.apache.spark.sql.TypedColumn&amp;#010;&amp;#010;scala&gt;      |      |      |      |      |      |      | simpleSum: org.apache.sp</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; defined class TestClass&amp;#010;&amp;#010;scala&gt; t: TestClass = TestClass@4976ca7b&amp;#010;&amp;#010;scala&gt; import t.testMethod&amp;#010;&amp;#010;scala&gt; defined class TestCaseClass&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:31: error: not found: value sc&amp;#010;       sc.parallelize(1 to 10).map(x =&gt; TestCaseClass(x)).collect()&amp;#010;       ^&amp;#010;&amp;#010;scala&gt; </li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; defined class Foo&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:28: error: not found: value sc&amp;#010;       val res = sc.parallelize((1 to 100).map(Foo), 10).collect()&amp;#010;                 ^&amp;#010;&amp;#010;scala&gt;      | _result_1540450705162: Int = 1&amp;#010;&amp;#010;scala&gt; </li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; defined class Foo&amp;#010;&amp;#010;scala&gt; list: List[(Int, Foo)] = List((1,Foo(1)), (1,Foo(2)))&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:28: error: not found: value sc&amp;#010;       val res = sc.parallelize(list).groupByKey().collect()&amp;#010;                 ^&amp;#010;&amp;#010;scala&gt;      | _result_1540450705617: Int = 1&amp;#010;&amp;#010;scala&gt; </li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; timeout: Int = 60000&amp;#010;&amp;#010;scala&gt; start: Long = 1540450706952&amp;#010;&amp;#010;scala&gt;      |      |      | &lt;console&gt;:31: error: not found: value sc&amp;#010;       while(sc.statusTracker.getExecutorInfos.size != 3 &amp;&amp;&amp;#010;             ^&amp;#010;&amp;#010;scala&gt;      |      | &amp;#010;scala&gt; import org.apache.spark.storage.StorageLevel._&amp;</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; defined class Click&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:33: error: not found: value spark&amp;#010;       spark.implicits.newProductSeqEncoder[Click]&amp;#010;       ^&amp;#010;&amp;#010;scala&gt;      | _result_1540450709137: Int = 1&amp;#010;&amp;#010;scala&gt; </li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetAggregatorSuiteBase.setUp(JavaDatasetAggregatorSuiteBase.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.jav</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaApplySchemaSuite.tearDown(JavaApplySchemaSuite.java:60)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetAggregatorSuiteBase.setUp(JavaDatasetAggregatorSuiteBase.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.jav</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaApplySchemaSuite.tearDown(JavaApplySchemaSuite.java:60)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetAggregatorSuiteBase.setUp(JavaDatasetAggregatorSuiteBase.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.jav</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaApplySchemaSuite.tearDown(JavaApplySchemaSuite.java:60)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaColumnExpressionSuite.setUp(JavaColumnExpressionSuite.java:41)
sun.reflect.NativeMethodAccessorImpl.invoke0(</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaBeanDeserializationSuite.tearDown(JavaBeanDeserializationSuite.java:41)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaColumnExpressionSuite.setUp(JavaColumnExpressionSuite.java:41)
sun.reflect.NativeMethodAccessorImpl.invoke0(</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaBeanDeserializationSuite.tearDown(JavaBeanDeserializationSuite.java:41)
</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaColumnExpressionSuite.setUp(JavaColumnExpressionSuite.java:41)
sun.reflect.NativeMethodAccessorImpl.invoke0(</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaColumnExpressionSuite.tearDown(JavaColumnExpressionSuite.java:46)
</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>RpcEnv has been stopped</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameSuite.setUp(JavaDataFrameSuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.Delega</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameSuite.setUp(JavaDataFrameSuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameSuite.setUp(JavaDataFrameSuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameSuite.setUp(JavaDataFrameSuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameSuite.setUp(JavaDataFrameSuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameSuite.setUp(JavaDataFrameSuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameSuite.setUp(JavaDataFrameSuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameSuite.setUp(JavaDataFrameSuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameSuite.setUp(JavaDataFrameSuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameSuite.setUp(JavaDataFrameSuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameSuite.setUp(JavaDataFrameSuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameSuite.setUp(JavaDataFrameSuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameSuite.setUp(JavaDataFrameSuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameSuite.setUp(JavaDataFrameSuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameSuite.setUp(JavaDataFrameSuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameSuite.setUp(JavaDataFrameSuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameSuite.setUp(JavaDataFrameSuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameSuite.setUp(JavaDataFrameSuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameSuite.setUp(JavaDataFrameSuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameSuite.setUp(JavaDataFrameSuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameSuite.setUp(JavaDataFrameSuite.java:54)
sun.reflect.GeneratedMethodAccessor1.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.setUp(JavaDataFrameSuite.java:54)
</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameSuite.setUp(JavaDataFrameSuite.java:54)
sun.reflect.GeneratedMethodAccessor1.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetAggregatorSuiteBase.setUp(JavaDatasetAggregatorSuiteBase.java:45)
sun.reflect.NativeMethodAccessorImp</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetAggregatorSuiteBase.setUp(JavaDatasetAggregatorSuiteBase.java:45)
sun.reflect.NativeMethodAccessorImp</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetAggregatorSuiteBase.setUp(JavaDatasetAggregatorSuiteBase.java:45)
sun.reflect.NativeMethodAccessorImp</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.Delegating</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeC</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaSaveLoadSuite.tearDown(JavaSaveLoadSuite.java:76)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeC</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaSaveLoadSuite.tearDown(JavaSaveLoadSuite.java:76)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeC</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaUDAFSuite.tearDown(JavaUDAFSuite.java:42)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMetho</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaUDFSuite.tearDown(JavaUDFSuite.java:50)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMetho</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaUDFSuite.tearDown(JavaUDFSuite.java:50)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMetho</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaUDFSuite.tearDown(JavaUDFSuite.java:50)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMetho</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaUDFSuite.tearDown(JavaUDFSuite.java:50)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMetho</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaUDFSuite.tearDown(JavaUDFSuite.java:50)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMetho</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaUDFSuite.tearDown(JavaUDFSuite.java:50)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.tearDown(JavaDataStreamReaderWriterSuite.java:49)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.tearDown(JavaDataStreamReaderWriterSuite.java:49)
</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.sql.hive.HiveMetastoreLazyInitializationSuite$$anonfun$1.apply(HiveMetastoreLazyInitializationSuite.scala:32)&amp;#010;org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)&amp;#010;org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)&amp;#010;org.scalatest.Transformer.apply(Trans</li></div><div><li>java.lang.ExceptionInInitializerError
	at org.apache.spark.sql.hive.JavaDataFrameSuite.setUp(JavaDataFrameSuite.java:50)
Caused by: java.lang.IllegalStateException: 
Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.hive.JavaDataFrameSuite.setUp(JavaDataFrameSuite.java:50)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>Could not initialize class org.apache.spark.sql.hive.test.TestHive$</li></div><div><li>Could not initialize class org.apache.spark.sql.hive.test.TestHive$</li></div><div><li>Could not initialize class org.apache.spark.sql.hive.test.TestHive$</li></div><div><li>Could not initialize class org.apache.spark.sql.hive.test.TestHive$</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:850)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(StreamingContext.scala:85)
org.apache.spark.streaming.api.java.JavaStreamingContext.&lt;init&gt;(JavaStreaming</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:75)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexDescendingWithStart</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexWithStart</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndexDescendingWithStart</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexDescending</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexWithStart</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexWithLast</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexWithSkip</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexWithMax</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexDescending</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndexDescendingWithLast</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexDescending</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexDescendingWithLast</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndex</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndexWithLast</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexWithStart</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexDescendingWithStart</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexWithLast</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexWithSkip</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndexDescending</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.testRefWithIntNaturalKey</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexDescending</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexDescendingWithStart</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexWithMax</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndex</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexWithLast</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexWithSkip</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexWithMax</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexDescendingWithLast</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexDescendingWithLast</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexDescendingWithStart</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndex</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexWithLast</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexWithSkip</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexWithStart</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndex</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexDescendingWithLast</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndexWithStart</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndex</div></li><li><div>org.apache.spark.util.kvstore.LevelDBSuite.testMultipleTypesWriteReadDelete</div></li><li><div>org.apache.spark.util.kvstore.LevelDBSuite.testObjectWriteReadDelete</div></li><li><div>org.apache.spark.util.kvstore.LevelDBSuite.testSkip</div></li><li><div>org.apache.spark.util.kvstore.LevelDBSuite.testMultipleObjectWriteReadDelete</div></li><li><div>org.apache.spark.util.kvstore.LevelDBSuite.testReopenAndVersionCheckDb</div></li><li><div>org.apache.spark.util.kvstore.LevelDBSuite.testMetadata</div></li><li><div>org.apache.spark.util.kvstore.LevelDBSuite.testUpdate</div></li><li><div>org.apache.spark.util.kvstore.LevelDBSuite.testNegativeIndexValues</div></li><li><div>org.apache.spark.sql.kafka010.KafkaSourceStressSuite.stress test with multiple topics and partitions</div></li><li><div>org.apache.spark.sql.hive.HiveSparkSubmitSuite.SPARK-8020: set sql conf in spark conf</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.spark.network.ChunkFetchIntegrationSuite.fetchFileChunk</div></li><li><div>org.apache.spark.JavaJdbcRDDSuite.testJavaJdbcRDD</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.leftOuterJoin</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.leftOuterJoin</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.foldReduce</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.foldReduce</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.mapsFromPairsToPairs</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.mapsFromPairsToPairs</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.flatMap</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.flatMap</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.foreach</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.foreach</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.map</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.map</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.zip</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.zip</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.keyBy</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.keyBy</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.groupBy</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.groupBy</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.mapPartitions</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.mapPartitions</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.foldByKey</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.foldByKey</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.mapOnPairRDD</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.mapOnPairRDD</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.sequenceFile</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.sequenceFile</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.collectPartitions</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.collectPartitions</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.reduceByKey</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.reduceByKey</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.foreachWithAnonymousClass</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.foreachWithAnonymousClass</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.collectAsMapWithIntArrayValues</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.collectAsMapWithIntArrayValues</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.zipPartitions</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.zipPartitions</div></li><li><div>test.org.apache.spark.JavaAPISuite.groupByOnPairRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.groupByOnPairRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.binaryFilesCaching</div></li><li><div>test.org.apache.spark.JavaAPISuite.binaryFilesCaching</div></li><li><div>test.org.apache.spark.JavaAPISuite.sparkContextUnion</div></li><li><div>test.org.apache.spark.JavaAPISuite.sparkContextUnion</div></li><li><div>test.org.apache.spark.JavaAPISuite.checkpointAndComputation</div></li><li><div>test.org.apache.spark.JavaAPISuite.checkpointAndComputation</div></li><li><div>test.org.apache.spark.JavaAPISuite.leftOuterJoin</div></li><li><div>test.org.apache.spark.JavaAPISuite.keyByOnPairRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.wholeTextFiles</div></li><li><div>test.org.apache.spark.JavaAPISuite.wholeTextFiles</div></li><li><div>test.org.apache.spark.JavaAPISuite.binaryFiles</div></li><li><div>test.org.apache.spark.JavaAPISuite.binaryFiles</div></li><li><div>test.org.apache.spark.JavaAPISuite.foldReduce</div></li><li><div>test.org.apache.spark.JavaAPISuite.foldReduce</div></li><li><div>test.org.apache.spark.JavaAPISuite.writeWithNewAPIHadoopFile</div></li><li><div>test.org.apache.spark.JavaAPISuite.writeWithNewAPIHadoopFile</div></li><li><div>test.org.apache.spark.JavaAPISuite.hadoopFile</div></li><li><div>test.org.apache.spark.JavaAPISuite.hadoopFile</div></li><li><div>test.org.apache.spark.JavaAPISuite.lookup</div></li><li><div>test.org.apache.spark.JavaAPISuite.lookup</div></li><li><div>test.org.apache.spark.JavaAPISuite.countAsync</div></li><li><div>test.org.apache.spark.JavaAPISuite.countAsync</div></li><li><div>test.org.apache.spark.JavaAPISuite.textFiles</div></li><li><div>test.org.apache.spark.JavaAPISuite.binaryRecords</div></li><li><div>test.org.apache.spark.JavaAPISuite.toLocalIterator</div></li><li><div>test.org.apache.spark.JavaAPISuite.toLocalIterator</div></li><li><div>test.org.apache.spark.JavaAPISuite.repartitionAndSortWithinPartitions</div></li><li><div>test.org.apache.spark.JavaAPISuite.repartitionAndSortWithinPartitions</div></li><li><div>test.org.apache.spark.JavaAPISuite.reduce</div></li><li><div>test.org.apache.spark.JavaAPISuite.reduce</div></li><li><div>test.org.apache.spark.JavaAPISuite.sample</div></li><li><div>test.org.apache.spark.JavaAPISuite.sample</div></li><li><div>test.org.apache.spark.JavaAPISuite.sortBy</div></li><li><div>test.org.apache.spark.JavaAPISuite.sortBy</div></li><li><div>test.org.apache.spark.JavaAPISuite.mapsFromPairsToPairs</div></li><li><div>test.org.apache.spark.JavaAPISuite.mapsFromPairsToPairs</div></li><li><div>test.org.apache.spark.JavaAPISuite.flatMap</div></li><li><div>test.org.apache.spark.JavaAPISuite.flatMap</div></li><li><div>test.org.apache.spark.JavaAPISuite.cogroup3</div></li><li><div>test.org.apache.spark.JavaAPISuite.cogroup4</div></li><li><div>test.org.apache.spark.JavaAPISuite.cogroup4</div></li><li><div>test.org.apache.spark.JavaAPISuite.randomSplit</div></li><li><div>test.org.apache.spark.JavaAPISuite.randomSplit</div></li><li><div>test.org.apache.spark.JavaAPISuite.persist</div></li><li><div>test.org.apache.spark.JavaAPISuite.persist</div></li><li><div>test.org.apache.spark.JavaAPISuite.foreach</div></li><li><div>test.org.apache.spark.JavaAPISuite.foreach</div></li><li><div>test.org.apache.spark.JavaAPISuite.hadoopFileCompressed</div></li><li><div>test.org.apache.spark.JavaAPISuite.hadoopFileCompressed</div></li><li><div>test.org.apache.spark.JavaAPISuite.textFilesCompressed</div></li><li><div>test.org.apache.spark.JavaAPISuite.textFilesCompressed</div></li><li><div>test.org.apache.spark.JavaAPISuite.testAsyncActionCancellation</div></li><li><div>test.org.apache.spark.JavaAPISuite.testAsyncActionCancellation</div></li><li><div>test.org.apache.spark.JavaAPISuite.checkpointAndRestore</div></li><li><div>test.org.apache.spark.JavaAPISuite.checkpointAndRestore</div></li><li><div>test.org.apache.spark.JavaAPISuite.sortByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.sortByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.aggregateByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.aggregateByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.map</div></li><li><div>test.org.apache.spark.JavaAPISuite.map</div></li><li><div>test.org.apache.spark.JavaAPISuite.max</div></li><li><div>test.org.apache.spark.JavaAPISuite.max</div></li><li><div>test.org.apache.spark.JavaAPISuite.min</div></li><li><div>test.org.apache.spark.JavaAPISuite.min</div></li><li><div>test.org.apache.spark.JavaAPISuite.top</div></li><li><div>test.org.apache.spark.JavaAPISuite.top</div></li><li><div>test.org.apache.spark.JavaAPISuite.zip</div></li><li><div>test.org.apache.spark.JavaAPISuite.zip</div></li><li><div>test.org.apache.spark.JavaAPISuite.fold</div></li><li><div>test.org.apache.spark.JavaAPISuite.fold</div></li><li><div>test.org.apache.spark.JavaAPISuite.glom</div></li><li><div>test.org.apache.spark.JavaAPISuite.glom</div></li><li><div>test.org.apache.spark.JavaAPISuite.take</div></li><li><div>test.org.apache.spark.JavaAPISuite.take</div></li><li><div>test.org.apache.spark.JavaAPISuite.javaDoubleRDDHistoGram</div></li><li><div>test.org.apache.spark.JavaAPISuite.javaDoubleRDDHistoGram</div></li><li><div>test.org.apache.spark.JavaAPISuite.collectUnderlyingScalaRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.collectUnderlyingScalaRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.keyBy</div></li><li><div>test.org.apache.spark.JavaAPISuite.keyBy</div></li><li><div>test.org.apache.spark.JavaAPISuite.mapPartitionsWithIndex</div></li><li><div>test.org.apache.spark.JavaAPISuite.mapPartitionsWithIndex</div></li><li><div>test.org.apache.spark.JavaAPISuite.sampleByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.sampleByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.intersection</div></li><li><div>test.org.apache.spark.JavaAPISuite.intersection</div></li><li><div>test.org.apache.spark.JavaAPISuite.aggregate</div></li><li><div>test.org.apache.spark.JavaAPISuite.aggregate</div></li><li><div>test.org.apache.spark.JavaAPISuite.cartesian</div></li><li><div>test.org.apache.spark.JavaAPISuite.cartesian</div></li><li><div>test.org.apache.spark.JavaAPISuite.countApproxDistinctByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.countApproxDistinctByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.readWithNewAPIHadoopFile</div></li><li><div>test.org.apache.spark.JavaAPISuite.readWithNewAPIHadoopFile</div></li><li><div>test.org.apache.spark.JavaAPISuite.testRegisterKryoClasses</div></li><li><div>test.org.apache.spark.JavaAPISuite.testRegisterKryoClasses</div></li><li><div>test.org.apache.spark.JavaAPISuite.groupBy</div></li><li><div>test.org.apache.spark.JavaAPISuite.groupBy</div></li><li><div>test.org.apache.spark.JavaAPISuite.sampleByKeyExact</div></li><li><div>test.org.apache.spark.JavaAPISuite.sampleByKeyExact</div></li><li><div>test.org.apache.spark.JavaAPISuite.mapPartitions</div></li><li><div>test.org.apache.spark.JavaAPISuite.takeOrdered</div></li><li><div>test.org.apache.spark.JavaAPISuite.foldByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.objectFilesOfInts</div></li><li><div>test.org.apache.spark.JavaAPISuite.treeAggregate</div></li><li><div>test.org.apache.spark.JavaAPISuite.testGetPersistentRDDs</div></li><li><div>test.org.apache.spark.JavaAPISuite.testGetPersistentRDDs</div></li><li><div>test.org.apache.spark.JavaAPISuite.approximateResults</div></li><li><div>test.org.apache.spark.JavaAPISuite.approximateResults</div></li><li><div>test.org.apache.spark.JavaAPISuite.treeReduce</div></li><li><div>test.org.apache.spark.JavaAPISuite.treeReduce</div></li><li><div>test.org.apache.spark.JavaAPISuite.collectAsMapAndSerialize</div></li><li><div>test.org.apache.spark.JavaAPISuite.collectAsMapAndSerialize</div></li><li><div>test.org.apache.spark.JavaAPISuite.countApproxDistinct</div></li><li><div>test.org.apache.spark.JavaAPISuite.countApproxDistinct</div></li><li><div>test.org.apache.spark.JavaAPISuite.javaDoubleRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.javaDoubleRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.mapOnPairRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.mapOnPairRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.testAsyncActionErrorWrapping</div></li><li><div>test.org.apache.spark.JavaAPISuite.testAsyncActionErrorWrapping</div></li><li><div>test.org.apache.spark.JavaAPISuite.naturalMax</div></li><li><div>test.org.apache.spark.JavaAPISuite.naturalMax</div></li><li><div>test.org.apache.spark.JavaAPISuite.naturalMin</div></li><li><div>test.org.apache.spark.JavaAPISuite.naturalMin</div></li><li><div>test.org.apache.spark.JavaAPISuite.sequenceFile</div></li><li><div>test.org.apache.spark.JavaAPISuite.sequenceFile</div></li><li><div>test.org.apache.spark.JavaAPISuite.collectPartitions</div></li><li><div>test.org.apache.spark.JavaAPISuite.collectPartitions</div></li><li><div>test.org.apache.spark.JavaAPISuite.cogroup</div></li><li><div>test.org.apache.spark.JavaAPISuite.cogroup</div></li><li><div>test.org.apache.spark.JavaAPISuite.reduceByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.reduceByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.repartition</div></li><li><div>test.org.apache.spark.JavaAPISuite.iterator</div></li><li><div>test.org.apache.spark.JavaAPISuite.iterator</div></li><li><div>test.org.apache.spark.JavaAPISuite.emptyRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.emptyRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.zipWithIndex</div></li><li><div>test.org.apache.spark.JavaAPISuite.zipWithIndex</div></li><li><div>test.org.apache.spark.JavaAPISuite.foreachPartition</div></li><li><div>test.org.apache.spark.JavaAPISuite.foreachPartition</div></li><li><div>test.org.apache.spark.JavaAPISuite.combineByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.combineByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.takeAsync</div></li><li><div>test.org.apache.spark.JavaAPISuite.takeAsync</div></li><li><div>test.org.apache.spark.JavaAPISuite.collectAsMapWithIntArrayValues</div></li><li><div>test.org.apache.spark.JavaAPISuite.collectAsMapWithIntArrayValues</div></li><li><div>test.org.apache.spark.JavaAPISuite.objectFilesOfComplexTypes</div></li><li><div>test.org.apache.spark.JavaAPISuite.objectFilesOfComplexTypes</div></li><li><div>test.org.apache.spark.JavaAPISuite.zipWithUniqueId</div></li><li><div>test.org.apache.spark.JavaAPISuite.zipWithUniqueId</div></li><li><div>test.org.apache.spark.JavaAPISuite.collectAsync</div></li><li><div>test.org.apache.spark.JavaAPISuite.collectAsync</div></li><li><div>test.org.apache.spark.JavaAPISuite.foreachAsync</div></li><li><div>test.org.apache.spark.JavaAPISuite.foreachAsync</div></li><li><div>test.org.apache.spark.JavaAPISuite.zipPartitions</div></li><li><div>test.org.apache.spark.JavaAPISuite.reduceOnJavaDoubleRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.reduceOnJavaDoubleRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.isEmpty</div></li><li><div>test.org.apache.spark.JavaAPISuite.isEmpty</div></li><li><div>test.org.apache.spark.JavaSparkContextSuite.javaSparkContext</div></li><li><div>test.org.apache.spark.JavaSparkContextSuite.scalaSparkContext</div></li><li><div>org.apache.spark.sql.avro.AvroFunctionsSuite.roundtrip in to_avro and from_avro - int and string</div></li><li><div>org.apache.spark.sql.avro.AvroFunctionsSuite.roundtrip in to_avro and from_avro - struct</div></li><li><div>org.apache.spark.sql.avro.AvroFunctionsSuite.roundtrip in to_avro and from_avro - array with null</div></li><li><div>org.apache.spark.streaming.kafka010.JavaDirectKafkaStreamSuite.testKafkaStream</div></li><li><div>org.apache.spark.streaming.kafka010.JavaKafkaRDDSuite.testKafkaRDD</div></li><li><div>org.apache.spark.graphx.EdgeRDDSuite.checkpointing</div></li><li><div>org.apache.spark.graphx.GraphLoaderSuite.GraphLoader.edgeListFile</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.joinVertices</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.collectNeighborIds</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.removeSelfEdges</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.filter</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.convertToCanonicalEdges</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.collectEdgesCycleDirectionOut</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.collectEdgesCycleDirectionIn</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.collectEdgesCycleDirectionEither</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.collectEdgesChainDirectionOut</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.collectEdgesChainDirectionIn</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.collectEdgesChainDirectionEither</div></li><li><div>org.apache.spark.graphx.GraphSuite.Graph.fromEdgeTuples</div></li><li><div>org.apache.spark.graphx.GraphSuite.Graph.fromEdges</div></li><li><div>org.apache.spark.graphx.GraphSuite.Graph.apply</div></li><li><div>org.apache.spark.graphx.GraphSuite.triplets</div></li><li><div>org.apache.spark.graphx.GraphSuite.partitionBy</div></li><li><div>org.apache.spark.graphx.GraphSuite.mapVertices</div></li><li><div>org.apache.spark.graphx.GraphSuite.mapVertices changing type with same erased type</div></li><li><div>org.apache.spark.graphx.GraphSuite.mapEdges</div></li><li><div>org.apache.spark.graphx.GraphSuite.mapTriplets</div></li><li><div>org.apache.spark.graphx.GraphSuite.reverse</div></li><li><div>org.apache.spark.graphx.GraphSuite.reverse with join elimination</div></li><li><div>org.apache.spark.graphx.GraphSuite.subgraph</div></li><li><div>org.apache.spark.graphx.GraphSuite.mask</div></li><li><div>org.apache.spark.graphx.GraphSuite.groupEdges</div></li><li><div>org.apache.spark.graphx.GraphSuite.aggregateMessages</div></li><li><div>org.apache.spark.graphx.GraphSuite.outerJoinVertices</div></li><li><div>org.apache.spark.graphx.GraphSuite.more edge partitions than vertex partitions</div></li><li><div>org.apache.spark.graphx.GraphSuite.checkpoint</div></li><li><div>org.apache.spark.graphx.GraphSuite.cache, getStorageLevel</div></li><li><div>org.apache.spark.graphx.GraphSuite.non-default number of edge partitions</div></li><li><div>org.apache.spark.graphx.GraphSuite.unpersist graph RDD</div></li><li><div>org.apache.spark.graphx.GraphSuite.SPARK-14219: pickRandomVertex</div></li><li><div>org.apache.spark.graphx.PregelSuite.1 iteration</div></li><li><div>org.apache.spark.graphx.PregelSuite.chain propagation</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.filter</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.mapValues</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.minus</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.minus with RDD[(VertexId, VD)]</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.minus with non-equal number of partitions</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.diff</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.diff with RDD[(VertexId, VD)]</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.diff vertices with non-equal number of partitions</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.leftJoin</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.leftJoin vertices with non-equal number of partitions</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.innerJoin</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.innerJoin vertices with the non-equal number of partitions</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.aggregateUsingIndex</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.mergeFunc</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.cache, getStorageLevel</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.checkpoint</div></li><li><div>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Grid Connected Components</div></li><li><div>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Reverse Grid Connected Components</div></li><li><div>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Chain Connected Components</div></li><li><div>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Reverse Chain Connected Components</div></li><li><div>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Connected Components on a Toy Connected Graph</div></li><li><div>org.apache.spark.graphx.lib.LabelPropagationSuite.Label Propagation</div></li><li><div>org.apache.spark.graphx.lib.PageRankSuite.Star PageRank</div></li><li><div>org.apache.spark.graphx.lib.PageRankSuite.Star PersonalPageRank</div></li><li><div>org.apache.spark.graphx.lib.PageRankSuite.Grid PageRank</div></li><li><div>org.apache.spark.graphx.lib.PageRankSuite.Chain PageRank</div></li><li><div>org.apache.spark.graphx.lib.PageRankSuite.Chain PersonalizedPageRank</div></li><li><div>org.apache.spark.graphx.lib.PageRankSuite.Loop with source PageRank</div></li><li><div>org.apache.spark.graphx.lib.PageRankSuite.Loop with sink PageRank</div></li><li><div>org.apache.spark.graphx.lib.SVDPlusPlusSuite.Test SVD++ with mean square error on training set</div></li><li><div>org.apache.spark.graphx.lib.ShortestPathsSuite.Shortest Path Computations</div></li><li><div>org.apache.spark.graphx.lib.StronglyConnectedComponentsSuite.Island Strongly Connected Components</div></li><li><div>org.apache.spark.graphx.lib.StronglyConnectedComponentsSuite.Cycle Strongly Connected Components</div></li><li><div>org.apache.spark.graphx.lib.StronglyConnectedComponentsSuite.2 Cycle Strongly Connected Components</div></li><li><div>org.apache.spark.graphx.lib.TriangleCountSuite.Count a single triangle</div></li><li><div>org.apache.spark.graphx.lib.TriangleCountSuite.Count two triangles</div></li><li><div>org.apache.spark.graphx.lib.TriangleCountSuite.Count two triangles with bi-directed edges</div></li><li><div>org.apache.spark.graphx.lib.TriangleCountSuite.Count a single triangle with duplicate edges</div></li><li><div>org.apache.spark.graphx.util.GraphGeneratorsSuite.GraphGenerators.logNormalGraph</div></li><li><div>org.apache.spark.graphx.util.GraphGeneratorsSuite.SPARK-5064 GraphGenerators.rmatGraph numEdges upper bound</div></li><li><div>org.apache.spark.graphx.util.PeriodicGraphCheckpointerSuite.Persisting</div></li><li><div>org.apache.spark.graphx.util.PeriodicGraphCheckpointerSuite.Checkpointing</div></li><li><div>org.apache.spark.ml.JavaPipelineSuite.pipeline</div></li><li><div>org.apache.spark.ml.JavaPipelineSuite.pipeline</div></li><li><div>org.apache.spark.ml.classification.JavaDecisionTreeClassifierSuite.runDT</div></li><li><div>org.apache.spark.ml.classification.JavaDecisionTreeClassifierSuite.runDT</div></li><li><div>org.apache.spark.ml.classification.JavaGBTClassifierSuite.runDT</div></li><li><div>org.apache.spark.ml.classification.JavaGBTClassifierSuite.runDT</div></li><li><div>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionWithSetters</div></li><li><div>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionWithSetters</div></li><li><div>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionTrainingSummary</div></li><li><div>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionTrainingSummary</div></li><li><div>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionPredictorClassifierMethods</div></li><li><div>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionPredictorClassifierMethods</div></li><li><div>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionDefaultParams</div></li><li><div>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionDefaultParams</div></li><li><div>org.apache.spark.ml.classification.JavaMultilayerPerceptronClassifierSuite.testMLPC</div></li><li><div>org.apache.spark.ml.classification.JavaMultilayerPerceptronClassifierSuite.testMLPC</div></li><li><div>org.apache.spark.ml.classification.JavaNaiveBayesSuite.testNaiveBayes</div></li><li><div>org.apache.spark.ml.classification.JavaNaiveBayesSuite.naiveBayesDefaultParams</div></li><li><div>org.apache.spark.ml.classification.JavaNaiveBayesSuite.naiveBayesDefaultParams</div></li><li><div>org.apache.spark.ml.classification.JavaOneVsRestSuite.oneVsRestDefaultParams</div></li><li><div>org.apache.spark.ml.classification.JavaOneVsRestSuite.oneVsRestDefaultParams</div></li><li><div>org.apache.spark.ml.classification.JavaRandomForestClassifierSuite.runDT</div></li><li><div>org.apache.spark.ml.clustering.JavaKMeansSuite.fitAndTransform</div></li><li><div>org.apache.spark.ml.clustering.JavaKMeansSuite.fitAndTransform</div></li><li><div>org.apache.spark.ml.feature.JavaBucketizerSuite.bucketizerTest</div></li><li><div>org.apache.spark.ml.feature.JavaBucketizerSuite.bucketizerMultipleColumnsTest</div></li><li><div>org.apache.spark.ml.feature.JavaBucketizerSuite.bucketizerMultipleColumnsTest</div></li><li><div>org.apache.spark.ml.feature.JavaDCTSuite.javaCompatibilityTest</div></li><li><div>org.apache.spark.ml.feature.JavaDCTSuite.javaCompatibilityTest</div></li><li><div>org.apache.spark.ml.feature.JavaHashingTFSuite.hashingTF</div></li><li><div>org.apache.spark.ml.feature.JavaNormalizerSuite.normalizer</div></li><li><div>org.apache.spark.ml.feature.JavaNormalizerSuite.normalizer</div></li><li><div>org.apache.spark.ml.feature.JavaPCASuite.testPCA</div></li><li><div>org.apache.spark.ml.feature.JavaPCASuite.testPCA</div></li><li><div>org.apache.spark.ml.feature.JavaPolynomialExpansionSuite.polynomialExpansionTest</div></li><li><div>org.apache.spark.ml.feature.JavaPolynomialExpansionSuite.polynomialExpansionTest</div></li><li><div>org.apache.spark.ml.feature.JavaStandardScalerSuite.standardScaler</div></li><li><div>org.apache.spark.ml.feature.JavaStandardScalerSuite.standardScaler</div></li><li><div>org.apache.spark.ml.feature.JavaStopWordsRemoverSuite.javaCompatibilityTest</div></li><li><div>org.apache.spark.ml.feature.JavaStringIndexerSuite.testStringIndexer</div></li><li><div>org.apache.spark.ml.feature.JavaStringIndexerSuite.testStringIndexer</div></li><li><div>org.apache.spark.ml.feature.JavaTokenizerSuite.regexTokenizer</div></li><li><div>org.apache.spark.ml.feature.JavaTokenizerSuite.regexTokenizer</div></li><li><div>org.apache.spark.ml.feature.JavaVectorAssemblerSuite.testVectorAssembler</div></li><li><div>org.apache.spark.ml.feature.JavaVectorAssemblerSuite.testVectorAssembler</div></li><li><div>org.apache.spark.ml.feature.JavaVectorIndexerSuite.vectorIndexerAPI</div></li><li><div>org.apache.spark.ml.feature.JavaVectorIndexerSuite.vectorIndexerAPI</div></li><li><div>org.apache.spark.ml.feature.JavaVectorSlicerSuite.vectorSlice</div></li><li><div>org.apache.spark.ml.feature.JavaVectorSlicerSuite.vectorSlice</div></li><li><div>org.apache.spark.ml.feature.JavaWord2VecSuite.testJavaWord2Vec</div></li><li><div>org.apache.spark.ml.regression.JavaDecisionTreeRegressorSuite.runDT</div></li><li><div>org.apache.spark.ml.regression.JavaDecisionTreeRegressorSuite.runDT</div></li><li><div>org.apache.spark.ml.regression.JavaGBTRegressorSuite.runDT</div></li><li><div>org.apache.spark.ml.regression.JavaGBTRegressorSuite.runDT</div></li><li><div>org.apache.spark.ml.regression.JavaLinearRegressionSuite.linearRegressionDefaultParams</div></li><li><div>org.apache.spark.ml.regression.JavaLinearRegressionSuite.linearRegressionDefaultParams</div></li><li><div>org.apache.spark.ml.regression.JavaLinearRegressionSuite.linearRegressionWithSetters</div></li><li><div>org.apache.spark.ml.regression.JavaLinearRegressionSuite.linearRegressionWithSetters</div></li><li><div>org.apache.spark.ml.regression.JavaRandomForestRegressorSuite.runDT</div></li><li><div>org.apache.spark.ml.regression.JavaRandomForestRegressorSuite.runDT</div></li><li><div>org.apache.spark.ml.source.libsvm.JavaLibSVMRelationSuite.verifyLibSVMDF</div></li><li><div>org.apache.spark.ml.stat.JavaKolmogorovSmirnovTestSuite.testKSTestNamedDistribution</div></li><li><div>org.apache.spark.ml.stat.JavaKolmogorovSmirnovTestSuite.testKSTestNamedDistribution</div></li><li><div>org.apache.spark.ml.stat.JavaKolmogorovSmirnovTestSuite.testKSTestCDF</div></li><li><div>org.apache.spark.ml.stat.JavaKolmogorovSmirnovTestSuite.testKSTestCDF</div></li><li><div>org.apache.spark.ml.stat.JavaSummarizerSuite.testSummarizer</div></li><li><div>org.apache.spark.ml.stat.JavaSummarizerSuite.testSummarizer</div></li><li><div>org.apache.spark.ml.tuning.JavaCrossValidatorSuite.crossValidationWithLogisticRegression</div></li><li><div>org.apache.spark.ml.tuning.JavaCrossValidatorSuite.crossValidationWithLogisticRegression</div></li><li><div>org.apache.spark.ml.util.JavaDefaultReadWriteSuite.testDefaultReadWrite</div></li><li><div>org.apache.spark.ml.util.JavaDefaultReadWriteSuite.testDefaultReadWrite</div></li><li><div>org.apache.spark.mllib.classification.JavaLogisticRegressionSuite.runLRUsingConstructor</div></li><li><div>org.apache.spark.mllib.classification.JavaLogisticRegressionSuite.runLRUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.classification.JavaLogisticRegressionSuite.runLRUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.testPredictJavaRDD</div></li><li><div>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.testPredictJavaRDD</div></li><li><div>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.runUsingConstructor</div></li><li><div>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.runUsingConstructor</div></li><li><div>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.runUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.runUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.testModelTypeSetters</div></li><li><div>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.testModelTypeSetters</div></li><li><div>org.apache.spark.mllib.classification.JavaSVMSuite.runSVMUsingConstructor</div></li><li><div>org.apache.spark.mllib.classification.JavaSVMSuite.runSVMUsingConstructor</div></li><li><div>org.apache.spark.mllib.classification.JavaSVMSuite.runSVMUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.classification.JavaSVMSuite.runSVMUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.classification.JavaStreamingLogisticRegressionSuite.javaAPI</div></li><li><div>org.apache.spark.mllib.classification.JavaStreamingLogisticRegressionSuite.javaAPI</div></li><li><div>org.apache.spark.mllib.clustering.JavaBisectingKMeansSuite.twoDimensionalData</div></li><li><div>org.apache.spark.mllib.clustering.JavaBisectingKMeansSuite.twoDimensionalData</div></li><li><div>org.apache.spark.mllib.clustering.JavaGaussianMixtureSuite.runGaussianMixture</div></li><li><div>org.apache.spark.mllib.clustering.JavaGaussianMixtureSuite.runGaussianMixture</div></li><li><div>org.apache.spark.mllib.clustering.JavaKMeansSuite.testPredictJavaRDD</div></li><li><div>org.apache.spark.mllib.clustering.JavaKMeansSuite.testPredictJavaRDD</div></li><li><div>org.apache.spark.mllib.clustering.JavaKMeansSuite.runKMeansUsingConstructor</div></li><li><div>org.apache.spark.mllib.clustering.JavaKMeansSuite.runKMeansUsingConstructor</div></li><li><div>org.apache.spark.mllib.clustering.JavaKMeansSuite.runKMeansUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.clustering.JavaKMeansSuite.runKMeansUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.clustering.JavaLDASuite.onlineOptimizerCompatibility</div></li><li><div>org.apache.spark.mllib.clustering.JavaLDASuite.distributedLDAModel</div></li><li><div>org.apache.spark.mllib.clustering.JavaLDASuite.distributedLDAModel</div></li><li><div>org.apache.spark.mllib.clustering.JavaLDASuite.localLDAModel</div></li><li><div>org.apache.spark.mllib.clustering.JavaLDASuite.localLDAModel</div></li><li><div>org.apache.spark.mllib.clustering.JavaLDASuite.localLdaMethods</div></li><li><div>org.apache.spark.mllib.clustering.JavaLDASuite.localLdaMethods</div></li><li><div>org.apache.spark.mllib.clustering.JavaStreamingKMeansSuite.javaAPI</div></li><li><div>org.apache.spark.mllib.clustering.JavaStreamingKMeansSuite.javaAPI</div></li><li><div>org.apache.spark.mllib.evaluation.JavaRankingMetricsSuite.rankingMetrics</div></li><li><div>org.apache.spark.mllib.evaluation.JavaRankingMetricsSuite.rankingMetrics</div></li><li><div>org.apache.spark.mllib.feature.JavaTfIdfSuite.tfIdfMinimumDocumentFrequency</div></li><li><div>org.apache.spark.mllib.feature.JavaTfIdfSuite.tfIdfMinimumDocumentFrequency</div></li><li><div>org.apache.spark.mllib.feature.JavaTfIdfSuite.tfIdf</div></li><li><div>org.apache.spark.mllib.feature.JavaTfIdfSuite.tfIdf</div></li><li><div>org.apache.spark.mllib.feature.JavaWord2VecSuite.word2Vec</div></li><li><div>org.apache.spark.mllib.feature.JavaWord2VecSuite.word2Vec</div></li><li><div>org.apache.spark.mllib.fpm.JavaAssociationRulesSuite.runAssociationRules</div></li><li><div>org.apache.spark.mllib.fpm.JavaAssociationRulesSuite.runAssociationRules</div></li><li><div>org.apache.spark.mllib.fpm.JavaFPGrowthSuite.runFPGrowthSaveLoad</div></li><li><div>org.apache.spark.mllib.fpm.JavaFPGrowthSuite.runFPGrowthSaveLoad</div></li><li><div>org.apache.spark.mllib.fpm.JavaFPGrowthSuite.runFPGrowth</div></li><li><div>org.apache.spark.mllib.fpm.JavaFPGrowthSuite.runFPGrowth</div></li><li><div>org.apache.spark.mllib.fpm.JavaPrefixSpanSuite.runPrefixSpan</div></li><li><div>org.apache.spark.mllib.fpm.JavaPrefixSpanSuite.runPrefixSpan</div></li><li><div>org.apache.spark.mllib.fpm.JavaPrefixSpanSuite.runPrefixSpanSaveLoad</div></li><li><div>org.apache.spark.mllib.fpm.JavaPrefixSpanSuite.runPrefixSpanSaveLoad</div></li><li><div>org.apache.spark.mllib.linalg.distributed.JavaRowMatrixSuite.rowMatrixQRDecomposition</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testNormalVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testNormalVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testArbitrary</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testArbitrary</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testLogNormalVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testLogNormalVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testExponentialVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testExponentialVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testUniformRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testUniformRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testRandomVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testRandomVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testGammaRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testGammaRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testUniformVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testUniformVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testPoissonRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testPoissonRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testNormalRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testNormalRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testPoissonVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testPoissonVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testGammaVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testGammaVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testExponentialRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testExponentialRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testLNormalRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testLNormalRDD</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runALSUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runALSUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSUsingConstructor</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSUsingConstructor</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runRecommend</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runRecommend</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSWithNegativeWeight</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSWithNegativeWeight</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runALSUsingConstructor</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runALSUsingConstructor</div></li><li><div>org.apache.spark.mllib.regression.JavaIsotonicRegressionSuite.testIsotonicRegressionJavaRDD</div></li><li><div>org.apache.spark.mllib.regression.JavaIsotonicRegressionSuite.testIsotonicRegressionJavaRDD</div></li><li><div>org.apache.spark.mllib.regression.JavaIsotonicRegressionSuite.testIsotonicRegressionPredictionsJavaRDD</div></li><li><div>org.apache.spark.mllib.regression.JavaIsotonicRegressionSuite.testIsotonicRegressionPredictionsJavaRDD</div></li><li><div>org.apache.spark.mllib.regression.JavaLassoSuite.runLassoUsingConstructor</div></li><li><div>org.apache.spark.mllib.regression.JavaLassoSuite.runLassoUsingConstructor</div></li><li><div>org.apache.spark.mllib.regression.JavaLassoSuite.runLassoUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.regression.JavaLassoSuite.runLassoUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.testPredictJavaRDD</div></li><li><div>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.testPredictJavaRDD</div></li><li><div>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.runLinearRegressionUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.runLinearRegressionUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.runLinearRegressionUsingConstructor</div></li><li><div>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.runLinearRegressionUsingConstructor</div></li><li><div>org.apache.spark.mllib.regression.JavaRidgeRegressionSuite.runRidgeRegressionUsingConstructor</div></li><li><div>org.apache.spark.mllib.regression.JavaRidgeRegressionSuite.runRidgeRegressionUsingConstructor</div></li><li><div>org.apache.spark.mllib.regression.JavaRidgeRegressionSuite.runRidgeRegressionUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.regression.JavaRidgeRegressionSuite.runRidgeRegressionUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.regression.JavaStreamingLinearRegressionSuite.javaAPI</div></li><li><div>org.apache.spark.mllib.regression.JavaStreamingLinearRegressionSuite.javaAPI</div></li><li><div>org.apache.spark.mllib.stat.JavaStatisticsSuite.testCorr</div></li><li><div>org.apache.spark.mllib.stat.JavaStatisticsSuite.testCorr</div></li><li><div>org.apache.spark.mllib.stat.JavaStatisticsSuite.chiSqTest</div></li><li><div>org.apache.spark.mllib.stat.JavaStatisticsSuite.chiSqTest</div></li><li><div>org.apache.spark.mllib.stat.JavaStatisticsSuite.streamingTest</div></li><li><div>org.apache.spark.mllib.stat.JavaStatisticsSuite.streamingTest</div></li><li><div>org.apache.spark.mllib.stat.JavaStatisticsSuite.kolmogorovSmirnovTest</div></li><li><div>org.apache.spark.mllib.tree.JavaDecisionTreeSuite.runDTUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.tree.JavaDecisionTreeSuite.runDTUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.tree.JavaDecisionTreeSuite.runDTUsingConstructor</div></li><li><div>org.apache.spark.mllib.tree.JavaDecisionTreeSuite.runDTUsingConstructor</div></li><li><div>org.apache.spark.mllib.util.JavaMLUtilsSuite.testConvertMatrixColumnsToAndFromML</div></li><li><div>org.apache.spark.mllib.util.JavaMLUtilsSuite.testConvertMatrixColumnsToAndFromML</div></li><li><div>org.apache.spark.mllib.util.JavaMLUtilsSuite.testConvertVectorColumnsToAndFromML</div></li><li><div>org.apache.spark.mllib.util.JavaMLUtilsSuite.testConvertVectorColumnsToAndFromML</div></li><li><div>org.apache.spark.repl.ReplSuite.SPARK-15236: use Hive catalog</div></li><li><div>org.apache.spark.repl.ReplSuite.SPARK-15236: use in-memory catalog</div></li><li><div>org.apache.spark.repl.ReplSuite.broadcast vars</div></li><li><div>org.apache.spark.repl.ReplSuite.line wrapper only initialized once when used as encoder outer scope</div></li><li><div>org.apache.spark.repl.ReplSuite.define case class and create Dataset together with paste mode</div></li><li><div>org.apache.spark.repl.ReplSuite.:replay should work correctly</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.simple foreach with accumulator</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.external vars</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.external classes</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.external functions</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.external functions that access vars</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.broadcast vars</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.interacting with files</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.local-cluster mode</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.SPARK-2576 importing implicits</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.Datasets and encoders</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.SPARK-2632 importing a method from non serializable class and not using it.</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.collecting objects of class defined in repl</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.collecting objects of class defined in repl - shuffling</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.replicating blocks of object with class defined in repl</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.newProductSeqEncoder with REPL defined class</div></li><li><div>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationCount</div></li><li><div>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationCount</div></li><li><div>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationSumDouble</div></li><li><div>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationSumDouble</div></li><li><div>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationSumLong</div></li><li><div>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationSumLong</div></li><li><div>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationAverage</div></li><li><div>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationAverage</div></li><li><div>test.org.apache.spark.sql.JavaApplySchemaSuite.applySchema</div></li><li><div>test.org.apache.spark.sql.JavaApplySchemaSuite.applySchema</div></li><li><div>test.org.apache.spark.sql.JavaApplySchemaSuite.dataFrameRDDOperations</div></li><li><div>test.org.apache.spark.sql.JavaApplySchemaSuite.dataFrameRDDOperations</div></li><li><div>test.org.apache.spark.sql.JavaApplySchemaSuite.applySchemaToJSON</div></li><li><div>test.org.apache.spark.sql.JavaApplySchemaSuite.applySchemaToJSON</div></li><li><div>test.org.apache.spark.sql.JavaBeanDeserializationSuite.testBeanWithArrayFieldDeserialization</div></li><li><div>test.org.apache.spark.sql.JavaBeanDeserializationSuite.testBeanWithArrayFieldDeserialization</div></li><li><div>test.org.apache.spark.sql.JavaBeanDeserializationSuite.testBeanWithMapFieldsDeserialization</div></li><li><div>test.org.apache.spark.sql.JavaBeanDeserializationSuite.testBeanWithMapFieldsDeserialization</div></li><li><div>test.org.apache.spark.sql.JavaColumnExpressionSuite.isInCollectionCheckExceptionMessage</div></li><li><div>test.org.apache.spark.sql.JavaColumnExpressionSuite.isInCollectionCheckExceptionMessage</div></li><li><div>test.org.apache.spark.sql.JavaColumnExpressionSuite.isInCollectionWorksCorrectlyOnJava</div></li><li><div>test.org.apache.spark.sql.JavaColumnExpressionSuite.isInCollectionWorksCorrectlyOnJava</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testFormatAPI</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testTextAPI</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testJsonAPI</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testLoadAPI</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testOptionsAPI</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testSaveModeAPI</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testCsvAPI</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testParquetAPI</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testTextFileAPI</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCollectAndTake</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCollectAndTake</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testJsonRDDToDataFrame</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testJsonRDDToDataFrame</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testVarargMethods</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testVarargMethods</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testBeanWithoutGetter</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testBeanWithoutGetter</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateStructTypeFromList</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateStructTypeFromList</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testSampleBy</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testSampleBy</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCrosstab</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCrosstab</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testUDF</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testUDF</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFromFromList</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFromFromList</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCircularReferenceBean</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCircularReferenceBean</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testFrequentItems</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testFrequentItems</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testSampleByColumn</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testSampleByColumn</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testExecution</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testExecution</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testTextLoad</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testTextLoad</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.pivot</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.pivot</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testGenericLoad</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testGenericLoad</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCountMinSketch</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCountMinSketch</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.pivotColumnValues</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.pivotColumnValues</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFrameFromJavaBeans</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFrameFromJavaBeans</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCorrelation</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCorrelation</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testBloomFilter</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testBloomFilter</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCovariance</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCovariance</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFrameFromLocalJavaBeans</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFrameFromLocalJavaBeans</div></li><li><div>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationCount</div></li><li><div>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationCount</div></li><li><div>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationSumDouble</div></li><li><div>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationSumLong</div></li><li><div>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationAnonClass</div></li><li><div>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationAverage</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testRuntimeNullabilityCheck</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean1</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean1</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean2</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean2</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean3</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean3</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testSerializeNull</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testSerializeNull</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testRandomSplit</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testRandomSplit</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testTypedFilterPreservingSchema</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testTypedFilterPreservingSchema</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJoin</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJoin</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testTake</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testTake</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testToLocalIterator</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testToLocalIterator</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testSpecificLists</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testSpecificLists</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testForeach</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJavaEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJavaEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testPrimitiveEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testPrimitiveEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testEmptyBean</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testEmptyBean</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testCommonOperation</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testCommonOperation</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testNullInTopLevelBean</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testNullInTopLevelBean</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testGroupBy</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testGroupBy</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testSetOperation</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testSetOperation</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testBeanWithEnum</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testBeanWithEnum</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testKryoEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.test</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.test</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJavaBeanEncoder2</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJavaBeanEncoder2</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testCollect</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testCollect</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testKryoEncoderErrorMessageForPrivateClass</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testKryoEncoderErrorMessageForPrivateClass</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJavaBeanEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJavaBeanEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testTupleEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testTupleEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testNestedTupleEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testNestedTupleEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testTupleEncoderSchema</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testTupleEncoderSchema</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testReduce</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testReduce</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testSelect</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testSelect</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJavaEncoderErrorMessageForPrivateClass</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJavaEncoderErrorMessageForPrivateClass</div></li><li><div>test.org.apache.spark.sql.JavaSaveLoadSuite.saveAndLoadWithSchema</div></li><li><div>test.org.apache.spark.sql.JavaSaveLoadSuite.saveAndLoadWithSchema</div></li><li><div>test.org.apache.spark.sql.JavaSaveLoadSuite.saveAndLoad</div></li><li><div>test.org.apache.spark.sql.JavaSaveLoadSuite.saveAndLoad</div></li><li><div>test.org.apache.spark.sql.JavaUDAFSuite.udf1Test</div></li><li><div>test.org.apache.spark.sql.JavaUDAFSuite.udf1Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf1Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf1Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf2Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf2Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf3Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf3Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf4Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf4Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf5Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf6Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf6Test</div></li><li><div>test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.testForeachBatchAPI</div></li><li><div>test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.testForeachBatchAPI</div></li><li><div>test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.testForeachAPI</div></li><li><div>test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.testForeachAPI</div></li><li><div>org.apache.spark.sql.hive.HiveMetastoreLazyInitializationSuite.lazily initialize Hive client</div></li><li><div>org.apache.spark.sql.hive.JavaDataFrameSuite.testUDAF</div></li><li><div>org.apache.spark.sql.hive.JavaDataFrameSuite.saveTableAndQueryIt</div></li><li><div>org.apache.spark.sql.hive.JavaMetastoreDataSourcesSuite.saveExternalTableAndQueryIt</div></li><li><div>org.apache.spark.sql.hive.JavaMetastoreDataSourcesSuite.saveTableAndQueryIt</div></li><li><div>org.apache.spark.sql.hive.JavaMetastoreDataSourcesSuite.saveExternalTableWithSchemaAndQueryIt</div></li><li><div>org.apache.spark.streaming.JavaMapWithStateSuite.testBasicFunction</div></li><li><div>org.apache.spark.streaming.JavaMapWithStateSuite.testBasicFunction</div></li><li><div>org.apache.spark.streaming.JavaReceiverAPISuite.testReceiver</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testStreamingContextTransform</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testFlatMapValues</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testFlatMapValues</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testMapPartitions</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testMapPartitions</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairFilter</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairFilter</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testCombineByKey</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testCombineByKey</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testMap</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testMap</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairToNormalRDDTransform</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairToNormalRDDTransform</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairReduceByKey</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairReduceByKey</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairMap</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairMap</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testFlatMap</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testFlatMap</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testReduceByKeyAndWindowWithInverse</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testReduceByKeyAndWindowWithInverse</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testReduceByWindow</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testReduceByWindow</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairFlatMap</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairFlatMap</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairToPairFlatMapWithChangingTypes</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairToPairFlatMapWithChangingTypes</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairMapPartitions</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairMapPartitions</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testVariousTransform</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testVariousTransform</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testTransformWith</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testTransformWith</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testVariousTransformWith</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testVariousTransformWith</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testReduceByKeyAndWindow</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testReduceByKeyAndWindow</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairTransform</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairTransform</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testFilter</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testFilter</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairMap2</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairMap2</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testMapValues</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testMapValues</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testReduce</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testReduce</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testUpdateStateByKey</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testUpdateStateByKey</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testTransform</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testTransform</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testStreamingContextTransform</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testFlatMapValues</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testReduceByWindowWithInverse</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testMapPartitions</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairFilter</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairFilter</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testRepartitionFewerPartitions</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testRepartitionFewerPartitions</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCombineByKey</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCombineByKey</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testContextGetOrCreate</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testContextGetOrCreate</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testWindowWithSlideDuration</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testWindowWithSlideDuration</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testQueueStream</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testQueueStream</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCountByValue</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCountByValue</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testMap</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testMap</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairToNormalRDDTransform</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairToNormalRDDTransform</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairReduceByKey</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairReduceByKey</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCount</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCount</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCheckpointMasterRecovery</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCheckpointMasterRecovery</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairMap</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairMap</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testUnion</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testUnion</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testFlatMap</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testFlatMap</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testReduceByKeyAndWindowWithInverse</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testReduceByKeyAndWindowWithInverse</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testGlom</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testGlom</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testJoin</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testJoin</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairFlatMap</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairToPairFlatMapWithChangingTypes</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairMapPartitions</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairMapPartitions</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testRepartitionMorePartitions</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testRepartitionMorePartitions</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testReduceByWindowWithoutInverse</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testReduceByWindowWithoutInverse</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testLeftOuterJoin</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testLeftOuterJoin</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testVariousTransform</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testVariousTransform</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testTransformWith</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testTransformWith</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testVariousTransformWith</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testVariousTransformWith</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testTextFileStream</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testTextFileStream</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairGroupByKey</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairGroupByKey</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCoGroup</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCoGroup</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testInitialization</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testInitialization</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testSocketString</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testSocketString</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testGroupByKeyAndWindow</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testGroupByKeyAndWindow</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testReduceByKeyAndWindow</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testReduceByKeyAndWindow</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testForeachRDD</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testForeachRDD</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testFileStream</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testFileStream</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairTransform</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairTransform</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testFilter</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testFilter</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairMap2</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairMap2</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testMapValues</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testMapValues</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testReduce</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testUpdateStateByKey</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testTransform</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testWindow</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCountByValueAndWindow</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCountByValueAndWindow</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testRawSocketStream</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testRawSocketStream</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testSocketTextStream</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testSocketTextStream</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testUpdateStateByKeyWithInitial</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testUpdateStateByKeyWithInitial</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testContextState</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testContextState</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.spark.network.ChunkFetchIntegrationSuite.fetchFileChunk</div></li><li><div>org.apache.spark.JavaJdbcRDDSuite.testJavaJdbcRDD</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.leftOuterJoin</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.foldReduce</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.mapsFromPairsToPairs</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.flatMap</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.foreach</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.map</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.map</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.zip</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.zip</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.keyBy</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.keyBy</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.groupBy</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.groupBy</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.mapPartitions</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.mapPartitions</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.foldByKey</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.foldByKey</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.mapOnPairRDD</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.sequenceFile</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.sequenceFile</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.collectPartitions</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.collectPartitions</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.reduceByKey</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.reduceByKey</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.foreachWithAnonymousClass</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.foreachWithAnonymousClass</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.collectAsMapWithIntArrayValues</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.collectAsMapWithIntArrayValues</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.zipPartitions</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.zipPartitions</div></li><li><div>test.org.apache.spark.JavaAPISuite.groupByOnPairRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.groupByOnPairRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.binaryFilesCaching</div></li><li><div>test.org.apache.spark.JavaAPISuite.binaryFilesCaching</div></li><li><div>test.org.apache.spark.JavaAPISuite.sparkContextUnion</div></li><li><div>test.org.apache.spark.JavaAPISuite.sparkContextUnion</div></li><li><div>test.org.apache.spark.JavaAPISuite.checkpointAndComputation</div></li><li><div>test.org.apache.spark.JavaAPISuite.checkpointAndComputation</div></li><li><div>test.org.apache.spark.JavaAPISuite.leftOuterJoin</div></li><li><div>test.org.apache.spark.JavaAPISuite.leftOuterJoin</div></li><li><div>test.org.apache.spark.JavaAPISuite.keyByOnPairRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.keyByOnPairRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.getNumPartitions</div></li><li><div>test.org.apache.spark.JavaAPISuite.getNumPartitions</div></li><li><div>test.org.apache.spark.JavaAPISuite.wholeTextFiles</div></li><li><div>test.org.apache.spark.JavaAPISuite.wholeTextFiles</div></li><li><div>test.org.apache.spark.JavaAPISuite.binaryFiles</div></li><li><div>test.org.apache.spark.JavaAPISuite.binaryFiles</div></li><li><div>test.org.apache.spark.JavaAPISuite.foldReduce</div></li><li><div>test.org.apache.spark.JavaAPISuite.writeWithNewAPIHadoopFile</div></li><li><div>test.org.apache.spark.JavaAPISuite.writeWithNewAPIHadoopFile</div></li><li><div>test.org.apache.spark.JavaAPISuite.hadoopFile</div></li><li><div>test.org.apache.spark.JavaAPISuite.hadoopFile</div></li><li><div>test.org.apache.spark.JavaAPISuite.lookup</div></li><li><div>test.org.apache.spark.JavaAPISuite.lookup</div></li><li><div>test.org.apache.spark.JavaAPISuite.countAsync</div></li><li><div>test.org.apache.spark.JavaAPISuite.countAsync</div></li><li><div>test.org.apache.spark.JavaAPISuite.textFiles</div></li><li><div>test.org.apache.spark.JavaAPISuite.textFiles</div></li><li><div>test.org.apache.spark.JavaAPISuite.binaryRecords</div></li><li><div>test.org.apache.spark.JavaAPISuite.binaryRecords</div></li><li><div>test.org.apache.spark.JavaAPISuite.toLocalIterator</div></li><li><div>test.org.apache.spark.JavaAPISuite.toLocalIterator</div></li><li><div>test.org.apache.spark.JavaAPISuite.repartitionAndSortWithinPartitions</div></li><li><div>test.org.apache.spark.JavaAPISuite.repartitionAndSortWithinPartitions</div></li><li><div>test.org.apache.spark.JavaAPISuite.reduce</div></li><li><div>test.org.apache.spark.JavaAPISuite.reduce</div></li><li><div>test.org.apache.spark.JavaAPISuite.sample</div></li><li><div>test.org.apache.spark.JavaAPISuite.sample</div></li><li><div>test.org.apache.spark.JavaAPISuite.sortBy</div></li><li><div>test.org.apache.spark.JavaAPISuite.sortBy</div></li><li><div>test.org.apache.spark.JavaAPISuite.mapsFromPairsToPairs</div></li><li><div>test.org.apache.spark.JavaAPISuite.mapsFromPairsToPairs</div></li><li><div>test.org.apache.spark.JavaAPISuite.flatMap</div></li><li><div>test.org.apache.spark.JavaAPISuite.flatMap</div></li><li><div>test.org.apache.spark.JavaAPISuite.cogroup3</div></li><li><div>test.org.apache.spark.JavaAPISuite.cogroup3</div></li><li><div>test.org.apache.spark.JavaAPISuite.cogroup4</div></li><li><div>test.org.apache.spark.JavaAPISuite.cogroup4</div></li><li><div>test.org.apache.spark.JavaAPISuite.randomSplit</div></li><li><div>test.org.apache.spark.JavaAPISuite.randomSplit</div></li><li><div>test.org.apache.spark.JavaAPISuite.persist</div></li><li><div>test.org.apache.spark.JavaAPISuite.persist</div></li><li><div>test.org.apache.spark.JavaAPISuite.foreach</div></li><li><div>test.org.apache.spark.JavaAPISuite.foreach</div></li><li><div>test.org.apache.spark.JavaAPISuite.hadoopFileCompressed</div></li><li><div>test.org.apache.spark.JavaAPISuite.hadoopFileCompressed</div></li><li><div>test.org.apache.spark.JavaAPISuite.textFilesCompressed</div></li><li><div>test.org.apache.spark.JavaAPISuite.textFilesCompressed</div></li><li><div>test.org.apache.spark.JavaAPISuite.testAsyncActionCancellation</div></li><li><div>test.org.apache.spark.JavaAPISuite.checkpointAndRestore</div></li><li><div>test.org.apache.spark.JavaAPISuite.checkpointAndRestore</div></li><li><div>test.org.apache.spark.JavaAPISuite.sortByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.sortByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.aggregateByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.aggregateByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.map</div></li><li><div>test.org.apache.spark.JavaAPISuite.map</div></li><li><div>test.org.apache.spark.JavaAPISuite.max</div></li><li><div>test.org.apache.spark.JavaAPISuite.max</div></li><li><div>test.org.apache.spark.JavaAPISuite.min</div></li><li><div>test.org.apache.spark.JavaAPISuite.min</div></li><li><div>test.org.apache.spark.JavaAPISuite.top</div></li><li><div>test.org.apache.spark.JavaAPISuite.top</div></li><li><div>test.org.apache.spark.JavaAPISuite.zip</div></li><li><div>test.org.apache.spark.JavaAPISuite.zip</div></li><li><div>test.org.apache.spark.JavaAPISuite.fold</div></li><li><div>test.org.apache.spark.JavaAPISuite.fold</div></li><li><div>test.org.apache.spark.JavaAPISuite.glom</div></li><li><div>test.org.apache.spark.JavaAPISuite.glom</div></li><li><div>test.org.apache.spark.JavaAPISuite.take</div></li><li><div>test.org.apache.spark.JavaAPISuite.take</div></li><li><div>test.org.apache.spark.JavaAPISuite.javaDoubleRDDHistoGram</div></li><li><div>test.org.apache.spark.JavaAPISuite.javaDoubleRDDHistoGram</div></li><li><div>test.org.apache.spark.JavaAPISuite.collectUnderlyingScalaRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.collectUnderlyingScalaRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.keyBy</div></li><li><div>test.org.apache.spark.JavaAPISuite.keyBy</div></li><li><div>test.org.apache.spark.JavaAPISuite.mapPartitionsWithIndex</div></li><li><div>test.org.apache.spark.JavaAPISuite.sampleByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.intersection</div></li><li><div>test.org.apache.spark.JavaAPISuite.aggregate</div></li><li><div>test.org.apache.spark.JavaAPISuite.cartesian</div></li><li><div>test.org.apache.spark.JavaAPISuite.cartesian</div></li><li><div>test.org.apache.spark.JavaAPISuite.countApproxDistinctByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.countApproxDistinctByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.readWithNewAPIHadoopFile</div></li><li><div>test.org.apache.spark.JavaAPISuite.readWithNewAPIHadoopFile</div></li><li><div>test.org.apache.spark.JavaAPISuite.groupBy</div></li><li><div>test.org.apache.spark.JavaAPISuite.groupBy</div></li><li><div>test.org.apache.spark.JavaAPISuite.sampleByKeyExact</div></li><li><div>test.org.apache.spark.JavaAPISuite.sampleByKeyExact</div></li><li><div>test.org.apache.spark.JavaAPISuite.mapPartitions</div></li><li><div>test.org.apache.spark.JavaAPISuite.takeOrdered</div></li><li><div>test.org.apache.spark.JavaAPISuite.takeOrdered</div></li><li><div>test.org.apache.spark.JavaAPISuite.foldByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.foldByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.objectFilesOfInts</div></li><li><div>test.org.apache.spark.JavaAPISuite.objectFilesOfInts</div></li><li><div>test.org.apache.spark.JavaAPISuite.treeAggregate</div></li><li><div>test.org.apache.spark.JavaAPISuite.treeAggregate</div></li><li><div>test.org.apache.spark.JavaAPISuite.testGetPersistentRDDs</div></li><li><div>test.org.apache.spark.JavaAPISuite.testGetPersistentRDDs</div></li><li><div>test.org.apache.spark.JavaAPISuite.approximateResults</div></li><li><div>test.org.apache.spark.JavaAPISuite.approximateResults</div></li><li><div>test.org.apache.spark.JavaAPISuite.treeReduce</div></li><li><div>test.org.apache.spark.JavaAPISuite.treeReduce</div></li><li><div>test.org.apache.spark.JavaAPISuite.collectAsMapAndSerialize</div></li><li><div>test.org.apache.spark.JavaAPISuite.collectAsMapAndSerialize</div></li><li><div>test.org.apache.spark.JavaAPISuite.countApproxDistinct</div></li><li><div>test.org.apache.spark.JavaAPISuite.countApproxDistinct</div></li><li><div>test.org.apache.spark.JavaAPISuite.javaDoubleRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.javaDoubleRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.mapOnPairRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.mapOnPairRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.testAsyncActionErrorWrapping</div></li><li><div>test.org.apache.spark.JavaAPISuite.testAsyncActionErrorWrapping</div></li><li><div>test.org.apache.spark.JavaAPISuite.naturalMax</div></li><li><div>test.org.apache.spark.JavaAPISuite.naturalMax</div></li><li><div>test.org.apache.spark.JavaAPISuite.naturalMin</div></li><li><div>test.org.apache.spark.JavaAPISuite.naturalMin</div></li><li><div>test.org.apache.spark.JavaAPISuite.sequenceFile</div></li><li><div>test.org.apache.spark.JavaAPISuite.sequenceFile</div></li><li><div>test.org.apache.spark.JavaAPISuite.collectPartitions</div></li><li><div>test.org.apache.spark.JavaAPISuite.collectPartitions</div></li><li><div>test.org.apache.spark.JavaAPISuite.cogroup</div></li><li><div>test.org.apache.spark.JavaAPISuite.cogroup</div></li><li><div>test.org.apache.spark.JavaAPISuite.reduceByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.reduceByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.repartition</div></li><li><div>test.org.apache.spark.JavaAPISuite.repartition</div></li><li><div>test.org.apache.spark.JavaAPISuite.iterator</div></li><li><div>test.org.apache.spark.JavaAPISuite.iterator</div></li><li><div>test.org.apache.spark.JavaAPISuite.emptyRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.emptyRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.zipWithIndex</div></li><li><div>test.org.apache.spark.JavaAPISuite.zipWithIndex</div></li><li><div>test.org.apache.spark.JavaAPISuite.foreachPartition</div></li><li><div>test.org.apache.spark.JavaAPISuite.foreachPartition</div></li><li><div>test.org.apache.spark.JavaAPISuite.combineByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.combineByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.takeAsync</div></li><li><div>test.org.apache.spark.JavaAPISuite.takeAsync</div></li><li><div>test.org.apache.spark.JavaAPISuite.collectAsMapWithIntArrayValues</div></li><li><div>test.org.apache.spark.JavaAPISuite.collectAsMapWithIntArrayValues</div></li><li><div>test.org.apache.spark.JavaAPISuite.objectFilesOfComplexTypes</div></li><li><div>test.org.apache.spark.JavaAPISuite.objectFilesOfComplexTypes</div></li><li><div>test.org.apache.spark.JavaAPISuite.zipWithUniqueId</div></li><li><div>test.org.apache.spark.JavaAPISuite.collectAsync</div></li><li><div>test.org.apache.spark.JavaAPISuite.foreachAsync</div></li><li><div>test.org.apache.spark.JavaAPISuite.foreachAsync</div></li><li><div>test.org.apache.spark.JavaAPISuite.zipPartitions</div></li><li><div>test.org.apache.spark.JavaAPISuite.zipPartitions</div></li><li><div>test.org.apache.spark.JavaAPISuite.reduceOnJavaDoubleRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.reduceOnJavaDoubleRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.isEmpty</div></li><li><div>test.org.apache.spark.JavaAPISuite.isEmpty</div></li><li><div>org.apache.spark.sql.avro.AvroFunctionsSuite.roundtrip in to_avro and from_avro - int and string</div></li><li><div>org.apache.spark.sql.avro.AvroFunctionsSuite.roundtrip in to_avro and from_avro - struct</div></li><li><div>org.apache.spark.sql.avro.AvroFunctionsSuite.roundtrip in to_avro and from_avro - array with null</div></li><li><div>org.apache.spark.streaming.kafka010.JavaDirectKafkaStreamSuite.testKafkaStream</div></li><li><div>org.apache.spark.streaming.kafka010.JavaKafkaRDDSuite.testKafkaRDD</div></li><li><div>org.apache.spark.graphx.EdgeRDDSuite.cache, getStorageLevel</div></li><li><div>org.apache.spark.graphx.EdgeRDDSuite.checkpointing</div></li><li><div>org.apache.spark.graphx.GraphLoaderSuite.GraphLoader.edgeListFile</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.joinVertices</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.collectNeighborIds</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.removeSelfEdges</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.filter</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.convertToCanonicalEdges</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.collectEdgesCycleDirectionOut</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.collectEdgesCycleDirectionIn</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.collectEdgesCycleDirectionEither</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.collectEdgesChainDirectionOut</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.collectEdgesChainDirectionIn</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.collectEdgesChainDirectionEither</div></li><li><div>org.apache.spark.graphx.GraphSuite.Graph.fromEdgeTuples</div></li><li><div>org.apache.spark.graphx.GraphSuite.Graph.fromEdges</div></li><li><div>org.apache.spark.graphx.GraphSuite.Graph.apply</div></li><li><div>org.apache.spark.graphx.GraphSuite.triplets</div></li><li><div>org.apache.spark.graphx.GraphSuite.partitionBy</div></li><li><div>org.apache.spark.graphx.GraphSuite.mapVertices</div></li><li><div>org.apache.spark.graphx.GraphSuite.mapVertices changing type with same erased type</div></li><li><div>org.apache.spark.graphx.GraphSuite.mapEdges</div></li><li><div>org.apache.spark.graphx.GraphSuite.mapTriplets</div></li><li><div>org.apache.spark.graphx.GraphSuite.reverse</div></li><li><div>org.apache.spark.graphx.GraphSuite.reverse with join elimination</div></li><li><div>org.apache.spark.graphx.GraphSuite.subgraph</div></li><li><div>org.apache.spark.graphx.GraphSuite.mask</div></li><li><div>org.apache.spark.graphx.GraphSuite.groupEdges</div></li><li><div>org.apache.spark.graphx.GraphSuite.aggregateMessages</div></li><li><div>org.apache.spark.graphx.GraphSuite.outerJoinVertices</div></li><li><div>org.apache.spark.graphx.GraphSuite.more edge partitions than vertex partitions</div></li><li><div>org.apache.spark.graphx.GraphSuite.checkpoint</div></li><li><div>org.apache.spark.graphx.GraphSuite.non-default number of edge partitions</div></li><li><div>org.apache.spark.graphx.GraphSuite.unpersist graph RDD</div></li><li><div>org.apache.spark.graphx.GraphSuite.SPARK-14219: pickRandomVertex</div></li><li><div>org.apache.spark.graphx.PregelSuite.1 iteration</div></li><li><div>org.apache.spark.graphx.PregelSuite.chain propagation</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.filter</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.mapValues</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.minus</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.minus with RDD[(VertexId, VD)]</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.minus with non-equal number of partitions</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.diff</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.diff with RDD[(VertexId, VD)]</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.diff vertices with non-equal number of partitions</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.leftJoin</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.leftJoin vertices with non-equal number of partitions</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.innerJoin</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.innerJoin vertices with the non-equal number of partitions</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.aggregateUsingIndex</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.mergeFunc</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.cache, getStorageLevel</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.checkpoint</div></li><li><div>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Grid Connected Components</div></li><li><div>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Reverse Grid Connected Components</div></li><li><div>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Chain Connected Components</div></li><li><div>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Reverse Chain Connected Components</div></li><li><div>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Connected Components on a Toy Connected Graph</div></li><li><div>org.apache.spark.graphx.lib.LabelPropagationSuite.Label Propagation</div></li><li><div>org.apache.spark.graphx.lib.PageRankSuite.Star PageRank</div></li><li><div>org.apache.spark.graphx.lib.PageRankSuite.Star PersonalPageRank</div></li><li><div>org.apache.spark.graphx.lib.PageRankSuite.Grid PageRank</div></li><li><div>org.apache.spark.graphx.lib.PageRankSuite.Chain PageRank</div></li><li><div>org.apache.spark.graphx.lib.PageRankSuite.Chain PersonalizedPageRank</div></li><li><div>org.apache.spark.graphx.lib.PageRankSuite.Loop with source PageRank</div></li><li><div>org.apache.spark.graphx.lib.PageRankSuite.Loop with sink PageRank</div></li><li><div>org.apache.spark.graphx.lib.SVDPlusPlusSuite.Test SVD++ with mean square error on training set</div></li><li><div>org.apache.spark.graphx.lib.ShortestPathsSuite.Shortest Path Computations</div></li><li><div>org.apache.spark.graphx.lib.StronglyConnectedComponentsSuite.Island Strongly Connected Components</div></li><li><div>org.apache.spark.graphx.lib.StronglyConnectedComponentsSuite.Cycle Strongly Connected Components</div></li><li><div>org.apache.spark.graphx.lib.StronglyConnectedComponentsSuite.2 Cycle Strongly Connected Components</div></li><li><div>org.apache.spark.graphx.lib.TriangleCountSuite.Count a single triangle</div></li><li><div>org.apache.spark.graphx.lib.TriangleCountSuite.Count two triangles</div></li><li><div>org.apache.spark.graphx.lib.TriangleCountSuite.Count two triangles with bi-directed edges</div></li><li><div>org.apache.spark.graphx.lib.TriangleCountSuite.Count a single triangle with duplicate edges</div></li><li><div>org.apache.spark.graphx.util.GraphGeneratorsSuite.GraphGenerators.logNormalGraph</div></li><li><div>org.apache.spark.graphx.util.GraphGeneratorsSuite.SPARK-5064 GraphGenerators.rmatGraph numEdges upper bound</div></li><li><div>org.apache.spark.graphx.util.PeriodicGraphCheckpointerSuite.Persisting</div></li><li><div>org.apache.spark.graphx.util.PeriodicGraphCheckpointerSuite.Checkpointing</div></li><li><div>org.apache.spark.ml.JavaPipelineSuite.pipeline</div></li><li><div>org.apache.spark.ml.JavaPipelineSuite.pipeline</div></li><li><div>org.apache.spark.ml.classification.JavaDecisionTreeClassifierSuite.runDT</div></li><li><div>org.apache.spark.ml.classification.JavaDecisionTreeClassifierSuite.runDT</div></li><li><div>org.apache.spark.ml.classification.JavaGBTClassifierSuite.runDT</div></li><li><div>org.apache.spark.ml.classification.JavaGBTClassifierSuite.runDT</div></li><li><div>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionWithSetters</div></li><li><div>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionWithSetters</div></li><li><div>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionTrainingSummary</div></li><li><div>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionTrainingSummary</div></li><li><div>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionPredictorClassifierMethods</div></li><li><div>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionPredictorClassifierMethods</div></li><li><div>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionDefaultParams</div></li><li><div>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionDefaultParams</div></li><li><div>org.apache.spark.ml.classification.JavaMultilayerPerceptronClassifierSuite.testMLPC</div></li><li><div>org.apache.spark.ml.classification.JavaMultilayerPerceptronClassifierSuite.testMLPC</div></li><li><div>org.apache.spark.ml.classification.JavaNaiveBayesSuite.testNaiveBayes</div></li><li><div>org.apache.spark.ml.classification.JavaNaiveBayesSuite.naiveBayesDefaultParams</div></li><li><div>org.apache.spark.ml.classification.JavaNaiveBayesSuite.naiveBayesDefaultParams</div></li><li><div>org.apache.spark.ml.classification.JavaOneVsRestSuite.oneVsRestDefaultParams</div></li><li><div>org.apache.spark.ml.classification.JavaOneVsRestSuite.oneVsRestDefaultParams</div></li><li><div>org.apache.spark.ml.classification.JavaRandomForestClassifierSuite.runDT</div></li><li><div>org.apache.spark.ml.classification.JavaRandomForestClassifierSuite.runDT</div></li><li><div>org.apache.spark.ml.clustering.JavaKMeansSuite.fitAndTransform</div></li><li><div>org.apache.spark.ml.clustering.JavaKMeansSuite.fitAndTransform</div></li><li><div>org.apache.spark.ml.feature.JavaBucketizerSuite.bucketizerTest</div></li><li><div>org.apache.spark.ml.feature.JavaBucketizerSuite.bucketizerMultipleColumnsTest</div></li><li><div>org.apache.spark.ml.feature.JavaBucketizerSuite.bucketizerMultipleColumnsTest</div></li><li><div>org.apache.spark.ml.feature.JavaDCTSuite.javaCompatibilityTest</div></li><li><div>org.apache.spark.ml.feature.JavaDCTSuite.javaCompatibilityTest</div></li><li><div>org.apache.spark.ml.feature.JavaHashingTFSuite.hashingTF</div></li><li><div>org.apache.spark.ml.feature.JavaHashingTFSuite.hashingTF</div></li><li><div>org.apache.spark.ml.feature.JavaNormalizerSuite.normalizer</div></li><li><div>org.apache.spark.ml.feature.JavaPCASuite.testPCA</div></li><li><div>org.apache.spark.ml.feature.JavaPCASuite.testPCA</div></li><li><div>org.apache.spark.ml.feature.JavaPolynomialExpansionSuite.polynomialExpansionTest</div></li><li><div>org.apache.spark.ml.feature.JavaStandardScalerSuite.standardScaler</div></li><li><div>org.apache.spark.ml.feature.JavaStandardScalerSuite.standardScaler</div></li><li><div>org.apache.spark.ml.feature.JavaStopWordsRemoverSuite.javaCompatibilityTest</div></li><li><div>org.apache.spark.ml.feature.JavaStopWordsRemoverSuite.javaCompatibilityTest</div></li><li><div>org.apache.spark.ml.feature.JavaStringIndexerSuite.testStringIndexer</div></li><li><div>org.apache.spark.ml.feature.JavaStringIndexerSuite.testStringIndexer</div></li><li><div>org.apache.spark.ml.feature.JavaTokenizerSuite.regexTokenizer</div></li><li><div>org.apache.spark.ml.feature.JavaTokenizerSuite.regexTokenizer</div></li><li><div>org.apache.spark.ml.feature.JavaVectorAssemblerSuite.testVectorAssembler</div></li><li><div>org.apache.spark.ml.feature.JavaVectorAssemblerSuite.testVectorAssembler</div></li><li><div>org.apache.spark.ml.feature.JavaVectorIndexerSuite.vectorIndexerAPI</div></li><li><div>org.apache.spark.ml.feature.JavaVectorIndexerSuite.vectorIndexerAPI</div></li><li><div>org.apache.spark.ml.feature.JavaVectorSlicerSuite.vectorSlice</div></li><li><div>org.apache.spark.ml.feature.JavaVectorSlicerSuite.vectorSlice</div></li><li><div>org.apache.spark.ml.feature.JavaWord2VecSuite.testJavaWord2Vec</div></li><li><div>org.apache.spark.ml.regression.JavaDecisionTreeRegressorSuite.runDT</div></li><li><div>org.apache.spark.ml.regression.JavaDecisionTreeRegressorSuite.runDT</div></li><li><div>org.apache.spark.ml.regression.JavaGBTRegressorSuite.runDT</div></li><li><div>org.apache.spark.ml.regression.JavaGBTRegressorSuite.runDT</div></li><li><div>org.apache.spark.ml.regression.JavaLinearRegressionSuite.linearRegressionDefaultParams</div></li><li><div>org.apache.spark.ml.regression.JavaLinearRegressionSuite.linearRegressionDefaultParams</div></li><li><div>org.apache.spark.ml.regression.JavaLinearRegressionSuite.linearRegressionWithSetters</div></li><li><div>org.apache.spark.ml.regression.JavaLinearRegressionSuite.linearRegressionWithSetters</div></li><li><div>org.apache.spark.ml.regression.JavaRandomForestRegressorSuite.runDT</div></li><li><div>org.apache.spark.ml.regression.JavaRandomForestRegressorSuite.runDT</div></li><li><div>org.apache.spark.ml.source.libsvm.JavaLibSVMRelationSuite.verifyLibSVMDF</div></li><li><div>org.apache.spark.ml.stat.JavaKolmogorovSmirnovTestSuite.testKSTestNamedDistribution</div></li><li><div>org.apache.spark.ml.stat.JavaKolmogorovSmirnovTestSuite.testKSTestNamedDistribution</div></li><li><div>org.apache.spark.ml.stat.JavaKolmogorovSmirnovTestSuite.testKSTestCDF</div></li><li><div>org.apache.spark.ml.stat.JavaKolmogorovSmirnovTestSuite.testKSTestCDF</div></li><li><div>org.apache.spark.ml.stat.JavaSummarizerSuite.testSummarizer</div></li><li><div>org.apache.spark.ml.stat.JavaSummarizerSuite.testSummarizer</div></li><li><div>org.apache.spark.ml.tuning.JavaCrossValidatorSuite.crossValidationWithLogisticRegression</div></li><li><div>org.apache.spark.ml.tuning.JavaCrossValidatorSuite.crossValidationWithLogisticRegression</div></li><li><div>org.apache.spark.ml.util.JavaDefaultReadWriteSuite.testDefaultReadWrite</div></li><li><div>org.apache.spark.ml.util.JavaDefaultReadWriteSuite.testDefaultReadWrite</div></li><li><div>org.apache.spark.mllib.classification.JavaLogisticRegressionSuite.runLRUsingConstructor</div></li><li><div>org.apache.spark.mllib.classification.JavaLogisticRegressionSuite.runLRUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.classification.JavaLogisticRegressionSuite.runLRUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.testPredictJavaRDD</div></li><li><div>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.testPredictJavaRDD</div></li><li><div>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.runUsingConstructor</div></li><li><div>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.runUsingConstructor</div></li><li><div>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.runUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.runUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.testModelTypeSetters</div></li><li><div>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.testModelTypeSetters</div></li><li><div>org.apache.spark.mllib.classification.JavaSVMSuite.runSVMUsingConstructor</div></li><li><div>org.apache.spark.mllib.classification.JavaSVMSuite.runSVMUsingConstructor</div></li><li><div>org.apache.spark.mllib.classification.JavaSVMSuite.runSVMUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.classification.JavaSVMSuite.runSVMUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.classification.JavaStreamingLogisticRegressionSuite.javaAPI</div></li><li><div>org.apache.spark.mllib.classification.JavaStreamingLogisticRegressionSuite.javaAPI</div></li><li><div>org.apache.spark.mllib.clustering.JavaBisectingKMeansSuite.twoDimensionalData</div></li><li><div>org.apache.spark.mllib.clustering.JavaBisectingKMeansSuite.twoDimensionalData</div></li><li><div>org.apache.spark.mllib.clustering.JavaGaussianMixtureSuite.runGaussianMixture</div></li><li><div>org.apache.spark.mllib.clustering.JavaGaussianMixtureSuite.runGaussianMixture</div></li><li><div>org.apache.spark.mllib.clustering.JavaKMeansSuite.testPredictJavaRDD</div></li><li><div>org.apache.spark.mllib.clustering.JavaKMeansSuite.runKMeansUsingConstructor</div></li><li><div>org.apache.spark.mllib.clustering.JavaKMeansSuite.runKMeansUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.clustering.JavaKMeansSuite.runKMeansUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.clustering.JavaLDASuite.onlineOptimizerCompatibility</div></li><li><div>org.apache.spark.mllib.clustering.JavaLDASuite.onlineOptimizerCompatibility</div></li><li><div>org.apache.spark.mllib.clustering.JavaLDASuite.distributedLDAModel</div></li><li><div>org.apache.spark.mllib.clustering.JavaLDASuite.distributedLDAModel</div></li><li><div>org.apache.spark.mllib.clustering.JavaLDASuite.localLDAModel</div></li><li><div>org.apache.spark.mllib.clustering.JavaLDASuite.localLDAModel</div></li><li><div>org.apache.spark.mllib.clustering.JavaLDASuite.localLdaMethods</div></li><li><div>org.apache.spark.mllib.clustering.JavaLDASuite.localLdaMethods</div></li><li><div>org.apache.spark.mllib.clustering.JavaStreamingKMeansSuite.javaAPI</div></li><li><div>org.apache.spark.mllib.evaluation.JavaRankingMetricsSuite.rankingMetrics</div></li><li><div>org.apache.spark.mllib.evaluation.JavaRankingMetricsSuite.rankingMetrics</div></li><li><div>org.apache.spark.mllib.feature.JavaTfIdfSuite.tfIdfMinimumDocumentFrequency</div></li><li><div>org.apache.spark.mllib.feature.JavaTfIdfSuite.tfIdfMinimumDocumentFrequency</div></li><li><div>org.apache.spark.mllib.feature.JavaTfIdfSuite.tfIdf</div></li><li><div>org.apache.spark.mllib.feature.JavaTfIdfSuite.tfIdf</div></li><li><div>org.apache.spark.mllib.feature.JavaWord2VecSuite.word2Vec</div></li><li><div>org.apache.spark.mllib.feature.JavaWord2VecSuite.word2Vec</div></li><li><div>org.apache.spark.mllib.fpm.JavaAssociationRulesSuite.runAssociationRules</div></li><li><div>org.apache.spark.mllib.fpm.JavaAssociationRulesSuite.runAssociationRules</div></li><li><div>org.apache.spark.mllib.fpm.JavaFPGrowthSuite.runFPGrowthSaveLoad</div></li><li><div>org.apache.spark.mllib.fpm.JavaFPGrowthSuite.runFPGrowthSaveLoad</div></li><li><div>org.apache.spark.mllib.fpm.JavaFPGrowthSuite.runFPGrowth</div></li><li><div>org.apache.spark.mllib.fpm.JavaPrefixSpanSuite.runPrefixSpan</div></li><li><div>org.apache.spark.mllib.fpm.JavaPrefixSpanSuite.runPrefixSpan</div></li><li><div>org.apache.spark.mllib.fpm.JavaPrefixSpanSuite.runPrefixSpanSaveLoad</div></li><li><div>org.apache.spark.mllib.fpm.JavaPrefixSpanSuite.runPrefixSpanSaveLoad</div></li><li><div>org.apache.spark.mllib.linalg.distributed.JavaRowMatrixSuite.rowMatrixQRDecomposition</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testNormalVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testNormalVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testArbitrary</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testArbitrary</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testLogNormalVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testLogNormalVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testExponentialVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testExponentialVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testUniformRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testUniformRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testRandomVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testRandomVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testGammaRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testGammaRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testUniformVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testUniformVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testPoissonRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testPoissonRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testNormalRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testNormalRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testPoissonVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testPoissonVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testGammaVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testGammaVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testExponentialRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testExponentialRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testLNormalRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testLNormalRDD</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runALSUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runALSUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSUsingConstructor</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSUsingConstructor</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runRecommend</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runRecommend</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSWithNegativeWeight</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSWithNegativeWeight</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runALSUsingConstructor</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runALSUsingConstructor</div></li><li><div>org.apache.spark.mllib.regression.JavaIsotonicRegressionSuite.testIsotonicRegressionJavaRDD</div></li><li><div>org.apache.spark.mllib.regression.JavaIsotonicRegressionSuite.testIsotonicRegressionJavaRDD</div></li><li><div>org.apache.spark.mllib.regression.JavaIsotonicRegressionSuite.testIsotonicRegressionPredictionsJavaRDD</div></li><li><div>org.apache.spark.mllib.regression.JavaIsotonicRegressionSuite.testIsotonicRegressionPredictionsJavaRDD</div></li><li><div>org.apache.spark.mllib.regression.JavaLassoSuite.runLassoUsingConstructor</div></li><li><div>org.apache.spark.mllib.regression.JavaLassoSuite.runLassoUsingConstructor</div></li><li><div>org.apache.spark.mllib.regression.JavaLassoSuite.runLassoUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.testPredictJavaRDD</div></li><li><div>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.runLinearRegressionUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.runLinearRegressionUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.runLinearRegressionUsingConstructor</div></li><li><div>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.runLinearRegressionUsingConstructor</div></li><li><div>org.apache.spark.mllib.regression.JavaRidgeRegressionSuite.runRidgeRegressionUsingConstructor</div></li><li><div>org.apache.spark.mllib.regression.JavaRidgeRegressionSuite.runRidgeRegressionUsingConstructor</div></li><li><div>org.apache.spark.mllib.regression.JavaRidgeRegressionSuite.runRidgeRegressionUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.regression.JavaRidgeRegressionSuite.runRidgeRegressionUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.regression.JavaStreamingLinearRegressionSuite.javaAPI</div></li><li><div>org.apache.spark.mllib.regression.JavaStreamingLinearRegressionSuite.javaAPI</div></li><li><div>org.apache.spark.mllib.stat.JavaStatisticsSuite.testCorr</div></li><li><div>org.apache.spark.mllib.stat.JavaStatisticsSuite.testCorr</div></li><li><div>org.apache.spark.mllib.stat.JavaStatisticsSuite.chiSqTest</div></li><li><div>org.apache.spark.mllib.stat.JavaStatisticsSuite.chiSqTest</div></li><li><div>org.apache.spark.mllib.stat.JavaStatisticsSuite.streamingTest</div></li><li><div>org.apache.spark.mllib.stat.JavaStatisticsSuite.streamingTest</div></li><li><div>org.apache.spark.mllib.stat.JavaStatisticsSuite.kolmogorovSmirnovTest</div></li><li><div>org.apache.spark.mllib.stat.JavaStatisticsSuite.kolmogorovSmirnovTest</div></li><li><div>org.apache.spark.mllib.tree.JavaDecisionTreeSuite.runDTUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.tree.JavaDecisionTreeSuite.runDTUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.tree.JavaDecisionTreeSuite.runDTUsingConstructor</div></li><li><div>org.apache.spark.mllib.tree.JavaDecisionTreeSuite.runDTUsingConstructor</div></li><li><div>org.apache.spark.mllib.util.JavaMLUtilsSuite.testConvertMatrixColumnsToAndFromML</div></li><li><div>org.apache.spark.mllib.util.JavaMLUtilsSuite.testConvertMatrixColumnsToAndFromML</div></li><li><div>org.apache.spark.mllib.util.JavaMLUtilsSuite.testConvertVectorColumnsToAndFromML</div></li><li><div>org.apache.spark.mllib.util.JavaMLUtilsSuite.testConvertVectorColumnsToAndFromML</div></li><li><div>org.apache.spark.repl.ReplSuite.SPARK-15236: use Hive catalog</div></li><li><div>org.apache.spark.repl.ReplSuite.SPARK-15236: use in-memory catalog</div></li><li><div>org.apache.spark.repl.ReplSuite.broadcast vars</div></li><li><div>org.apache.spark.repl.ReplSuite.line wrapper only initialized once when used as encoder outer scope</div></li><li><div>org.apache.spark.repl.ReplSuite.define case class and create Dataset together with paste mode</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.simple foreach with accumulator</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.external vars</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.external classes</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.external functions</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.external functions that access vars</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.broadcast vars</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.interacting with files</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.local-cluster mode</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.SPARK-2576 importing implicits</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.Datasets and encoders</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.SPARK-2632 importing a method from non serializable class and not using it.</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.collecting objects of class defined in repl</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.collecting objects of class defined in repl - shuffling</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.replicating blocks of object with class defined in repl</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.newProductSeqEncoder with REPL defined class</div></li><li><div>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationCount</div></li><li><div>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationCount</div></li><li><div>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationSumDouble</div></li><li><div>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationSumDouble</div></li><li><div>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationSumLong</div></li><li><div>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationSumLong</div></li><li><div>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationAverage</div></li><li><div>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationAverage</div></li><li><div>test.org.apache.spark.sql.JavaApplySchemaSuite.applySchema</div></li><li><div>test.org.apache.spark.sql.JavaApplySchemaSuite.applySchema</div></li><li><div>test.org.apache.spark.sql.JavaApplySchemaSuite.dataFrameRDDOperations</div></li><li><div>test.org.apache.spark.sql.JavaApplySchemaSuite.dataFrameRDDOperations</div></li><li><div>test.org.apache.spark.sql.JavaApplySchemaSuite.applySchemaToJSON</div></li><li><div>test.org.apache.spark.sql.JavaApplySchemaSuite.applySchemaToJSON</div></li><li><div>test.org.apache.spark.sql.JavaBeanDeserializationSuite.testBeanWithArrayFieldDeserialization</div></li><li><div>test.org.apache.spark.sql.JavaBeanDeserializationSuite.testBeanWithMapFieldsDeserialization</div></li><li><div>test.org.apache.spark.sql.JavaColumnExpressionSuite.isInCollectionCheckExceptionMessage</div></li><li><div>test.org.apache.spark.sql.JavaColumnExpressionSuite.isInCollectionCheckExceptionMessage</div></li><li><div>test.org.apache.spark.sql.JavaColumnExpressionSuite.isInCollectionWorksCorrectlyOnJava</div></li><li><div>test.org.apache.spark.sql.JavaColumnExpressionSuite.isInCollectionWorksCorrectlyOnJava</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testFormatAPI</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testTextAPI</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testJsonAPI</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testLoadAPI</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testOptionsAPI</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testSaveModeAPI</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testCsvAPI</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testParquetAPI</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testTextFileAPI</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCollectAndTake</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCollectAndTake</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testJsonRDDToDataFrame</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testJsonRDDToDataFrame</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testVarargMethods</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testVarargMethods</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testBeanWithoutGetter</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testBeanWithoutGetter</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateStructTypeFromList</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateStructTypeFromList</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testSampleBy</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testSampleBy</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCrosstab</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCrosstab</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testUDF</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testUDF</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFromFromList</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFromFromList</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCircularReferenceBean</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCircularReferenceBean</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testFrequentItems</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testFrequentItems</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testSampleByColumn</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testSampleByColumn</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testExecution</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testExecution</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testTextLoad</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testTextLoad</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.pivot</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.pivot</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testGenericLoad</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testGenericLoad</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCountMinSketch</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCountMinSketch</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.pivotColumnValues</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.pivotColumnValues</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFrameFromJavaBeans</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFrameFromJavaBeans</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCorrelation</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCorrelation</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testBloomFilter</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testBloomFilter</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCovariance</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCovariance</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFrameFromLocalJavaBeans</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFrameFromLocalJavaBeans</div></li><li><div>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationCount</div></li><li><div>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationCount</div></li><li><div>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationSumDouble</div></li><li><div>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationSumDouble</div></li><li><div>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationSumLong</div></li><li><div>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationSumLong</div></li><li><div>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationAnonClass</div></li><li><div>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationAnonClass</div></li><li><div>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationAverage</div></li><li><div>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationAverage</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testRuntimeNullabilityCheck</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean1</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean1</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean2</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean2</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean3</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean3</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testSerializeNull</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testSerializeNull</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testRandomSplit</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testRandomSplit</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testTypedFilterPreservingSchema</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testTypedFilterPreservingSchema</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJoin</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJoin</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testTake</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testTake</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testToLocalIterator</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testToLocalIterator</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testSpecificLists</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testSpecificLists</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testForeach</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testForeach</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJavaEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJavaEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testPrimitiveEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testPrimitiveEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testEmptyBean</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testEmptyBean</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testCommonOperation</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testCommonOperation</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testNullInTopLevelBean</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testNullInTopLevelBean</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testGroupBy</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testGroupBy</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testSetOperation</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testBeanWithEnum</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testKryoEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.test</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.test</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJavaBeanEncoder2</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJavaBeanEncoder2</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testCollect</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testCollect</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testKryoEncoderErrorMessageForPrivateClass</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testKryoEncoderErrorMessageForPrivateClass</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJavaBeanEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJavaBeanEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testTupleEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testTupleEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testNestedTupleEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testNestedTupleEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testTupleEncoderSchema</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testTupleEncoderSchema</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testReduce</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testReduce</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testSelect</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testSelect</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJavaEncoderErrorMessageForPrivateClass</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJavaEncoderErrorMessageForPrivateClass</div></li><li><div>test.org.apache.spark.sql.JavaSaveLoadSuite.saveAndLoadWithSchema</div></li><li><div>test.org.apache.spark.sql.JavaSaveLoadSuite.saveAndLoadWithSchema</div></li><li><div>test.org.apache.spark.sql.JavaSaveLoadSuite.saveAndLoad</div></li><li><div>test.org.apache.spark.sql.JavaSaveLoadSuite.saveAndLoad</div></li><li><div>test.org.apache.spark.sql.JavaUDAFSuite.udf1Test</div></li><li><div>test.org.apache.spark.sql.JavaUDAFSuite.udf1Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf1Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf1Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf2Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf2Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf3Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf3Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf4Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf4Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf5Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf5Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf6Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf6Test</div></li><li><div>test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.testForeachBatchAPI</div></li><li><div>test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.testForeachBatchAPI</div></li><li><div>test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.testForeachAPI</div></li><li><div>test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.testForeachAPI</div></li><li><div>org.apache.spark.sql.hive.HiveMetastoreLazyInitializationSuite.lazily initialize Hive client</div></li><li><div>org.apache.spark.sql.hive.JavaDataFrameSuite.testUDAF</div></li><li><div>org.apache.spark.sql.hive.JavaDataFrameSuite.saveTableAndQueryIt</div></li><li><div>org.apache.spark.sql.hive.JavaMetastoreDataSourcesSuite.saveExternalTableAndQueryIt</div></li><li><div>org.apache.spark.sql.hive.JavaMetastoreDataSourcesSuite.saveTableAndQueryIt</div></li><li><div>org.apache.spark.sql.hive.JavaMetastoreDataSourcesSuite.saveExternalTableWithSchemaAndQueryIt</div></li><li><div>org.apache.spark.streaming.JavaMapWithStateSuite.testBasicFunction</div></li><li><div>org.apache.spark.streaming.JavaMapWithStateSuite.testBasicFunction</div></li><li><div>org.apache.spark.streaming.JavaReceiverAPISuite.testReceiver</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testStreamingContextTransform</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testStreamingContextTransform</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testFlatMapValues</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testFlatMapValues</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testMapPartitions</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testMapPartitions</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairFilter</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairFilter</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testCombineByKey</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testCombineByKey</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testMap</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testMap</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairToNormalRDDTransform</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairToNormalRDDTransform</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairReduceByKey</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairReduceByKey</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairMap</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairMap</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testFlatMap</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testFlatMap</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testReduceByKeyAndWindowWithInverse</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testReduceByKeyAndWindowWithInverse</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testReduceByWindow</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testReduceByWindow</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairFlatMap</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairFlatMap</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairToPairFlatMapWithChangingTypes</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairToPairFlatMapWithChangingTypes</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairMapPartitions</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairMapPartitions</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testVariousTransform</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testVariousTransform</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testTransformWith</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testTransformWith</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testVariousTransformWith</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testVariousTransformWith</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testReduceByKeyAndWindow</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testReduceByKeyAndWindow</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairTransform</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairTransform</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testFilter</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testFilter</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairMap2</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairMap2</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testMapValues</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testMapValues</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testReduce</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testReduce</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testUpdateStateByKey</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testUpdateStateByKey</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testTransform</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testTransform</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testStreamingContextTransform</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testFlatMapValues</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testReduceByWindowWithInverse</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testMapPartitions</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairFilter</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairFilter</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testRepartitionFewerPartitions</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testRepartitionFewerPartitions</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCombineByKey</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCombineByKey</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testContextGetOrCreate</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testContextGetOrCreate</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testWindowWithSlideDuration</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testWindowWithSlideDuration</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testQueueStream</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testQueueStream</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCountByValue</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCountByValue</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testMap</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testMap</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairToNormalRDDTransform</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairToNormalRDDTransform</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairReduceByKey</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairReduceByKey</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCount</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCount</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCheckpointMasterRecovery</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCheckpointMasterRecovery</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairMap</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testUnion</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testUnion</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testFlatMap</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testFlatMap</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testReduceByKeyAndWindowWithInverse</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testReduceByKeyAndWindowWithInverse</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testGlom</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testJoin</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testJoin</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairFlatMap</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairFlatMap</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairToPairFlatMapWithChangingTypes</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairToPairFlatMapWithChangingTypes</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairMapPartitions</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairMapPartitions</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testRepartitionMorePartitions</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testRepartitionMorePartitions</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testReduceByWindowWithoutInverse</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testReduceByWindowWithoutInverse</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testLeftOuterJoin</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testLeftOuterJoin</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testVariousTransform</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testVariousTransform</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testTransformWith</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testTransformWith</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testVariousTransformWith</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testVariousTransformWith</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testTextFileStream</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testTextFileStream</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairGroupByKey</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairGroupByKey</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCoGroup</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCoGroup</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testInitialization</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testInitialization</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testSocketString</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testSocketString</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testGroupByKeyAndWindow</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testGroupByKeyAndWindow</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testReduceByKeyAndWindow</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testReduceByKeyAndWindow</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testForeachRDD</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testForeachRDD</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testFileStream</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testFileStream</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairTransform</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairTransform</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testFilter</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testFilter</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairMap2</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairMap2</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testMapValues</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testMapValues</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testReduce</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testReduce</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testUpdateStateByKey</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testUpdateStateByKey</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testTransform</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testTransform</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testWindow</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testWindow</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCountByValueAndWindow</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCountByValueAndWindow</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testSocketTextStream</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testSocketTextStream</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testUpdateStateByKeyWithInitial</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testUpdateStateByKeyWithInitial</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testContextState</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testContextState</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexDescendingWithStart</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexWithStart</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndexDescendingWithStart</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexDescending</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexWithStart</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexWithLast</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexWithSkip</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexWithMax</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexDescending</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndexDescendingWithLast</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexDescending</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexDescendingWithLast</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndex</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndexWithLast</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexWithStart</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexDescendingWithStart</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexWithLast</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexWithSkip</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndexDescending</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.testRefWithIntNaturalKey</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexDescending</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexDescendingWithStart</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexWithMax</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndex</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexWithLast</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexWithSkip</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexWithMax</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexDescendingWithLast</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexDescendingWithLast</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexDescendingWithStart</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndex</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexWithLast</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexWithSkip</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexWithStart</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndex</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexDescendingWithLast</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndexWithStart</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndex</div></li><li><div>org.apache.spark.util.kvstore.LevelDBSuite.testMultipleTypesWriteReadDelete</div></li><li><div>org.apache.spark.util.kvstore.LevelDBSuite.testObjectWriteReadDelete</div></li><li><div>org.apache.spark.util.kvstore.LevelDBSuite.testSkip</div></li><li><div>org.apache.spark.util.kvstore.LevelDBSuite.testMultipleObjectWriteReadDelete</div></li><li><div>org.apache.spark.util.kvstore.LevelDBSuite.testReopenAndVersionCheckDb</div></li><li><div>org.apache.spark.util.kvstore.LevelDBSuite.testMetadata</div></li><li><div>org.apache.spark.util.kvstore.LevelDBSuite.testUpdate</div></li><li><div>org.apache.spark.util.kvstore.LevelDBSuite.testNegativeIndexValues</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.spark.network.ChunkFetchIntegrationSuite.fetchFileChunk</div></li><li><div>org.apache.spark.JavaJdbcRDDSuite.testJavaJdbcRDD</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.leftOuterJoin</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.foldReduce</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.mapsFromPairsToPairs</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.flatMap</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.foreach</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.map</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.map</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.zip</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.zip</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.keyBy</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.keyBy</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.groupBy</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.groupBy</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.mapPartitions</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.mapPartitions</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.foldByKey</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.foldByKey</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.mapOnPairRDD</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.mapOnPairRDD</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.sequenceFile</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.sequenceFile</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.collectPartitions</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.collectPartitions</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.reduceByKey</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.reduceByKey</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.foreachWithAnonymousClass</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.foreachWithAnonymousClass</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.collectAsMapWithIntArrayValues</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.collectAsMapWithIntArrayValues</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.zipPartitions</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.zipPartitions</div></li><li><div>test.org.apache.spark.JavaAPISuite.groupByOnPairRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.binaryFilesCaching</div></li><li><div>test.org.apache.spark.JavaAPISuite.sparkContextUnion</div></li><li><div>test.org.apache.spark.JavaAPISuite.checkpointAndComputation</div></li><li><div>test.org.apache.spark.JavaAPISuite.checkpointAndComputation</div></li><li><div>test.org.apache.spark.JavaAPISuite.leftOuterJoin</div></li><li><div>test.org.apache.spark.JavaAPISuite.leftOuterJoin</div></li><li><div>test.org.apache.spark.JavaAPISuite.keyByOnPairRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.keyByOnPairRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.getNumPartitions</div></li><li><div>test.org.apache.spark.JavaAPISuite.getNumPartitions</div></li><li><div>test.org.apache.spark.JavaAPISuite.wholeTextFiles</div></li><li><div>test.org.apache.spark.JavaAPISuite.wholeTextFiles</div></li><li><div>test.org.apache.spark.JavaAPISuite.binaryFiles</div></li><li><div>test.org.apache.spark.JavaAPISuite.binaryFiles</div></li><li><div>test.org.apache.spark.JavaAPISuite.foldReduce</div></li><li><div>test.org.apache.spark.JavaAPISuite.foldReduce</div></li><li><div>test.org.apache.spark.JavaAPISuite.writeWithNewAPIHadoopFile</div></li><li><div>test.org.apache.spark.JavaAPISuite.writeWithNewAPIHadoopFile</div></li><li><div>test.org.apache.spark.JavaAPISuite.hadoopFile</div></li><li><div>test.org.apache.spark.JavaAPISuite.hadoopFile</div></li><li><div>test.org.apache.spark.JavaAPISuite.lookup</div></li><li><div>test.org.apache.spark.JavaAPISuite.lookup</div></li><li><div>test.org.apache.spark.JavaAPISuite.countAsync</div></li><li><div>test.org.apache.spark.JavaAPISuite.countAsync</div></li><li><div>test.org.apache.spark.JavaAPISuite.textFiles</div></li><li><div>test.org.apache.spark.JavaAPISuite.textFiles</div></li><li><div>test.org.apache.spark.JavaAPISuite.binaryRecords</div></li><li><div>test.org.apache.spark.JavaAPISuite.binaryRecords</div></li><li><div>test.org.apache.spark.JavaAPISuite.toLocalIterator</div></li><li><div>test.org.apache.spark.JavaAPISuite.toLocalIterator</div></li><li><div>test.org.apache.spark.JavaAPISuite.repartitionAndSortWithinPartitions</div></li><li><div>test.org.apache.spark.JavaAPISuite.repartitionAndSortWithinPartitions</div></li><li><div>test.org.apache.spark.JavaAPISuite.reduce</div></li><li><div>test.org.apache.spark.JavaAPISuite.reduce</div></li><li><div>test.org.apache.spark.JavaAPISuite.sample</div></li><li><div>test.org.apache.spark.JavaAPISuite.sample</div></li><li><div>test.org.apache.spark.JavaAPISuite.sortBy</div></li><li><div>test.org.apache.spark.JavaAPISuite.sortBy</div></li><li><div>test.org.apache.spark.JavaAPISuite.mapsFromPairsToPairs</div></li><li><div>test.org.apache.spark.JavaAPISuite.mapsFromPairsToPairs</div></li><li><div>test.org.apache.spark.JavaAPISuite.flatMap</div></li><li><div>test.org.apache.spark.JavaAPISuite.flatMap</div></li><li><div>test.org.apache.spark.JavaAPISuite.cogroup3</div></li><li><div>test.org.apache.spark.JavaAPISuite.cogroup3</div></li><li><div>test.org.apache.spark.JavaAPISuite.cogroup4</div></li><li><div>test.org.apache.spark.JavaAPISuite.cogroup4</div></li><li><div>test.org.apache.spark.JavaAPISuite.randomSplit</div></li><li><div>test.org.apache.spark.JavaAPISuite.randomSplit</div></li><li><div>test.org.apache.spark.JavaAPISuite.persist</div></li><li><div>test.org.apache.spark.JavaAPISuite.persist</div></li><li><div>test.org.apache.spark.JavaAPISuite.foreach</div></li><li><div>test.org.apache.spark.JavaAPISuite.foreach</div></li><li><div>test.org.apache.spark.JavaAPISuite.hadoopFileCompressed</div></li><li><div>test.org.apache.spark.JavaAPISuite.hadoopFileCompressed</div></li><li><div>test.org.apache.spark.JavaAPISuite.textFilesCompressed</div></li><li><div>test.org.apache.spark.JavaAPISuite.checkpointAndRestore</div></li><li><div>test.org.apache.spark.JavaAPISuite.checkpointAndRestore</div></li><li><div>test.org.apache.spark.JavaAPISuite.sortByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.sortByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.aggregateByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.aggregateByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.map</div></li><li><div>test.org.apache.spark.JavaAPISuite.map</div></li><li><div>test.org.apache.spark.JavaAPISuite.max</div></li><li><div>test.org.apache.spark.JavaAPISuite.max</div></li><li><div>test.org.apache.spark.JavaAPISuite.min</div></li><li><div>test.org.apache.spark.JavaAPISuite.min</div></li><li><div>test.org.apache.spark.JavaAPISuite.top</div></li><li><div>test.org.apache.spark.JavaAPISuite.top</div></li><li><div>test.org.apache.spark.JavaAPISuite.zip</div></li><li><div>test.org.apache.spark.JavaAPISuite.zip</div></li><li><div>test.org.apache.spark.JavaAPISuite.fold</div></li><li><div>test.org.apache.spark.JavaAPISuite.fold</div></li><li><div>test.org.apache.spark.JavaAPISuite.glom</div></li><li><div>test.org.apache.spark.JavaAPISuite.glom</div></li><li><div>test.org.apache.spark.JavaAPISuite.take</div></li><li><div>test.org.apache.spark.JavaAPISuite.take</div></li><li><div>test.org.apache.spark.JavaAPISuite.javaDoubleRDDHistoGram</div></li><li><div>test.org.apache.spark.JavaAPISuite.collectUnderlyingScalaRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.collectUnderlyingScalaRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.keyBy</div></li><li><div>test.org.apache.spark.JavaAPISuite.keyBy</div></li><li><div>test.org.apache.spark.JavaAPISuite.mapPartitionsWithIndex</div></li><li><div>test.org.apache.spark.JavaAPISuite.mapPartitionsWithIndex</div></li><li><div>test.org.apache.spark.JavaAPISuite.sampleByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.sampleByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.intersection</div></li><li><div>test.org.apache.spark.JavaAPISuite.intersection</div></li><li><div>test.org.apache.spark.JavaAPISuite.aggregate</div></li><li><div>test.org.apache.spark.JavaAPISuite.aggregate</div></li><li><div>test.org.apache.spark.JavaAPISuite.cartesian</div></li><li><div>test.org.apache.spark.JavaAPISuite.cartesian</div></li><li><div>test.org.apache.spark.JavaAPISuite.countApproxDistinctByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.countApproxDistinctByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.readWithNewAPIHadoopFile</div></li><li><div>test.org.apache.spark.JavaAPISuite.readWithNewAPIHadoopFile</div></li><li><div>test.org.apache.spark.JavaAPISuite.testRegisterKryoClasses</div></li><li><div>test.org.apache.spark.JavaAPISuite.testRegisterKryoClasses</div></li><li><div>test.org.apache.spark.JavaAPISuite.groupBy</div></li><li><div>test.org.apache.spark.JavaAPISuite.groupBy</div></li><li><div>test.org.apache.spark.JavaAPISuite.sampleByKeyExact</div></li><li><div>test.org.apache.spark.JavaAPISuite.mapPartitions</div></li><li><div>test.org.apache.spark.JavaAPISuite.takeOrdered</div></li><li><div>test.org.apache.spark.JavaAPISuite.takeOrdered</div></li><li><div>test.org.apache.spark.JavaAPISuite.foldByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.foldByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.objectFilesOfInts</div></li><li><div>test.org.apache.spark.JavaAPISuite.objectFilesOfInts</div></li><li><div>test.org.apache.spark.JavaAPISuite.treeAggregate</div></li><li><div>test.org.apache.spark.JavaAPISuite.treeAggregate</div></li><li><div>test.org.apache.spark.JavaAPISuite.testGetPersistentRDDs</div></li><li><div>test.org.apache.spark.JavaAPISuite.testGetPersistentRDDs</div></li><li><div>test.org.apache.spark.JavaAPISuite.approximateResults</div></li><li><div>test.org.apache.spark.JavaAPISuite.approximateResults</div></li><li><div>test.org.apache.spark.JavaAPISuite.treeReduce</div></li><li><div>test.org.apache.spark.JavaAPISuite.treeReduce</div></li><li><div>test.org.apache.spark.JavaAPISuite.collectAsMapAndSerialize</div></li><li><div>test.org.apache.spark.JavaAPISuite.collectAsMapAndSerialize</div></li><li><div>test.org.apache.spark.JavaAPISuite.countApproxDistinct</div></li><li><div>test.org.apache.spark.JavaAPISuite.countApproxDistinct</div></li><li><div>test.org.apache.spark.JavaAPISuite.javaDoubleRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.javaDoubleRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.mapOnPairRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.mapOnPairRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.testAsyncActionErrorWrapping</div></li><li><div>test.org.apache.spark.JavaAPISuite.testAsyncActionErrorWrapping</div></li><li><div>test.org.apache.spark.JavaAPISuite.naturalMax</div></li><li><div>test.org.apache.spark.JavaAPISuite.naturalMax</div></li><li><div>test.org.apache.spark.JavaAPISuite.naturalMin</div></li><li><div>test.org.apache.spark.JavaAPISuite.naturalMin</div></li><li><div>test.org.apache.spark.JavaAPISuite.sequenceFile</div></li><li><div>test.org.apache.spark.JavaAPISuite.sequenceFile</div></li><li><div>test.org.apache.spark.JavaAPISuite.collectPartitions</div></li><li><div>test.org.apache.spark.JavaAPISuite.collectPartitions</div></li><li><div>test.org.apache.spark.JavaAPISuite.cogroup</div></li><li><div>test.org.apache.spark.JavaAPISuite.cogroup</div></li><li><div>test.org.apache.spark.JavaAPISuite.reduceByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.reduceByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.repartition</div></li><li><div>test.org.apache.spark.JavaAPISuite.repartition</div></li><li><div>test.org.apache.spark.JavaAPISuite.iterator</div></li><li><div>test.org.apache.spark.JavaAPISuite.iterator</div></li><li><div>test.org.apache.spark.JavaAPISuite.emptyRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.emptyRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.zipWithIndex</div></li><li><div>test.org.apache.spark.JavaAPISuite.zipWithIndex</div></li><li><div>test.org.apache.spark.JavaAPISuite.foreachPartition</div></li><li><div>test.org.apache.spark.JavaAPISuite.foreachPartition</div></li><li><div>test.org.apache.spark.JavaAPISuite.combineByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.combineByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.takeAsync</div></li><li><div>test.org.apache.spark.JavaAPISuite.takeAsync</div></li><li><div>test.org.apache.spark.JavaAPISuite.collectAsMapWithIntArrayValues</div></li><li><div>test.org.apache.spark.JavaAPISuite.collectAsMapWithIntArrayValues</div></li><li><div>test.org.apache.spark.JavaAPISuite.objectFilesOfComplexTypes</div></li><li><div>test.org.apache.spark.JavaAPISuite.objectFilesOfComplexTypes</div></li><li><div>test.org.apache.spark.JavaAPISuite.zipWithUniqueId</div></li><li><div>test.org.apache.spark.JavaAPISuite.zipWithUniqueId</div></li><li><div>test.org.apache.spark.JavaAPISuite.collectAsync</div></li><li><div>test.org.apache.spark.JavaAPISuite.collectAsync</div></li><li><div>test.org.apache.spark.JavaAPISuite.foreachAsync</div></li><li><div>test.org.apache.spark.JavaAPISuite.foreachAsync</div></li><li><div>test.org.apache.spark.JavaAPISuite.zipPartitions</div></li><li><div>test.org.apache.spark.JavaAPISuite.zipPartitions</div></li><li><div>test.org.apache.spark.JavaAPISuite.reduceOnJavaDoubleRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.reduceOnJavaDoubleRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.isEmpty</div></li><li><div>test.org.apache.spark.JavaAPISuite.isEmpty</div></li><li><div>test.org.apache.spark.JavaSparkContextSuite.scalaSparkContext</div></li><li><div>org.apache.spark.sql.avro.AvroFunctionsSuite.roundtrip in to_avro and from_avro - int and string</div></li><li><div>org.apache.spark.sql.avro.AvroFunctionsSuite.roundtrip in to_avro and from_avro - struct</div></li><li><div>org.apache.spark.sql.avro.AvroFunctionsSuite.roundtrip in to_avro and from_avro - array with null</div></li><li><div>org.apache.spark.streaming.kafka010.JavaDirectKafkaStreamSuite.testKafkaStream</div></li><li><div>org.apache.spark.streaming.kafka010.JavaKafkaRDDSuite.testKafkaRDD</div></li><li><div>org.apache.spark.graphx.EdgeRDDSuite.cache, getStorageLevel</div></li><li><div>org.apache.spark.graphx.EdgeRDDSuite.checkpointing</div></li><li><div>org.apache.spark.graphx.GraphLoaderSuite.GraphLoader.edgeListFile</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.joinVertices</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.collectNeighborIds</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.removeSelfEdges</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.filter</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.convertToCanonicalEdges</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.collectEdgesCycleDirectionOut</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.collectEdgesCycleDirectionIn</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.collectEdgesCycleDirectionEither</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.collectEdgesChainDirectionOut</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.collectEdgesChainDirectionIn</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.collectEdgesChainDirectionEither</div></li><li><div>org.apache.spark.graphx.GraphSuite.Graph.fromEdgeTuples</div></li><li><div>org.apache.spark.graphx.GraphSuite.Graph.fromEdges</div></li><li><div>org.apache.spark.graphx.GraphSuite.Graph.apply</div></li><li><div>org.apache.spark.graphx.GraphSuite.triplets</div></li><li><div>org.apache.spark.graphx.GraphSuite.partitionBy</div></li><li><div>org.apache.spark.graphx.GraphSuite.mapVertices</div></li><li><div>org.apache.spark.graphx.GraphSuite.mapVertices changing type with same erased type</div></li><li><div>org.apache.spark.graphx.GraphSuite.mapEdges</div></li><li><div>org.apache.spark.graphx.GraphSuite.mapTriplets</div></li><li><div>org.apache.spark.graphx.GraphSuite.reverse</div></li><li><div>org.apache.spark.graphx.GraphSuite.reverse with join elimination</div></li><li><div>org.apache.spark.graphx.GraphSuite.subgraph</div></li><li><div>org.apache.spark.graphx.GraphSuite.mask</div></li><li><div>org.apache.spark.graphx.GraphSuite.groupEdges</div></li><li><div>org.apache.spark.graphx.GraphSuite.aggregateMessages</div></li><li><div>org.apache.spark.graphx.GraphSuite.outerJoinVertices</div></li><li><div>org.apache.spark.graphx.GraphSuite.more edge partitions than vertex partitions</div></li><li><div>org.apache.spark.graphx.GraphSuite.checkpoint</div></li><li><div>org.apache.spark.graphx.GraphSuite.cache, getStorageLevel</div></li><li><div>org.apache.spark.graphx.GraphSuite.non-default number of edge partitions</div></li><li><div>org.apache.spark.graphx.GraphSuite.unpersist graph RDD</div></li><li><div>org.apache.spark.graphx.GraphSuite.SPARK-14219: pickRandomVertex</div></li><li><div>org.apache.spark.graphx.PregelSuite.1 iteration</div></li><li><div>org.apache.spark.graphx.PregelSuite.chain propagation</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.filter</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.mapValues</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.minus</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.minus with RDD[(VertexId, VD)]</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.minus with non-equal number of partitions</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.diff</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.diff with RDD[(VertexId, VD)]</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.diff vertices with non-equal number of partitions</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.leftJoin</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.leftJoin vertices with non-equal number of partitions</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.innerJoin</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.innerJoin vertices with the non-equal number of partitions</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.aggregateUsingIndex</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.mergeFunc</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.cache, getStorageLevel</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.checkpoint</div></li><li><div>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Grid Connected Components</div></li><li><div>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Reverse Grid Connected Components</div></li><li><div>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Chain Connected Components</div></li><li><div>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Reverse Chain Connected Components</div></li><li><div>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Connected Components on a Toy Connected Graph</div></li><li><div>org.apache.spark.graphx.lib.LabelPropagationSuite.Label Propagation</div></li><li><div>org.apache.spark.graphx.lib.PageRankSuite.Star PageRank</div></li><li><div>org.apache.spark.graphx.lib.PageRankSuite.Star PersonalPageRank</div></li><li><div>org.apache.spark.graphx.lib.PageRankSuite.Grid PageRank</div></li><li><div>org.apache.spark.graphx.lib.PageRankSuite.Chain PageRank</div></li><li><div>org.apache.spark.graphx.lib.PageRankSuite.Chain PersonalizedPageRank</div></li><li><div>org.apache.spark.graphx.lib.PageRankSuite.Loop with source PageRank</div></li><li><div>org.apache.spark.graphx.lib.PageRankSuite.Loop with sink PageRank</div></li><li><div>org.apache.spark.graphx.lib.SVDPlusPlusSuite.Test SVD++ with mean square error on training set</div></li><li><div>org.apache.spark.graphx.lib.ShortestPathsSuite.Shortest Path Computations</div></li><li><div>org.apache.spark.graphx.lib.StronglyConnectedComponentsSuite.Island Strongly Connected Components</div></li><li><div>org.apache.spark.graphx.lib.StronglyConnectedComponentsSuite.Cycle Strongly Connected Components</div></li><li><div>org.apache.spark.graphx.lib.StronglyConnectedComponentsSuite.2 Cycle Strongly Connected Components</div></li><li><div>org.apache.spark.graphx.lib.TriangleCountSuite.Count a single triangle</div></li><li><div>org.apache.spark.graphx.lib.TriangleCountSuite.Count two triangles</div></li><li><div>org.apache.spark.graphx.lib.TriangleCountSuite.Count two triangles with bi-directed edges</div></li><li><div>org.apache.spark.graphx.lib.TriangleCountSuite.Count a single triangle with duplicate edges</div></li><li><div>org.apache.spark.graphx.util.GraphGeneratorsSuite.GraphGenerators.logNormalGraph</div></li><li><div>org.apache.spark.graphx.util.GraphGeneratorsSuite.SPARK-5064 GraphGenerators.rmatGraph numEdges upper bound</div></li><li><div>org.apache.spark.graphx.util.PeriodicGraphCheckpointerSuite.Persisting</div></li><li><div>org.apache.spark.graphx.util.PeriodicGraphCheckpointerSuite.Checkpointing</div></li><li><div>org.apache.spark.ml.JavaPipelineSuite.pipeline</div></li><li><div>org.apache.spark.ml.JavaPipelineSuite.pipeline</div></li><li><div>org.apache.spark.ml.classification.JavaDecisionTreeClassifierSuite.runDT</div></li><li><div>org.apache.spark.ml.classification.JavaDecisionTreeClassifierSuite.runDT</div></li><li><div>org.apache.spark.ml.classification.JavaGBTClassifierSuite.runDT</div></li><li><div>org.apache.spark.ml.classification.JavaGBTClassifierSuite.runDT</div></li><li><div>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionWithSetters</div></li><li><div>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionWithSetters</div></li><li><div>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionTrainingSummary</div></li><li><div>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionTrainingSummary</div></li><li><div>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionPredictorClassifierMethods</div></li><li><div>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionPredictorClassifierMethods</div></li><li><div>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionDefaultParams</div></li><li><div>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionDefaultParams</div></li><li><div>org.apache.spark.ml.classification.JavaMultilayerPerceptronClassifierSuite.testMLPC</div></li><li><div>org.apache.spark.ml.classification.JavaMultilayerPerceptronClassifierSuite.testMLPC</div></li><li><div>org.apache.spark.ml.classification.JavaNaiveBayesSuite.testNaiveBayes</div></li><li><div>org.apache.spark.ml.classification.JavaNaiveBayesSuite.testNaiveBayes</div></li><li><div>org.apache.spark.ml.classification.JavaNaiveBayesSuite.naiveBayesDefaultParams</div></li><li><div>org.apache.spark.ml.classification.JavaNaiveBayesSuite.naiveBayesDefaultParams</div></li><li><div>org.apache.spark.ml.classification.JavaOneVsRestSuite.oneVsRestDefaultParams</div></li><li><div>org.apache.spark.ml.classification.JavaOneVsRestSuite.oneVsRestDefaultParams</div></li><li><div>org.apache.spark.ml.classification.JavaRandomForestClassifierSuite.runDT</div></li><li><div>org.apache.spark.ml.classification.JavaRandomForestClassifierSuite.runDT</div></li><li><div>org.apache.spark.ml.clustering.JavaKMeansSuite.fitAndTransform</div></li><li><div>org.apache.spark.ml.clustering.JavaKMeansSuite.fitAndTransform</div></li><li><div>org.apache.spark.ml.feature.JavaBucketizerSuite.bucketizerTest</div></li><li><div>org.apache.spark.ml.feature.JavaBucketizerSuite.bucketizerTest</div></li><li><div>org.apache.spark.ml.feature.JavaBucketizerSuite.bucketizerMultipleColumnsTest</div></li><li><div>org.apache.spark.ml.feature.JavaBucketizerSuite.bucketizerMultipleColumnsTest</div></li><li><div>org.apache.spark.ml.feature.JavaDCTSuite.javaCompatibilityTest</div></li><li><div>org.apache.spark.ml.feature.JavaDCTSuite.javaCompatibilityTest</div></li><li><div>org.apache.spark.ml.feature.JavaHashingTFSuite.hashingTF</div></li><li><div>org.apache.spark.ml.feature.JavaHashingTFSuite.hashingTF</div></li><li><div>org.apache.spark.ml.feature.JavaNormalizerSuite.normalizer</div></li><li><div>org.apache.spark.ml.feature.JavaNormalizerSuite.normalizer</div></li><li><div>org.apache.spark.ml.feature.JavaPCASuite.testPCA</div></li><li><div>org.apache.spark.ml.feature.JavaPCASuite.testPCA</div></li><li><div>org.apache.spark.ml.feature.JavaPolynomialExpansionSuite.polynomialExpansionTest</div></li><li><div>org.apache.spark.ml.feature.JavaPolynomialExpansionSuite.polynomialExpansionTest</div></li><li><div>org.apache.spark.ml.feature.JavaStandardScalerSuite.standardScaler</div></li><li><div>org.apache.spark.ml.feature.JavaStandardScalerSuite.standardScaler</div></li><li><div>org.apache.spark.ml.feature.JavaStopWordsRemoverSuite.javaCompatibilityTest</div></li><li><div>org.apache.spark.ml.feature.JavaStopWordsRemoverSuite.javaCompatibilityTest</div></li><li><div>org.apache.spark.ml.feature.JavaStringIndexerSuite.testStringIndexer</div></li><li><div>org.apache.spark.ml.feature.JavaStringIndexerSuite.testStringIndexer</div></li><li><div>org.apache.spark.ml.feature.JavaTokenizerSuite.regexTokenizer</div></li><li><div>org.apache.spark.ml.feature.JavaTokenizerSuite.regexTokenizer</div></li><li><div>org.apache.spark.ml.feature.JavaVectorAssemblerSuite.testVectorAssembler</div></li><li><div>org.apache.spark.ml.feature.JavaVectorAssemblerSuite.testVectorAssembler</div></li><li><div>org.apache.spark.ml.feature.JavaVectorIndexerSuite.vectorIndexerAPI</div></li><li><div>org.apache.spark.ml.feature.JavaVectorIndexerSuite.vectorIndexerAPI</div></li><li><div>org.apache.spark.ml.feature.JavaVectorSlicerSuite.vectorSlice</div></li><li><div>org.apache.spark.ml.feature.JavaVectorSlicerSuite.vectorSlice</div></li><li><div>org.apache.spark.ml.feature.JavaWord2VecSuite.testJavaWord2Vec</div></li><li><div>org.apache.spark.ml.feature.JavaWord2VecSuite.testJavaWord2Vec</div></li><li><div>org.apache.spark.ml.regression.JavaDecisionTreeRegressorSuite.runDT</div></li><li><div>org.apache.spark.ml.regression.JavaDecisionTreeRegressorSuite.runDT</div></li><li><div>org.apache.spark.ml.regression.JavaGBTRegressorSuite.runDT</div></li><li><div>org.apache.spark.ml.regression.JavaGBTRegressorSuite.runDT</div></li><li><div>org.apache.spark.ml.regression.JavaLinearRegressionSuite.linearRegressionDefaultParams</div></li><li><div>org.apache.spark.ml.regression.JavaLinearRegressionSuite.linearRegressionDefaultParams</div></li><li><div>org.apache.spark.ml.regression.JavaLinearRegressionSuite.linearRegressionWithSetters</div></li><li><div>org.apache.spark.ml.regression.JavaLinearRegressionSuite.linearRegressionWithSetters</div></li><li><div>org.apache.spark.ml.regression.JavaRandomForestRegressorSuite.runDT</div></li><li><div>org.apache.spark.ml.source.libsvm.JavaLibSVMRelationSuite.verifyLibSVMDF</div></li><li><div>org.apache.spark.ml.source.libsvm.JavaLibSVMRelationSuite.verifyLibSVMDF</div></li><li><div>org.apache.spark.ml.stat.JavaKolmogorovSmirnovTestSuite.testKSTestNamedDistribution</div></li><li><div>org.apache.spark.ml.stat.JavaKolmogorovSmirnovTestSuite.testKSTestCDF</div></li><li><div>org.apache.spark.ml.stat.JavaSummarizerSuite.testSummarizer</div></li><li><div>org.apache.spark.ml.stat.JavaSummarizerSuite.testSummarizer</div></li><li><div>org.apache.spark.ml.tuning.JavaCrossValidatorSuite.crossValidationWithLogisticRegression</div></li><li><div>org.apache.spark.ml.tuning.JavaCrossValidatorSuite.crossValidationWithLogisticRegression</div></li><li><div>org.apache.spark.ml.util.JavaDefaultReadWriteSuite.testDefaultReadWrite</div></li><li><div>org.apache.spark.ml.util.JavaDefaultReadWriteSuite.testDefaultReadWrite</div></li><li><div>org.apache.spark.mllib.classification.JavaLogisticRegressionSuite.runLRUsingConstructor</div></li><li><div>org.apache.spark.mllib.classification.JavaLogisticRegressionSuite.runLRUsingConstructor</div></li><li><div>org.apache.spark.mllib.classification.JavaLogisticRegressionSuite.runLRUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.classification.JavaLogisticRegressionSuite.runLRUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.testPredictJavaRDD</div></li><li><div>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.testPredictJavaRDD</div></li><li><div>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.runUsingConstructor</div></li><li><div>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.runUsingConstructor</div></li><li><div>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.runUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.runUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.testModelTypeSetters</div></li><li><div>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.testModelTypeSetters</div></li><li><div>org.apache.spark.mllib.classification.JavaSVMSuite.runSVMUsingConstructor</div></li><li><div>org.apache.spark.mllib.classification.JavaSVMSuite.runSVMUsingConstructor</div></li><li><div>org.apache.spark.mllib.classification.JavaSVMSuite.runSVMUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.classification.JavaSVMSuite.runSVMUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.classification.JavaStreamingLogisticRegressionSuite.javaAPI</div></li><li><div>org.apache.spark.mllib.classification.JavaStreamingLogisticRegressionSuite.javaAPI</div></li><li><div>org.apache.spark.mllib.clustering.JavaBisectingKMeansSuite.twoDimensionalData</div></li><li><div>org.apache.spark.mllib.clustering.JavaBisectingKMeansSuite.twoDimensionalData</div></li><li><div>org.apache.spark.mllib.clustering.JavaGaussianMixtureSuite.runGaussianMixture</div></li><li><div>org.apache.spark.mllib.clustering.JavaGaussianMixtureSuite.runGaussianMixture</div></li><li><div>org.apache.spark.mllib.clustering.JavaKMeansSuite.testPredictJavaRDD</div></li><li><div>org.apache.spark.mllib.clustering.JavaKMeansSuite.runKMeansUsingConstructor</div></li><li><div>org.apache.spark.mllib.clustering.JavaKMeansSuite.runKMeansUsingConstructor</div></li><li><div>org.apache.spark.mllib.clustering.JavaKMeansSuite.runKMeansUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.clustering.JavaKMeansSuite.runKMeansUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.clustering.JavaLDASuite.onlineOptimizerCompatibility</div></li><li><div>org.apache.spark.mllib.clustering.JavaLDASuite.onlineOptimizerCompatibility</div></li><li><div>org.apache.spark.mllib.clustering.JavaLDASuite.distributedLDAModel</div></li><li><div>org.apache.spark.mllib.clustering.JavaLDASuite.distributedLDAModel</div></li><li><div>org.apache.spark.mllib.clustering.JavaLDASuite.localLDAModel</div></li><li><div>org.apache.spark.mllib.clustering.JavaLDASuite.localLDAModel</div></li><li><div>org.apache.spark.mllib.clustering.JavaLDASuite.localLdaMethods</div></li><li><div>org.apache.spark.mllib.clustering.JavaLDASuite.localLdaMethods</div></li><li><div>org.apache.spark.mllib.clustering.JavaStreamingKMeansSuite.javaAPI</div></li><li><div>org.apache.spark.mllib.evaluation.JavaRankingMetricsSuite.rankingMetrics</div></li><li><div>org.apache.spark.mllib.evaluation.JavaRankingMetricsSuite.rankingMetrics</div></li><li><div>org.apache.spark.mllib.feature.JavaTfIdfSuite.tfIdfMinimumDocumentFrequency</div></li><li><div>org.apache.spark.mllib.feature.JavaTfIdfSuite.tfIdfMinimumDocumentFrequency</div></li><li><div>org.apache.spark.mllib.feature.JavaTfIdfSuite.tfIdf</div></li><li><div>org.apache.spark.mllib.feature.JavaTfIdfSuite.tfIdf</div></li><li><div>org.apache.spark.mllib.feature.JavaWord2VecSuite.word2Vec</div></li><li><div>org.apache.spark.mllib.feature.JavaWord2VecSuite.word2Vec</div></li><li><div>org.apache.spark.mllib.fpm.JavaAssociationRulesSuite.runAssociationRules</div></li><li><div>org.apache.spark.mllib.fpm.JavaAssociationRulesSuite.runAssociationRules</div></li><li><div>org.apache.spark.mllib.fpm.JavaFPGrowthSuite.runFPGrowthSaveLoad</div></li><li><div>org.apache.spark.mllib.fpm.JavaFPGrowthSuite.runFPGrowthSaveLoad</div></li><li><div>org.apache.spark.mllib.fpm.JavaFPGrowthSuite.runFPGrowth</div></li><li><div>org.apache.spark.mllib.fpm.JavaFPGrowthSuite.runFPGrowth</div></li><li><div>org.apache.spark.mllib.fpm.JavaPrefixSpanSuite.runPrefixSpan</div></li><li><div>org.apache.spark.mllib.fpm.JavaPrefixSpanSuite.runPrefixSpan</div></li><li><div>org.apache.spark.mllib.fpm.JavaPrefixSpanSuite.runPrefixSpanSaveLoad</div></li><li><div>org.apache.spark.mllib.fpm.JavaPrefixSpanSuite.runPrefixSpanSaveLoad</div></li><li><div>org.apache.spark.mllib.linalg.distributed.JavaRowMatrixSuite.rowMatrixQRDecomposition</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testNormalVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testArbitrary</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testArbitrary</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testLogNormalVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testLogNormalVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testExponentialVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testExponentialVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testUniformRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testUniformRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testRandomVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testRandomVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testGammaRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testGammaRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testUniformVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testUniformVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testPoissonRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testPoissonRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testNormalRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testNormalRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testPoissonVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testPoissonVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testGammaVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testGammaVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testExponentialRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testExponentialRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testLNormalRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testLNormalRDD</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runALSUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runALSUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSUsingConstructor</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSUsingConstructor</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runRecommend</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runRecommend</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSWithNegativeWeight</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSWithNegativeWeight</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runALSUsingConstructor</div></li><li><div>org.apache.spark.mllib.regression.JavaIsotonicRegressionSuite.testIsotonicRegressionJavaRDD</div></li><li><div>org.apache.spark.mllib.regression.JavaIsotonicRegressionSuite.testIsotonicRegressionPredictionsJavaRDD</div></li><li><div>org.apache.spark.mllib.regression.JavaLassoSuite.runLassoUsingConstructor</div></li><li><div>org.apache.spark.mllib.regression.JavaLassoSuite.runLassoUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.testPredictJavaRDD</div></li><li><div>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.testPredictJavaRDD</div></li><li><div>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.runLinearRegressionUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.runLinearRegressionUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.runLinearRegressionUsingConstructor</div></li><li><div>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.runLinearRegressionUsingConstructor</div></li><li><div>org.apache.spark.mllib.regression.JavaRidgeRegressionSuite.runRidgeRegressionUsingConstructor</div></li><li><div>org.apache.spark.mllib.regression.JavaRidgeRegressionSuite.runRidgeRegressionUsingConstructor</div></li><li><div>org.apache.spark.mllib.regression.JavaRidgeRegressionSuite.runRidgeRegressionUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.regression.JavaRidgeRegressionSuite.runRidgeRegressionUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.regression.JavaStreamingLinearRegressionSuite.javaAPI</div></li><li><div>org.apache.spark.mllib.stat.JavaStatisticsSuite.testCorr</div></li><li><div>org.apache.spark.mllib.stat.JavaStatisticsSuite.testCorr</div></li><li><div>org.apache.spark.mllib.stat.JavaStatisticsSuite.chiSqTest</div></li><li><div>org.apache.spark.mllib.stat.JavaStatisticsSuite.chiSqTest</div></li><li><div>org.apache.spark.mllib.stat.JavaStatisticsSuite.streamingTest</div></li><li><div>org.apache.spark.mllib.stat.JavaStatisticsSuite.streamingTest</div></li><li><div>org.apache.spark.mllib.stat.JavaStatisticsSuite.kolmogorovSmirnovTest</div></li><li><div>org.apache.spark.mllib.stat.JavaStatisticsSuite.kolmogorovSmirnovTest</div></li><li><div>org.apache.spark.mllib.tree.JavaDecisionTreeSuite.runDTUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.tree.JavaDecisionTreeSuite.runDTUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.tree.JavaDecisionTreeSuite.runDTUsingConstructor</div></li><li><div>org.apache.spark.mllib.tree.JavaDecisionTreeSuite.runDTUsingConstructor</div></li><li><div>org.apache.spark.mllib.util.JavaMLUtilsSuite.testConvertMatrixColumnsToAndFromML</div></li><li><div>org.apache.spark.mllib.util.JavaMLUtilsSuite.testConvertVectorColumnsToAndFromML</div></li><li><div>org.apache.spark.mllib.util.JavaMLUtilsSuite.testConvertVectorColumnsToAndFromML</div></li><li><div>org.apache.spark.repl.ReplSuite.SPARK-15236: use Hive catalog</div></li><li><div>org.apache.spark.repl.ReplSuite.SPARK-15236: use in-memory catalog</div></li><li><div>org.apache.spark.repl.ReplSuite.broadcast vars</div></li><li><div>org.apache.spark.repl.ReplSuite.line wrapper only initialized once when used as encoder outer scope</div></li><li><div>org.apache.spark.repl.ReplSuite.define case class and create Dataset together with paste mode</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.simple foreach with accumulator</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.external vars</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.external classes</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.external functions</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.external functions that access vars</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.broadcast vars</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.interacting with files</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.local-cluster mode</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.SPARK-2576 importing implicits</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.Datasets and encoders</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.SPARK-2632 importing a method from non serializable class and not using it.</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.collecting objects of class defined in repl</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.collecting objects of class defined in repl - shuffling</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.replicating blocks of object with class defined in repl</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.newProductSeqEncoder with REPL defined class</div></li><li><div>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationCount</div></li><li><div>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationCount</div></li><li><div>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationSumDouble</div></li><li><div>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationSumDouble</div></li><li><div>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationSumLong</div></li><li><div>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationSumLong</div></li><li><div>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationAverage</div></li><li><div>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationAverage</div></li><li><div>test.org.apache.spark.sql.JavaApplySchemaSuite.applySchema</div></li><li><div>test.org.apache.spark.sql.JavaApplySchemaSuite.applySchema</div></li><li><div>test.org.apache.spark.sql.JavaApplySchemaSuite.dataFrameRDDOperations</div></li><li><div>test.org.apache.spark.sql.JavaApplySchemaSuite.dataFrameRDDOperations</div></li><li><div>test.org.apache.spark.sql.JavaApplySchemaSuite.applySchemaToJSON</div></li><li><div>test.org.apache.spark.sql.JavaApplySchemaSuite.applySchemaToJSON</div></li><li><div>test.org.apache.spark.sql.JavaBeanDeserializationSuite.testBeanWithArrayFieldDeserialization</div></li><li><div>test.org.apache.spark.sql.JavaBeanDeserializationSuite.testBeanWithArrayFieldDeserialization</div></li><li><div>test.org.apache.spark.sql.JavaBeanDeserializationSuite.testBeanWithMapFieldsDeserialization</div></li><li><div>test.org.apache.spark.sql.JavaBeanDeserializationSuite.testBeanWithMapFieldsDeserialization</div></li><li><div>test.org.apache.spark.sql.JavaColumnExpressionSuite.isInCollectionCheckExceptionMessage</div></li><li><div>test.org.apache.spark.sql.JavaColumnExpressionSuite.isInCollectionWorksCorrectlyOnJava</div></li><li><div>test.org.apache.spark.sql.JavaColumnExpressionSuite.isInCollectionWorksCorrectlyOnJava</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testFormatAPI</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testTextAPI</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testJsonAPI</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testLoadAPI</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testOptionsAPI</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testSaveModeAPI</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testCsvAPI</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testParquetAPI</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testTextFileAPI</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCollectAndTake</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCollectAndTake</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testJsonRDDToDataFrame</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testJsonRDDToDataFrame</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testVarargMethods</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testVarargMethods</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testBeanWithoutGetter</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testBeanWithoutGetter</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateStructTypeFromList</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateStructTypeFromList</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testSampleBy</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testSampleBy</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCrosstab</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCrosstab</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testUDF</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testUDF</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFromFromList</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFromFromList</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCircularReferenceBean</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCircularReferenceBean</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testFrequentItems</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testFrequentItems</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testSampleByColumn</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testSampleByColumn</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testExecution</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testExecution</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testTextLoad</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testTextLoad</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.pivot</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.pivot</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testGenericLoad</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testGenericLoad</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCountMinSketch</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCountMinSketch</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.pivotColumnValues</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.pivotColumnValues</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFrameFromJavaBeans</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFrameFromJavaBeans</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCorrelation</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCorrelation</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testBloomFilter</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCovariance</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCovariance</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFrameFromLocalJavaBeans</div></li><li><div>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationCount</div></li><li><div>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationSumDouble</div></li><li><div>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationSumLong</div></li><li><div>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationSumLong</div></li><li><div>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationAnonClass</div></li><li><div>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationAnonClass</div></li><li><div>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationAverage</div></li><li><div>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationAverage</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testRuntimeNullabilityCheck</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean1</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean1</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean2</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean2</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean3</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean3</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testSerializeNull</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testSerializeNull</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testRandomSplit</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testRandomSplit</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testTypedFilterPreservingSchema</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testTypedFilterPreservingSchema</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJoin</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJoin</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testTake</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testTake</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testToLocalIterator</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testToLocalIterator</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testSpecificLists</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testSpecificLists</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testForeach</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testForeach</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJavaEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJavaEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testPrimitiveEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testPrimitiveEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testEmptyBean</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testEmptyBean</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testCommonOperation</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testCommonOperation</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testNullInTopLevelBean</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testNullInTopLevelBean</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testGroupBy</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testGroupBy</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testSetOperation</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testSetOperation</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testBeanWithEnum</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testBeanWithEnum</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testKryoEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testKryoEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.test</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.test</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJavaBeanEncoder2</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJavaBeanEncoder2</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testCollect</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testCollect</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testKryoEncoderErrorMessageForPrivateClass</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testKryoEncoderErrorMessageForPrivateClass</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJavaBeanEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJavaBeanEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testTupleEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testTupleEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testNestedTupleEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testNestedTupleEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testTupleEncoderSchema</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testTupleEncoderSchema</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testReduce</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testReduce</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testSelect</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testSelect</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJavaEncoderErrorMessageForPrivateClass</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJavaEncoderErrorMessageForPrivateClass</div></li><li><div>test.org.apache.spark.sql.JavaSaveLoadSuite.saveAndLoadWithSchema</div></li><li><div>test.org.apache.spark.sql.JavaSaveLoadSuite.saveAndLoadWithSchema</div></li><li><div>test.org.apache.spark.sql.JavaSaveLoadSuite.saveAndLoad</div></li><li><div>test.org.apache.spark.sql.JavaSaveLoadSuite.saveAndLoad</div></li><li><div>test.org.apache.spark.sql.JavaUDAFSuite.udf1Test</div></li><li><div>test.org.apache.spark.sql.JavaUDAFSuite.udf1Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf1Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf1Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf2Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf2Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf3Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf3Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf4Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf4Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf5Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf5Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf6Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf6Test</div></li><li><div>test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.testForeachBatchAPI</div></li><li><div>test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.testForeachBatchAPI</div></li><li><div>test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.testForeachAPI</div></li><li><div>test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.testForeachAPI</div></li><li><div>org.apache.spark.sql.hive.HiveMetastoreLazyInitializationSuite.lazily initialize Hive client</div></li><li><div>org.apache.spark.sql.hive.JavaDataFrameSuite.testUDAF</div></li><li><div>org.apache.spark.sql.hive.JavaDataFrameSuite.saveTableAndQueryIt</div></li><li><div>org.apache.spark.sql.hive.JavaMetastoreDataSourcesSuite.saveExternalTableAndQueryIt</div></li><li><div>org.apache.spark.sql.hive.JavaMetastoreDataSourcesSuite.saveTableAndQueryIt</div></li><li><div>org.apache.spark.sql.hive.JavaMetastoreDataSourcesSuite.saveExternalTableWithSchemaAndQueryIt</div></li><li><div>org.apache.spark.streaming.JavaMapWithStateSuite.testBasicFunction</div></li><li><div>org.apache.spark.streaming.JavaMapWithStateSuite.testBasicFunction</div></li><li><div>org.apache.spark.streaming.JavaReceiverAPISuite.testReceiver</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testStreamingContextTransform</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testFlatMapValues</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testFlatMapValues</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testMapPartitions</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testMapPartitions</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairFilter</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairFilter</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testCombineByKey</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testCombineByKey</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testMap</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testMap</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairToNormalRDDTransform</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairToNormalRDDTransform</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairReduceByKey</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairReduceByKey</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairMap</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairMap</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testFlatMap</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testFlatMap</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testReduceByKeyAndWindowWithInverse</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testReduceByKeyAndWindowWithInverse</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testReduceByWindow</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testReduceByWindow</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairFlatMap</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairFlatMap</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairToPairFlatMapWithChangingTypes</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairMapPartitions</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testVariousTransform</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testVariousTransform</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testTransformWith</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testTransformWith</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testVariousTransformWith</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testVariousTransformWith</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testReduceByKeyAndWindow</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testReduceByKeyAndWindow</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairTransform</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairTransform</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testFilter</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testFilter</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairMap2</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairMap2</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testMapValues</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testReduce</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testUpdateStateByKey</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testTransform</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testStreamingContextTransform</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testFlatMapValues</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testReduceByWindowWithInverse</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testMapPartitions</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairFilter</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairFilter</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testRepartitionFewerPartitions</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testRepartitionFewerPartitions</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCombineByKey</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCombineByKey</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testContextGetOrCreate</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testContextGetOrCreate</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testWindowWithSlideDuration</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testWindowWithSlideDuration</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testQueueStream</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testQueueStream</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCountByValue</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCountByValue</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testMap</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testMap</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairToNormalRDDTransform</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairToNormalRDDTransform</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairReduceByKey</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairReduceByKey</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCount</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCount</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCheckpointMasterRecovery</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCheckpointMasterRecovery</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairMap</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairMap</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testUnion</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testUnion</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testFlatMap</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testFlatMap</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testReduceByKeyAndWindowWithInverse</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testReduceByKeyAndWindowWithInverse</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testGlom</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testJoin</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testJoin</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairFlatMap</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairFlatMap</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairToPairFlatMapWithChangingTypes</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairToPairFlatMapWithChangingTypes</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairMapPartitions</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairMapPartitions</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testRepartitionMorePartitions</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testRepartitionMorePartitions</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testReduceByWindowWithoutInverse</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testReduceByWindowWithoutInverse</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testLeftOuterJoin</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testLeftOuterJoin</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testVariousTransform</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testVariousTransform</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testTransformWith</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testTransformWith</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testVariousTransformWith</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testVariousTransformWith</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testTextFileStream</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testTextFileStream</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairGroupByKey</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairGroupByKey</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCoGroup</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCoGroup</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testSocketString</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testSocketString</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testGroupByKeyAndWindow</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testGroupByKeyAndWindow</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testReduceByKeyAndWindow</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testReduceByKeyAndWindow</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testForeachRDD</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testForeachRDD</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testFileStream</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testFileStream</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairTransform</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairTransform</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testFilter</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testFilter</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairMap2</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairMap2</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testMapValues</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testMapValues</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testReduce</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testReduce</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testUpdateStateByKey</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testUpdateStateByKey</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testTransform</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testTransform</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testWindow</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testWindow</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCountByValueAndWindow</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCountByValueAndWindow</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testRawSocketStream</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testRawSocketStream</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testUpdateStateByKeyWithInitial</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testUpdateStateByKeyWithInitial</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testContextState</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testContextState</div></li></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="sqoop"><div style="font-weight:bold;" class="panel-heading">SQOOP<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Yussuf)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/trunk</div><div><b>Last Revision: </b>6dd6a4fc863074d570919b183fdf5c20e86c5e0b</div><div><b>Last Run: </b>02-11-2018 01:55 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC UBUNTU18</th><th>X86 UBUNTU18</th><th>PPC RHEL72</th><th>X86 RHEL72</th><th>PPC RHEL75</th><th>X86 RHEL75</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 888</div><div>Failed Count : 0</div><div>Skipped Count : 44</div></td><td><div>Total Count : 888</div><div>Failed Count : 0</div><div>Skipped Count : 44</div></td><td><div>Total Count : 887</div><div>Failed Count : 1</div><div>Skipped Count : 44</div></td><td><div>Total Count : 888</div><div>Failed Count : 0</div><div>Skipped Count : 44</div></td><td><div>Total Count : 887</div><div>Failed Count : 1</div><div>Skipped Count : 44</div></td><td><div>Total Count : 888</div><div>Failed Count : 0</div><div>Skipped Count : 44</div></td><td><div>Total Count : 887</div><div>Failed Count : 1</div><div>Skipped Count : 44</div></td><td><div>Total Count : 888</div><div>Failed Count : 0</div><div>Skipped Count : 44</div></td></tr><tr><td>Result</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.sqoop.mapreduce.db.netezza.TestNetezzaExternalTableExportMapper.testPassingJDBC</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.sqoop.mapreduce.db.netezza.TestNetezzaExternalTableExportMapper.testPassingJDBC</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.sqoop.mapreduce.db.netezza.TestNetezzaExternalTableExportMapper.testPassingJDBC</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>Timeout occurred. Please note the time in the report does not reflect the time until the timeout.</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>Timeout occurred. Please note the time in the report does not reflect the time until the timeout.</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>Timeout occurred. Please note the time in the report does not reflect the time until the timeout.</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.sqoop.mapreduce.db.netezza.TestNetezzaExternalTableExportMapper.testPassingJDBC</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.sqoop.mapreduce.db.netezza.TestNetezzaExternalTableExportMapper.testPassingJDBC</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.sqoop.mapreduce.db.netezza.TestNetezzaExternalTableExportMapper.testPassingJDBC</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="storm"><div style="font-weight:bold;" class="panel-heading">STORM<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Alisha)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>dcf637ff666e3c08ef81911034576019d35ca215</div><div><b>Last Run: </b>01-11-2018 01:18 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC UBUNTU18</th><th>X86 UBUNTU18</th><th>PPC RHEL72</th><th>X86 RHEL72</th><th>PPC RHEL75</th><th>X86 RHEL75</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 1162</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 1162</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 1162</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 1162</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 1162</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 1038</div><div>Failed Count : 1</div><div>Skipped Count : 4</div></td><td><div>Total Count : 1162</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 1162</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td></tr><tr><td>Result</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.storm.TickTupleTest.testTickTupleWorksWithSystemBolt</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>It took over 60000ms to shut down slot Thread[SLOT_1024,5,main]</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.storm.TickTupleTest.testTickTupleWorksWithSystemBolt</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="tez"><div style="font-weight:bold;" class="panel-heading">TEZ<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Prajyot)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>efc73318342e40dac30ec321119c6536b67c0a64</div><div><b>Last Run: </b>01-11-2018 03:53 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC UBUNTU18</th><th>X86 UBUNTU18</th><th>PPC RHEL72</th><th>X86 RHEL72</th><th>PPC RHEL75</th><th>X86 RHEL75</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 1836</div><div>Failed Count : 2</div><div>Skipped Count : 14</div></td><td><div>Total Count : 1836</div><div>Failed Count : 1</div><div>Skipped Count : 14</div></td><td><div>Total Count : 1833</div><div>Failed Count : 3</div><div>Skipped Count : 14</div></td><td><div>Total Count : 1836</div><div>Failed Count : 1</div><div>Skipped Count : 14</div></td><td><div>Total Count : 1836</div><div>Failed Count : 2</div><div>Skipped Count : 14</div></td><td><div>Total Count : 1836</div><div>Failed Count : 2</div><div>Skipped Count : 14</div></td><td><div>Total Count : 1833</div><div>Failed Count : 3</div><div>Skipped Count : 14</div></td><td><div>Total Count : 1836</div><div>Failed Count : 1</div><div>Skipped Count : 14</div></td></tr><tr><td>Result</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"><div><li>org.apache.tez.dag.app.TestPreemption.testPreemptionWithSession</li></div><div><li>org.apache.tez.tests.TestExtServicesWithLocalMode.test1</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.tez.tests.TestExtServicesWithLocalMode.test1</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.tez.tests.TestExtServicesWithLocalMode.test1</li></div><div><li>org.apache.tez.auxservices.TestShuffleHandler.testRecoveryFromOtherVersions</li></div><div><li>org.apache.tez.auxservices.TestShuffleHandler.testRecovery</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.tez.tests.TestExtServicesWithLocalMode.test1</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.tez.dag.app.TestMockDAGAppMaster.testInternalPreemption</li></div><div><li>org.apache.tez.tests.TestExtServicesWithLocalMode.test1</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.tez.client.TestTezClient.testPreWarmWithTimeout</li></div><div><li>org.apache.tez.tests.TestExtServicesWithLocalMode.test1</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.tez.tests.TestExtServicesWithLocalMode.test1</li></div><div><li>org.apache.tez.auxservices.TestShuffleHandler.testRecoveryFromOtherVersions</li></div><div><li>org.apache.tez.auxservices.TestShuffleHandler.testRecovery</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.tez.tests.TestExtServicesWithLocalMode.test1</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"><div><li>test timed out after 30000 milliseconds</li></div><div><li>org.apache.tez.dag.api.TezUncheckedException: java.lang.reflect.InvocationTargetException</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.tez.dag.api.TezUncheckedException: java.lang.reflect.InvocationTargetException</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.tez.dag.api.TezUncheckedException: java.lang.reflect.InvocationTargetException</li></div><div><li>Could not load library. Reasons: [no leveldbjni64-1.8 in java.library.path, no leveldbjni-1.8 in java.library.path, no leveldbjni in java.library.path, /tmp/libleveldbjni-64-1-4555171526148784567.8: /tmp/libleveldbjni-64-1-4555171526148784567.8: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64-bit platform)]</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.tez.dag.api.TezUncheckedException: java.lang.reflect.InvocationTargetException</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;KILLED&gt; but was:&lt;FAILED&gt;</li></div><div><li>org.apache.tez.dag.api.TezUncheckedException: java.lang.reflect.InvocationTargetException</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Time taken is not as expected</li></div><div><li>org.apache.tez.dag.api.TezUncheckedException: java.lang.reflect.InvocationTargetException</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.tez.dag.api.TezUncheckedException: java.lang.reflect.InvocationTargetException</li></div><div><li>Could not load library. Reasons: [no leveldbjni64-1.8 in java.library.path, no leveldbjni-1.8 in java.library.path, no leveldbjni in java.library.path, /tmp/libleveldbjni-64-1-8373695035469616384.8: /tmp/libleveldbjni-64-1-8373695035469616384.8: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64-bit platform)]</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.tez.dag.api.TezUncheckedException: java.lang.reflect.InvocationTargetException</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.tez.dag.app.TestPreemption.testPreemptionWithSession</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.tez.auxservices.TestShuffleHandler.testRecoveryFromOtherVersions</div></li><li><div>org.apache.tez.auxservices.TestShuffleHandler.testRecovery</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.tez.dag.app.TestMockDAGAppMaster.testInternalPreemption</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.tez.client.TestTezClient.testPreWarmWithTimeout</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.tez.auxservices.TestShuffleHandler.testRecoveryFromOtherVersions</div></li><li><div>org.apache.tez.auxservices.TestShuffleHandler.testRecovery</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="zeppelin"><div style="font-weight:bold;" class="panel-heading">ZEPPELIN<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Alisha)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>d37cc1b611dc761b1ec69c29d6b8ceaa84f301f4</div><div><b>Last Run: </b>14-09-2018 03:06 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC UBUNTU18</th><th>X86 UBUNTU18</th><th>PPC RHEL72</th><th>X86 RHEL72</th><th>PPC RHEL75</th><th>X86 RHEL75</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 868</div><div>Failed Count : 1</div><div>Skipped Count : 5</div></td><td><div>Total Count : 869</div><div>Failed Count : 3</div><div>Skipped Count : 5</div></td><td><div>Total Count : 869</div><div>Failed Count : 3</div><div>Skipped Count : 5</div></td><td><div>Total Count : 869</div><div>Failed Count : 2</div><div>Skipped Count : 5</div></td><td><div>Total Count : 872</div><div>Failed Count : 8</div><div>Skipped Count : 5</div></td><td><div>Total Count : 858</div><div>Failed Count : 3</div><div>Skipped Count : 5</div></td><td><div>Total Count : 859</div><div>Failed Count : 17</div><div>Skipped Count : 5</div></td><td><div>Total Count : 869</div><div>Failed Count : 3</div><div>Skipped Count : 5</div></td></tr><tr><td>Result</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"><div><li>org.apache.zeppelin.rest.NotebookRestApiTest.testRunWithServerRestart</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.zeppelin.rest.InterpreterRestApiTest.testRestartInterpreterPerNote</li></div><div><li>org.apache.zeppelin.rest.NotebookRestApiTest.testRunWithServerRestart</li></div><div><li>org.apache.zeppelin.rest.ZeppelinRestApiTest.org.apache.zeppelin.rest.ZeppelinRestApiTest</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.zeppelin.ignite.IgniteInterpreterTest.testInterpretInvalidInput</li></div><div><li>org.apache.zeppelin.ignite.IgniteInterpreterTest.testInterpretInvalidInput</li></div><div><li>org.apache.zeppelin.rest.NotebookRestApiTest.testRunWithServerRestart</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.zeppelin.rest.NotebookRestApiTest.testRunWithServerRestart</li></div><div><li>org.apache.zeppelin.rest.ZeppelinRestApiTest.org.apache.zeppelin.rest.ZeppelinRestApiTest</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.zeppelin.ignite.IgniteInterpreterTest.testInterpret</li></div><div><li>org.apache.zeppelin.ignite.IgniteInterpreterTest.testInterpret</li></div><div><li>org.apache.zeppelin.ignite.IgniteInterpreterTest.testInterpretInvalidInput</li></div><div><li>org.apache.zeppelin.ignite.IgniteInterpreterTest.testInterpretInvalidInput</li></div><div><li>org.apache.zeppelin.ignite.IgniteSqlInterpreterTest.testSql</li></div><div><li>org.apache.zeppelin.ignite.IgniteSqlInterpreterTest.testSql</li></div><div><li>org.apache.zeppelin.rest.NotebookRestApiTest.testRunWithServerRestart</li></div><div><li>org.apache.zeppelin.rest.ZeppelinRestApiTest.org.apache.zeppelin.rest.ZeppelinRestApiTest</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.zeppelin.spark.IPySparkInterpreterTest.testRedefinitionZeppelinContext</li></div><div><li>org.apache.zeppelin.rest.NotebookRestApiTest.testRunWithServerRestart</li></div><div><li>org.apache.zeppelin.helium.HeliumBundleFactoryTest.bundlePackage</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.zeppelin.python.IPythonInterpreterTest.testIPythonPlotting</li></div><div><li>org.apache.zeppelin.python.IPythonInterpreterTest.testZeppelinContext</li></div><div><li>org.apache.zeppelin.python.PythonInterpreterMatplotlibTest.testClose</li></div><div><li>org.apache.zeppelin.python.PythonInterpreterMatplotlibTest.showPlot</li></div><div><li>org.apache.zeppelin.python.PythonInterpreterMatplotlibTest.dependenciesAreInstalled</li></div><div><li>org.apache.zeppelin.python.PythonInterpreterMatplotlibTest.testNoClose</li></div><div><li>org.apache.zeppelin.python.PythonInterpreterTest.testZeppelinContext</li></div><div><li>org.apache.zeppelin.spark.IPySparkInterpreterTest.testIPySpark</li></div><div><li>org.apache.zeppelin.spark.IPySparkInterpreterTest.testIPythonPlotting</li></div><div><li>org.apache.zeppelin.spark.IPySparkInterpreterTest.testZeppelinContext</li></div><div><li>org.apache.zeppelin.spark.PySparkInterpreterTest.testPySpark</li></div><div><li>org.apache.zeppelin.spark.PySparkInterpreterTest.testZeppelinContext</li></div><div><li>org.apache.zeppelin.rest.InterpreterRestApiTest.testRestartInterpreterPerNote</li></div><div><li>org.apache.zeppelin.rest.NotebookRestApiTest.testRunWithServerRestart</li></div><div><li>org.apache.zeppelin.rest.ZeppelinRestApiTest.org.apache.zeppelin.rest.ZeppelinRestApiTest</li></div><div><li>org.apache.zeppelin.rest.ZeppelinSparkClusterTest.pySparkDepLoaderTest[3]</li></div><div><li>org.apache.zeppelin.rest.ZeppelinSparkClusterTest.testZeppelinContextResource[3]</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.zeppelin.rest.InterpreterRestApiTest.testRestartInterpreterPerNote</li></div><div><li>org.apache.zeppelin.rest.NotebookRestApiTest.testRunWithServerRestart</li></div><div><li>org.apache.zeppelin.rest.ZeppelinRestApiTest.org.apache.zeppelin.rest.ZeppelinRestApiTest</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"><div><li>java.lang.NullPointerException: null
	at org.apache.zeppelin.service.JobManagerService$NoteJobInfo.&lt;init&gt;(JobManagerService.java:124)
	at org.apache.zeppelin.service.JobManagerService.getNoteJobInfo(JobManagerService.java:52)
	at org.apache.zeppelin.socket.NotebookServer$NotebookInformationListener.onParagraphStatusChange(NotebookServer.java:1896)
	at org.apache.zeppelin.socket.NotebookServer.onSt</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>shared interpreter restart:
Expected: HTTP response &lt;200&gt; from /api/interpreter/setting/restart/md
     but: got &lt;500&gt; Request failed.</li></div><div><li>java.lang.NullPointerException: null
	at org.apache.zeppelin.service.JobManagerService$NoteJobInfo.&lt;init&gt;(JobManagerService.java:124)
	at org.apache.zeppelin.service.JobManagerService.getNoteJobInfo(JobManagerService.java:52)
	at org.apache.zeppelin.socket.NotebookServer$NotebookInformationListener.onParagraphStatusChange(NotebookServer.java:1896)
	at org.apache.zeppelin.socket.NotebookServer.onSt</li></div><div><li>java.lang.NullPointerException: null
	at org.apache.zeppelin.service.JobManagerService$NoteJobInfo.&lt;init&gt;(JobManagerService.java:124)
	at org.apache.zeppelin.service.JobManagerService.getNoteJobInfo(JobManagerService.java:52)
	at org.apache.zeppelin.socket.NotebookServer$NotebookInformationListener.onParagraphStatusChange(NotebookServer.java:1896)
	at org.apache.zeppelin.socket.NotebookServer.onSt</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Unable to establish plain connection. Was remote cluster configured with SSL? [rmtAddr=/127.0.0.1:47507, errMsg="Failed to deserialize object with given class loader: sun.misc.Launcher$AppClassLoader@135fbaa4"]</li></div><div><li>java.lang.NullPointerException: null
	at org.apache.zeppelin.ignite.IgniteInterpreterTest.tearDown(IgniteInterpreterTest.java:75)
</li></div><div><li>java.lang.NullPointerException: null
	at org.apache.zeppelin.service.JobManagerService$NoteJobInfo.&lt;init&gt;(JobManagerService.java:124)
	at org.apache.zeppelin.service.JobManagerService.getNoteJobInfo(JobManagerService.java:52)
	at org.apache.zeppelin.socket.NotebookServer$NotebookInformationListener.onParagraphStatusChange(NotebookServer.java:1896)
	at org.apache.zeppelin.socket.NotebookServer.onSt</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>java.lang.NullPointerException: null
	at org.apache.zeppelin.service.JobManagerService$NoteJobInfo.&lt;init&gt;(JobManagerService.java:124)
	at org.apache.zeppelin.service.JobManagerService.getNoteJobInfo(JobManagerService.java:52)
	at org.apache.zeppelin.socket.NotebookServer$NotebookInformationListener.onParagraphStatusChange(NotebookServer.java:1896)
	at org.apache.zeppelin.socket.NotebookServer.onSt</li></div><div><li>java.lang.NullPointerException: null
	at org.apache.zeppelin.service.JobManagerService$NoteJobInfo.&lt;init&gt;(JobManagerService.java:124)
	at org.apache.zeppelin.service.JobManagerService.getNoteJobInfo(JobManagerService.java:52)
	at org.apache.zeppelin.socket.NotebookServer$NotebookInformationListener.onParagraphStatusChange(NotebookServer.java:1896)
	at org.apache.zeppelin.socket.NotebookServer.onSt</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Unable to establish plain connection. Was remote cluster configured with SSL? [rmtAddr=/127.0.0.1:47507, errMsg="Failed to deserialize object with given class loader: sun.misc.Launcher$AppClassLoader@75b84c92"]</li></div><div><li>java.lang.NullPointerException: null
	at org.apache.zeppelin.ignite.IgniteInterpreterTest.tearDown(IgniteInterpreterTest.java:75)
</li></div><div><li>Unable to establish plain connection. Was remote cluster configured with SSL? [rmtAddr=/127.0.0.1:47507, errMsg="Failed to deserialize object with given class loader: sun.misc.Launcher$AppClassLoader@75b84c92"]</li></div><div><li>java.lang.NullPointerException: null
	at org.apache.zeppelin.ignite.IgniteInterpreterTest.tearDown(IgniteInterpreterTest.java:75)
</li></div><div><li>Unable to establish plain connection. Was remote cluster configured with SSL? [rmtAddr=/127.0.0.1:47507, errMsg="Failed to deserialize object with given class loader: sun.misc.Launcher$AppClassLoader@75b84c92"]</li></div><div><li>java.lang.NullPointerException: null
	at org.apache.zeppelin.ignite.IgniteSqlInterpreterTest.tearDown(IgniteSqlInterpreterTest.java:87)
</li></div><div><li>java.lang.NullPointerException: null
	at org.apache.zeppelin.service.JobManagerService$NoteJobInfo.&lt;init&gt;(JobManagerService.java:124)
	at org.apache.zeppelin.service.JobManagerService.getNoteJobInfo(JobManagerService.java:52)
	at org.apache.zeppelin.socket.NotebookServer$NotebookInformationListener.onParagraphStatusChange(NotebookServer.java:1896)
	at org.apache.zeppelin.socket.NotebookServer.onSt</li></div><div><li>java.lang.NullPointerException: null
	at org.apache.zeppelin.service.JobManagerService$NoteJobInfo.&lt;init&gt;(JobManagerService.java:124)
	at org.apache.zeppelin.service.JobManagerService.getNoteJobInfo(JobManagerService.java:52)
	at org.apache.zeppelin.socket.NotebookServer$NotebookInformationListener.onParagraphStatusChange(NotebookServer.java:1896)
	at org.apache.zeppelin.socket.NotebookServer.onSt</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Fail to open IPythonInterpreter</li></div><div><li>java.lang.NullPointerException: null
	at org.apache.zeppelin.service.JobManagerService$NoteJobInfo.&lt;init&gt;(JobManagerService.java:124)
	at org.apache.zeppelin.service.JobManagerService.getNoteJobInfo(JobManagerService.java:52)
	at org.apache.zeppelin.socket.NotebookServer$NotebookInformationListener.onParagraphStatusChange(NotebookServer.java:1896)
	at org.apache.zeppelin.socket.NotebookServer.onSt</li></div><div><li>com.github.eirslett.maven.plugins.frontend.lib.TaskRunnerException: 'yarn install --fetch-retries=2 --fetch-retry-factor=1 --fetch-retry-mintimeout=5000 --registry=http://registry.npmjs.org/' failed. (error code 1)</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>expected:&lt;1&gt; but was:&lt;2&gt;</li></div><div><li>Index: 0, Size: 0</li></div><div><li>Index: 0, Size: 0</li></div><div><li>[%text Fail to execute line 1: import matplotlib
Traceback (most recent call last):
  File "/tmp/1536900576226-0/zeppelin_python.py", line 158, in &lt;module&gt;
    exec(code, _zcUserQueryNameSpace)
  File "&lt;stdin&gt;", line 1, in &lt;module&gt;
  File "/usr/lib64/python2.7/site-packages/matplotlib/__init__.py", line 124, in &lt;module&gt;
    from . import cbook
ImportError: cannot import name cbook
] expected:&lt;SUCC</li></div><div><li>Index: 0, Size: 0</li></div><div><li>expected:&lt;1&gt; but was:&lt;2&gt;</li></div><div><li>expected:&lt;[]+---+---+
| _1| _2|
...&gt; but was:&lt;[/usr/lib64/python2.7/site-packages/pandas/core/nanops.py:39: UserWarning: The installed version of bottleneck 0.7.0 is not supported in pandas and will be not be used
The minimum supported version is 1.0.0

  ver=ver, min_ver=_MIN_BOTTLENECK_VERSION), UserWarning)
]+---+---+
| _1| _2|
...&gt;</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>expected:&lt;id name
1 a
2 b
3 c
[]&gt; but was:&lt;id name
1 a
2 b
3 c
[/usr/lib64/python2.7/site-packages/pandas/core/nanops.py:39: UserWarning: The installed version of bottleneck 0.7.0 is not supported in pandas and will be not be used
The minimum supported version is 1.0.0
  ver=ver, min_ver=_MIN_BOTTLENECK_VERSION), UserWarning)
]&gt;</li></div><div><li>expected:&lt;[]+---+---+
| _1| _2|
...&gt; but was:&lt;[/usr/lib64/python2.7/site-packages/pandas/core/nanops.py:39: UserWarning: The installed version of bottleneck 0.7.0 is not supported in pandas and will be not be used
The minimum supported version is 1.0.0

  ver=ver, min_ver=_MIN_BOTTLENECK_VERSION), UserWarning)
]+---+---+
| _1| _2|
...&gt;</li></div><div><li>expected:&lt;1&gt; but was:&lt;2&gt;</li></div><div><li>shared interpreter restart:
Expected: HTTP response &lt;200&gt; from /api/interpreter/setting/restart/md
     but: got &lt;500&gt; Request failed.</li></div><div><li>java.lang.NullPointerException: null
	at org.apache.zeppelin.service.JobManagerService$NoteJobInfo.&lt;init&gt;(JobManagerService.java:124)
	at org.apache.zeppelin.service.JobManagerService.getNoteJobInfo(JobManagerService.java:52)
	at org.apache.zeppelin.socket.NotebookServer$NotebookInformationListener.onParagraphStatusChange(NotebookServer.java:1896)
	at org.apache.zeppelin.socket.NotebookServer.onSt</li></div><div><li>java.lang.NullPointerException: null
	at org.apache.zeppelin.service.JobManagerService$NoteJobInfo.&lt;init&gt;(JobManagerService.java:124)
	at org.apache.zeppelin.service.JobManagerService.getNoteJobInfo(JobManagerService.java:52)
	at org.apache.zeppelin.socket.NotebookServer$NotebookInformationListener.onParagraphStatusChange(NotebookServer.java:1896)
	at org.apache.zeppelin.socket.NotebookServer.onSt</li></div><div><li>expected:&lt;[]2
&gt; but was:&lt;[/usr/lib64/python2.7/site-packages/pandas/core/nanops.py:39: UserWarning: The installed version of bottleneck 0.7.0 is not supported in pandas and will be not be used
The minimum supported version is 1.0.0

  ver=ver, min_ver=_MIN_BOTTLENECK_VERSION), UserWarning)
]2
&gt;</li></div><div><li>expected:&lt;[]hello world
&gt; but was:&lt;[/usr/lib64/python2.7/site-packages/pandas/core/nanops.py:39: UserWarning: The installed version of bottleneck 0.7.0 is not supported in pandas and will be not be used
The minimum supported version is 1.0.0

  ver=ver, min_ver=_MIN_BOTTLENECK_VERSION), UserWarning)
]hello world
&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>shared interpreter restart:
Expected: HTTP response &lt;200&gt; from /api/interpreter/setting/restart/md
     but: got &lt;500&gt; Request failed.</li></div><div><li>java.lang.NullPointerException: null
	at org.apache.zeppelin.service.JobManagerService$NoteJobInfo.&lt;init&gt;(JobManagerService.java:124)
	at org.apache.zeppelin.service.JobManagerService.getNoteJobInfo(JobManagerService.java:52)
	at org.apache.zeppelin.socket.NotebookServer$NotebookInformationListener.onParagraphStatusChange(NotebookServer.java:1896)
	at org.apache.zeppelin.socket.NotebookServer.onSt</li></div><div><li>java.lang.NullPointerException: null
	at org.apache.zeppelin.service.JobManagerService$NoteJobInfo.&lt;init&gt;(JobManagerService.java:124)
	at org.apache.zeppelin.service.JobManagerService.getNoteJobInfo(JobManagerService.java:52)
	at org.apache.zeppelin.socket.NotebookServer$NotebookInformationListener.onParagraphStatusChange(NotebookServer.java:1896)
	at org.apache.zeppelin.socket.NotebookServer.onSt</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.zeppelin.rest.InterpreterRestApiTest.testRestartInterpreterPerNote</div></li><li><div>org.apache.zeppelin.rest.ZeppelinRestApiTest.org.apache.zeppelin.rest.ZeppelinRestApiTest</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.zeppelin.ignite.IgniteInterpreterTest.testInterpretInvalidInput</div></li><li><div>org.apache.zeppelin.ignite.IgniteInterpreterTest.testInterpretInvalidInput</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.zeppelin.rest.ZeppelinRestApiTest.org.apache.zeppelin.rest.ZeppelinRestApiTest</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.zeppelin.ignite.IgniteInterpreterTest.testInterpret</div></li><li><div>org.apache.zeppelin.ignite.IgniteInterpreterTest.testInterpret</div></li><li><div>org.apache.zeppelin.ignite.IgniteInterpreterTest.testInterpretInvalidInput</div></li><li><div>org.apache.zeppelin.ignite.IgniteInterpreterTest.testInterpretInvalidInput</div></li><li><div>org.apache.zeppelin.ignite.IgniteSqlInterpreterTest.testSql</div></li><li><div>org.apache.zeppelin.ignite.IgniteSqlInterpreterTest.testSql</div></li><li><div>org.apache.zeppelin.rest.ZeppelinRestApiTest.org.apache.zeppelin.rest.ZeppelinRestApiTest</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.zeppelin.spark.IPySparkInterpreterTest.testRedefinitionZeppelinContext</div></li><li><div>org.apache.zeppelin.helium.HeliumBundleFactoryTest.bundlePackage</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.zeppelin.python.IPythonInterpreterTest.testIPythonPlotting</div></li><li><div>org.apache.zeppelin.python.IPythonInterpreterTest.testZeppelinContext</div></li><li><div>org.apache.zeppelin.python.PythonInterpreterMatplotlibTest.testClose</div></li><li><div>org.apache.zeppelin.python.PythonInterpreterMatplotlibTest.showPlot</div></li><li><div>org.apache.zeppelin.python.PythonInterpreterMatplotlibTest.dependenciesAreInstalled</div></li><li><div>org.apache.zeppelin.python.PythonInterpreterMatplotlibTest.testNoClose</div></li><li><div>org.apache.zeppelin.python.PythonInterpreterTest.testZeppelinContext</div></li><li><div>org.apache.zeppelin.spark.IPySparkInterpreterTest.testIPySpark</div></li><li><div>org.apache.zeppelin.spark.IPySparkInterpreterTest.testIPythonPlotting</div></li><li><div>org.apache.zeppelin.spark.IPySparkInterpreterTest.testZeppelinContext</div></li><li><div>org.apache.zeppelin.spark.PySparkInterpreterTest.testPySpark</div></li><li><div>org.apache.zeppelin.spark.PySparkInterpreterTest.testZeppelinContext</div></li><li><div>org.apache.zeppelin.rest.ZeppelinSparkClusterTest.pySparkDepLoaderTest[3]</div></li><li><div>org.apache.zeppelin.rest.ZeppelinSparkClusterTest.testZeppelinContextResource[3]</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="zookeeper"><div style="font-weight:bold;" class="panel-heading">ZOOKEEPER<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Pravin)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>1ce2ca8107438d283581d18d064a25bd6b74adf7</div><div><b>Last Run: </b>26-10-2018 03:51 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC UBUNTU18</th><th>X86 UBUNTU18</th><th>PPC RHEL72</th><th>X86 RHEL72</th><th>PPC RHEL75</th><th>X86 RHEL75</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 1802</div><div>Failed Count : 1</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1802</div><div>Failed Count : 1</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1802</div><div>Failed Count : 1</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1802</div><div>Failed Count : 1</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1802</div><div>Failed Count : 2</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1802</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1802</div><div>Failed Count : 1</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1802</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td></tr><tr><td>Result</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"><div><li>org.apache.zookeeper.server.quorum.QuorumPeerMainTest.testQuorumV6</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.zookeeper.server.quorum.QuorumPeerMainTest.testQuorumV6</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.zookeeper.server.quorum.QuorumPeerMainTest.testQuorumV6</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.zookeeper.server.quorum.QuorumPeerMainTest.testQuorumV6</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.zookeeper.server.quorum.QuorumPeerMainTest.testQuorumV6</li></div><div><li>org.apache.zookeeper.test.WatchEventWhenAutoResetTest.testNodeDataChanged</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.zookeeper.server.quorum.QuorumPeerMainTest.testQuorumV6</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"><div><li>waiting for server 1 being up</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>waiting for server 1 being up</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>waiting for server 1 being up</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>waiting for server 1 being up</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>waiting for server 1 being up</li></div><div><li>expected:&lt;NodeDataChanged&gt; but was:&lt;NodeDeleted&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>waiting for server 1 being up</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.zookeeper.server.quorum.QuorumPeerMainTest.testQuorumV6</div></li><li><div>org.apache.zookeeper.test.WatchEventWhenAutoResetTest.testNodeDataChanged</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.zookeeper.server.quorum.QuorumPeerMainTest.testQuorumV6</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div id="ubuntu16" style="font-weight:bold;font-size:12;display:none" class="panel panel-info" name="summary"><div class="panel-heading"><div class="panel-title">UBUNTU16 SUMMARY</div></div><div class="bs-callout bs-callout-info"><div><b>OS: </b>UBUNTU 16.04</div></div><table style="font-size:14" class="table table-striped"><tbody><tr><th>Package Name</th><th>PPC</th><th>X86</th><th></th></tr><tr><td><a href="#" id="anchor_accumulo" onclick="showme(this.id);">ACCUMULO</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_ambari" onclick="showme(this.id);">AMBARI</a></td><td><img src="resources/red.png" align="top" style="width: 16px; height: 16px;" title="FAILURE"></img>0</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td></tr><tr><td><a href="#" id="anchor_atlas" onclick="showme(this.id);">ATLAS</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_calcite" onclick="showme(this.id);">CALCITE</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_datafu" onclick="showme(this.id);">DATAFU</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_druid" onclick="showme(this.id);">DRUID</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>6 (4)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2</td></tr><tr><td><a href="#" id="anchor_falcon" onclick="showme(this.id);">FALCON</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_flume" onclick="showme(this.id);">FLUME</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_hadoop" onclick="showme(this.id);">HADOOP</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>67 (10)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>61 (4)</td></tr><tr><td><a href="#" id="anchor_hbase" onclick="showme(this.id);">HBASE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>7 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>7 (1)</td></tr><tr><td><a href="#" id="anchor_hive" onclick="showme(this.id);">HIVE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3 (2)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td></tr><tr><td><a href="#" id="anchor_kafka" onclick="showme(this.id);">KAFKA</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td></tr><tr><td><a href="#" id="anchor_knox" onclick="showme(this.id);">KNOX</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_metron" onclick="showme(this.id);">METRON</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>13 (13)</td></tr><tr><td><a href="#" id="anchor_oozie" onclick="showme(this.id);">OOZIE</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (2)</td></tr><tr><td><a href="#" id="anchor_phoenix" onclick="showme(this.id);">PHOENIX</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_pig" onclick="showme(this.id);">PIG</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>9</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>9</td></tr><tr><td><a href="#" id="anchor_ranger" onclick="showme(this.id);">RANGER</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_slider" onclick="showme(this.id);">SLIDER</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_spark" onclick="showme(this.id);">SPARK</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_sqoop" onclick="showme(this.id);">SQOOP</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_storm" onclick="showme(this.id);">STORM</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_tez" onclick="showme(this.id);">TEZ</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td></tr><tr><td><a href="#" id="anchor_zeppelin" onclick="showme(this.id);">ZEPPELIN</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3 (2)</td></tr><tr><td><a href="#" id="anchor_zookeeper" onclick="showme(this.id);">ZOOKEEPER</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td></tr></tbody></table></div><div id="ubuntu18" style="font-weight:bold;font-size:12;display:none" class="panel panel-info" name="summary"><div class="panel-heading"><div class="panel-title">UBUNTU18 SUMMARY</div></div><div class="bs-callout bs-callout-info"><div><b>OS: </b>UBUNTU 18.04</div></div><table style="font-size:14" class="table table-striped"><tbody><tr><th>Package Name</th><th>PPC</th><th>X86</th><th></th></tr><tr><td><a href="#" id="anchor_accumulo" onclick="showme(this.id);">ACCUMULO</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_ambari" onclick="showme(this.id);">AMBARI</a></td><td><img src="resources/red.png" align="top" style="width: 16px; height: 16px;" title="FAILURE"></img>0</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td></tr><tr><td><a href="#" id="anchor_atlas" onclick="showme(this.id);">ATLAS</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_calcite" onclick="showme(this.id);">CALCITE</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_datafu" onclick="showme(this.id);">DATAFU</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_druid" onclick="showme(this.id);">DRUID</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>6 (4)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2</td></tr><tr><td><a href="#" id="anchor_falcon" onclick="showme(this.id);">FALCON</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_flume" onclick="showme(this.id);">FLUME</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_hadoop" onclick="showme(this.id);">HADOOP</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>69 (6)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>71 (8)</td></tr><tr><td><a href="#" id="anchor_hbase" onclick="showme(this.id);">HBASE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>84 (82)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>8 (6)</td></tr><tr><td><a href="#" id="anchor_hive" onclick="showme(this.id);">HIVE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3</td></tr><tr><td><a href="#" id="anchor_kafka" onclick="showme(this.id);">KAFKA</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_knox" onclick="showme(this.id);">KNOX</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_metron" onclick="showme(this.id);">METRON</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>13 (13)</td></tr><tr><td><a href="#" id="anchor_oozie" onclick="showme(this.id);">OOZIE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>4 (4)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_phoenix" onclick="showme(this.id);">PHOENIX</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_pig" onclick="showme(this.id);">PIG</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>9</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>9</td></tr><tr><td><a href="#" id="anchor_ranger" onclick="showme(this.id);">RANGER</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_slider" onclick="showme(this.id);">SLIDER</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_spark" onclick="showme(this.id);">SPARK</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>48 (48)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>831 (831)</td></tr><tr><td><a href="#" id="anchor_sqoop" onclick="showme(this.id);">SQOOP</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_storm" onclick="showme(this.id);">STORM</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_tez" onclick="showme(this.id);">TEZ</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3 (2)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td></tr><tr><td><a href="#" id="anchor_zeppelin" onclick="showme(this.id);">ZEPPELIN</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3 (2)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (1)</td></tr><tr><td><a href="#" id="anchor_zookeeper" onclick="showme(this.id);">ZOOKEEPER</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td></tr></tbody></table></div><div id="rhel72" style="font-weight:bold;font-size:12;display:none" class="panel panel-info" name="summary"><div class="panel-heading"><div class="panel-title">RHEL72 SUMMARY</div></div><div class="bs-callout bs-callout-info"><div><b>OS: </b>RHEL 7.2</div></div><table style="font-size:14" class="table table-striped"><tbody><tr><th>Package Name</th><th>PPC</th><th>X86</th><th></th></tr><tr><td><a href="#" id="anchor_accumulo" onclick="showme(this.id);">ACCUMULO</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (2)</td></tr><tr><td><a href="#" id="anchor_ambari" onclick="showme(this.id);">AMBARI</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_atlas" onclick="showme(this.id);">ATLAS</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_calcite" onclick="showme(this.id);">CALCITE</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_datafu" onclick="showme(this.id);">DATAFU</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_druid" onclick="showme(this.id);">DRUID</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>6 (4)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2</td></tr><tr><td><a href="#" id="anchor_falcon" onclick="showme(this.id);">FALCON</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_flume" onclick="showme(this.id);">FLUME</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_hadoop" onclick="showme(this.id);">HADOOP</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>212 (158)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>63 (9)</td></tr><tr><td><a href="#" id="anchor_hbase" onclick="showme(this.id);">HBASE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (2)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>9 (9)</td></tr><tr><td><a href="#" id="anchor_hive" onclick="showme(this.id);">HIVE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>4 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3</td></tr><tr><td><a href="#" id="anchor_kafka" onclick="showme(this.id);">KAFKA</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_knox" onclick="showme(this.id);">KNOX</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_metron" onclick="showme(this.id);">METRON</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>13 (13)</td></tr><tr><td><a href="#" id="anchor_oozie" onclick="showme(this.id);">OOZIE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_phoenix" onclick="showme(this.id);">PHOENIX</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_pig" onclick="showme(this.id);">PIG</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>9</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>9</td></tr><tr><td><a href="#" id="anchor_ranger" onclick="showme(this.id);">RANGER</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_slider" onclick="showme(this.id);">SLIDER</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_spark" onclick="showme(this.id);">SPARK</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>827 (827)</td></tr><tr><td><a href="#" id="anchor_sqoop" onclick="showme(this.id);">SQOOP</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_storm" onclick="showme(this.id);">STORM</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td></tr><tr><td><a href="#" id="anchor_tez" onclick="showme(this.id);">TEZ</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (1)</td></tr><tr><td><a href="#" id="anchor_zeppelin" onclick="showme(this.id);">ZEPPELIN</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>8 (7)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3 (2)</td></tr><tr><td><a href="#" id="anchor_zookeeper" onclick="showme(this.id);">ZOOKEEPER</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (2)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr></tbody></table></div><div id="rhel75" style="font-weight:bold;font-size:12;display:none" class="panel panel-info" name="summary"><div class="panel-heading"><div class="panel-title">RHEL75 SUMMARY</div></div><div class="bs-callout bs-callout-info"><div><b>OS: </b>RHEL 7.5</div></div><table style="font-size:14" class="table table-striped"><tbody><tr><th>Package Name</th><th>PPC</th><th>X86</th><th></th></tr><tr><td><a href="#" id="anchor_accumulo" onclick="showme(this.id);">ACCUMULO</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_ambari" onclick="showme(this.id);">AMBARI</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (2)</td></tr><tr><td><a href="#" id="anchor_atlas" onclick="showme(this.id);">ATLAS</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_calcite" onclick="showme(this.id);">CALCITE</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_datafu" onclick="showme(this.id);">DATAFU</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_druid" onclick="showme(this.id);">DRUID</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>6 (4)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2</td></tr><tr><td><a href="#" id="anchor_falcon" onclick="showme(this.id);">FALCON</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_flume" onclick="showme(this.id);">FLUME</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>5 (5)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_hadoop" onclick="showme(this.id);">HADOOP</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>60 (8)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>68 (16)</td></tr><tr><td><a href="#" id="anchor_hbase" onclick="showme(this.id);">HBASE</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (2)</td></tr><tr><td><a href="#" id="anchor_hive" onclick="showme(this.id);">HIVE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td></tr><tr><td><a href="#" id="anchor_kafka" onclick="showme(this.id);">KAFKA</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td></tr><tr><td><a href="#" id="anchor_knox" onclick="showme(this.id);">KNOX</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_metron" onclick="showme(this.id);">METRON</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>13 (13)</td></tr><tr><td><a href="#" id="anchor_oozie" onclick="showme(this.id);">OOZIE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (2)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_phoenix" onclick="showme(this.id);">PHOENIX</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_pig" onclick="showme(this.id);">PIG</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>9</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>9</td></tr><tr><td><a href="#" id="anchor_ranger" onclick="showme(this.id);">RANGER</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_slider" onclick="showme(this.id);">SLIDER</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_spark" onclick="showme(this.id);">SPARK</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>46 (46)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>823 (823)</td></tr><tr><td><a href="#" id="anchor_sqoop" onclick="showme(this.id);">SQOOP</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_storm" onclick="showme(this.id);">STORM</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_tez" onclick="showme(this.id);">TEZ</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3 (2)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td></tr><tr><td><a href="#" id="anchor_zeppelin" onclick="showme(this.id);">ZEPPELIN</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>17 (14)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3</td></tr><tr><td><a href="#" id="anchor_zookeeper" onclick="showme(this.id);">ZOOKEEPER</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr></tbody></table></div><div id="ppcx86" style="display:block;font-weight:bold" class="panel panel-info" name="summary"><div class="panel-heading"><div class="panel-title">FULL SUMMARY</div></div><table style="font-size:14" id="summarytable" class="table table-striped"><tbody><tr><th></th></tr><tr><th>Package Name</th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC UBUNTU18</th><th>X86 UBUNTU18</th><th>PPC RHEL72</th><th>X86 RHEL72</th><th>PPC RHEL75</th><th>X86 RHEL75</th></tr><tr><td><a href="#" id="anchor_accumulo" onclick="showme(this.id);">ACCUMULO</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (2)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_ambari" onclick="showme(this.id);">AMBARI</a></td><td>N/A</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td>N/A</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (2)</td></tr><tr><td><a href="#" id="anchor_atlas" onclick="showme(this.id);">ATLAS</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_calcite" onclick="showme(this.id);">CALCITE</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_datafu" onclick="showme(this.id);">DATAFU</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_druid" onclick="showme(this.id);">DRUID</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>6 (4)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>6 (4)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>6 (4)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>6 (4)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2</td></tr><tr><td><a href="#" id="anchor_falcon" onclick="showme(this.id);">FALCON</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_flume" onclick="showme(this.id);">FLUME</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>5 (5)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_hadoop" onclick="showme(this.id);">HADOOP</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>67 (10)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>61 (4)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>69 (6)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>71 (8)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>212 (158)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>63 (9)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>60 (8)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>68 (16)</td></tr><tr><td><a href="#" id="anchor_hbase" onclick="showme(this.id);">HBASE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>7 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>7 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>84 (82)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>8 (6)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (2)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>9 (9)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (2)</td></tr><tr><td><a href="#" id="anchor_hive" onclick="showme(this.id);">HIVE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3 (2)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>4 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td></tr><tr><td><a href="#" id="anchor_kafka" onclick="showme(this.id);">KAFKA</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td></tr><tr><td><a href="#" id="anchor_knox" onclick="showme(this.id);">KNOX</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_metron" onclick="showme(this.id);">METRON</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>13 (13)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>13 (13)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>13 (13)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>13 (13)</td></tr><tr><td><a href="#" id="anchor_oozie" onclick="showme(this.id);">OOZIE</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (2)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>4 (4)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (2)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_phoenix" onclick="showme(this.id);">PHOENIX</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_pig" onclick="showme(this.id);">PIG</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>9</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>9</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>9</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>9</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>9</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>9</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>9</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>9</td></tr><tr><td><a href="#" id="anchor_ranger" onclick="showme(this.id);">RANGER</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_slider" onclick="showme(this.id);">SLIDER</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_spark" onclick="showme(this.id);">SPARK</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>48 (48)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>831 (831)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>827 (827)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>46 (46)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>823 (823)</td></tr><tr><td><a href="#" id="anchor_sqoop" onclick="showme(this.id);">SQOOP</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_storm" onclick="showme(this.id);">STORM</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_tez" onclick="showme(this.id);">TEZ</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3 (2)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3 (2)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td></tr><tr><td><a href="#" id="anchor_zeppelin" onclick="showme(this.id);">ZEPPELIN</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3 (2)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3 (2)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>8 (7)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3 (2)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>17 (14)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3</td></tr><tr><td><a href="#" id="anchor_zookeeper" onclick="showme(this.id);">ZOOKEEPER</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (2)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr></tbody></table></div></div></body></html>