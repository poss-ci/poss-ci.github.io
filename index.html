<html><head><script src="resources/jquery.min.js"></script><link href="resources/bootstrap.min.css" rel="stylesheet"></link><link href="resources/bootstrap-theme.min.css" rel="stylesheet"></link><script src="resources/bootstrap.min.js"></script><script src="helper.js">function hideAll(){console.log("hideAll")}function showme(e){console.log("showme");var l,n=e.substring(7),o=document.getElementsByName("data");for(l=0;l&lt;o.length;l++)o[l].style.display="none";var t=document.getElementsByName("summary");for(l=0;l&lt;t.length;l++)t[l].style.display="none";document.getElementById(n).style.display="block"}</script><style>table, th, td { vertical-align:top; padding: 3px} table {table-layout:fixed} td {word-wrap:break-word} .bs-callout { padding: 5px; margin: 5px 0; border: 1px solid #eee; border-left-width: 5px; border-radius: 3px; font-weight:normal; }.bs-callout-info {border-left-color: #5bc0de;}</style></head><body><nav class="navbar navbar-light"><div style="background-color: #F0F8FF;" class="container-fluid"><ul class="nav nav-pills"><li role="presentation"><a style="font-weight:bold" href="#" id="anchor_ppcx86" onclick="showme(this.id);">FULL SUMMARY</a></li><li role="presentation"><a style="font-weight:bold" href="#" id="anchor_ubuntu16" onclick="showme(this.id);">UBUNTU16</a></li><li role="presentation"><a style="font-weight:bold" href="#" id="anchor_ubuntu18" onclick="showme(this.id);">UBUNTU18</a></li><li role="presentation"><a style="font-weight:bold" href="#" id="anchor_rhel72" onclick="showme(this.id);">RHEL72</a></li><li role="presentation"><a style="font-weight:bold" href="#" id="anchor_rhel75" onclick="showme(this.id);">RHEL75</a></li><li role="presentation"><a style="font-weight:bold" href="#" id="anchor_developers" onclick="showme(this.id);">DEVELOPERS</a></li><p style="float:right;color:grey;font-size:13;padding-top:5px" role="presentation">15-12-2018 20:30 UTC</p></ul><div style="float:right;color:grey;font-size:12">Notations:<img src="resources/red.png" style="width: 16px; height: 16px;">Build failed </img><img src="resources/blue.png" style="width: 16px; height: 16px;">Build success with no failure </img><img src="resources/yellow.png" style="width: 16px; height: 16px;">N (M) Build success with N test failures &amp; M unique failures </img></div></div></nav><div style="table-cell" class="col-sm-2 col-md-2 sidebar"><div class="list-group"><a href="#" class="list-group-item list-group-item-action active" onclick="showme(this.id);" id="anchor_ppcx86">Packages</a><a class="list-group-item list-group-item-action" href="#" id="anchor_accumulo" onclick="showme(this.id);" title="Owned by Prajyot">ACCUMULO</a><a class="list-group-item list-group-item-action" href="#" id="anchor_ambari" onclick="showme(this.id);" title="Owned by Prajyot">AMBARI</a><a class="list-group-item list-group-item-action" href="#" id="anchor_atlas" onclick="showme(this.id);" title="Owned by Yussuf">ATLAS</a><a class="list-group-item list-group-item-action" href="#" id="anchor_calcite" onclick="showme(this.id);" title="Owned by Pravin">CALCITE</a><a class="list-group-item list-group-item-action" href="#" id="anchor_datafu" onclick="showme(this.id);" title="Owned by N/A">DATAFU</a><a class="list-group-item list-group-item-action" href="#" id="anchor_druid" onclick="showme(this.id);" title="Owned by N/A">DRUID</a><a class="list-group-item list-group-item-action" href="#" id="anchor_falcon" onclick="showme(this.id);" title="Owned by Yussuf">FALCON</a><a class="list-group-item list-group-item-action" href="#" id="anchor_flume" onclick="showme(this.id);" title="Owned by Pravin">FLUME</a><a class="list-group-item list-group-item-action" href="#" id="anchor_hadoop" onclick="showme(this.id);" title="Owned by Pravin">HADOOP</a><a class="list-group-item list-group-item-action" href="#" id="anchor_hbase" onclick="showme(this.id);" title="Owned by Prajyot">HBASE</a><a class="list-group-item list-group-item-action" href="#" id="anchor_hive" onclick="showme(this.id);" title="Owned by Alisha">HIVE</a><a class="list-group-item list-group-item-action" href="#" id="anchor_kafka" onclick="showme(this.id);" title="Owned by Prajyot">KAFKA</a><a class="list-group-item list-group-item-action" href="#" id="anchor_knox" onclick="showme(this.id);" title="Owned by Yussuf">KNOX</a><a class="list-group-item list-group-item-action" href="#" id="anchor_metron" onclick="showme(this.id);" title="Owned by Pravin">METRON</a><a class="list-group-item list-group-item-action" href="#" id="anchor_oozie" onclick="showme(this.id);" title="Owned by Alisha">OOZIE</a><a class="list-group-item list-group-item-action" href="#" id="anchor_phoenix" onclick="showme(this.id);" title="Owned by Prajyot">PHOENIX</a><a class="list-group-item list-group-item-action" href="#" id="anchor_pig" onclick="showme(this.id);" title="Owned by Yussuf">PIG</a><a class="list-group-item list-group-item-action" href="#" id="anchor_ranger" onclick="showme(this.id);" title="Owned by Yussuf">RANGER</a><a class="list-group-item list-group-item-action" href="#" id="anchor_slider" onclick="showme(this.id);" title="Owned by Yussuf">SLIDER</a><a class="list-group-item list-group-item-action" href="#" id="anchor_spark" onclick="showme(this.id);" title="Owned by Prajyot">SPARK</a><a class="list-group-item list-group-item-action" href="#" id="anchor_sqoop" onclick="showme(this.id);" title="Owned by Yussuf">SQOOP</a><a class="list-group-item list-group-item-action" href="#" id="anchor_storm" onclick="showme(this.id);" title="Owned by Alisha">STORM</a><a class="list-group-item list-group-item-action" href="#" id="anchor_tez" onclick="showme(this.id);" title="Owned by Prajyot">TEZ</a><a class="list-group-item list-group-item-action" href="#" id="anchor_zeppelin" onclick="showme(this.id);" title="Owned by Alisha">ZEPPELIN</a><a class="list-group-item list-group-item-action" href="#" id="anchor_zookeeper" onclick="showme(this.id);" title="Owned by Pravin">ZOOKEEPER</a></div></div><div style="display: table-cell"><div id="developers" style="display:block;font-weight:bold;display:none;" class="panel panel-info" name="summary"><div class="panel-heading"><div class="panel-title">DEVELOPERS</div></div><div class="panel-body"><table style="font-size:15" id="summarytable" class="table table-striped"><tr><td style="width: 100px;font-weight:bold">ALISHA</td><td><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_storm">STORM </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_zeppelin">ZEPPELIN </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_oozie">OOZIE </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_hive">HIVE </button></td></tr><tr><td style="width: 100px;font-weight:bold">PRAVIN</td><td><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_metron">METRON </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_zookeeper">ZOOKEEPER </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_hadoop">HADOOP </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_calcite">CALCITE </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_flume">FLUME </button></td></tr><tr><td style="width: 100px;font-weight:bold">PRAJYOT</td><td><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_kafka">KAFKA </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_tez">TEZ </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_hbase">HBASE </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_accumulo">ACCUMULO </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_phoenix">PHOENIX </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_spark">SPARK </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_ambari">AMBARI </button></td></tr><tr><td style="width: 100px;font-weight:bold">YUSSUF</td><td><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_knox">KNOX </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_atlas">ATLAS </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_sqoop">SQOOP </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_slider">SLIDER </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_falcon">FALCON </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_pig">PIG </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_ranger">RANGER </button></td></tr></table></div></div></div><div style="display: table-cell"><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="accumulo"><div style="font-weight:bold;" class="panel-heading">ACCUMULO<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Prajyot)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>c8d19d45f361f49a4d42b07310393b7bdf68b3b4</div><div><b>Last Run: </b>11-12-2018 20:26 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC UBUNTU18</th><th>X86 UBUNTU18</th><th>PPC RHEL72</th><th>X86 RHEL72</th><th>PPC RHEL75</th><th>X86 RHEL75</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 1718</div><div>Failed Count : 0</div><div>Skipped Count : 6</div></td><td><div>Total Count : 1718</div><div>Failed Count : 0</div><div>Skipped Count : 6</div></td><td><div>Total Count : 1718</div><div>Failed Count : 0</div><div>Skipped Count : 6</div></td><td><div>Total Count : 1718</div><div>Failed Count : 2</div><div>Skipped Count : 6</div></td><td><div>Total Count : 1718</div><div>Failed Count : 0</div><div>Skipped Count : 6</div></td><td><div>Total Count : 1718</div><div>Failed Count : 0</div><div>Skipped Count : 6</div></td><td><div>Total Count : 1718</div><div>Failed Count : 1</div><div>Skipped Count : 6</div></td><td><div>Total Count : 1718</div><div>Failed Count : 0</div><div>Skipped Count : 6</div></td></tr><tr><td>Result</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.accumulo.minicluster.MiniAccumuloClusterTest.testPerTableClasspath</li></div><div><li>org.apache.accumulo.minicluster.MiniAccumuloClusterTest.test</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.accumulo.test.fate.zookeeper.ZooLockTest.testTryLock</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>test timed out after 60000 milliseconds</li></div><div><li>test timed out after 30000 milliseconds</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>KeeperErrorCode = ConnectionLoss for /zltest-1253243994-l2</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.accumulo.minicluster.MiniAccumuloClusterTest.testPerTableClasspath</div></li><li><div>org.apache.accumulo.minicluster.MiniAccumuloClusterTest.test</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.accumulo.test.fate.zookeeper.ZooLockTest.testTryLock</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="ambari"><div style="font-weight:bold;" class="panel-heading">AMBARI<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Prajyot)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> origin/trunk</div><div><b>Last Revision: </b>48bedc1c1878f853f8660106559fb92ff3dfefdd</div><div><b>Last Run: </b>29-11-2018 04:55 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC UBUNTU18</th><th>X86 UBUNTU18</th><th>PPC RHEL72</th><th>X86 RHEL72</th><th>PPC RHEL75</th><th>X86 RHEL75</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 0</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 5259</div><div>Failed Count : 0</div><div>Skipped Count : 72</div></td><td><div>Total Count : 0</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 5259</div><div>Failed Count : 0</div><div>Skipped Count : 72</div></td><td><div>Total Count : 5259</div><div>Failed Count : 1</div><div>Skipped Count : 72</div></td><td><div>Total Count : 5259</div><div>Failed Count : 0</div><div>Skipped Count : 72</div></td><td><div>Total Count : 5259</div><div>Failed Count : 0</div><div>Skipped Count : 72</div></td><td><div>Total Count : 5259</div><div>Failed Count : 0</div><div>Skipped Count : 72</div></td></tr><tr><td>Result</td><td>N/A</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td>N/A</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.ambari.server.state.cluster.ClusterDeadlockTest.testDeadlockWhileRestartingComponents</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>[Deadlocked Thread:
------------------
"Thread-22" Id=55 WAITING on java.util.concurrent.locks.ReentrantReadWriteLock$NonfairSync@5f66a0da
 at sun.misc.Unsafe.park(Native Method)
 -  waiting on java.util.concurrent.locks.ReentrantReadWriteLock$NonfairSync@5f66a0da
 at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
 at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAnd</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.ambari.server.state.cluster.ClusterDeadlockTest.testDeadlockWhileRestartingComponents</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="atlas"><div style="font-weight:bold;" class="panel-heading">ATLAS<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Yussuf)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>97e131a58849d17145d9ccc0997c050866e532c3</div><div><b>Last Run: </b>05-12-2018 15:22 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC UBUNTU18</th><th>X86 UBUNTU18</th><th>PPC RHEL72</th><th>X86 RHEL72</th><th>PPC RHEL75</th><th>X86 RHEL75</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 969</div><div>Failed Count : 1</div><div>Skipped Count : 30</div></td><td><div>Total Count : 968</div><div>Failed Count : 0</div><div>Skipped Count : 22</div></td><td><div>Total Count : 968</div><div>Failed Count : 0</div><div>Skipped Count : 22</div></td><td><div>Total Count : 968</div><div>Failed Count : 0</div><div>Skipped Count : 22</div></td><td><div>Total Count : 968</div><div>Failed Count : 0</div><div>Skipped Count : 22</div></td><td><div>Total Count : 968</div><div>Failed Count : 0</div><div>Skipped Count : 22</div></td><td><div>Total Count : 968</div><div>Failed Count : 0</div><div>Skipped Count : 22</div></td><td><div>Total Count : 968</div><div>Failed Count : 0</div><div>Skipped Count : 22</div></td></tr><tr><td>Result</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"><div><li>org.apache.atlas.notification.NotificationHookConsumerKafkaTest.testConsumerConsumesNewMessageWithAutoCommitDisabled</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"><div><li>
Wanted but not invoked:
atlasEntityStore.createOrUpdate(
    &lt;any&gt;,
    &lt;any&gt;
);
-&gt; at org.apache.atlas.notification.NotificationHookConsumerKafkaTest.testConsumerConsumesNewMessageWithAutoCommitDisabled(NotificationHookConsumerKafkaTest.java:112)
Actually, there were zero interactions with this mock.
</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.atlas.notification.NotificationHookConsumerKafkaTest.testConsumerConsumesNewMessageWithAutoCommitDisabled</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="calcite"><div style="font-weight:bold;" class="panel-heading">CALCITE<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Pravin)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>f3655e15a11a9fb266af290cb390e690d4301c09</div><div><b>Last Run: </b>10-12-2018 01:44 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC UBUNTU18</th><th>X86 UBUNTU18</th><th>PPC RHEL72</th><th>X86 RHEL72</th><th>PPC RHEL75</th><th>X86 RHEL75</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 6506</div><div>Failed Count : 0</div><div>Skipped Count : 188</div></td><td><div>Total Count : 6517</div><div>Failed Count : 0</div><div>Skipped Count : 188</div></td><td><div>Total Count : 6506</div><div>Failed Count : 0</div><div>Skipped Count : 188</div></td><td><div>Total Count : 6517</div><div>Failed Count : 0</div><div>Skipped Count : 188</div></td><td><div>Total Count : 6506</div><div>Failed Count : 0</div><div>Skipped Count : 188</div></td><td><div>Total Count : 6517</div><div>Failed Count : 0</div><div>Skipped Count : 188</div></td><td><div>Total Count : 6506</div><div>Failed Count : 0</div><div>Skipped Count : 188</div></td><td><div>Total Count : 6517</div><div>Failed Count : 0</div><div>Skipped Count : 188</div></td></tr><tr><td>Result</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="datafu"><div style="font-weight:bold;" class="panel-heading">DATAFU<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(N/A)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>857cf164c30883d739c4895c9a9c758880526435</div><div><b>Last Run: </b>10-12-2018 01:11 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC UBUNTU18</th><th>X86 UBUNTU18</th><th>PPC RHEL72</th><th>X86 RHEL72</th><th>PPC RHEL75</th><th>X86 RHEL75</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 278</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 278</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 278</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 278</div><div>Failed Count : 1</div><div>Skipped Count : 0</div></td><td><div>Total Count : 278</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 278</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 278</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 278</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td></tr><tr><td>Result</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>datafu.test.pig.sampling.SimpleRandomSampleTest.testStratifiedSample</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>java.lang.AssertionError: Expected (A8,1) but found (A8,2) expected:&lt;(A8,1)&gt; but was:&lt;(A8,2)&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>datafu.test.pig.sampling.SimpleRandomSampleTest.testStratifiedSample</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="druid"><div style="font-weight:bold;" class="panel-heading">DRUID<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(N/A)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>c780aacc03e30b929ba14f70d7f278811fd8ba44</div><div><b>Last Run: </b>17-10-2018 06:25 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC UBUNTU18</th><th>X86 UBUNTU18</th><th>PPC RHEL72</th><th>X86 RHEL72</th><th>PPC RHEL75</th><th>X86 RHEL75</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 90903</div><div>Failed Count : 6</div><div>Skipped Count : 91</div></td><td><div>Total Count : 90903</div><div>Failed Count : 2</div><div>Skipped Count : 91</div></td><td><div>Total Count : 90903</div><div>Failed Count : 6</div><div>Skipped Count : 91</div></td><td><div>Total Count : 90903</div><div>Failed Count : 2</div><div>Skipped Count : 91</div></td><td><div>Total Count : 90903</div><div>Failed Count : 6</div><div>Skipped Count : 91</div></td><td><div>Total Count : 90903</div><div>Failed Count : 2</div><div>Skipped Count : 91</div></td><td><div>Total Count : 90903</div><div>Failed Count : 6</div><div>Skipped Count : 91</div></td><td><div>Total Count : 90903</div><div>Failed Count : 2</div><div>Skipped Count : 91</div></td></tr><tr><td>Result</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"><div><li>org.apache.druid.java.util.metrics.MonitorsTest.testDefaultFeed</li></div><div><li>org.apache.druid.java.util.metrics.MonitorsTest.testSetFeed</li></div><div><li>org.apache.druid.java.util.metrics.SigarLoadTest.testSigarLoad</li></div><div><li>org.apache.druid.java.util.metrics.SigarPidDiscovererTest.simpleTest</li></div><div><li>org.apache.druid.storage.s3.TestAWSCredentialsProvider.testWithFileSessionCredentials</li></div><div><li>org.apache.druid.storage.s3.TestAWSCredentialsProvider.testWithFixedAWSKeys</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.druid.storage.s3.TestAWSCredentialsProvider.testWithFileSessionCredentials</li></div><div><li>org.apache.druid.storage.s3.TestAWSCredentialsProvider.testWithFixedAWSKeys</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.druid.java.util.metrics.MonitorsTest.testDefaultFeed</li></div><div><li>org.apache.druid.java.util.metrics.MonitorsTest.testSetFeed</li></div><div><li>org.apache.druid.java.util.metrics.SigarLoadTest.testSigarLoad</li></div><div><li>org.apache.druid.java.util.metrics.SigarPidDiscovererTest.simpleTest</li></div><div><li>org.apache.druid.storage.s3.TestAWSCredentialsProvider.testWithFileSessionCredentials</li></div><div><li>org.apache.druid.storage.s3.TestAWSCredentialsProvider.testWithFixedAWSKeys</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.druid.storage.s3.TestAWSCredentialsProvider.testWithFileSessionCredentials</li></div><div><li>org.apache.druid.storage.s3.TestAWSCredentialsProvider.testWithFixedAWSKeys</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.druid.java.util.metrics.MonitorsTest.testDefaultFeed</li></div><div><li>org.apache.druid.java.util.metrics.MonitorsTest.testSetFeed</li></div><div><li>org.apache.druid.java.util.metrics.SigarLoadTest.testSigarLoad</li></div><div><li>org.apache.druid.java.util.metrics.SigarPidDiscovererTest.simpleTest</li></div><div><li>org.apache.druid.storage.s3.TestAWSCredentialsProvider.testWithFileSessionCredentials</li></div><div><li>org.apache.druid.storage.s3.TestAWSCredentialsProvider.testWithFixedAWSKeys</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.druid.storage.s3.TestAWSCredentialsProvider.testWithFileSessionCredentials</li></div><div><li>org.apache.druid.storage.s3.TestAWSCredentialsProvider.testWithFixedAWSKeys</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.druid.java.util.metrics.MonitorsTest.testDefaultFeed</li></div><div><li>org.apache.druid.java.util.metrics.MonitorsTest.testSetFeed</li></div><div><li>org.apache.druid.java.util.metrics.SigarLoadTest.testSigarLoad</li></div><div><li>org.apache.druid.java.util.metrics.SigarPidDiscovererTest.simpleTest</li></div><div><li>org.apache.druid.storage.s3.TestAWSCredentialsProvider.testWithFileSessionCredentials</li></div><div><li>org.apache.druid.storage.s3.TestAWSCredentialsProvider.testWithFixedAWSKeys</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.druid.storage.s3.TestAWSCredentialsProvider.testWithFileSessionCredentials</li></div><div><li>org.apache.druid.storage.s3.TestAWSCredentialsProvider.testWithFixedAWSKeys</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"><div><li>org.hyperic.sigar.Sigar.getPid()J</li></div><div><li>org.hyperic.sigar.Sigar.getPid()J</li></div><div><li>org.hyperic.sigar.Sigar.getPid()J</li></div><div><li>org.hyperic.sigar.Sigar.getPid()J</li></div><div><li>Unable to find a region via the region provider chain. Must provide an explicit region in the builder or setup environment to supply a region.</li></div><div><li>Unable to find a region via the region provider chain. Must provide an explicit region in the builder or setup environment to supply a region.</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Unable to find a region via the region provider chain. Must provide an explicit region in the builder or setup environment to supply a region.</li></div><div><li>Unable to find a region via the region provider chain. Must provide an explicit region in the builder or setup environment to supply a region.</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.hyperic.sigar.Sigar.getPid()J</li></div><div><li>org.hyperic.sigar.Sigar.getPid()J</li></div><div><li>org.hyperic.sigar.Sigar.getPid()J</li></div><div><li>org.hyperic.sigar.Sigar.getPid()J</li></div><div><li>Unable to find a region via the region provider chain. Must provide an explicit region in the builder or setup environment to supply a region.</li></div><div><li>Unable to find a region via the region provider chain. Must provide an explicit region in the builder or setup environment to supply a region.</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Unable to find a region via the region provider chain. Must provide an explicit region in the builder or setup environment to supply a region.</li></div><div><li>Unable to find a region via the region provider chain. Must provide an explicit region in the builder or setup environment to supply a region.</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.hyperic.sigar.Sigar.getPid()J</li></div><div><li>org.hyperic.sigar.Sigar.getPid()J</li></div><div><li>org.hyperic.sigar.Sigar.getPid()J</li></div><div><li>org.hyperic.sigar.Sigar.getPid()J</li></div><div><li>Unable to find a region via the region provider chain. Must provide an explicit region in the builder or setup environment to supply a region.</li></div><div><li>Unable to find a region via the region provider chain. Must provide an explicit region in the builder or setup environment to supply a region.</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Unable to find a region via the region provider chain. Must provide an explicit region in the builder or setup environment to supply a region.</li></div><div><li>Unable to find a region via the region provider chain. Must provide an explicit region in the builder or setup environment to supply a region.</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.hyperic.sigar.Sigar.getPid()J</li></div><div><li>org.hyperic.sigar.Sigar.getPid()J</li></div><div><li>org.hyperic.sigar.Sigar.getPid()J</li></div><div><li>org.hyperic.sigar.Sigar.getPid()J</li></div><div><li>Unable to find a region via the region provider chain. Must provide an explicit region in the builder or setup environment to supply a region.</li></div><div><li>Unable to find a region via the region provider chain. Must provide an explicit region in the builder or setup environment to supply a region.</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Unable to find a region via the region provider chain. Must provide an explicit region in the builder or setup environment to supply a region.</li></div><div><li>Unable to find a region via the region provider chain. Must provide an explicit region in the builder or setup environment to supply a region.</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.druid.java.util.metrics.MonitorsTest.testDefaultFeed</div></li><li><div>org.apache.druid.java.util.metrics.MonitorsTest.testSetFeed</div></li><li><div>org.apache.druid.java.util.metrics.SigarLoadTest.testSigarLoad</div></li><li><div>org.apache.druid.java.util.metrics.SigarPidDiscovererTest.simpleTest</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.druid.java.util.metrics.MonitorsTest.testDefaultFeed</div></li><li><div>org.apache.druid.java.util.metrics.MonitorsTest.testSetFeed</div></li><li><div>org.apache.druid.java.util.metrics.SigarLoadTest.testSigarLoad</div></li><li><div>org.apache.druid.java.util.metrics.SigarPidDiscovererTest.simpleTest</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.druid.java.util.metrics.MonitorsTest.testDefaultFeed</div></li><li><div>org.apache.druid.java.util.metrics.MonitorsTest.testSetFeed</div></li><li><div>org.apache.druid.java.util.metrics.SigarLoadTest.testSigarLoad</div></li><li><div>org.apache.druid.java.util.metrics.SigarPidDiscovererTest.simpleTest</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.druid.java.util.metrics.MonitorsTest.testDefaultFeed</div></li><li><div>org.apache.druid.java.util.metrics.MonitorsTest.testSetFeed</div></li><li><div>org.apache.druid.java.util.metrics.SigarLoadTest.testSigarLoad</div></li><li><div>org.apache.druid.java.util.metrics.SigarPidDiscovererTest.simpleTest</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="falcon"><div style="font-weight:bold;" class="panel-heading">FALCON<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Yussuf)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>470e5e9f5de9ba1b6149dec60e87d3a04270eda3</div><div><b>Last Run: </b>05-12-2018 03:27 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC UBUNTU18</th><th>X86 UBUNTU18</th><th>PPC RHEL72</th><th>X86 RHEL72</th><th>PPC RHEL75</th><th>X86 RHEL75</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 1001</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 1002</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 1000</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 1003</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 1001</div><div>Failed Count : 1</div><div>Skipped Count : 0</div></td><td><div>Total Count : 1003</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 998</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 1001</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td></tr><tr><td>Result</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.falcon.notification.service.SchedulerServiceTest.testDeRegistration</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>expected [1] but found [null]</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.falcon.notification.service.SchedulerServiceTest.testDeRegistration</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="flume"><div style="font-weight:bold;" class="panel-heading">FLUME<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Pravin)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/trunk</div><div><b>Last Revision: </b>5d29402f8def1fd3bb3f24c9d30bebb1e3806619</div><div><b>Last Run: </b>11-12-2018 01:48 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC UBUNTU18</th><th>X86 UBUNTU18</th><th>PPC RHEL72</th><th>X86 RHEL72</th><th>PPC RHEL75</th><th>X86 RHEL75</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 1325</div><div>Failed Count : 1</div><div>Skipped Count : 7</div></td><td><div>Total Count : 1325</div><div>Failed Count : 15</div><div>Skipped Count : 7</div></td><td><div>Total Count : 1324</div><div>Failed Count : 0</div><div>Skipped Count : 7</div></td><td><div>Total Count : 1325</div><div>Failed Count : 2</div><div>Skipped Count : 7</div></td><td><div>Total Count : 1325</div><div>Failed Count : 0</div><div>Skipped Count : 7</div></td><td><div>Total Count : 1325</div><div>Failed Count : 0</div><div>Skipped Count : 7</div></td><td><div>Total Count : 1325</div><div>Failed Count : 0</div><div>Skipped Count : 7</div></td><td><div>Total Count : 1322</div><div>Failed Count : 0</div><div>Skipped Count : 7</div></td></tr><tr><td>Result</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"><div><li>org.apache.flume.channel.file.TestLog.testMinimumRequiredSpaceTooSmallForPut</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.flume.channel.file.TestLog.testMinimumRequiredSpaceTooSmallForPut</li></div><div><li>org.apache.flume.client.avro.TestSpoolingFileLineReader.testDestinationExistsAndSameFileWindows</li></div><div><li>org.apache.flume.client.avro.TestSpoolingFileLineReader.testDestinationExistsAndSameFileNotOnWindows</li></div><div><li>org.apache.flume.client.avro.TestSpoolingFileLineReader.testBasicCommitFailure</li></div><div><li>org.apache.flume.client.avro.TestSpoolingFileLineReader.testBasicCommitFailureAndBufferSizeChanges</li></div><div><li>org.apache.flume.client.avro.TestSpoolingFileLineReader.testDestinationExistsAndDifferentFile</li></div><div><li>org.apache.flume.client.avro.TestSpoolingFileLineReader.testBehaviorWithEmptyFile</li></div><div><li>org.apache.flume.client.avro.TestSpoolingFileLineReader.testBatchedReadsWithinAFile</li></div><div><li>org.apache.flume.client.avro.TestSpoolingFileLineReader.testBatchedReadsAcrossFileBoundary</li></div><div><li>org.apache.flume.client.avro.TestSpoolingFileLineReader.testEmptyDirectoryAfterCommittingFile</li></div><div><li>org.apache.flume.client.avro.TestSpoolingFileLineReader.testLineExceedsMaxLineLength</li></div><div><li>org.apache.flume.client.avro.TestSpoolingFileLineReader.testBasicSpooling</li></div><div><li>org.apache.flume.client.avro.TestSpoolingFileLineReader.testInitiallyEmptyDirectory</li></div><div><li>org.apache.flume.client.avro.TestSpoolingFileLineReader.testFileChangesDuringRead</li></div><div><li>org.apache.flume.client.avro.TestSpoolingFileLineReader.testNameCorrespondsToLatestRead</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.flume.channel.file.TestLog.testMinimumRequiredSpaceTooSmallForPut</li></div><div><li>org.apache.flume.source.TestSyslogUtils.TestRfc3164Dates</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"><div><li>java.lang.AssertionError
	at org.apache.flume.channel.file.TestLog.doTestMinimumRequiredSpaceTooSmallForPut(TestLog.java:241)
	at org.apache.flume.channel.file.TestLog.testMinimumRequiredSpaceTooSmallForPut(TestLog.java:210)
</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>java.lang.AssertionError
	at org.apache.flume.channel.file.TestLog.doTestMinimumRequiredSpaceTooSmallForPut(TestLog.java:241)
	at org.apache.flume.channel.file.TestLog.testMinimumRequiredSpaceTooSmallForPut(TestLog.java:210)
</li></div><div><li>Platform not recognized</li></div><div><li>Unexpected exception, expected&lt;java.lang.IllegalStateException&gt; but was&lt;java.lang.NoClassDefFoundError&gt;</li></div><div><li>Could not initialize class java.nio.file.FileSystems$DefaultFileSystemHolder</li></div><div><li>Could not initialize class java.nio.file.FileSystems$DefaultFileSystemHolder</li></div><div><li>Unexpected exception, expected&lt;java.lang.IllegalStateException&gt; but was&lt;java.lang.NoClassDefFoundError&gt;</li></div><div><li>Could not initialize class java.nio.file.FileSystems$DefaultFileSystemHolder</li></div><div><li>Could not initialize class java.nio.file.FileSystems$DefaultFileSystemHolder</li></div><div><li>Could not initialize class java.nio.file.FileSystems$DefaultFileSystemHolder</li></div><div><li>Could not initialize class java.nio.file.FileSystems$DefaultFileSystemHolder</li></div><div><li>Could not initialize class java.nio.file.FileSystems$DefaultFileSystemHolder</li></div><div><li>Could not initialize class java.nio.file.FileSystems$DefaultFileSystemHolder</li></div><div><li>Could not initialize class java.nio.file.FileSystems$DefaultFileSystemHolder</li></div><div><li>Unexpected exception, expected&lt;java.lang.IllegalStateException&gt; but was&lt;java.lang.NoClassDefFoundError&gt;</li></div><div><li>Could not initialize class java.nio.file.FileSystems$DefaultFileSystemHolder</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>java.lang.AssertionError
	at org.apache.flume.channel.file.TestLog.doTestMinimumRequiredSpaceTooSmallForPut(TestLog.java:241)
	at org.apache.flume.channel.file.TestLog.testMinimumRequiredSpaceTooSmallForPut(TestLog.java:210)
</li></div><div><li>expected:&lt;15206[689]81000&gt; but was:&lt;15206[725]81000&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.flume.client.avro.TestSpoolingFileLineReader.testDestinationExistsAndSameFileWindows</div></li><li><div>org.apache.flume.client.avro.TestSpoolingFileLineReader.testDestinationExistsAndSameFileNotOnWindows</div></li><li><div>org.apache.flume.client.avro.TestSpoolingFileLineReader.testBasicCommitFailure</div></li><li><div>org.apache.flume.client.avro.TestSpoolingFileLineReader.testBasicCommitFailureAndBufferSizeChanges</div></li><li><div>org.apache.flume.client.avro.TestSpoolingFileLineReader.testDestinationExistsAndDifferentFile</div></li><li><div>org.apache.flume.client.avro.TestSpoolingFileLineReader.testBehaviorWithEmptyFile</div></li><li><div>org.apache.flume.client.avro.TestSpoolingFileLineReader.testBatchedReadsWithinAFile</div></li><li><div>org.apache.flume.client.avro.TestSpoolingFileLineReader.testBatchedReadsAcrossFileBoundary</div></li><li><div>org.apache.flume.client.avro.TestSpoolingFileLineReader.testEmptyDirectoryAfterCommittingFile</div></li><li><div>org.apache.flume.client.avro.TestSpoolingFileLineReader.testLineExceedsMaxLineLength</div></li><li><div>org.apache.flume.client.avro.TestSpoolingFileLineReader.testBasicSpooling</div></li><li><div>org.apache.flume.client.avro.TestSpoolingFileLineReader.testInitiallyEmptyDirectory</div></li><li><div>org.apache.flume.client.avro.TestSpoolingFileLineReader.testFileChangesDuringRead</div></li><li><div>org.apache.flume.client.avro.TestSpoolingFileLineReader.testNameCorrespondsToLatestRead</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.flume.channel.file.TestLog.testMinimumRequiredSpaceTooSmallForPut</div></li><li><div>org.apache.flume.source.TestSyslogUtils.TestRfc3164Dates</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="hadoop"><div style="font-weight:bold;" class="panel-heading">HADOOP<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Pravin)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/trunk</div><div><b>Last Revision: </b>3044b78bd0191883d5f9daf2601a58a268beed06</div><div><b>Last Run: </b>03-12-2018 13:31 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC UBUNTU18</th><th>X86 UBUNTU18</th><th>PPC RHEL72</th><th>X86 RHEL72</th><th>PPC RHEL75</th><th>X86 RHEL75</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 20325</div><div>Failed Count : 39</div><div>Skipped Count : 1191</div></td><td><div>Total Count : 20031</div><div>Failed Count : 20</div><div>Skipped Count : 1191</div></td><td><div>Total Count : 19497</div><div>Failed Count : 23</div><div>Skipped Count : 1144</div></td><td><div>Total Count : 19142</div><div>Failed Count : 20</div><div>Skipped Count : 1143</div></td><td><div>Total Count : 20395</div><div>Failed Count : 10</div><div>Skipped Count : 1191</div></td><td><div>Total Count : 20040</div><div>Failed Count : 11</div><div>Skipped Count : 1191</div></td><td><div>Total Count : 20412</div><div>Failed Count : 25</div><div>Skipped Count : 1191</div></td><td><div>Total Count : 20036</div><div>Failed Count : 25</div><div>Skipped Count : 1190</div></td></tr><tr><td>Result</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider.testTokenServiceCreationWithLegacyFormat</li></div><div><li>org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider.testTokenServiceCreationWithUriFormat</li></div><div><li>org.apache.hadoop.util.TestShell.testEnvVarsWithInheritance</li></div><div><li>org.apache.hadoop.registry.secure.TestSecureLogins.testValidKerberosName</li></div><div><li>org.apache.hadoop.fs.contract.router.web.TestRouterWebHDFSContractAppend.testRenameFileBeingAppended</li></div><div><li>org.apache.hadoop.hdfs.server.federation.router.TestRouterSafemode.testRouterRpcSafeMode</li></div><div><li>org.apache.hadoop.hdfs.TestDFSRemove.testRemove</li></div><div><li>org.apache.hadoop.hdfs.TestDFSShell.testCopyCommandsWithPreserveOption</li></div><div><li>org.apache.hadoop.hdfs.TestLeaseRecovery.testBlockSynchronization</li></div><div><li>org.apache.hadoop.hdfs.TestReadStripedFileWithMissingBlocks.testReadFileWithMissingBlocks</li></div><div><li>org.apache.hadoop.hdfs.TestRollingUpgrade.testCheckpointWithMultipleNN</li></div><div><li>org.apache.hadoop.hdfs.server.balancer.TestBalancer.testMaxIterationTime</li></div><div><li>org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFSStriped.testRead</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.TestBlockRecovery.testRaceBetweenReplicaRecoveryAndFinalizeBlock</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.TestDataNodeRollingUpgrade.testWithLayoutChangeAndFinalize</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.TestDirectoryScanner.testThrottling</li></div><div><li>org.apache.hadoop.hdfs.server.namenode.ha.TestHAAppend.testMultipleAppendsDuringCatchupTailing</li></div><div><li>org.apache.hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA.testRetryCacheOnStandbyNN</li></div><div><li>org.apache.hadoop.hdfs.server.sps.TestExternalStoragePolicySatisfier.testMultipleLevelDirectoryForSatisfyStoragePolicy</li></div><div><li>org.apache.hadoop.mapreduce.v2.app.launcher.TestContainerLauncher.testSlowNM</li></div><div><li>org.apache.hadoop.mapred.TestTaskLogAppender.testTaskLogAppender</li></div><div><li>org.apache.hadoop.mapred.TestMRTimelineEventHandling.testMapreduceJobTimelineServiceEnabled</li></div><div><li>org.apache.hadoop.mapreduce.lib.input.TestFixedLengthInputFormat.testNegativeRecordLength</li></div><div><li>org.apache.hadoop.mapreduce.lib.input.TestFixedLengthInputFormat.testPartialRecordCompressedIn</li></div><div><li>org.apache.hadoop.yarn.service.TestYarnNativeServices.testCreateFlexStopDestroyService</li></div><div><li>org.apache.hadoop.yarn.service.monitor.probe.TestDefaultProbe.testDefaultProbe[2]</li></div><div><li>org.apache.hadoop.yarn.client.TestApplicationClientProtocolOnHA.testMoveApplicationAcrossQueuesOnHA</li></div><div><li>org.apache.hadoop.yarn.client.TestApplicationClientProtocolOnHA.testForceKillApplicationOnHA</li></div><div><li>org.apache.hadoop.yarn.client.TestApplicationClientProtocolOnHA.testGetClusterNodesOnHA</li></div><div><li>org.apache.hadoop.yarn.client.TestApplicationClientProtocolOnHA.testGetContainerReportOnHA</li></div><div><li>org.apache.hadoop.yarn.util.TestProcfsBasedProcessTree.testProcessTree</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.testContainerUpgradeSuccessExplicitRollback</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.recovery.TestFSRMStateStore.testFSRMStateStoreClientRetry</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestIncreaseAllocationExpirer.testDecreaseAfterIncreaseWithAllocationExpiration</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.TestPlacementConstraintsUtil.testInterAppConstraintsByAppID</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.security.TestAMRMTokens.testMasterKeyRollOver[0]</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.security.TestDelegationTokenRenewer.testRMRestartWithExpiredToken</li></div><div><li>org.apache.hadoop.yarn.server.router.webapp.TestRouterWebServicesREST.testUpdateReservationXML</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.security.TestTimelineAuthFilterForV2.testPutTimelineEntities[2]</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider.testTokenServiceCreationWithLegacyFormat</li></div><div><li>org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider.testTokenServiceCreationWithUriFormat</li></div><div><li>org.apache.hadoop.util.TestShell.testEnvVarsWithInheritance</li></div><div><li>org.apache.hadoop.registry.secure.TestSecureLogins.testValidKerberosName</li></div><div><li>org.apache.hadoop.fs.contract.router.web.TestRouterWebHDFSContractAppend.testRenameFileBeingAppended</li></div><div><li>org.apache.hadoop.hdfs.server.balancer.TestBalancerWithMultipleNameNodes.test1OutOf2BlockpoolsWithBlockPoolPolicy</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.TestBlockRecovery.testRaceBetweenReplicaRecoveryAndFinalizeBlock</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting.testHotSwapOutFailedVolumeAndReporting</li></div><div><li>org.apache.hadoop.hdfs.server.namenode.ha.TestDFSUpgradeWithHA.testRollbackWithJournalNodes</li></div><div><li>org.apache.hadoop.mapreduce.v2.app.webapp.TestAMWebServices.testAMSlash</li></div><div><li>org.apache.hadoop.yarn.service.monitor.probe.TestDefaultProbe.testDefaultProbe[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithContainerResourceChange[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithContainerResourceChange[1]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestNMClient.testNMClient</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestNMClient.testNMClientNoCleanupOnStop</li></div><div><li>org.apache.hadoop.yarn.util.TestProcfsBasedProcessTree.testProcessTree</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.TestContainerLaunch.testKillProcessGroup</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.TestContainersMonitor.testContainerKillOnMemoryOverflow</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacitySchedulerSchedulingRequestUpdate.testNodePartitionPendingResourceUpdate</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestQueueManagementDynamicEditPolicy.testEditSchedule</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.io.compress.TestCompressorDecompressor.testCompressorDecompressor</li></div><div><li>org.apache.hadoop.io.compress.TestCompressorDecompressor.testCompressorDecompressorWithExeedBufferLimit</li></div><div><li>org.apache.hadoop.io.compress.snappy.TestSnappyCompressorDecompressor.testSnappyCompressDecompressInMultiThreads</li></div><div><li>org.apache.hadoop.io.compress.snappy.TestSnappyCompressorDecompressor.testSnappyCompressDecompress</li></div><div><li>org.apache.hadoop.util.TestShell.testEnvVarsWithInheritance</li></div><div><li>org.apache.hadoop.registry.secure.TestSecureLogins.testValidKerberosName</li></div><div><li>org.apache.hadoop.fs.contract.router.web.TestRouterWebHDFSContractAppend.testRenameFileBeingAppended</li></div><div><li>org.apache.hadoop.hdfs.server.balancer.TestBalancerRPCDelay.testBalancerRPCDelay</li></div><div><li>org.apache.hadoop.hdfs.server.balancer.TestBalancerWithMultipleNameNodes.testBalancingBlockpoolsWithBlockPoolPolicy</li></div><div><li>org.apache.hadoop.hdfs.server.namenode.TestEditLogRace.testDeadlock[0]</li></div><div><li>org.apache.hadoop.hdfs.server.namenode.ha.TestDFSUpgradeWithHA.testRollbackWithJournalNodes</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testAuthUrlConnectTimeout[timeoutSource=ConnectionFactory]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testConnectTimeout[timeoutSource=ConnectionFactory]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testRedirectConnectTimeout[timeoutSource=ConnectionFactory]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testTwoStepWriteConnectTimeout[timeoutSource=ConnectionFactory]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testAuthUrlConnectTimeout[timeoutSource=Configuration]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testConnectTimeout[timeoutSource=Configuration]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testRedirectConnectTimeout[timeoutSource=Configuration]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testTwoStepWriteConnectTimeout[timeoutSource=Configuration]</li></div><div><li>org.apache.hadoop.yarn.service.monitor.probe.TestDefaultProbe.testDefaultProbe[2]</li></div><div><li>org.apache.hadoop.yarn.util.TestProcfsBasedProcessTree.testProcessTree</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.TestAbstractYarnScheduler.testContainerReleaseWithAllocationTags[CAPACITY]</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestQueueManagementDynamicEditPolicy.testEditSchedule</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider.testTokenServiceCreationWithLegacyFormat</li></div><div><li>org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider.testTokenServiceCreationWithUriFormat</li></div><div><li>org.apache.hadoop.io.compress.TestCompressorDecompressor.testCompressorDecompressor</li></div><div><li>org.apache.hadoop.io.compress.TestCompressorDecompressor.testCompressorDecompressorWithExeedBufferLimit</li></div><div><li>org.apache.hadoop.io.compress.snappy.TestSnappyCompressorDecompressor.testSnappyCompressDecompressInMultiThreads</li></div><div><li>org.apache.hadoop.io.compress.snappy.TestSnappyCompressorDecompressor.testSnappyCompressDecompress</li></div><div><li>org.apache.hadoop.io.nativeio.TestNativeIO.testStat</li></div><div><li>org.apache.hadoop.io.nativeio.TestNativeIO.testMultiThreadedStat</li></div><div><li>org.apache.hadoop.util.TestShell.testEnvVarsWithInheritance</li></div><div><li>org.apache.hadoop.registry.secure.TestSecureLogins.testValidKerberosName</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting.testHotSwapOutFailedVolumeAndReporting</li></div><div><li>org.apache.hadoop.hdfs.server.namenode.TestNameNodeMXBean.testDecommissioningNodes</li></div><div><li>org.apache.hadoop.hdfs.server.namenode.TestPersistentStoragePolicySatisfier.testWithRestarts</li></div><div><li>org.apache.hadoop.yarn.service.monitor.probe.TestDefaultProbe.testDefaultProbe[2]</li></div><div><li>org.apache.hadoop.yarn.util.TestProcfsBasedProcessTree.testProcessTree</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.TestApplicationMasterServiceCapacity.testUpdateTrackingUrl</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.TestApplicationMasterServiceFair.testUpdateTrackingUrl</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestIncreaseAllocationExpirer.testDecreaseAfterIncreaseWithAllocationExpiration</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestQueueManagementDynamicEditPolicy.testEditSchedule</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestContinuousScheduling.testFairSchedulerContinuousSchedulingInitTime</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.util.TestShell.testEnvVarsWithInheritance</li></div><div><li>org.apache.hadoop.registry.secure.TestSecureLogins.testValidKerberosName</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.TestNNHandlesBlockReportPerStorage.blockReport_09</li></div><div><li>org.apache.hadoop.mapreduce.v2.app.TestRuntimeEstimators.testLegacyEstimator</li></div><div><li>org.apache.hadoop.yarn.service.monitor.probe.TestDefaultProbe.testDefaultProbe[2]</li></div><div><li>org.apache.hadoop.yarn.util.TestProcfsBasedProcessTree.testProcessTree</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.testContainerUpgradeSuccessExplicitRollback</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.TestContainerLaunch.testKillProcessGroup</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.TestContainersMonitor.testContainerKillOnMemoryOverflow</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestQueueManagementDynamicEditPolicy.testEditSchedule</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider.testTokenServiceCreationWithLegacyFormat</li></div><div><li>org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider.testTokenServiceCreationWithUriFormat</li></div><div><li>org.apache.hadoop.util.TestShell.testEnvVarsWithInheritance</li></div><div><li>org.apache.hadoop.registry.secure.TestSecureLogins.testValidKerberosName</li></div><div><li>org.apache.hadoop.fs.contract.router.web.TestRouterWebHDFSContractAppend.testRenameFileBeingAppended</li></div><div><li>org.apache.hadoop.hdfs.TestFileCorruption.testCorruptionWithDiskFailure</li></div><div><li>org.apache.hadoop.hdfs.server.namenode.TestUpgradeDomainBlockPlacementPolicy.testPlacement</li></div><div><li>org.apache.hadoop.hdfs.server.namenode.ha.TestDFSUpgradeWithHA.testRollbackWithJournalNodes</li></div><div><li>org.apache.hadoop.yarn.service.monitor.probe.TestDefaultProbe.testDefaultProbe[2]</li></div><div><li>org.apache.hadoop.yarn.util.TestProcfsBasedProcessTree.testProcessTree</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestIncreaseAllocationExpirer.testContainerIncreaseAllocationExpiration</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.util.TestShell.testEnvVarsWithInheritance</li></div><div><li>org.apache.hadoop.registry.secure.TestSecureLogins.testValidKerberosName</li></div><div><li>org.apache.hadoop.hdfs.TestErasureCodingExerciseAPIs.testQuota</li></div><div><li>org.apache.hadoop.hdfs.TestQuota.testQuotaCommands</li></div><div><li>org.apache.hadoop.hdfs.TestQuota.testNamespaceCommands</li></div><div><li>org.apache.hadoop.hdfs.TestQuota.testMaxSpaceQuotas</li></div><div><li>org.apache.hadoop.hdfs.TestQuota.testSpaceCommands</li></div><div><li>org.apache.hadoop.hdfs.TestQuota.testBlockAllocationAdjustsUsageConservatively</li></div><div><li>org.apache.hadoop.hdfs.TestQuota.testMultipleFilesSmallerThanOneBlock</li></div><div><li>org.apache.hadoop.hdfs.TestQuota.testHugeFileCount</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting.testHotSwapOutFailedVolumeAndReporting</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestLazyPersistReplicaRecovery.testDnRestartWithSavedReplicas</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testAuthUrlConnectTimeout[timeoutSource=ConnectionFactory]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testConnectTimeout[timeoutSource=ConnectionFactory]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testRedirectConnectTimeout[timeoutSource=ConnectionFactory]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testTwoStepWriteConnectTimeout[timeoutSource=ConnectionFactory]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testAuthUrlConnectTimeout[timeoutSource=Configuration]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testConnectTimeout[timeoutSource=Configuration]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testRedirectConnectTimeout[timeoutSource=Configuration]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testTwoStepWriteConnectTimeout[timeoutSource=Configuration]</li></div><div><li>org.apache.hadoop.mapred.TestMRTimelineEventHandling.testMapreduceJobTimelineServiceEnabled</li></div><div><li>org.apache.hadoop.yarn.service.monitor.probe.TestDefaultProbe.testDefaultProbe[2]</li></div><div><li>org.apache.hadoop.yarn.util.TestProcfsBasedProcessTree.testProcessTree</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.amrmproxy.TestFederationInterceptor.testMultipleSubClusters</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestQueueManagementDynamicEditPolicy.testEditSchedule</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider.testTokenServiceCreationWithLegacyFormat</li></div><div><li>org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider.testTokenServiceCreationWithUriFormat</li></div><div><li>org.apache.hadoop.util.TestShell.testEnvVarsWithInheritance</li></div><div><li>org.apache.hadoop.registry.secure.TestSecureLogins.testValidKerberosName</li></div><div><li>org.apache.hadoop.hdfs.server.blockmanagement.TestUnderReplicatedBlocks.testSetRepIncWithUnderReplicatedBlocks</li></div><div><li>org.apache.hadoop.hdfs.server.namenode.sps.TestBlockStorageMovementAttemptedItems.testNoBlockMovementAttemptFinishedReportAdded</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testAuthUrlConnectTimeout[timeoutSource=ConnectionFactory]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testConnectTimeout[timeoutSource=ConnectionFactory]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testRedirectConnectTimeout[timeoutSource=ConnectionFactory]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testTwoStepWriteConnectTimeout[timeoutSource=ConnectionFactory]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testAuthUrlConnectTimeout[timeoutSource=Configuration]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testConnectTimeout[timeoutSource=Configuration]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testRedirectConnectTimeout[timeoutSource=Configuration]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testTwoStepWriteConnectTimeout[timeoutSource=Configuration]</li></div><div><li>org.apache.hadoop.yarn.sls.appmaster.TestAMSimulator.testAMSimulatorWithNodeLabels[1]</li></div><div><li>org.apache.hadoop.yarn.service.TestYarnNativeServices.testCreateFlexStopDestroyService</li></div><div><li>org.apache.hadoop.yarn.service.monitor.probe.TestDefaultProbe.testDefaultProbe[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithContainerResourceChange[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMProxy.testAMRMProxyTokenRenewal</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestNMClient.testNMClient</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestNMClient.testNMClientNoCleanupOnStop</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestTimelineClientV2Impl.testSyncCall</li></div><div><li>org.apache.hadoop.yarn.util.TestProcfsBasedProcessTree.testProcessTree</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.TestCapacitySchedulerMetrics.testCSMetrics</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestQueueManagementDynamicEditPolicy.testEditSchedule</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;host1[]:9600&gt; but was:&lt;host1[.persistent.co.in.]:9600&gt;</li></div><div><li>expected:&lt;host1[]:9600&gt; but was:&lt;host1[.persistent.co.in.]:9600&gt;</li></div><div><li>String index out of range: -1</li></div><div><li>No rules applied to zookeeper/localhost</li></div><div><li>Failed to append to non-existent file /test/test/target for client 127.0.0.1
 at org.apache.hadoop.hdfs.server.namenode.FSDirAppendOp.appendFile(FSDirAppendOp.java:104)
 at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.appendFile(FSNamesystem.java:2720)
 at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.append(NameNodeRpcServer.java:820)
 at org.apache.hadoop.hdfs.protocolPB.Client</li></div><div><li>java.util.concurrent.ExecutionException: java.io.IOException: Cannot find locations for /testfile.txt, because the default nameservice is disabled to read or write</li></div><div><li>All blocks should be gone. start=16384 max=20384 final=20384 expected:&lt;16384&gt; but was:&lt;20384&gt;</li></div><div><li>expected:&lt;1543891272993&gt; but was:&lt;1543891274052&gt;</li></div><div><li>Found 2 lease, expected 1</li></div><div><li>test timed out after 300000 milliseconds</li></div><div><li>new checkpoint does not exist</li></div><div><li>Unexpected iteration runtime: 4008ms &gt; 3.5s</li></div><div><li>OP_READ_BLOCK: access token is invalid, when it is expected to be valid</li></div><div><li>test timed out after 20000 milliseconds</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.hdfs.server.datanode.TestDataNodeRollingUpgrade.deleteAndEnsureInTrash(TestDataNodeRollingUpgrade.java:141)
	at org.apache.hadoop.hdfs.server.datanode.TestDataNodeRollingUpgrade.testWithLayoutChangeAndFinalize(TestD</li></div><div><li>Throttle is too permissive</li></div><div><li>inode should complete in ~60000 ms.
Expected: is &lt;true&gt;
     but: was &lt;false&gt;</li></div><div><li>File is not open for writing: /ec/RS-3-2 (inode 16398) Holder DFSClient_NONMAPREDUCE_585547392_1 does not have any open files.
 at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2905)
 at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState(FSDirWriteFileOp.java:599)
 at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(F</li></div><div><li>Timed out waiting for condition. Thread diagnostics:
Timestamp: 2018-12-03 09:56:38,864

"org.eclipse.jetty.server.session.HashSessionManager@4b81d2aeTimer" daemon prio=5 tid=10989 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.</li></div><div><li>test timed out after 15000 milliseconds</li></div><div><li>test timed out after 5000 milliseconds</li></div><div><li>Job didn't finish in 30 seconds</li></div><div><li>test timed out after 5000 milliseconds</li></div><div><li>No such file or directory</li></div><div><li>expected:&lt;1&gt; but was:&lt;3&gt;</li></div><div><li>Expected success for Probe Status, time="Tue Dec 04 10:49:12 UTC 2018", outcome="failure", message="Failure in Default probe: IP presence with DNS checking and DNS server address 8.8.8.8", exception="java.io.IOException: comp-0: DNS checking is enabled, but lookup for example.com is not available yet"</li></div><div><li>Your endpoint configuration is wrong; For more details see:  http://wiki.apache.org/hadoop/UnsetHostnameOrPort</li></div><div><li>Your endpoint configuration is wrong; For more details see:  http://wiki.apache.org/hadoop/UnsetHostnameOrPort</li></div><div><li>Your endpoint configuration is wrong; For more details see:  http://wiki.apache.org/hadoop/UnsetHostnameOrPort</li></div><div><li>Your endpoint configuration is wrong; For more details see:  http://wiki.apache.org/hadoop/UnsetHostnameOrPort</li></div><div><li>Child process owned by init escaped process tree.</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.doRestartTests(TestContainerManager.java:482)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.testContainerUpgradeSuc</li></div><div><li>test timed out after 30000 milliseconds</li></div><div><li>expected:&lt;3072&gt; but was:&lt;4096&gt;</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertFalse(Assert.java:64)
	at org.junit.Assert.assertFalse(Assert.java:74)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.TestPlacementConstraintsUtil.testInterAppConstraintsByAppID(TestPlacementConstraintsUtil.java:965)
	at sun.reflect.N</li></div><div><li>appattempt_1543902967925_0001_000001 not found in AMRMTokenSecretManager.</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertNotNull(Assert.java:712)
	at org.junit.Assert.assertNotNull(Assert.java:722)
	at org.apache.hadoop.yarn.server.resourcemanager.security.TestDelegationTokenRenewer.testRMRestartWithExpiredToken(TestDelegationTokenRenewer.java:1067)
	at sun.reflect.NativeMetho</li></div><div><li>test timed out after 2000 milliseconds</li></div><div><li>Entities should have been published successfully.</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;host1[]:9600&gt; but was:&lt;host1[.persistent.co.in.]:9600&gt;</li></div><div><li>expected:&lt;host1[]:9600&gt; but was:&lt;host1[.persistent.co.in.]:9600&gt;</li></div><div><li>String index out of range: -1</li></div><div><li>No rules applied to zookeeper/localhost</li></div><div><li>Failed to append to non-existent file /test/test/target for client 127.0.0.1
 at org.apache.hadoop.hdfs.server.namenode.FSDirAppendOp.appendFile(FSDirAppendOp.java:104)
 at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.appendFile(FSNamesystem.java:2720)
 at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.append(NameNodeRpcServer.java:820)
 at org.apache.hadoop.hdfs.protocolPB.Client</li></div><div><li>test timed out after 600000 milliseconds</li></div><div><li>Recovery should be initiated successfully</li></div><div><li>Actual async detected volume failures should be greater or equal than [Ljava.lang.String;@12367c55</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.hdfs.server.namenode.ha.TestDFSUpgradeWithHA.testRollbackWithJournalNodes(TestDFSUpgradeWithHA.java:688)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invok</li></div><div><li>java.net.BindException: Address already in use</li></div><div><li>Expected success for Probe Status, time="Tue Dec 04 02:02:27 UTC 2018", outcome="failure", message="Failure in Default probe: IP presence with DNS checking and DNS server address 8.8.8.8", exception="java.io.IOException: comp-0: DNS checking is enabled, but lookup for example.com is not available yet"</li></div><div><li>expected:&lt;1&gt; but was:&lt;0&gt;</li></div><div><li>expected:&lt;1&gt; but was:&lt;0&gt;</li></div><div><li>test timed out after 200000 milliseconds</li></div><div><li>test timed out after 180000 milliseconds</li></div><div><li>ProcessTree shouldn't be alive</li></div><div><li>Process is still alive!</li></div><div><li>Process is still alive!</li></div><div><li>expected:&lt;0&gt; but was:&lt;8192&gt;</li></div><div><li>expected:&lt;0.5&gt; but was:&lt;0.0&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li> Expected to find 'testCompressorDecompressor error !!!' but got unexpected exception: java.lang.NullPointerException
 at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:187)
 at com.google.common.base.Joiner.toString(Joiner.java:532)
 at com.google.common.base.Joiner.appendTo(Joiner.java:124)
 at com.google.common.base.Joiner.appendTo(Joiner.java:181)
 at com.google.common.ba</li></div><div><li> Expected to find 'testCompressorDecompressorWithExeedBufferLimit error !!!' but got unexpected exception: java.lang.NullPointerException
 at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:187)
 at com.google.common.base.Joiner.toString(Joiner.java:532)
 at com.google.common.base.Joiner.appendTo(Joiner.java:124)
 at com.google.common.base.Joiner.appendTo(Joiner.java:181)
 at </li></div><div><li>Deferred</li></div><div><li>Could not decompress data. Input is invalid.</li></div><div><li>String index out of range: -1</li></div><div><li>No rules applied to zookeeper/localhost</li></div><div><li>Failed to append to non-existent file /test/test/target for client 127.0.0.1
 at org.apache.hadoop.hdfs.server.namenode.FSDirAppendOp.appendFile(FSDirAppendOp.java:104)
 at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.appendFile(FSNamesystem.java:2720)
 at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.append(NameNodeRpcServer.java:820)
 at org.apache.hadoop.hdfs.protocolPB.Client</li></div><div><li>java.lang.NullPointerException
	at java.util.LinkedList$ListItr.next(LinkedList.java:893)
	at org.mockito.internal.stubbing.InvocationContainerImpl.findAnswerFor(InvocationContainerImpl.java:71)
	at org.mockito.internal.MockHandler.handle(MockHandler.java:93)
	at org.mockito.internal.creation.MethodInterceptorFilter.intercept(MethodInterceptorFilter.java:47)
	at org.apache.hadoop.hdfs.server.namen</li></div><div><li>test timed out after 600000 milliseconds</li></div><div><li>Timed out waiting for condition. Thread diagnostics:
Timestamp: 2018-12-03 10:06:20,029

"Log spammer 9"  prio=5 tid=433 blocked
java.lang.Thread.State: BLOCKED
        at org.apache.hadoop.hdfs.server.namenode.FSEditLog.logEdit(FSEditLog.java:456)
        at org.apache.hadoop.hdfs.server.namenode.TestEditLogRace$4.call(TestEditLogRace.java:616)
        at org.apache.hadoop.hdfs.server.namenode.Te</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.hdfs.server.namenode.ha.TestDFSUpgradeWithHA.testRollbackWithJournalNodes(TestDFSUpgradeWithHA.java:688)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invok</li></div><div><li> Expected to find 'localhost:41627: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:41627: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li> Expected to find 'localhost:37229: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:37229: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li> Expected to find 'localhost:37936: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:37936: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li>expected timeout</li></div><div><li> Expected to find 'localhost:46062: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:46062: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li> Expected to find 'localhost:38888: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:38888: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li> Expected to find 'localhost:36402: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:36402: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li>expected timeout</li></div><div><li>Expected success for Probe Status, time="Mon Dec 03 19:57:51 CST 2018", outcome="failure", message="Failure in Default probe: IP presence with DNS checking and DNS server address 8.8.8.8", exception="java.io.IOException: comp-0: DNS checking is enabled, but lookup for example.com is not available yet"</li></div><div><li>Child process owned by init escaped process tree.</li></div><div><li>Timed out waiting for condition. Thread diagnostics:
Timestamp: 2018-12-03 04:08:20,405

"Timer-13" daemon prio=5 tid=60 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.util.TimerThread.mainLoop(Timer.java:552)
        at java.util.TimerThread.run(Timer.java:505)
"surefire-forkedjvm-ping-30s" daemon prio=5 tid=9 timed_waiting
java</li></div><div><li>expected:&lt;0.5&gt; but was:&lt;0.0&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;host1[]:9600&gt; but was:&lt;host1[.persistent.co.in.]:9600&gt;</li></div><div><li>expected:&lt;host1[]:9600&gt; but was:&lt;host1[.persistent.co.in.]:9600&gt;</li></div><div><li> Expected to find 'testCompressorDecompressor error !!!' but got unexpected exception: java.lang.NullPointerException
 at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:187)
 at com.google.common.base.Joiner.toString(Joiner.java:532)
 at com.google.common.base.Joiner.appendTo(Joiner.java:124)
 at com.google.common.base.Joiner.appendTo(Joiner.java:181)
 at com.google.common.ba</li></div><div><li> Expected to find 'testCompressorDecompressorWithExeedBufferLimit error !!!' but got unexpected exception: java.lang.NullPointerException
 at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:187)
 at com.google.common.base.Joiner.toString(Joiner.java:532)
 at com.google.common.base.Joiner.appendTo(Joiner.java:124)
 at com.google.common.base.Joiner.appendTo(Joiner.java:181)
 at </li></div><div><li>Deferred</li></div><div><li>Could not decompress data. Input is invalid.</li></div><div><li>length != 10(unixSymbolicPermission=-rw-r--r--.)</li></div><div><li>java.lang.IllegalArgumentException: length != 10(unixSymbolicPermission=-rw-r--r--.)</li></div><div><li>String index out of range: -1</li></div><div><li>No rules applied to zookeeper/localhost</li></div><div><li>Actual async detected volume failures should be greater or equal than [Ljava.lang.String;@7843f3fe</li></div><div><li>expected:&lt;...0,"lastBlockReport":[0},"127.0.0.1:45098":{"infoAddr":"127.0.0.1:46560","infoSecureAddr":"127.0.0.1:0","xferaddr":"127.0.0.1:45098","lastContact":0,"usedSpace":49152,"adminState":"In Service","nonDfsUsedSpace":202361507840,"capacity":422487998464,"numBlocks":0,"version":"3.3.0-SNAPSHOT","used":49152,"remaining":198618050560,"blockScheduled":0,"blockPoolUsed":49152,"blockPoolUsedPercen</li></div><div><li>Timed out waiting for condition. Thread diagnostics:
Timestamp: 2018-12-03 09:04:24,771

"IPC Server idle connection scanner for port 40914" daemon prio=5 tid=922 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.util.TimerThread.mainLoop(Timer.java:552)
        at java.util.TimerThread.run(Timer.java:505)
"org.eclipse.jetty.server.</li></div><div><li>Expected success for Probe Status, time="Mon Dec 03 18:32:14 CST 2018", outcome="failure", message="Failure in Default probe: IP presence with DNS checking and DNS server address 8.8.8.8", exception="java.io.IOException: comp-0: DNS checking is enabled, but lookup for example.com is not available yet"</li></div><div><li>Child process owned by init escaped process tree.</li></div><div><li>expected:&lt;[hadoop.apache.org]&gt; but was:&lt;[N/A]&gt;</li></div><div><li>expected:&lt;[hadoop.apache.org]&gt; but was:&lt;[N/A]&gt;</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestIncreaseAllocationExpirer.testDecreaseAfterIncreaseWithAllocationExpiration(TestIncreaseAllocationExpirer.java:450)
	at sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>expected:&lt;0.5&gt; but was:&lt;0.0&gt;</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestContinuousScheduling.testFairSchedulerContinuousSchedulingInitTime(TestContinuousScheduling.java:387)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>String index out of range: -1</li></div><div><li>No rules applied to zookeeper/localhost</li></div><div><li>Wrong number of PendingReplication blocks expected:&lt;2&gt; but was:&lt;1&gt;</li></div><div><li>We got the wrong number of successful speculations. expected:&lt;3&gt; but was:&lt;2&gt;</li></div><div><li>Expected success for Probe Status, time="Tue Dec 04 02:06:23 UTC 2018", outcome="failure", message="Failure in Default probe: IP presence with DNS checking and DNS server address 8.8.8.8", exception="java.io.IOException: comp-0: DNS checking is enabled, but lookup for example.com is not available yet"</li></div><div><li>ProcessTree shouldn't be alive</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.doRestartTests(TestContainerManager.java:482)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.testContainerUpgradeSuc</li></div><div><li>Process is still alive!</li></div><div><li>Process is still alive!</li></div><div><li>expected:&lt;0.5&gt; but was:&lt;0.0&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;host1[]:9600&gt; but was:&lt;host1[.persistent.co.in.]:9600&gt;</li></div><div><li>expected:&lt;host1[]:9600&gt; but was:&lt;host1[.persistent.co.in.]:9600&gt;</li></div><div><li>String index out of range: -1</li></div><div><li>No rules applied to zookeeper/localhost</li></div><div><li>Failed to append to non-existent file /test/test/target for client 127.0.0.1
 at org.apache.hadoop.hdfs.server.namenode.FSDirAppendOp.appendFile(FSDirAppendOp.java:104)
 at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.appendFile(FSNamesystem.java:2720)
 at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.append(NameNodeRpcServer.java:820)
 at org.apache.hadoop.hdfs.protocolPB.Client</li></div><div><li>Test resulted in an unexpected exit</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.hdfs.server.namenode.TestUpgradeDomainBlockPlacementPolicy.testPlacement(TestUpgradeDomainBlockPlacementPolicy.java:207)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAc</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.hdfs.server.namenode.ha.TestDFSUpgradeWithHA.testRollbackWithJournalNodes(TestDFSUpgradeWithHA.java:688)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invok</li></div><div><li>Expected success for Probe Status, time="Tue Dec 04 00:29:33 UTC 2018", outcome="failure", message="Failure in Default probe: IP presence with DNS checking and DNS server address 8.8.8.8", exception="java.io.IOException: comp-0: DNS checking is enabled, but lookup for example.com is not available yet"</li></div><div><li>Child process owned by init escaped process tree.</li></div><div><li>expected:&lt;2048&gt; but was:&lt;4096&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>String index out of range: -1</li></div><div><li>No rules applied to zookeeper/localhost</li></div><div><li>expected:&lt;           6               5            none             inf            1            0                  0 &gt; but was:&lt;           6               5            none             inf &gt;</li></div><div><li>expected:&lt;           3               0            9600            4480            2            1               1024 &gt; but was:&lt;           3               0            9600            4480 &gt;</li></div><div><li>expected:&lt;           6               3            none             inf            3            0                  0 &gt; but was:&lt;           6               3            none             inf &gt;</li></div><div><li>expected:&lt;9223372036854775806 9223372036854775805              10              10            1            0                  0 &gt; but was:&lt;9223372036854775806 9223372036854775805              10              10 &gt;</li></div><div><li>expected:&lt;        none             inf           12288           12288            3            0                  0 &gt; but was:&lt;        none             inf           12288           12288 &gt;</li></div><div><li>expected:&lt;        none             inf            1536             768            1            1                256 &gt; but was:&lt;        none             inf            1536             768 &gt;</li></div><div><li>expected:&lt;        none             inf          196608           15360            1           59              60416 &gt; but was:&lt;        none             inf          196608           15360 &gt;</li></div><div><li>expected:&lt;        none             inf            none             inf            3            6                  0 &gt; but was:&lt;        none             inf            none             inf &gt;</li></div><div><li>Actual async detected volume failures should be greater or equal than [Ljava.lang.String;@2dc46616</li></div><div><li>Timed out waiting for condition. Thread diagnostics:
Timestamp: 2018-12-04 06:35:16,537

"VolumeScannerThread(/var/lib/jenkins/workspace/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2)" daemon prio=5 tid=232 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(</li></div><div><li> Expected to find 'localhost:38104: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:38104: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li> Expected to find 'localhost:36293: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:36293: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li> Expected to find 'localhost:37337: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:37337: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li>expected timeout</li></div><div><li> Expected to find 'localhost:38298: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:38298: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li> Expected to find 'localhost:37851: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:37851: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li> Expected to find 'localhost:37348: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:37348: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li>expected timeout</li></div><div><li>Job didn't finish in 30 seconds</li></div><div><li>Expected success for Probe Status, time="Tue Dec 04 15:41:49 UTC 2018", outcome="failure", message="Failure in Default probe: IP presence with DNS checking and DNS server address 8.8.8.8", exception="java.io.IOException: comp-0: DNS checking is enabled, but lookup for example.com is not available yet"</li></div><div><li>Child process owned by init escaped process tree.</li></div><div><li>java.lang.reflect.UndeclaredThrowableException
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1894)
	at org.apache.hadoop.yarn.server.nodemanager.amrmproxy.TestFederationInterceptor.testMultipleSubClusters(TestFederationInterceptor.java:304)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMeth</li></div><div><li>expected:&lt;0.5&gt; but was:&lt;0.0&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;host1[]:9600&gt; but was:&lt;host1[.persistent.co.in.]:9600&gt;</li></div><div><li>expected:&lt;host1[]:9600&gt; but was:&lt;host1[.persistent.co.in.]:9600&gt;</li></div><div><li>String index out of range: -1</li></div><div><li>No rules applied to zookeeper/localhost</li></div><div><li>test timed out after 120000 milliseconds</li></div><div><li>Item doesn't exist in the attempted list expected:&lt;1&gt; but was:&lt;0&gt;</li></div><div><li> Expected to find 'localhost:44674: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:44674: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li> Expected to find 'localhost:44645: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:44645: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li> Expected to find 'localhost:43853: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:43853: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li>expected timeout</li></div><div><li> Expected to find 'localhost:34017: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:34017: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li> Expected to find 'localhost:32889: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:32889: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li> Expected to find 'localhost:43489: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:43489: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li>expected timeout</li></div><div><li>java.io.IOException: Unable to delete directory /var/lib/jenkins/workspace/hadoop/hadoop-tools/hadoop-sls/target/test-dir/output4919486531515700293/metrics.</li></div><div><li>expected:&lt;1&gt; but was:&lt;2&gt;</li></div><div><li>Expected success for Probe Status, time="Tue Dec 04 00:44:11 UTC 2018", outcome="failure", message="Failure in Default probe: IP presence with DNS checking and DNS server address 8.8.8.8", exception="java.io.IOException: comp-0: DNS checking is enabled, but lookup for example.com is not available yet"</li></div><div><li>expected:&lt;1&gt; but was:&lt;0&gt;</li></div><div><li>Application attempt appattempt_1543873440786_0001_000001 doesn't exist in ApplicationMasterService cache.
 at org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.allocate(ApplicationMasterService.java:398)
 at org.apache.hadoop.yarn.server.nodemanager.amrmproxy.DefaultRequestInterceptor$3.allocate(DefaultRequestInterceptor.java:224)
 at org.apache.hadoop.yarn.server.nodemanager.</li></div><div><li>test timed out after 200000 milliseconds</li></div><div><li>test timed out after 180000 milliseconds</li></div><div><li>TimelineEntities not published as desired expected:&lt;3&gt; but was:&lt;4&gt;</li></div><div><li>Child process owned by init escaped process tree.</li></div><div><li>expected:&lt;2&gt; but was:&lt;1&gt;</li></div><div><li>expected:&lt;0.5&gt; but was:&lt;0.0&gt;</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.hdfs.server.federation.router.TestRouterSafemode.testRouterRpcSafeMode</div></li><li><div>org.apache.hadoop.hdfs.TestDFSRemove.testRemove</div></li><li><div>org.apache.hadoop.hdfs.TestDFSShell.testCopyCommandsWithPreserveOption</div></li><li><div>org.apache.hadoop.hdfs.TestLeaseRecovery.testBlockSynchronization</div></li><li><div>org.apache.hadoop.hdfs.TestReadStripedFileWithMissingBlocks.testReadFileWithMissingBlocks</div></li><li><div>org.apache.hadoop.hdfs.TestRollingUpgrade.testCheckpointWithMultipleNN</div></li><li><div>org.apache.hadoop.hdfs.server.balancer.TestBalancer.testMaxIterationTime</div></li><li><div>org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFSStriped.testRead</div></li><li><div>org.apache.hadoop.hdfs.server.datanode.TestDataNodeRollingUpgrade.testWithLayoutChangeAndFinalize</div></li><li><div>org.apache.hadoop.hdfs.server.datanode.TestDirectoryScanner.testThrottling</div></li><li><div>org.apache.hadoop.hdfs.server.namenode.ha.TestHAAppend.testMultipleAppendsDuringCatchupTailing</div></li><li><div>org.apache.hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA.testRetryCacheOnStandbyNN</div></li><li><div>org.apache.hadoop.hdfs.server.sps.TestExternalStoragePolicySatisfier.testMultipleLevelDirectoryForSatisfyStoragePolicy</div></li><li><div>org.apache.hadoop.mapreduce.v2.app.launcher.TestContainerLauncher.testSlowNM</div></li><li><div>org.apache.hadoop.mapred.TestTaskLogAppender.testTaskLogAppender</div></li><li><div>org.apache.hadoop.mapred.TestMRTimelineEventHandling.testMapreduceJobTimelineServiceEnabled</div></li><li><div>org.apache.hadoop.mapreduce.lib.input.TestFixedLengthInputFormat.testNegativeRecordLength</div></li><li><div>org.apache.hadoop.mapreduce.lib.input.TestFixedLengthInputFormat.testPartialRecordCompressedIn</div></li><li><div>org.apache.hadoop.yarn.service.TestYarnNativeServices.testCreateFlexStopDestroyService</div></li><li><div>org.apache.hadoop.yarn.client.TestApplicationClientProtocolOnHA.testMoveApplicationAcrossQueuesOnHA</div></li><li><div>org.apache.hadoop.yarn.client.TestApplicationClientProtocolOnHA.testForceKillApplicationOnHA</div></li><li><div>org.apache.hadoop.yarn.client.TestApplicationClientProtocolOnHA.testGetClusterNodesOnHA</div></li><li><div>org.apache.hadoop.yarn.client.TestApplicationClientProtocolOnHA.testGetContainerReportOnHA</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.testContainerUpgradeSuccessExplicitRollback</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.recovery.TestFSRMStateStore.testFSRMStateStoreClientRetry</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestIncreaseAllocationExpirer.testDecreaseAfterIncreaseWithAllocationExpiration</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.TestPlacementConstraintsUtil.testInterAppConstraintsByAppID</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.security.TestAMRMTokens.testMasterKeyRollOver[0]</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.security.TestDelegationTokenRenewer.testRMRestartWithExpiredToken</div></li><li><div>org.apache.hadoop.yarn.server.router.webapp.TestRouterWebServicesREST.testUpdateReservationXML</div></li><li><div>org.apache.hadoop.yarn.server.timelineservice.security.TestTimelineAuthFilterForV2.testPutTimelineEntities[2]</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.hdfs.server.balancer.TestBalancerWithMultipleNameNodes.test1OutOf2BlockpoolsWithBlockPoolPolicy</div></li><li><div>org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting.testHotSwapOutFailedVolumeAndReporting</div></li><li><div>org.apache.hadoop.hdfs.server.namenode.ha.TestDFSUpgradeWithHA.testRollbackWithJournalNodes</div></li><li><div>org.apache.hadoop.mapreduce.v2.app.webapp.TestAMWebServices.testAMSlash</div></li><li><div>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithContainerResourceChange[0]</div></li><li><div>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithContainerResourceChange[1]</div></li><li><div>org.apache.hadoop.yarn.client.api.impl.TestNMClient.testNMClient</div></li><li><div>org.apache.hadoop.yarn.client.api.impl.TestNMClient.testNMClientNoCleanupOnStop</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.TestContainerLaunch.testKillProcessGroup</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.TestContainersMonitor.testContainerKillOnMemoryOverflow</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacitySchedulerSchedulingRequestUpdate.testNodePartitionPendingResourceUpdate</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestQueueManagementDynamicEditPolicy.testEditSchedule</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.fs.contract.router.web.TestRouterWebHDFSContractAppend.testRenameFileBeingAppended</div></li><li><div>org.apache.hadoop.hdfs.server.balancer.TestBalancerRPCDelay.testBalancerRPCDelay</div></li><li><div>org.apache.hadoop.hdfs.server.balancer.TestBalancerWithMultipleNameNodes.testBalancingBlockpoolsWithBlockPoolPolicy</div></li><li><div>org.apache.hadoop.hdfs.server.namenode.TestEditLogRace.testDeadlock[0]</div></li><li><div>org.apache.hadoop.hdfs.server.namenode.ha.TestDFSUpgradeWithHA.testRollbackWithJournalNodes</div></li><li><div>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testAuthUrlConnectTimeout[timeoutSource=ConnectionFactory]</div></li><li><div>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testConnectTimeout[timeoutSource=ConnectionFactory]</div></li><li><div>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testRedirectConnectTimeout[timeoutSource=ConnectionFactory]</div></li><li><div>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testTwoStepWriteConnectTimeout[timeoutSource=ConnectionFactory]</div></li><li><div>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testAuthUrlConnectTimeout[timeoutSource=Configuration]</div></li><li><div>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testConnectTimeout[timeoutSource=Configuration]</div></li><li><div>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testRedirectConnectTimeout[timeoutSource=Configuration]</div></li><li><div>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testTwoStepWriteConnectTimeout[timeoutSource=Configuration]</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.scheduler.TestAbstractYarnScheduler.testContainerReleaseWithAllocationTags[CAPACITY]</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider.testTokenServiceCreationWithLegacyFormat</div></li><li><div>org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider.testTokenServiceCreationWithUriFormat</div></li><li><div>org.apache.hadoop.io.nativeio.TestNativeIO.testStat</div></li><li><div>org.apache.hadoop.io.nativeio.TestNativeIO.testMultiThreadedStat</div></li><li><div>org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting.testHotSwapOutFailedVolumeAndReporting</div></li><li><div>org.apache.hadoop.hdfs.server.namenode.TestNameNodeMXBean.testDecommissioningNodes</div></li><li><div>org.apache.hadoop.hdfs.server.namenode.TestPersistentStoragePolicySatisfier.testWithRestarts</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.TestApplicationMasterServiceCapacity.testUpdateTrackingUrl</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.TestApplicationMasterServiceFair.testUpdateTrackingUrl</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestIncreaseAllocationExpirer.testDecreaseAfterIncreaseWithAllocationExpiration</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestContinuousScheduling.testFairSchedulerContinuousSchedulingInitTime</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.hdfs.server.datanode.TestNNHandlesBlockReportPerStorage.blockReport_09</div></li><li><div>org.apache.hadoop.mapreduce.v2.app.TestRuntimeEstimators.testLegacyEstimator</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.testContainerUpgradeSuccessExplicitRollback</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.TestContainerLaunch.testKillProcessGroup</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.TestContainersMonitor.testContainerKillOnMemoryOverflow</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestQueueManagementDynamicEditPolicy.testEditSchedule</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider.testTokenServiceCreationWithLegacyFormat</div></li><li><div>org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider.testTokenServiceCreationWithUriFormat</div></li><li><div>org.apache.hadoop.fs.contract.router.web.TestRouterWebHDFSContractAppend.testRenameFileBeingAppended</div></li><li><div>org.apache.hadoop.hdfs.TestFileCorruption.testCorruptionWithDiskFailure</div></li><li><div>org.apache.hadoop.hdfs.server.namenode.TestUpgradeDomainBlockPlacementPolicy.testPlacement</div></li><li><div>org.apache.hadoop.hdfs.server.namenode.ha.TestDFSUpgradeWithHA.testRollbackWithJournalNodes</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestIncreaseAllocationExpirer.testContainerIncreaseAllocationExpiration</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.hdfs.TestErasureCodingExerciseAPIs.testQuota</div></li><li><div>org.apache.hadoop.hdfs.TestQuota.testQuotaCommands</div></li><li><div>org.apache.hadoop.hdfs.TestQuota.testNamespaceCommands</div></li><li><div>org.apache.hadoop.hdfs.TestQuota.testMaxSpaceQuotas</div></li><li><div>org.apache.hadoop.hdfs.TestQuota.testSpaceCommands</div></li><li><div>org.apache.hadoop.hdfs.TestQuota.testBlockAllocationAdjustsUsageConservatively</div></li><li><div>org.apache.hadoop.hdfs.TestQuota.testMultipleFilesSmallerThanOneBlock</div></li><li><div>org.apache.hadoop.hdfs.TestQuota.testHugeFileCount</div></li><li><div>org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting.testHotSwapOutFailedVolumeAndReporting</div></li><li><div>org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestLazyPersistReplicaRecovery.testDnRestartWithSavedReplicas</div></li><li><div>org.apache.hadoop.mapred.TestMRTimelineEventHandling.testMapreduceJobTimelineServiceEnabled</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.amrmproxy.TestFederationInterceptor.testMultipleSubClusters</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider.testTokenServiceCreationWithLegacyFormat</div></li><li><div>org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider.testTokenServiceCreationWithUriFormat</div></li><li><div>org.apache.hadoop.hdfs.server.blockmanagement.TestUnderReplicatedBlocks.testSetRepIncWithUnderReplicatedBlocks</div></li><li><div>org.apache.hadoop.hdfs.server.namenode.sps.TestBlockStorageMovementAttemptedItems.testNoBlockMovementAttemptFinishedReportAdded</div></li><li><div>org.apache.hadoop.yarn.sls.appmaster.TestAMSimulator.testAMSimulatorWithNodeLabels[1]</div></li><li><div>org.apache.hadoop.yarn.service.TestYarnNativeServices.testCreateFlexStopDestroyService</div></li><li><div>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithContainerResourceChange[0]</div></li><li><div>org.apache.hadoop.yarn.client.api.impl.TestAMRMProxy.testAMRMProxyTokenRenewal</div></li><li><div>org.apache.hadoop.yarn.client.api.impl.TestNMClient.testNMClient</div></li><li><div>org.apache.hadoop.yarn.client.api.impl.TestNMClient.testNMClientNoCleanupOnStop</div></li><li><div>org.apache.hadoop.yarn.client.api.impl.TestTimelineClientV2Impl.testSyncCall</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.TestCapacitySchedulerMetrics.testCSMetrics</div></li></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="hbase"><div style="font-weight:bold;" class="panel-heading">HBASE<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Prajyot)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>f32d2618430f70e1b0db92785294b2c7892cc02b</div><div><b>Last Run: </b>13-12-2018 14:31 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC UBUNTU18</th><th>X86 UBUNTU18</th><th>PPC RHEL72</th><th>X86 RHEL72</th><th>PPC RHEL75</th><th>X86 RHEL75</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 4710</div><div>Failed Count : 51</div><div>Skipped Count : 41</div></td><td><div>Total Count : 4738</div><div>Failed Count : 6</div><div>Skipped Count : 41</div></td><td><div>Total Count : 4725</div><div>Failed Count : 21</div><div>Skipped Count : 41</div></td><td><div>Total Count : 4739</div><div>Failed Count : 9</div><div>Skipped Count : 41</div></td><td><div>Total Count : 4718</div><div>Failed Count : 23</div><div>Skipped Count : 41</div></td><td><div>Total Count : 4738</div><div>Failed Count : 0</div><div>Skipped Count : 41</div></td><td><div>Total Count : 4708</div><div>Failed Count : 40</div><div>Skipped Count : 41</div></td><td><div>Total Count : 4738</div><div>Failed Count : 1</div><div>Skipped Count : 41</div></td></tr><tr><td>Result</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.hbase.coprocessor.example.TestWriteHeavyIncrementObserverWithMemStoreCompaction.test</li></div><div><li>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.testExportWithTargetName</li></div><div><li>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.testExportFileSystemState</li></div><div><li>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.testConsecutiveExports</li></div><div><li>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.testEmptyExportFileSystemState</li></div><div><li>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory</li></div><div><li>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory</li></div><div><li>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.testExportFileSystemStateWithSkipTmp</li></div><div><li>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.testExportFileSystemStateWithSkipTmp</li></div><div><li>org.apache.hadoop.hbase.rest.TestGetAndPutResource.org.apache.hadoop.hbase.rest.TestGetAndPutResource</li></div><div><li>org.apache.hadoop.hbase.rest.TestGetAndPutResource.org.apache.hadoop.hbase.rest.TestGetAndPutResource</li></div><div><li>org.apache.hadoop.hbase.rest.TestGetAndPutResource.testSingleCellGetPutPB</li></div><div><li>org.apache.hadoop.hbase.rsgroup.TestRSGroupsAdmin1.org.apache.hadoop.hbase.rsgroup.TestRSGroupsAdmin1</li></div><div><li>org.apache.hadoop.hbase.rsgroup.TestRSGroupsAdmin1.org.apache.hadoop.hbase.rsgroup.TestRSGroupsAdmin1</li></div><div><li>org.apache.hadoop.hbase.TestClientOperationTimeout.testPutTimeout</li></div><div><li>org.apache.hadoop.hbase.TestClientOperationTimeout.testGetTimeout</li></div><div><li>org.apache.hadoop.hbase.TestClientOperationTimeout.testScanTimeout</li></div><div><li>org.apache.hadoop.hbase.client.TestAsyncDecommissionAdminApi.testAsyncDecommissionRegionServers[0]</li></div><div><li>org.apache.hadoop.hbase.client.TestAsyncDecommissionAdminApi.testAsyncDecommissionRegionServers[1]</li></div><div><li>org.apache.hadoop.hbase.client.TestReplicaWithCluster.testCreateDeleteTable</li></div><div><li>org.apache.hadoop.hbase.client.TestReplicaWithCluster.org.apache.hadoop.hbase.client.TestReplicaWithCluster</li></div><div><li>org.apache.hadoop.hbase.client.TestReplicaWithCluster.org.apache.hadoop.hbase.client.TestReplicaWithCluster</li></div><div><li>org.apache.hadoop.hbase.client.TestSeparateClientZKCluster.testMetaMoveDuringClientZkClusterRestart</li></div><div><li>org.apache.hadoop.hbase.client.TestSeparateClientZKCluster.testBasicOperation</li></div><div><li>org.apache.hadoop.hbase.client.TestSplitOrMergeStatus.testSplitSwitch</li></div><div><li>org.apache.hadoop.hbase.client.TestTableFavoredNodes.testMergeTable</li></div><div><li>org.apache.hadoop.hbase.client.replication.TestReplicationAdmin.org.apache.hadoop.hbase.client.replication.TestReplicationAdmin</li></div><div><li>org.apache.hadoop.hbase.client.replication.TestReplicationAdmin.org.apache.hadoop.hbase.client.replication.TestReplicationAdmin</li></div><div><li>org.apache.hadoop.hbase.client.replication.TestReplicationAdminWithClusters.org.apache.hadoop.hbase.client.replication.TestReplicationAdminWithClusters</li></div><div><li>org.apache.hadoop.hbase.client.replication.TestReplicationAdminWithClusters.org.apache.hadoop.hbase.client.replication.TestReplicationAdminWithClusters</li></div><div><li>org.apache.hadoop.hbase.master.TestAssignmentManagerMetrics.testRITAssignmentManagerMetrics</li></div><div><li>org.apache.hadoop.hbase.master.TestMasterMetricsWrapper.testOfflineRegion</li></div><div><li>org.apache.hadoop.hbase.master.TestMasterRepairMode.testNewCluster</li></div><div><li>org.apache.hadoop.hbase.master.assignment.TestCloseRegionWhileRSCrash.testRetryBackoff</li></div><div><li>org.apache.hadoop.hbase.master.assignment.TestSplitTableRegionProcedure.testRecoveryAndDoubleExecution</li></div><div><li>org.apache.hadoop.hbase.master.assignment.TestSplitTableRegionProcedure.org.apache.hadoop.hbase.master.assignment.TestSplitTableRegionProcedure</li></div><div><li>org.apache.hadoop.hbase.master.assignment.TestSplitTableRegionProcedure.org.apache.hadoop.hbase.master.assignment.TestSplitTableRegionProcedure</li></div><div><li>org.apache.hadoop.hbase.master.cleaner.TestSnapshotFromMaster.testAsyncSnapshotWillNotBlockSnapshotHFileCleaner</li></div><div><li>org.apache.hadoop.hbase.master.cleaner.TestSnapshotFromMaster.testSnapshotHFileArchiving</li></div><div><li>org.apache.hadoop.hbase.master.locking.TestLockManager.testMasterLockAcquireTimeout</li></div><div><li>org.apache.hadoop.hbase.master.locking.TestLockProcedure.testMultipleLocks</li></div><div><li>org.apache.hadoop.hbase.namespace.TestNamespaceAuditor.testRegionMerge</li></div><div><li>org.apache.hadoop.hbase.regionserver.TestCleanupMetaWAL.testCleanupMetaWAL</li></div><div><li>org.apache.hadoop.hbase.regionserver.TestCompactionFileNotFound.testSplitAfterRefresh</li></div><div><li>org.apache.hadoop.hbase.regionserver.TestHRegionFileSystem.testBlockStoragePolicy</li></div><div><li>org.apache.hadoop.hbase.regionserver.TestMajorCompaction.testTimeBasedMajorCompaction[2]</li></div><div><li>org.apache.hadoop.hbase.regionserver.TestRegionServerNoMaster.org.apache.hadoop.hbase.regionserver.TestRegionServerNoMaster</li></div><div><li>org.apache.hadoop.hbase.regionserver.TestRegionServerNoMaster.org.apache.hadoop.hbase.regionserver.TestRegionServerNoMaster</li></div><div><li>org.apache.hadoop.hbase.regionserver.TestScannerHeartbeatMessages.testHeartbeatWithSparseRowFilter</li></div><div><li>org.apache.hadoop.hbase.regionserver.throttle.TestFlushWithThroughputController.testFlushControl</li></div><div><li>org.apache.hadoop.hbase.replication.TestReplicationEndpoint.testWALEntryFilterUpdateValidation</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.testExportWithTargetName</li></div><div><li>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.testExportFileSystemState</li></div><div><li>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.testConsecutiveExports</li></div><div><li>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.testEmptyExportFileSystemState</li></div><div><li>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.testExportFileSystemStateWithSkipTmp</li></div><div><li>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.testExportRetry</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.testExportWithTargetName</li></div><div><li>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.testExportFileSystemState</li></div><div><li>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.testConsecutiveExports</li></div><div><li>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.testEmptyExportFileSystemState</li></div><div><li>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.testExportFileSystemStateWithSkipTmp</li></div><div><li>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.testExportRetry</li></div><div><li>org.apache.hadoop.hbase.rest.TestGetAndPutResource.org.apache.hadoop.hbase.rest.TestGetAndPutResource</li></div><div><li>org.apache.hadoop.hbase.rest.TestGetAndPutResource.org.apache.hadoop.hbase.rest.TestGetAndPutResource</li></div><div><li>org.apache.hadoop.hbase.client.TestAsyncDecommissionAdminApi.testAsyncDecommissionRegionServers[0]</li></div><div><li>org.apache.hadoop.hbase.client.TestAsyncDecommissionAdminApi.testAsyncDecommissionRegionServers[1]</li></div><div><li>org.apache.hadoop.hbase.client.TestResultScannerCursor.testHeartbeatWithSparseFilterReversed</li></div><div><li>org.apache.hadoop.hbase.client.replication.TestReplicationAdminWithClusters.testDisableAndEnableReplication</li></div><div><li>org.apache.hadoop.hbase.client.replication.TestReplicationAdminWithClusters.org.apache.hadoop.hbase.client.replication.TestReplicationAdminWithClusters</li></div><div><li>org.apache.hadoop.hbase.client.replication.TestReplicationAdminWithClusters.org.apache.hadoop.hbase.client.replication.TestReplicationAdminWithClusters</li></div><div><li>org.apache.hadoop.hbase.master.TestMasterMetricsWrapper.testOfflineRegion</li></div><div><li>org.apache.hadoop.hbase.master.locking.TestLockManager.testMasterLockAcquireTimeout</li></div><div><li>org.apache.hadoop.hbase.namespace.TestNamespaceAuditor.testRegionMerge</li></div><div><li>org.apache.hadoop.hbase.regionserver.TestCleanupMetaWAL.testCleanupMetaWAL</li></div><div><li>org.apache.hadoop.hbase.replication.multiwal.TestReplicationEndpointWithMultipleAsyncWAL.testInterClusterReplication</li></div><div><li>org.apache.hadoop.hbase.replication.multiwal.TestReplicationEndpointWithMultipleWAL.testCustomReplicationEndpoint</li></div><div><li>org.apache.hadoop.hbase.replication.regionserver.TestDrainReplicationQueuesForStandBy.test</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.testExportWithTargetName</li></div><div><li>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.testExportFileSystemState</li></div><div><li>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.testConsecutiveExports</li></div><div><li>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.testEmptyExportFileSystemState</li></div><div><li>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.testExportFileSystemStateWithSkipTmp</li></div><div><li>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.testExportRetry</li></div><div><li>org.apache.hadoop.hbase.regionserver.TestCompactionWithCoprocessor.testStopStartCompaction</li></div><div><li>org.apache.hadoop.hbase.regionserver.TestRegionServerAbortTimeout.org.apache.hadoop.hbase.regionserver.TestRegionServerAbortTimeout</li></div><div><li>org.apache.hadoop.hbase.replication.regionserver.TestReplicator.testReplicatorWithErrors</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.hbase.rest.TestGetAndPutResource.org.apache.hadoop.hbase.rest.TestGetAndPutResource</li></div><div><li>org.apache.hadoop.hbase.rest.TestGetAndPutResource.org.apache.hadoop.hbase.rest.TestGetAndPutResource</li></div><div><li>org.apache.hadoop.hbase.rest.TestGetAndPutResource.testSuffixGlobbingXML</li></div><div><li>org.apache.hadoop.hbase.TestClientOperationTimeout.testPutTimeout</li></div><div><li>org.apache.hadoop.hbase.client.TestSeparateClientZKCluster.testMetaMoveDuringClientZkClusterRestart</li></div><div><li>org.apache.hadoop.hbase.client.replication.TestReplicationAdminWithClusters.org.apache.hadoop.hbase.client.replication.TestReplicationAdminWithClusters</li></div><div><li>org.apache.hadoop.hbase.client.replication.TestReplicationAdminWithClusters.org.apache.hadoop.hbase.client.replication.TestReplicationAdminWithClusters</li></div><div><li>org.apache.hadoop.hbase.io.asyncfs.TestFanOutOneBlockAsyncDFSOutput.testHeartbeat</li></div><div><li>org.apache.hadoop.hbase.master.TestAssignmentManagerMetrics.testRITAssignmentManagerMetrics</li></div><div><li>org.apache.hadoop.hbase.master.TestMasterMetricsWrapper.testOfflineRegion</li></div><div><li>org.apache.hadoop.hbase.master.assignment.TestAssignmentManagerUtil.org.apache.hadoop.hbase.master.assignment.TestAssignmentManagerUtil</li></div><div><li>org.apache.hadoop.hbase.master.cleaner.TestSnapshotFromMaster.testSnapshotHFileArchiving</li></div><div><li>org.apache.hadoop.hbase.master.locking.TestLockProcedure.testMultipleLocks</li></div><div><li>org.apache.hadoop.hbase.namespace.TestNamespaceAuditor.testRegionMerge</li></div><div><li>org.apache.hadoop.hbase.regionserver.TestCleanupMetaWAL.testCleanupMetaWAL</li></div><div><li>org.apache.hadoop.hbase.regionserver.TestCompactionFileNotFound.testSplitAfterRefresh</li></div><div><li>org.apache.hadoop.hbase.regionserver.TestRegionServerNoMaster.org.apache.hadoop.hbase.regionserver.TestRegionServerNoMaster</li></div><div><li>org.apache.hadoop.hbase.regionserver.TestRegionServerNoMaster.org.apache.hadoop.hbase.regionserver.TestRegionServerNoMaster</li></div><div><li>org.apache.hadoop.hbase.replication.TestSerialReplication.testRegionMerge</li></div><div><li>org.apache.hadoop.hbase.replication.multiwal.TestReplicationEndpointWithMultipleAsyncWAL.testCustomReplicationEndpoint</li></div><div><li>org.apache.hadoop.hbase.replication.multiwal.TestReplicationEndpointWithMultipleAsyncWAL.testWALEntryFilterAddValidation</li></div><div><li>org.apache.hadoop.hbase.replication.multiwal.TestReplicationEndpointWithMultipleWAL.testWALEntryFilterAddValidation</li></div><div><li>org.apache.hadoop.hbase.tool.TestCanaryTool.testBasicCanaryWorks</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.testExportFileSystemStateWithSkipTmp</li></div><div><li>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory</li></div><div><li>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory</li></div><div><li>org.apache.hadoop.hbase.rest.TestGetAndPutResource.org.apache.hadoop.hbase.rest.TestGetAndPutResource</li></div><div><li>org.apache.hadoop.hbase.rest.TestGetAndPutResource.org.apache.hadoop.hbase.rest.TestGetAndPutResource</li></div><div><li>org.apache.hadoop.hbase.rsgroup.TestRSGroupsAdmin1.org.apache.hadoop.hbase.rsgroup.TestRSGroupsAdmin1</li></div><div><li>org.apache.hadoop.hbase.rsgroup.TestRSGroupsAdmin1.org.apache.hadoop.hbase.rsgroup.TestRSGroupsAdmin1</li></div><div><li>org.apache.hadoop.hbase.TestClientOperationTimeout.testPutTimeout</li></div><div><li>org.apache.hadoop.hbase.TestInfoServers.testMasterServerReadOnly</li></div><div><li>org.apache.hadoop.hbase.TestInfoServers.testInfoServersStatusPages</li></div><div><li>org.apache.hadoop.hbase.client.TestAsyncDecommissionAdminApi.testAsyncDecommissionRegionServers[0]</li></div><div><li>org.apache.hadoop.hbase.client.TestAsyncDecommissionAdminApi.testAsyncDecommissionRegionServers[1]</li></div><div><li>org.apache.hadoop.hbase.client.TestReplicaWithCluster.testReplicaAndReplication</li></div><div><li>org.apache.hadoop.hbase.client.TestReplicaWithCluster.org.apache.hadoop.hbase.client.TestReplicaWithCluster</li></div><div><li>org.apache.hadoop.hbase.client.TestReplicaWithCluster.org.apache.hadoop.hbase.client.TestReplicaWithCluster</li></div><div><li>org.apache.hadoop.hbase.client.TestTableFavoredNodes.testMergeTable</li></div><div><li>org.apache.hadoop.hbase.client.replication.TestReplicationAdmin.testConcurrentPeerOperations</li></div><div><li>org.apache.hadoop.hbase.client.replication.TestReplicationAdmin.org.apache.hadoop.hbase.client.replication.TestReplicationAdmin</li></div><div><li>org.apache.hadoop.hbase.client.replication.TestReplicationAdmin.org.apache.hadoop.hbase.client.replication.TestReplicationAdmin</li></div><div><li>org.apache.hadoop.hbase.client.replication.TestReplicationAdminWithClusters.org.apache.hadoop.hbase.client.replication.TestReplicationAdminWithClusters</li></div><div><li>org.apache.hadoop.hbase.client.replication.TestReplicationAdminWithClusters.org.apache.hadoop.hbase.client.replication.TestReplicationAdminWithClusters</li></div><div><li>org.apache.hadoop.hbase.master.TestAssignmentManagerMetrics.testRITAssignmentManagerMetrics</li></div><div><li>org.apache.hadoop.hbase.master.TestMasterMetricsWrapper.testOfflineRegion</li></div><div><li>org.apache.hadoop.hbase.master.assignment.TestReportOnlineRegionsRace.testRace</li></div><div><li>org.apache.hadoop.hbase.master.assignment.TestReportRegionStateTransitionFromDeadServer.test</li></div><div><li>org.apache.hadoop.hbase.master.assignment.TestSplitTableRegionProcedure.testSplitTableRegion</li></div><div><li>org.apache.hadoop.hbase.master.assignment.TestSplitTableRegionProcedure.org.apache.hadoop.hbase.master.assignment.TestSplitTableRegionProcedure</li></div><div><li>org.apache.hadoop.hbase.master.assignment.TestSplitTableRegionProcedure.org.apache.hadoop.hbase.master.assignment.TestSplitTableRegionProcedure</li></div><div><li>org.apache.hadoop.hbase.master.cleaner.TestLogsCleaner.testZooKeeperAbortDuringGetListOfReplicators</li></div><div><li>org.apache.hadoop.hbase.master.cleaner.TestSnapshotFromMaster.testAsyncSnapshotWillNotBlockSnapshotHFileCleaner</li></div><div><li>org.apache.hadoop.hbase.master.snapshot.TestSnapshotWhileRSCrashes.test</li></div><div><li>org.apache.hadoop.hbase.namespace.TestNamespaceAuditor.testRegionMerge</li></div><div><li>org.apache.hadoop.hbase.regionserver.TestCompactionFileNotFound.testSplitAfterRefresh</li></div><div><li>org.apache.hadoop.hbase.regionserver.TestShutdownWhileWALBroken.test[0: WAL=asyncfs]</li></div><div><li>org.apache.hadoop.hbase.regionserver.throttle.TestFlushWithThroughputController.testFlushControlForStripedStore</li></div><div><li>org.apache.hadoop.hbase.regionserver.throttle.TestFlushWithThroughputController.testFlushControl</li></div><div><li>org.apache.hadoop.hbase.replication.TestSerialReplication.testRegionMerge</li></div><div><li>org.apache.hadoop.hbase.replication.multiwal.TestReplicationEndpointWithMultipleAsyncWAL.testWALEntryFilterUpdateValidation</li></div><div><li>org.apache.hadoop.hbase.replication.multiwal.TestReplicationEndpointWithMultipleWAL.testWALEntryFilterAddValidation</li></div><div><li>org.apache.hadoop.hbase.tool.TestCanaryTool.testBasicCanaryWorks</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.hbase.master.cleaner.TestSnapshotFromMaster.testSnapshotHFileArchiving</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div><div><li>test timed out after 780 seconds</li></div><div><li>Appears to be stuck in thread RS-EventLoopGroup-5-4</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div><div><li>Interrupt while waiting on Operation: DISABLE, Table Name: default:testtb-testExportFileSystemStateWithSkipTmp, procId: 73</li></div><div><li>test timed out after 780 seconds</li></div><div><li>Appears to be stuck in thread RS-EventLoopGroup-5-2</li></div><div><li>Interrupt while waiting on Operation: DISABLE, Table Name: default:TestRowResource, procId: 133</li></div><div><li>test timed out after 780 seconds</li></div><div><li>Appears to be stuck in thread Default-IPC-NioEventLoopGroup-7-2</li></div><div><li>callTimeout=500, callDuration=705: Call to aa13f1202f42/172.17.0.2:40510 failed on local exception: org.apache.hadoop.hbase.ipc.CallTimeoutException: Call id=2, waitTime=476, rpcTimeout=471 </li></div><div><li>callTimeout=500, callDuration=611: Call to aa13f1202f42/172.17.0.2:40510 failed on local exception: org.apache.hadoop.hbase.ipc.CallTimeoutException: Call id=4, waitTime=505, rpcTimeout=496 </li></div><div><li>callTimeout=500, callDuration=610: Call to aa13f1202f42/172.17.0.2:40510 failed on local exception: org.apache.hadoop.hbase.ipc.CallTimeoutException: Call id=6, waitTime=504, rpcTimeout=496 </li></div><div><li>Could not find region hbase:meta,,1.1588230740 on server aa13f1202f42,42924,1544734710113</li></div><div><li>java.lang.AssertionError
	at org.apache.hadoop.hbase.client.TestAsyncDecommissionAdminApi.testAsyncDecommissionRegionServers(TestAsyncDecommissionAdminApi.java:50)
</li></div><div><li>Interrupt while waiting on Operation: CREATE, Table Name: default:testCreateDeleteTable, procId: 420</li></div><div><li>test timed out after 780 seconds</li></div><div><li>Appears to be stuck in thread Socket Reader #1 for port 44273</li></div><div><li>Failed after attempts=3, exceptions:
Thu Dec 13 20:47:13 UTC 2018, RpcRetryingCaller{globalStartTime=1544734030252, pause=100, maxAttempts=3}, org.apache.hadoop.hbase.client.RetriesExhaustedException: Failed after attempts=3, exceptions:
Thu Dec 13 20:47:13 UTC 2018, RpcRetryingCaller{globalStartTime=1544734033019, pause=100, maxAttempts=3}, java.io.IOException: Call to aa13f1202f42/172.17.0.2:339</li></div><div><li>Failed after attempts=7, exceptions:
Thu Dec 13 20:47:16 UTC 2018, RpcRetryingCaller{globalStartTime=1544734036918, pause=100, maxAttempts=7}, java.net.ConnectException: Call to aa13f1202f42/172.17.0.2:33957 failed on connection exception: org.apache.hbase.thirdparty.io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: aa13f1202f42/172.17.0.2:33957
Thu Dec 13 20:47:17 UT</li></div><div><li>org.apache.hadoop.hbase.client.DoNotRetryRegionException: 53f3010b9ac974309c859f6c84f19978 is no OPEN; state=SPLITTING
 at org.apache.hadoop.hbase.master.assignment.RegionStateNode.checkOnline(RegionStateNode.java:291)
 at org.apache.hadoop.hbase.master.procedure.AbstractStateMachineTableProcedure.checkOnline(AbstractStateMachineTableProcedure.java:191)
 at org.apache.hadoop.hbase.master.assignmen</li></div><div><li>The procedure 11 is still running</li></div><div><li>test timed out after 780 seconds</li></div><div><li>Appears to be stuck in thread DataXceiver for client DFSClient_NONMAPREDUCE_-2059134530_13 at /127.0.0.1:50778 [Receiving block BP-1782796859-172.17.0.2-1544730517464:blk_1073741829_1005]</li></div><div><li>test timed out after 780 seconds</li></div><div><li>Appears to be stuck in thread Socket Reader #1 for port 41426</li></div><div><li>java.util.concurrent.TimeoutException: The procedure 4 is still running</li></div><div><li>test timed out after 30000 milliseconds</li></div><div><li>Shutting down</li></div><div><li>Failed after attempts=16, exceptions:
Thu Dec 13 18:30:31 UTC 2018, RpcRetryingCaller{globalStartTime=1544725831582, pause=100, maxAttempts=16}, java.net.ConnectException: Call to aa13f1202f42/172.17.0.2:45472 failed on connection exception: org.apache.hbase.thirdparty.io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: aa13f1202f42/172.17.0.2:45472
Thu Dec 13 18:30:31 </li></div><div><li>Interrupt while waiting on Operation: DISABLE, Table Name: default:testRecoveryAndDoubleExecution, procId: 97</li></div><div><li>test timed out after 780 seconds</li></div><div><li>Appears to be stuck in thread Socket Reader #1 for port 35125</li></div><div><li>Waiting timed out after [10,000] msec</li></div><div><li>Archived hfiles [] and table hfiles [d19babf6af024a99b78e5872118e655b] is missing snapshot file:f70ee7eee54d45b698eb779d4982e631</li></div><div><li>java.lang.AssertionError
	at org.apache.hadoop.hbase.master.locking.TestLockManager.testMasterLockAcquireTimeout(TestLockManager.java:137)
</li></div><div><li>java.lang.AssertionError
	at org.apache.hadoop.hbase.master.locking.TestLockProcedure.testMultipleLocks(TestLockProcedure.java:337)
</li></div><div><li>The procedure 305 is still running</li></div><div><li>Waiting timed out after [10,000] msec</li></div><div><li>Waiting timed out after [20,000] msec</li></div><div><li>Shutting down</li></div><div><li>expected:&lt;2&gt; but was:&lt;1&gt;</li></div><div><li>Shutting down</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.hbase.regionserver.TestRegionServerNoMaster.afterClass(TestRegionServerNoMaster.java:147)
</li></div><div><li>Heartbeat messages are enabled, exceptions should NOT be thrown. Exception trace:org.apache.hadoop.hbase.client.RetriesExhaustedException: Failed after attempts=16, exceptions:
Thu Dec 13 17:28:44 UTC 2018, null, java.net.SocketTimeoutException: callTimeout=1000, callDuration=1015: Call to aa13f1202f42/172.17.0.2:38830 failed on local exception: org.apache.hadoop.hbase.ipc.CallTimeoutException: Ca</li></div><div><li>java.lang.AssertionError
	at org.apache.hadoop.hbase.regionserver.throttle.TestFlushWithThroughputController.testFlushWithThroughputLimit(TestFlushWithThroughputController.java:154)
	at org.apache.hadoop.hbase.regionserver.throttle.TestFlushWithThroughputController.testFlushControl(TestFlushWithThroughputController.java:160)
</li></div><div><li>Waiting timed out after [3,000] msec Still waiting for log roll on regionservers: [aa13f1202f42,33049,1544741387726, aa13f1202f42,41438,1544741387864]</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div><div><li>test timed out after 780 seconds</li></div><div><li>Appears to be stuck in thread DataXceiver for client DFSClient_NONMAPREDUCE_796778454_13 at /127.0.0.1:48156 [Receiving block BP-986296105-172.17.0.2-1544739929423:blk_1073741829_1005]</li></div><div><li>Could not find region testAsyncDecommissionRegionServers,,1544727683211.f3c81ec9f43494af126ad1353a946ba5. on server 2989394c6186,45427,1544727675228</li></div><div><li>java.lang.AssertionError
	at org.apache.hadoop.hbase.client.TestAsyncDecommissionAdminApi.testAsyncDecommissionRegionServers(TestAsyncDecommissionAdminApi.java:50)
</li></div><div><li>Failed after attempts=16, exceptions:
Thu Dec 13 12:51:01 CST 2018, null, java.net.SocketTimeoutException: callTimeout=4000, callDuration=4437: Call to 2989394c6186/172.17.0.2:46558 failed on local exception: org.apache.hadoop.hbase.ipc.CallTimeoutException: Call id=20, waitTime=4005, rpcTimeout=3999 row '' on table 'TestScanCursor' at region=TestScanCursor,,1544727052618.65e082dc1888a6f87000104f3</li></div><div><li>Interrupt while waiting on Operation: REMOVE_REPLICATION_PEER, peerId: 2</li></div><div><li>test timed out after 780 seconds</li></div><div><li>Appears to be stuck in thread Socket Reader #1 for port 39881</li></div><div><li>test timed out after 30000 milliseconds</li></div><div><li>java.lang.AssertionError
	at org.apache.hadoop.hbase.master.locking.TestLockManager.testMasterLockAcquireTimeout(TestLockManager.java:140)
</li></div><div><li>The procedure 298 is still running</li></div><div><li>Waiting timed out after [10,000] msec</li></div><div><li>Waiting timed out after [30,000] msec Failed to replicate all edits, expected = 2500 replicated = 1200</li></div><div><li>Waiting timed out after [3,000] msec Still waiting for log roll on regionservers: [2989394c6186,38682,1544730576524, 2989394c6186,36052,1544730576640]</li></div><div><li>Index: 0, Size: 0</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div><div><li>expected:&lt;1&gt; but was:&lt;2&gt;</li></div><div><li>java.util.concurrent.TimeoutException: The procedure 16 is still running</li></div><div><li>We did not replicate enough rows expected:&lt;10&gt; but was:&lt;9&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>test timed out after 780 seconds</li></div><div><li>Appears to be stuck in thread DataXceiver for client DFSClient_NONMAPREDUCE_-1285145818_13 at /127.0.0.1:46696 [Receiving block BP-454245857-172.17.0.2-1544769957441:blk_1073741829_1005]</li></div><div><li>Interrupt while waiting on Operation: CREATE, Table Name: default:TestRowResource, procId: 158</li></div><div><li>callTimeout=500, callDuration=675: Call to 2e1809059360/172.17.0.2:44717 failed on local exception: org.apache.hadoop.hbase.ipc.CallTimeoutException: Call id=2, waitTime=405, rpcTimeout=403 </li></div><div><li>Failed after attempts=3, exceptions:
Fri Dec 14 03:12:06 UTC 2018, RpcRetryingCaller{globalStartTime=1544757125362, pause=100, maxAttempts=3}, java.io.IOException: Call to 2e1809059360/172.17.0.2:33418 failed on local exception: org.apache.hadoop.hbase.ipc.FailedServerException: This server is in the failed servers list: 2e1809059360/172.17.0.2:33418
Fri Dec 14 03:12:06 UTC 2018, RpcRetryingCaller</li></div><div><li>test timed out after 780 seconds</li></div><div><li>Appears to be stuck in thread Socket Reader #1 for port 42095</li></div><div><li>java.io.IOException: stream already broken</li></div><div><li>java.util.concurrent.TimeoutException: The procedure 4 is still running</li></div><div><li>test timed out after 30000 milliseconds</li></div><div><li>Shutting down</li></div><div><li>Archived hfiles [] and table hfiles [9d5e213e5864454cab21c45ffa53a74c] is missing snapshot file:3ce2a8bd83234eadbdac4dde8c4757e3</li></div><div><li>java.lang.AssertionError
	at org.apache.hadoop.hbase.master.locking.TestLockProcedure.testMultipleLocks(TestLockProcedure.java:322)
</li></div><div><li>The procedure 305 is still running</li></div><div><li>Waiting timed out after [10,000] msec</li></div><div><li>Waiting timed out after [20,000] msec</li></div><div><li>Shutting down</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.hbase.regionserver.TestRegionServerNoMaster.afterClass(TestRegionServerNoMaster.java:147)
</li></div><div><li>The procedure 119 is still running</li></div><div><li>Waiting timed out after [3,000] msec Still waiting for log roll on regionservers: [2e1809059360,43360,1544760480632, 2e1809059360,38497,1544760480733]</li></div><div><li>Waiting timed out after [3,000] msec Still waiting for log roll on regionservers: [2e1809059360,43360,1544760480632, 2e1809059360,38497,1544760480733]</li></div><div><li>Waiting timed out after [3,000] msec Still waiting for log roll on regionservers: [2e1809059360,39227,1544760460417, 2e1809059360,42636,1544760460508]</li></div><div><li>expected:&lt;0&gt; but was:&lt;3&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>Interrupt while waiting on Operation: DISABLE, Table Name: default:testtb-testExportFileSystemStateWithSkipTmp, procId: 76</li></div><div><li>test timed out after 780 seconds</li></div><div><li>Appears to be stuck in thread RS-EventLoopGroup-5-2</li></div><div><li>test timed out after 780 seconds</li></div><div><li>Appears to be stuck in thread DataXceiver for client DFSClient_NONMAPREDUCE_1454281731_13 at /127.0.0.1:45014 [Receiving block BP-1052642045-172.17.0.2-1544795371369:blk_1073741829_1005]</li></div><div><li>test timed out after 780 seconds</li></div><div><li>Appears to be stuck in thread Default-IPC-NioEventLoopGroup-7-2</li></div><div><li>callTimeout=500, callDuration=622: Call to 763842c8b1e9/172.17.0.2:44369 failed on local exception: org.apache.hadoop.hbase.ipc.CallTimeoutException: Call id=2, waitTime=477, rpcTimeout=470 </li></div><div><li>Read timed out</li></div><div><li>Read timed out</li></div><div><li>Could not find region hbase:meta,,1.1588230740 on server 763842c8b1e9,46584,1544778146978</li></div><div><li>java.lang.AssertionError
	at org.apache.hadoop.hbase.client.TestAsyncDecommissionAdminApi.testAsyncDecommissionRegionServers(TestAsyncDecommissionAdminApi.java:50)
</li></div><div><li>Interrupt while waiting on Operation: DISABLE, Table Name: default:testReplicaAndReplication, procId: 163</li></div><div><li>test timed out after 780 seconds</li></div><div><li>Appears to be stuck in thread Socket Reader #1 for port 34367</li></div><div><li>The procedure 11 is still running</li></div><div><li>java.lang.InterruptedException
	at org.apache.hadoop.hbase.client.replication.TestReplicationAdmin.testConcurrentPeerOperations(TestReplicationAdmin.java:169)
</li></div><div><li>test timed out after 780 seconds</li></div><div><li>Appears to be stuck in thread DataXceiver for client DFSClient_NONMAPREDUCE_-747941751_13 at /127.0.0.1:35268 [Receiving block BP-663130118-172.17.0.2-1544774083888:blk_1073741829_1005]</li></div><div><li>test timed out after 780 seconds</li></div><div><li>Appears to be stuck in thread Socket Reader #1 for port 40278</li></div><div><li>java.util.concurrent.TimeoutException: The procedure 4 is still running</li></div><div><li>test timed out after 30000 milliseconds</li></div><div><li>Waiting timed out after [10,000] msec</li></div><div><li>Waiting timed out after [30,000] msec</li></div><div><li>Interrupt while waiting on Operation: DISABLE, Table Name: default:testSplitTableRegion, procId: 129</li></div><div><li>test timed out after 780 seconds</li></div><div><li>Appears to be stuck in thread Socket Reader #1 for port 35163</li></div><div><li>test timed out after 10000 milliseconds</li></div><div><li>Waiting timed out after [10,000] msec</li></div><div><li>Waiting timed out after [10,000] msec</li></div><div><li>The procedure 298 is still running</li></div><div><li>Waiting timed out after [20,000] msec</li></div><div><li>Waiting timed out after [30,000] msec Failover is not finished yet</li></div><div><li>java.lang.AssertionError
	at org.apache.hadoop.hbase.regionserver.throttle.TestFlushWithThroughputController.testFlushWithThroughputLimit(TestFlushWithThroughputController.java:154)
	at org.apache.hadoop.hbase.regionserver.throttle.TestFlushWithThroughputController.testFlushControlForStripedStore(TestFlushWithThroughputController.java:227)
</li></div><div><li>java.lang.AssertionError
	at org.apache.hadoop.hbase.regionserver.throttle.TestFlushWithThroughputController.testFlushWithThroughputLimit(TestFlushWithThroughputController.java:154)
	at org.apache.hadoop.hbase.regionserver.throttle.TestFlushWithThroughputController.testFlushControl(TestFlushWithThroughputController.java:160)
</li></div><div><li>The procedure 119 is still running</li></div><div><li>Waiting timed out after [3,000] msec Still waiting for log roll on regionservers: [763842c8b1e9,40228,1544782314317, 763842c8b1e9,33177,1544782314404]</li></div><div><li>Waiting timed out after [3,000] msec Still waiting for log roll on regionservers: [763842c8b1e9,38727,1544782313236]</li></div><div><li>expected:&lt;0&gt; but was:&lt;3&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Archived hfiles [] and table hfiles [b4d5828ca83d466680bae3f5474a8ea5] is missing snapshot file:d5d1c956cd3c4a3a9a537166c2524490</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.hbase.coprocessor.example.TestWriteHeavyIncrementObserverWithMemStoreCompaction.test</div></li><li><div>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory</div></li><li><div>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory</div></li><li><div>org.apache.hadoop.hbase.rest.TestGetAndPutResource.org.apache.hadoop.hbase.rest.TestGetAndPutResource</div></li><li><div>org.apache.hadoop.hbase.rest.TestGetAndPutResource.org.apache.hadoop.hbase.rest.TestGetAndPutResource</div></li><li><div>org.apache.hadoop.hbase.rest.TestGetAndPutResource.testSingleCellGetPutPB</div></li><li><div>org.apache.hadoop.hbase.rsgroup.TestRSGroupsAdmin1.org.apache.hadoop.hbase.rsgroup.TestRSGroupsAdmin1</div></li><li><div>org.apache.hadoop.hbase.rsgroup.TestRSGroupsAdmin1.org.apache.hadoop.hbase.rsgroup.TestRSGroupsAdmin1</div></li><li><div>org.apache.hadoop.hbase.TestClientOperationTimeout.testPutTimeout</div></li><li><div>org.apache.hadoop.hbase.TestClientOperationTimeout.testGetTimeout</div></li><li><div>org.apache.hadoop.hbase.TestClientOperationTimeout.testScanTimeout</div></li><li><div>org.apache.hadoop.hbase.client.TestAsyncDecommissionAdminApi.testAsyncDecommissionRegionServers[0]</div></li><li><div>org.apache.hadoop.hbase.client.TestAsyncDecommissionAdminApi.testAsyncDecommissionRegionServers[1]</div></li><li><div>org.apache.hadoop.hbase.client.TestReplicaWithCluster.testCreateDeleteTable</div></li><li><div>org.apache.hadoop.hbase.client.TestReplicaWithCluster.org.apache.hadoop.hbase.client.TestReplicaWithCluster</div></li><li><div>org.apache.hadoop.hbase.client.TestReplicaWithCluster.org.apache.hadoop.hbase.client.TestReplicaWithCluster</div></li><li><div>org.apache.hadoop.hbase.client.TestSeparateClientZKCluster.testMetaMoveDuringClientZkClusterRestart</div></li><li><div>org.apache.hadoop.hbase.client.TestSeparateClientZKCluster.testBasicOperation</div></li><li><div>org.apache.hadoop.hbase.client.TestSplitOrMergeStatus.testSplitSwitch</div></li><li><div>org.apache.hadoop.hbase.client.TestTableFavoredNodes.testMergeTable</div></li><li><div>org.apache.hadoop.hbase.client.replication.TestReplicationAdmin.org.apache.hadoop.hbase.client.replication.TestReplicationAdmin</div></li><li><div>org.apache.hadoop.hbase.client.replication.TestReplicationAdmin.org.apache.hadoop.hbase.client.replication.TestReplicationAdmin</div></li><li><div>org.apache.hadoop.hbase.client.replication.TestReplicationAdminWithClusters.org.apache.hadoop.hbase.client.replication.TestReplicationAdminWithClusters</div></li><li><div>org.apache.hadoop.hbase.client.replication.TestReplicationAdminWithClusters.org.apache.hadoop.hbase.client.replication.TestReplicationAdminWithClusters</div></li><li><div>org.apache.hadoop.hbase.master.TestAssignmentManagerMetrics.testRITAssignmentManagerMetrics</div></li><li><div>org.apache.hadoop.hbase.master.TestMasterMetricsWrapper.testOfflineRegion</div></li><li><div>org.apache.hadoop.hbase.master.TestMasterRepairMode.testNewCluster</div></li><li><div>org.apache.hadoop.hbase.master.assignment.TestCloseRegionWhileRSCrash.testRetryBackoff</div></li><li><div>org.apache.hadoop.hbase.master.assignment.TestSplitTableRegionProcedure.testRecoveryAndDoubleExecution</div></li><li><div>org.apache.hadoop.hbase.master.assignment.TestSplitTableRegionProcedure.org.apache.hadoop.hbase.master.assignment.TestSplitTableRegionProcedure</div></li><li><div>org.apache.hadoop.hbase.master.assignment.TestSplitTableRegionProcedure.org.apache.hadoop.hbase.master.assignment.TestSplitTableRegionProcedure</div></li><li><div>org.apache.hadoop.hbase.master.cleaner.TestSnapshotFromMaster.testAsyncSnapshotWillNotBlockSnapshotHFileCleaner</div></li><li><div>org.apache.hadoop.hbase.master.cleaner.TestSnapshotFromMaster.testSnapshotHFileArchiving</div></li><li><div>org.apache.hadoop.hbase.master.locking.TestLockManager.testMasterLockAcquireTimeout</div></li><li><div>org.apache.hadoop.hbase.master.locking.TestLockProcedure.testMultipleLocks</div></li><li><div>org.apache.hadoop.hbase.namespace.TestNamespaceAuditor.testRegionMerge</div></li><li><div>org.apache.hadoop.hbase.regionserver.TestCleanupMetaWAL.testCleanupMetaWAL</div></li><li><div>org.apache.hadoop.hbase.regionserver.TestCompactionFileNotFound.testSplitAfterRefresh</div></li><li><div>org.apache.hadoop.hbase.regionserver.TestHRegionFileSystem.testBlockStoragePolicy</div></li><li><div>org.apache.hadoop.hbase.regionserver.TestMajorCompaction.testTimeBasedMajorCompaction[2]</div></li><li><div>org.apache.hadoop.hbase.regionserver.TestRegionServerNoMaster.org.apache.hadoop.hbase.regionserver.TestRegionServerNoMaster</div></li><li><div>org.apache.hadoop.hbase.regionserver.TestRegionServerNoMaster.org.apache.hadoop.hbase.regionserver.TestRegionServerNoMaster</div></li><li><div>org.apache.hadoop.hbase.regionserver.TestScannerHeartbeatMessages.testHeartbeatWithSparseRowFilter</div></li><li><div>org.apache.hadoop.hbase.regionserver.throttle.TestFlushWithThroughputController.testFlushControl</div></li><li><div>org.apache.hadoop.hbase.replication.TestReplicationEndpoint.testWALEntryFilterUpdateValidation</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.testExportRetry</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.hbase.rest.TestGetAndPutResource.org.apache.hadoop.hbase.rest.TestGetAndPutResource</div></li><li><div>org.apache.hadoop.hbase.rest.TestGetAndPutResource.org.apache.hadoop.hbase.rest.TestGetAndPutResource</div></li><li><div>org.apache.hadoop.hbase.client.TestAsyncDecommissionAdminApi.testAsyncDecommissionRegionServers[0]</div></li><li><div>org.apache.hadoop.hbase.client.TestAsyncDecommissionAdminApi.testAsyncDecommissionRegionServers[1]</div></li><li><div>org.apache.hadoop.hbase.client.TestResultScannerCursor.testHeartbeatWithSparseFilterReversed</div></li><li><div>org.apache.hadoop.hbase.client.replication.TestReplicationAdminWithClusters.testDisableAndEnableReplication</div></li><li><div>org.apache.hadoop.hbase.client.replication.TestReplicationAdminWithClusters.org.apache.hadoop.hbase.client.replication.TestReplicationAdminWithClusters</div></li><li><div>org.apache.hadoop.hbase.client.replication.TestReplicationAdminWithClusters.org.apache.hadoop.hbase.client.replication.TestReplicationAdminWithClusters</div></li><li><div>org.apache.hadoop.hbase.master.TestMasterMetricsWrapper.testOfflineRegion</div></li><li><div>org.apache.hadoop.hbase.master.locking.TestLockManager.testMasterLockAcquireTimeout</div></li><li><div>org.apache.hadoop.hbase.namespace.TestNamespaceAuditor.testRegionMerge</div></li><li><div>org.apache.hadoop.hbase.regionserver.TestCleanupMetaWAL.testCleanupMetaWAL</div></li><li><div>org.apache.hadoop.hbase.replication.multiwal.TestReplicationEndpointWithMultipleAsyncWAL.testInterClusterReplication</div></li><li><div>org.apache.hadoop.hbase.replication.multiwal.TestReplicationEndpointWithMultipleWAL.testCustomReplicationEndpoint</div></li><li><div>org.apache.hadoop.hbase.replication.regionserver.TestDrainReplicationQueuesForStandBy.test</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.hbase.regionserver.TestCompactionWithCoprocessor.testStopStartCompaction</div></li><li><div>org.apache.hadoop.hbase.regionserver.TestRegionServerAbortTimeout.org.apache.hadoop.hbase.regionserver.TestRegionServerAbortTimeout</div></li><li><div>org.apache.hadoop.hbase.replication.regionserver.TestReplicator.testReplicatorWithErrors</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.hbase.rest.TestGetAndPutResource.org.apache.hadoop.hbase.rest.TestGetAndPutResource</div></li><li><div>org.apache.hadoop.hbase.rest.TestGetAndPutResource.org.apache.hadoop.hbase.rest.TestGetAndPutResource</div></li><li><div>org.apache.hadoop.hbase.rest.TestGetAndPutResource.testSuffixGlobbingXML</div></li><li><div>org.apache.hadoop.hbase.TestClientOperationTimeout.testPutTimeout</div></li><li><div>org.apache.hadoop.hbase.client.TestSeparateClientZKCluster.testMetaMoveDuringClientZkClusterRestart</div></li><li><div>org.apache.hadoop.hbase.client.replication.TestReplicationAdminWithClusters.org.apache.hadoop.hbase.client.replication.TestReplicationAdminWithClusters</div></li><li><div>org.apache.hadoop.hbase.client.replication.TestReplicationAdminWithClusters.org.apache.hadoop.hbase.client.replication.TestReplicationAdminWithClusters</div></li><li><div>org.apache.hadoop.hbase.io.asyncfs.TestFanOutOneBlockAsyncDFSOutput.testHeartbeat</div></li><li><div>org.apache.hadoop.hbase.master.TestAssignmentManagerMetrics.testRITAssignmentManagerMetrics</div></li><li><div>org.apache.hadoop.hbase.master.TestMasterMetricsWrapper.testOfflineRegion</div></li><li><div>org.apache.hadoop.hbase.master.assignment.TestAssignmentManagerUtil.org.apache.hadoop.hbase.master.assignment.TestAssignmentManagerUtil</div></li><li><div>org.apache.hadoop.hbase.master.cleaner.TestSnapshotFromMaster.testSnapshotHFileArchiving</div></li><li><div>org.apache.hadoop.hbase.master.locking.TestLockProcedure.testMultipleLocks</div></li><li><div>org.apache.hadoop.hbase.namespace.TestNamespaceAuditor.testRegionMerge</div></li><li><div>org.apache.hadoop.hbase.regionserver.TestCleanupMetaWAL.testCleanupMetaWAL</div></li><li><div>org.apache.hadoop.hbase.regionserver.TestCompactionFileNotFound.testSplitAfterRefresh</div></li><li><div>org.apache.hadoop.hbase.regionserver.TestRegionServerNoMaster.org.apache.hadoop.hbase.regionserver.TestRegionServerNoMaster</div></li><li><div>org.apache.hadoop.hbase.regionserver.TestRegionServerNoMaster.org.apache.hadoop.hbase.regionserver.TestRegionServerNoMaster</div></li><li><div>org.apache.hadoop.hbase.replication.TestSerialReplication.testRegionMerge</div></li><li><div>org.apache.hadoop.hbase.replication.multiwal.TestReplicationEndpointWithMultipleAsyncWAL.testCustomReplicationEndpoint</div></li><li><div>org.apache.hadoop.hbase.replication.multiwal.TestReplicationEndpointWithMultipleAsyncWAL.testWALEntryFilterAddValidation</div></li><li><div>org.apache.hadoop.hbase.replication.multiwal.TestReplicationEndpointWithMultipleWAL.testWALEntryFilterAddValidation</div></li><li><div>org.apache.hadoop.hbase.tool.TestCanaryTool.testBasicCanaryWorks</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.testExportFileSystemStateWithSkipTmp</div></li><li><div>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory</div></li><li><div>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory</div></li><li><div>org.apache.hadoop.hbase.rest.TestGetAndPutResource.org.apache.hadoop.hbase.rest.TestGetAndPutResource</div></li><li><div>org.apache.hadoop.hbase.rest.TestGetAndPutResource.org.apache.hadoop.hbase.rest.TestGetAndPutResource</div></li><li><div>org.apache.hadoop.hbase.rsgroup.TestRSGroupsAdmin1.org.apache.hadoop.hbase.rsgroup.TestRSGroupsAdmin1</div></li><li><div>org.apache.hadoop.hbase.rsgroup.TestRSGroupsAdmin1.org.apache.hadoop.hbase.rsgroup.TestRSGroupsAdmin1</div></li><li><div>org.apache.hadoop.hbase.TestClientOperationTimeout.testPutTimeout</div></li><li><div>org.apache.hadoop.hbase.TestInfoServers.testMasterServerReadOnly</div></li><li><div>org.apache.hadoop.hbase.TestInfoServers.testInfoServersStatusPages</div></li><li><div>org.apache.hadoop.hbase.client.TestAsyncDecommissionAdminApi.testAsyncDecommissionRegionServers[0]</div></li><li><div>org.apache.hadoop.hbase.client.TestAsyncDecommissionAdminApi.testAsyncDecommissionRegionServers[1]</div></li><li><div>org.apache.hadoop.hbase.client.TestReplicaWithCluster.testReplicaAndReplication</div></li><li><div>org.apache.hadoop.hbase.client.TestReplicaWithCluster.org.apache.hadoop.hbase.client.TestReplicaWithCluster</div></li><li><div>org.apache.hadoop.hbase.client.TestReplicaWithCluster.org.apache.hadoop.hbase.client.TestReplicaWithCluster</div></li><li><div>org.apache.hadoop.hbase.client.TestTableFavoredNodes.testMergeTable</div></li><li><div>org.apache.hadoop.hbase.client.replication.TestReplicationAdmin.testConcurrentPeerOperations</div></li><li><div>org.apache.hadoop.hbase.client.replication.TestReplicationAdmin.org.apache.hadoop.hbase.client.replication.TestReplicationAdmin</div></li><li><div>org.apache.hadoop.hbase.client.replication.TestReplicationAdmin.org.apache.hadoop.hbase.client.replication.TestReplicationAdmin</div></li><li><div>org.apache.hadoop.hbase.client.replication.TestReplicationAdminWithClusters.org.apache.hadoop.hbase.client.replication.TestReplicationAdminWithClusters</div></li><li><div>org.apache.hadoop.hbase.client.replication.TestReplicationAdminWithClusters.org.apache.hadoop.hbase.client.replication.TestReplicationAdminWithClusters</div></li><li><div>org.apache.hadoop.hbase.master.TestAssignmentManagerMetrics.testRITAssignmentManagerMetrics</div></li><li><div>org.apache.hadoop.hbase.master.TestMasterMetricsWrapper.testOfflineRegion</div></li><li><div>org.apache.hadoop.hbase.master.assignment.TestReportOnlineRegionsRace.testRace</div></li><li><div>org.apache.hadoop.hbase.master.assignment.TestReportRegionStateTransitionFromDeadServer.test</div></li><li><div>org.apache.hadoop.hbase.master.assignment.TestSplitTableRegionProcedure.testSplitTableRegion</div></li><li><div>org.apache.hadoop.hbase.master.assignment.TestSplitTableRegionProcedure.org.apache.hadoop.hbase.master.assignment.TestSplitTableRegionProcedure</div></li><li><div>org.apache.hadoop.hbase.master.assignment.TestSplitTableRegionProcedure.org.apache.hadoop.hbase.master.assignment.TestSplitTableRegionProcedure</div></li><li><div>org.apache.hadoop.hbase.master.cleaner.TestLogsCleaner.testZooKeeperAbortDuringGetListOfReplicators</div></li><li><div>org.apache.hadoop.hbase.master.cleaner.TestSnapshotFromMaster.testAsyncSnapshotWillNotBlockSnapshotHFileCleaner</div></li><li><div>org.apache.hadoop.hbase.master.snapshot.TestSnapshotWhileRSCrashes.test</div></li><li><div>org.apache.hadoop.hbase.namespace.TestNamespaceAuditor.testRegionMerge</div></li><li><div>org.apache.hadoop.hbase.regionserver.TestCompactionFileNotFound.testSplitAfterRefresh</div></li><li><div>org.apache.hadoop.hbase.regionserver.TestShutdownWhileWALBroken.test[0: WAL=asyncfs]</div></li><li><div>org.apache.hadoop.hbase.regionserver.throttle.TestFlushWithThroughputController.testFlushControlForStripedStore</div></li><li><div>org.apache.hadoop.hbase.regionserver.throttle.TestFlushWithThroughputController.testFlushControl</div></li><li><div>org.apache.hadoop.hbase.replication.TestSerialReplication.testRegionMerge</div></li><li><div>org.apache.hadoop.hbase.replication.multiwal.TestReplicationEndpointWithMultipleAsyncWAL.testWALEntryFilterUpdateValidation</div></li><li><div>org.apache.hadoop.hbase.replication.multiwal.TestReplicationEndpointWithMultipleWAL.testWALEntryFilterAddValidation</div></li><li><div>org.apache.hadoop.hbase.tool.TestCanaryTool.testBasicCanaryWorks</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.hbase.master.cleaner.TestSnapshotFromMaster.testSnapshotHFileArchiving</div></li></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="hive"><div style="font-weight:bold;" class="panel-heading">HIVE<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Alisha)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>9925eb1087fd8f09e93a444bfb79471680b80c64</div><div><b>Last Run: </b>11-12-2018 15:00 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC UBUNTU18</th><th>X86 UBUNTU18</th><th>PPC RHEL72</th><th>X86 RHEL72</th><th>PPC RHEL75</th><th>X86 RHEL75</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 7693</div><div>Failed Count : 3</div><div>Skipped Count : 246</div></td><td><div>Total Count : 7693</div><div>Failed Count : 1</div><div>Skipped Count : 246</div></td><td><div>Total Count : 7696</div><div>Failed Count : 3</div><div>Skipped Count : 246</div></td><td><div>Total Count : 7696</div><div>Failed Count : 3</div><div>Skipped Count : 246</div></td><td><div>Total Count : 7696</div><div>Failed Count : 1</div><div>Skipped Count : 246</div></td><td><div>Total Count : 7696</div><div>Failed Count : 3</div><div>Skipped Count : 246</div></td><td><div>Total Count : 7696</div><div>Failed Count : 3</div><div>Skipped Count : 246</div></td><td><div>Total Count : 7666</div><div>Failed Count : 1</div><div>Skipped Count : 246</div></td></tr><tr><td>Result</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hive.hcatalog.pig.TestAvroHCatStorer.testWriteDecimalX</li></div><div><li>org.apache.hive.hcatalog.pig.TestAvroHCatStorer.testWriteDecimal</li></div><div><li>org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskCommunicator.testFinishableStateUpdateFailure</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskCommunicator.testFinishableStateUpdateFailure</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hive.hcatalog.pig.TestAvroHCatStorer.testWriteDecimalX</li></div><div><li>org.apache.hive.hcatalog.pig.TestAvroHCatStorer.testWriteDecimal</li></div><div><li>org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskCommunicator.testFinishableStateUpdateFailure</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hive.hcatalog.pig.TestAvroHCatStorer.testWriteDecimalX</li></div><div><li>org.apache.hive.hcatalog.pig.TestAvroHCatStorer.testWriteDecimal</li></div><div><li>org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskCommunicator.testFinishableStateUpdateFailure</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskCommunicator.testFinishableStateUpdateFailure</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hive.hcatalog.pig.TestAvroHCatStorer.testWriteDecimalX</li></div><div><li>org.apache.hive.hcatalog.pig.TestAvroHCatStorer.testWriteDecimal</li></div><div><li>org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskCommunicator.testFinishableStateUpdateFailure</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hive.hcatalog.pig.TestAvroHCatStorer.testWriteDecimalX</li></div><div><li>org.apache.hive.hcatalog.pig.TestAvroHCatStorer.testWriteDecimal</li></div><div><li>org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskCommunicator.testFinishableStateUpdateFailure</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskCommunicator.testFinishableStateUpdateFailure</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"><div><li>Failed to execute "create table junitTypeTest1(f1 decimal(2)) stored as AVRO TBLPROPERTIES ('transactional'='false')". Driver returned 1 Error: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. java.lang.RuntimeException: MetaException(message:java.lang.NoSuchMethodError org.codehaus.jackson.JsonNode.asInt()I)</li></div><div><li>Failed to execute "create table junitTypeTest1(f1 decimal) stored as AVRO TBLPROPERTIES ('transactional'='false')". Driver returned 1 Error: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. java.lang.RuntimeException: MetaException(message:java.lang.NoSuchMethodError org.codehaus.jackson.JsonNode.asInt()I)</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos$SignableVertexSpec$Builder.setUser(LlapDaemonProtocolProtos.java:4899)
	at org.apache.hadoop.hive.llap.tez.Converters.constructSignableVertexSpec(Converters.java:135)
	at org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.constructSubmitWorkRequest(LlapTaskCommunicator.java:845)
	at org.apac</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>java.lang.NullPointerException
	at org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos$SignableVertexSpec$Builder.setUser(LlapDaemonProtocolProtos.java:4899)
	at org.apache.hadoop.hive.llap.tez.Converters.constructSignableVertexSpec(Converters.java:135)
	at org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.constructSubmitWorkRequest(LlapTaskCommunicator.java:845)
	at org.apac</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Failed to execute "create table junitTypeTest1(f1 decimal(2)) stored as AVRO TBLPROPERTIES ('transactional'='false')". Driver returned 1 Error: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. java.lang.RuntimeException: MetaException(message:java.lang.NoSuchMethodError org.codehaus.jackson.JsonNode.asInt()I)</li></div><div><li>Failed to execute "create table junitTypeTest1(f1 decimal) stored as AVRO TBLPROPERTIES ('transactional'='false')". Driver returned 1 Error: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. java.lang.RuntimeException: MetaException(message:java.lang.NoSuchMethodError org.codehaus.jackson.JsonNode.asInt()I)</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos$SignableVertexSpec$Builder.setUser(LlapDaemonProtocolProtos.java:4899)
	at org.apache.hadoop.hive.llap.tez.Converters.constructSignableVertexSpec(Converters.java:135)
	at org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.constructSubmitWorkRequest(LlapTaskCommunicator.java:845)
	at org.apac</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Failed to execute "create table junitTypeTest1(f1 decimal(2)) stored as AVRO TBLPROPERTIES ('transactional'='false')". Driver returned 1 Error: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. java.lang.RuntimeException: MetaException(message:java.lang.NoSuchMethodError org.codehaus.jackson.JsonNode.asInt()I)</li></div><div><li>Failed to execute "create table junitTypeTest1(f1 decimal) stored as AVRO TBLPROPERTIES ('transactional'='false')". Driver returned 1 Error: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. java.lang.RuntimeException: MetaException(message:java.lang.NoSuchMethodError org.codehaus.jackson.JsonNode.asInt()I)</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos$SignableVertexSpec$Builder.setUser(LlapDaemonProtocolProtos.java:4899)
	at org.apache.hadoop.hive.llap.tez.Converters.constructSignableVertexSpec(Converters.java:135)
	at org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.constructSubmitWorkRequest(LlapTaskCommunicator.java:845)
	at org.apac</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>java.lang.NullPointerException
	at org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos$SignableVertexSpec$Builder.setUser(LlapDaemonProtocolProtos.java:4899)
	at org.apache.hadoop.hive.llap.tez.Converters.constructSignableVertexSpec(Converters.java:135)
	at org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.constructSubmitWorkRequest(LlapTaskCommunicator.java:845)
	at org.apac</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Failed to execute "create table junitTypeTest1(f1 decimal(2)) stored as AVRO TBLPROPERTIES ('transactional'='false')". Driver returned 1 Error: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. java.lang.RuntimeException: MetaException(message:java.lang.NoSuchMethodError org.codehaus.jackson.JsonNode.asInt()I)</li></div><div><li>Failed to execute "create table junitTypeTest1(f1 decimal) stored as AVRO TBLPROPERTIES ('transactional'='false')". Driver returned 1 Error: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. java.lang.RuntimeException: MetaException(message:java.lang.NoSuchMethodError org.codehaus.jackson.JsonNode.asInt()I)</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos$SignableVertexSpec$Builder.setUser(LlapDaemonProtocolProtos.java:4899)
	at org.apache.hadoop.hive.llap.tez.Converters.constructSignableVertexSpec(Converters.java:135)
	at org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.constructSubmitWorkRequest(LlapTaskCommunicator.java:845)
	at org.apac</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Failed to execute "create table junitTypeTest1(f1 decimal(2)) stored as AVRO TBLPROPERTIES ('transactional'='false')". Driver returned 1 Error: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. java.lang.RuntimeException: MetaException(message:java.lang.NoSuchMethodError org.codehaus.jackson.JsonNode.asInt()I)</li></div><div><li>Failed to execute "create table junitTypeTest1(f1 decimal) stored as AVRO TBLPROPERTIES ('transactional'='false')". Driver returned 1 Error: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. java.lang.RuntimeException: MetaException(message:java.lang.NoSuchMethodError org.codehaus.jackson.JsonNode.asInt()I)</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos$SignableVertexSpec$Builder.setUser(LlapDaemonProtocolProtos.java:4899)
	at org.apache.hadoop.hive.llap.tez.Converters.constructSignableVertexSpec(Converters.java:135)
	at org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.constructSubmitWorkRequest(LlapTaskCommunicator.java:845)
	at org.apac</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>java.lang.NullPointerException
	at org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos$SignableVertexSpec$Builder.setUser(LlapDaemonProtocolProtos.java:4899)
	at org.apache.hadoop.hive.llap.tez.Converters.constructSignableVertexSpec(Converters.java:135)
	at org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.constructSubmitWorkRequest(LlapTaskCommunicator.java:845)
	at org.apac</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hive.hcatalog.pig.TestAvroHCatStorer.testWriteDecimalX</div></li><li><div>org.apache.hive.hcatalog.pig.TestAvroHCatStorer.testWriteDecimal</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hive.hcatalog.pig.TestAvroHCatStorer.testWriteDecimalX</div></li><li><div>org.apache.hive.hcatalog.pig.TestAvroHCatStorer.testWriteDecimal</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hive.hcatalog.pig.TestAvroHCatStorer.testWriteDecimalX</div></li><li><div>org.apache.hive.hcatalog.pig.TestAvroHCatStorer.testWriteDecimal</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="kafka"><div style="font-weight:bold;" class="panel-heading">KAFKA<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Prajyot)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/trunk</div><div><b>Last Revision: </b>4fb5520901d9908d2ca16b6f56242b778f6d2f05</div><div><b>Last Run: </b>29-11-2018 19:00 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC UBUNTU18</th><th>X86 UBUNTU18</th><th>PPC RHEL72</th><th>X86 RHEL72</th><th>PPC RHEL75</th><th>X86 RHEL75</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 10253</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 4983</div><div>Failed Count : 0</div><div>Skipped Count : 1</div></td><td><div>Total Count : 10253</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 10253</div><div>Failed Count : 1</div><div>Skipped Count : 5</div></td><td><div>Total Count : 10253</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 10253</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 10253</div><div>Failed Count : 1</div><div>Skipped Count : 5</div></td><td><div>Total Count : 10253</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td></tr><tr><td>Result</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>kafka.api.UserQuotaTest.testThrottledProducerConsumer</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>kafka.api.CustomQuotaCallbackTest.testCustomQuotaCallback</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>java.lang.AssertionError: Client with id=QuotasTestProducer-1 should have been throttled</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>java.lang.AssertionError: Too many quotaLimit calls Map(PRODUCE -&gt; 1, FETCH -&gt; 1, REQUEST -&gt; 3)</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>kafka.api.UserQuotaTest.testThrottledProducerConsumer</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>kafka.api.CustomQuotaCallbackTest.testCustomQuotaCallback</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="knox"><div style="font-weight:bold;" class="panel-heading">KNOX<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Yussuf)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>be7f4f37ea0d716a6767ce41ef9f31fdf9912ff7</div><div><b>Last Run: </b>11-12-2018 01:04 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC UBUNTU18</th><th>X86 UBUNTU18</th><th>PPC RHEL72</th><th>X86 RHEL72</th><th>PPC RHEL75</th><th>X86 RHEL75</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 1104</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1104</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1104</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1104</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1104</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1104</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1104</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1104</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td></tr><tr><td>Result</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="metron"><div style="font-weight:bold;" class="panel-heading">METRON<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Pravin)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>89a2beda4f07911c8b3cd7dee8a2c3426838d161</div><div><b>Last Run: </b>29-11-2018 06:13 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC UBUNTU18</th><th>X86 UBUNTU18</th><th>PPC RHEL72</th><th>X86 RHEL72</th><th>PPC RHEL75</th><th>X86 RHEL75</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 2048</div><div>Failed Count : 14</div><div>Skipped Count : 2</div></td><td><div>Total Count : 2048</div><div>Failed Count : 13</div><div>Skipped Count : 2</div></td><td><div>Total Count : 2048</div><div>Failed Count : 13</div><div>Skipped Count : 2</div></td><td><div>Total Count : 2048</div><div>Failed Count : 13</div><div>Skipped Count : 2</div></td><td><div>Total Count : 2048</div><div>Failed Count : 13</div><div>Skipped Count : 2</div></td><td><div>Total Count : 2048</div><div>Failed Count : 13</div><div>Skipped Count : 2</div></td><td><div>Total Count : 2048</div><div>Failed Count : 13</div><div>Skipped Count : 2</div></td><td><div>Total Count : 2048</div><div>Failed Count : 13</div><div>Skipped Count : 2</div></td></tr><tr><td>Result</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testBytesMatch[0]</li></div><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testStringMatch[0]</li></div><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testBytesMatch[1]</li></div><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testStringMatch[1]</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathStringReturned</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathFormatVariable</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathFormatConstant</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testSingleFileIfNoStreamClosed</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testHandleAttemptsRotateIfStreamClosed</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathMultipleFunctions</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFile</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteMultipleFiles</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFileWithNull</li></div><div><li>org.apache.metron.stellar.dsl.functions.RestFunctionsTest.restGetShouldTimeout</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testBytesMatch[0]</li></div><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testStringMatch[0]</li></div><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testBytesMatch[1]</li></div><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testStringMatch[1]</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathStringReturned</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathFormatVariable</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathFormatConstant</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testSingleFileIfNoStreamClosed</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testHandleAttemptsRotateIfStreamClosed</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathMultipleFunctions</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFile</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteMultipleFiles</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFileWithNull</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testBytesMatch[0]</li></div><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testStringMatch[0]</li></div><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testBytesMatch[1]</li></div><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testStringMatch[1]</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathStringReturned</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathFormatVariable</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathFormatConstant</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testSingleFileIfNoStreamClosed</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testHandleAttemptsRotateIfStreamClosed</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathMultipleFunctions</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFile</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteMultipleFiles</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFileWithNull</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testBytesMatch[0]</li></div><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testStringMatch[0]</li></div><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testBytesMatch[1]</li></div><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testStringMatch[1]</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathStringReturned</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathFormatVariable</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathFormatConstant</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testSingleFileIfNoStreamClosed</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testHandleAttemptsRotateIfStreamClosed</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathMultipleFunctions</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFile</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteMultipleFiles</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFileWithNull</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testBytesMatch[0]</li></div><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testStringMatch[0]</li></div><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testBytesMatch[1]</li></div><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testStringMatch[1]</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathStringReturned</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathFormatVariable</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathFormatConstant</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testSingleFileIfNoStreamClosed</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testHandleAttemptsRotateIfStreamClosed</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathMultipleFunctions</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFile</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteMultipleFiles</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFileWithNull</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testBytesMatch[0]</li></div><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testStringMatch[0]</li></div><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testBytesMatch[1]</li></div><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testStringMatch[1]</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathStringReturned</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathFormatVariable</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathFormatConstant</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testSingleFileIfNoStreamClosed</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testHandleAttemptsRotateIfStreamClosed</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathMultipleFunctions</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFile</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteMultipleFiles</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFileWithNull</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testBytesMatch[0]</li></div><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testStringMatch[0]</li></div><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testBytesMatch[1]</li></div><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testStringMatch[1]</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathStringReturned</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathFormatVariable</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathFormatConstant</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testSingleFileIfNoStreamClosed</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testHandleAttemptsRotateIfStreamClosed</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathMultipleFunctions</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFile</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteMultipleFiles</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFileWithNull</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testBytesMatch[0]</li></div><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testStringMatch[0]</li></div><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testBytesMatch[1]</li></div><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testStringMatch[1]</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathStringReturned</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathFormatVariable</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathFormatConstant</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testSingleFileIfNoStreamClosed</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testHandleAttemptsRotateIfStreamClosed</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathMultipleFunctions</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFile</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteMultipleFiles</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFileWithNull</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"><div><li>Unable to parse BYTEARRAY_MATCHER(pattern, data): Unable to parse: BYTEARRAY_MATCHER(pattern, data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing,pattern=missing</li></div><div><li>Unable to parse BYTEARRAY_MATCHER(pattern, data): Unable to parse: BYTEARRAY_MATCHER(pattern, data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing,pattern=missing</li></div><div><li>Unable to parse BYTEARRAY_MATCHER('2f56abd814bc56420489ca38e7faf8cec3d4', data): Unable to parse: BYTEARRAY_MATCHER('2f56abd814bc56420489ca38e7faf8cec3d4', data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing</li></div><div><li>Unable to parse BYTEARRAY_MATCHER('`metron`', data): Unable to parse: BYTEARRAY_MATCHER('`metron`', data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing</li></div><div><li>Unable to parse: TO_UPPER(FORMAT(MAP_GET('key', {'key': 'AbC%s'}), test.key)) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables test.key=test.value</li></div><div><li>Unable to parse: FORMAT('%s/%s/%s', test.key, test.key.2, test.key.3) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables test.key.2=test.value.2,test.key=test.value,test.key.3=test.value.3</li></div><div><li>Unable to parse: FORMAT('/test/folder/') due to: org/apache/http/conn/HttpClientConnectionManager</li></div><div><li>java.lang.NullPointerException
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testSingleFileIfNoStreamClosed(HdfsWriterTest.java:447)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method</li></div><div><li>expected:&lt;1&gt; but was:&lt;0&gt;</li></div><div><li>Unable to parse: FORMAT('%s', test.key) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables test.key=test.value</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFile(HdfsWriterTest.java:313)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:6</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteMultipleFiles(HdfsWriterTest.java:356)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.jav</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFileWithNull(HdfsWriterTest.java:408)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImp</li></div><div><li>expected null, but was:&lt;{get=success}&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Unable to parse BYTEARRAY_MATCHER(pattern, data): Unable to parse: BYTEARRAY_MATCHER(pattern, data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing,pattern=missing</li></div><div><li>Unable to parse BYTEARRAY_MATCHER(pattern, data): Unable to parse: BYTEARRAY_MATCHER(pattern, data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing,pattern=missing</li></div><div><li>Unable to parse BYTEARRAY_MATCHER('2f56abd814bc56420489ca38e7faf8cec3d4', data): Unable to parse: BYTEARRAY_MATCHER('2f56abd814bc56420489ca38e7faf8cec3d4', data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing</li></div><div><li>Unable to parse BYTEARRAY_MATCHER('`metron`', data): Unable to parse: BYTEARRAY_MATCHER('`metron`', data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing</li></div><div><li>Unable to parse: TO_UPPER(FORMAT(MAP_GET('key', {'key': 'AbC%s'}), test.key)) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables test.key=test.value</li></div><div><li>Unable to parse: FORMAT('%s/%s/%s', test.key, test.key.2, test.key.3) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables test.key.2=test.value.2,test.key=test.value,test.key.3=test.value.3</li></div><div><li>Unable to parse: FORMAT('/test/folder/') due to: org/apache/http/conn/HttpClientConnectionManager</li></div><div><li>java.lang.NullPointerException
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testSingleFileIfNoStreamClosed(HdfsWriterTest.java:447)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method</li></div><div><li>expected:&lt;1&gt; but was:&lt;0&gt;</li></div><div><li>Unable to parse: FORMAT('%s', test.key) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables test.key=test.value</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFile(HdfsWriterTest.java:313)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:6</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteMultipleFiles(HdfsWriterTest.java:356)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.jav</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFileWithNull(HdfsWriterTest.java:408)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImp</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Unable to parse BYTEARRAY_MATCHER(pattern, data): Unable to parse: BYTEARRAY_MATCHER(pattern, data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing,pattern=missing</li></div><div><li>Unable to parse BYTEARRAY_MATCHER(pattern, data): Unable to parse: BYTEARRAY_MATCHER(pattern, data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing,pattern=missing</li></div><div><li>Unable to parse BYTEARRAY_MATCHER('2f56abd814bc56420489ca38e7faf8cec3d4', data): Unable to parse: BYTEARRAY_MATCHER('2f56abd814bc56420489ca38e7faf8cec3d4', data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing</li></div><div><li>Unable to parse BYTEARRAY_MATCHER('`metron`', data): Unable to parse: BYTEARRAY_MATCHER('`metron`', data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing</li></div><div><li>Unable to parse: TO_UPPER(FORMAT(MAP_GET('key', {'key': 'AbC%s'}), test.key)) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables test.key=test.value</li></div><div><li>Unable to parse: FORMAT('%s/%s/%s', test.key, test.key.2, test.key.3) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables test.key.2=test.value.2,test.key=test.value,test.key.3=test.value.3</li></div><div><li>Unable to parse: FORMAT('/test/folder/') due to: org/apache/http/conn/HttpClientConnectionManager</li></div><div><li>java.lang.NullPointerException
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testSingleFileIfNoStreamClosed(HdfsWriterTest.java:447)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method</li></div><div><li>expected:&lt;1&gt; but was:&lt;0&gt;</li></div><div><li>Unable to parse: FORMAT('%s', test.key) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables test.key=test.value</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFile(HdfsWriterTest.java:313)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:6</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteMultipleFiles(HdfsWriterTest.java:356)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.jav</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFileWithNull(HdfsWriterTest.java:408)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImp</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Unable to parse BYTEARRAY_MATCHER(pattern, data): Unable to parse: BYTEARRAY_MATCHER(pattern, data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing,pattern=missing</li></div><div><li>Unable to parse BYTEARRAY_MATCHER(pattern, data): Unable to parse: BYTEARRAY_MATCHER(pattern, data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing,pattern=missing</li></div><div><li>Unable to parse BYTEARRAY_MATCHER('2f56abd814bc56420489ca38e7faf8cec3d4', data): Unable to parse: BYTEARRAY_MATCHER('2f56abd814bc56420489ca38e7faf8cec3d4', data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing</li></div><div><li>Unable to parse BYTEARRAY_MATCHER('`metron`', data): Unable to parse: BYTEARRAY_MATCHER('`metron`', data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing</li></div><div><li>Unable to parse: TO_UPPER(FORMAT(MAP_GET('key', {'key': 'AbC%s'}), test.key)) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables test.key=test.value</li></div><div><li>Unable to parse: FORMAT('%s/%s/%s', test.key, test.key.2, test.key.3) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables test.key.2=test.value.2,test.key=test.value,test.key.3=test.value.3</li></div><div><li>Unable to parse: FORMAT('/test/folder/') due to: org/apache/http/conn/HttpClientConnectionManager</li></div><div><li>java.lang.NullPointerException
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testSingleFileIfNoStreamClosed(HdfsWriterTest.java:447)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method</li></div><div><li>expected:&lt;1&gt; but was:&lt;0&gt;</li></div><div><li>Unable to parse: FORMAT('%s', test.key) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables test.key=test.value</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFile(HdfsWriterTest.java:313)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:6</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteMultipleFiles(HdfsWriterTest.java:356)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.jav</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFileWithNull(HdfsWriterTest.java:408)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImp</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Unable to parse BYTEARRAY_MATCHER(pattern, data): Unable to parse: BYTEARRAY_MATCHER(pattern, data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing,pattern=missing</li></div><div><li>Unable to parse BYTEARRAY_MATCHER(pattern, data): Unable to parse: BYTEARRAY_MATCHER(pattern, data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing,pattern=missing</li></div><div><li>Unable to parse BYTEARRAY_MATCHER('2f56abd814bc56420489ca38e7faf8cec3d4', data): Unable to parse: BYTEARRAY_MATCHER('2f56abd814bc56420489ca38e7faf8cec3d4', data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing</li></div><div><li>Unable to parse BYTEARRAY_MATCHER('`metron`', data): Unable to parse: BYTEARRAY_MATCHER('`metron`', data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing</li></div><div><li>Unable to parse: TO_UPPER(FORMAT(MAP_GET('key', {'key': 'AbC%s'}), test.key)) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables test.key=test.value</li></div><div><li>Unable to parse: FORMAT('%s/%s/%s', test.key, test.key.2, test.key.3) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables test.key.2=test.value.2,test.key=test.value,test.key.3=test.value.3</li></div><div><li>Unable to parse: FORMAT('/test/folder/') due to: org/apache/http/conn/HttpClientConnectionManager</li></div><div><li>java.lang.NullPointerException
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testSingleFileIfNoStreamClosed(HdfsWriterTest.java:447)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method</li></div><div><li>expected:&lt;1&gt; but was:&lt;0&gt;</li></div><div><li>Unable to parse: FORMAT('%s', test.key) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables test.key=test.value</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFile(HdfsWriterTest.java:313)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:6</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteMultipleFiles(HdfsWriterTest.java:356)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.jav</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFileWithNull(HdfsWriterTest.java:408)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImp</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Unable to parse BYTEARRAY_MATCHER(pattern, data): Unable to parse: BYTEARRAY_MATCHER(pattern, data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing,pattern=missing</li></div><div><li>Unable to parse BYTEARRAY_MATCHER(pattern, data): Unable to parse: BYTEARRAY_MATCHER(pattern, data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing,pattern=missing</li></div><div><li>Unable to parse BYTEARRAY_MATCHER('2f56abd814bc56420489ca38e7faf8cec3d4', data): Unable to parse: BYTEARRAY_MATCHER('2f56abd814bc56420489ca38e7faf8cec3d4', data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing</li></div><div><li>Unable to parse BYTEARRAY_MATCHER('`metron`', data): Unable to parse: BYTEARRAY_MATCHER('`metron`', data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing</li></div><div><li>Unable to parse: TO_UPPER(FORMAT(MAP_GET('key', {'key': 'AbC%s'}), test.key)) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables test.key=test.value</li></div><div><li>Unable to parse: FORMAT('%s/%s/%s', test.key, test.key.2, test.key.3) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables test.key.2=test.value.2,test.key=test.value,test.key.3=test.value.3</li></div><div><li>Unable to parse: FORMAT('/test/folder/') due to: org/apache/http/conn/HttpClientConnectionManager</li></div><div><li>java.lang.NullPointerException
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testSingleFileIfNoStreamClosed(HdfsWriterTest.java:447)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method</li></div><div><li>expected:&lt;1&gt; but was:&lt;0&gt;</li></div><div><li>Unable to parse: FORMAT('%s', test.key) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables test.key=test.value</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFile(HdfsWriterTest.java:313)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:6</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteMultipleFiles(HdfsWriterTest.java:356)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.jav</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFileWithNull(HdfsWriterTest.java:408)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImp</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Unable to parse BYTEARRAY_MATCHER(pattern, data): Unable to parse: BYTEARRAY_MATCHER(pattern, data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing,pattern=missing</li></div><div><li>Unable to parse BYTEARRAY_MATCHER(pattern, data): Unable to parse: BYTEARRAY_MATCHER(pattern, data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing,pattern=missing</li></div><div><li>Unable to parse BYTEARRAY_MATCHER('2f56abd814bc56420489ca38e7faf8cec3d4', data): Unable to parse: BYTEARRAY_MATCHER('2f56abd814bc56420489ca38e7faf8cec3d4', data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing</li></div><div><li>Unable to parse BYTEARRAY_MATCHER('`metron`', data): Unable to parse: BYTEARRAY_MATCHER('`metron`', data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing</li></div><div><li>Unable to parse: TO_UPPER(FORMAT(MAP_GET('key', {'key': 'AbC%s'}), test.key)) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables test.key=test.value</li></div><div><li>Unable to parse: FORMAT('%s/%s/%s', test.key, test.key.2, test.key.3) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables test.key.2=test.value.2,test.key=test.value,test.key.3=test.value.3</li></div><div><li>Unable to parse: FORMAT('/test/folder/') due to: org/apache/http/conn/HttpClientConnectionManager</li></div><div><li>java.lang.NullPointerException
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testSingleFileIfNoStreamClosed(HdfsWriterTest.java:447)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method</li></div><div><li>expected:&lt;1&gt; but was:&lt;0&gt;</li></div><div><li>Unable to parse: FORMAT('%s', test.key) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables test.key=test.value</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFile(HdfsWriterTest.java:313)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:6</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteMultipleFiles(HdfsWriterTest.java:356)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.jav</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFileWithNull(HdfsWriterTest.java:408)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImp</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Unable to parse BYTEARRAY_MATCHER(pattern, data): Unable to parse: BYTEARRAY_MATCHER(pattern, data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing,pattern=missing</li></div><div><li>Unable to parse BYTEARRAY_MATCHER(pattern, data): Unable to parse: BYTEARRAY_MATCHER(pattern, data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing,pattern=missing</li></div><div><li>Unable to parse BYTEARRAY_MATCHER('2f56abd814bc56420489ca38e7faf8cec3d4', data): Unable to parse: BYTEARRAY_MATCHER('2f56abd814bc56420489ca38e7faf8cec3d4', data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing</li></div><div><li>Unable to parse BYTEARRAY_MATCHER('`metron`', data): Unable to parse: BYTEARRAY_MATCHER('`metron`', data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing</li></div><div><li>Unable to parse: TO_UPPER(FORMAT(MAP_GET('key', {'key': 'AbC%s'}), test.key)) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables test.key=test.value</li></div><div><li>Unable to parse: FORMAT('%s/%s/%s', test.key, test.key.2, test.key.3) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables test.key.2=test.value.2,test.key=test.value,test.key.3=test.value.3</li></div><div><li>Unable to parse: FORMAT('/test/folder/') due to: org/apache/http/conn/HttpClientConnectionManager</li></div><div><li>java.lang.NullPointerException
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testSingleFileIfNoStreamClosed(HdfsWriterTest.java:447)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method</li></div><div><li>expected:&lt;1&gt; but was:&lt;0&gt;</li></div><div><li>Unable to parse: FORMAT('%s', test.key) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables test.key=test.value</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFile(HdfsWriterTest.java:313)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:6</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteMultipleFiles(HdfsWriterTest.java:356)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.jav</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFileWithNull(HdfsWriterTest.java:408)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImp</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.metron.stellar.dsl.functions.RestFunctionsTest.restGetShouldTimeout</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="oozie"><div style="font-weight:bold;" class="panel-heading">OOZIE<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Alisha)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>7ef418f7feafb904c1d2d41d39be52d64a854666</div><div><b>Last Run: </b>29-11-2018 21:58 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC UBUNTU18</th><th>X86 UBUNTU18</th><th>PPC RHEL72</th><th>X86 RHEL72</th><th>PPC RHEL75</th><th>X86 RHEL75</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 3075</div><div>Failed Count : 1</div><div>Skipped Count : 2</div></td><td><div>Total Count : 3075</div><div>Failed Count : 1</div><div>Skipped Count : 2</div></td><td><div>Total Count : 3075</div><div>Failed Count : 3</div><div>Skipped Count : 2</div></td><td><div>Total Count : 3075</div><div>Failed Count : 3</div><div>Skipped Count : 2</div></td><td><div>Total Count : 3075</div><div>Failed Count : 638</div><div>Skipped Count : 2</div></td><td><div>Total Count : 3075</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 3075</div><div>Failed Count : 2</div><div>Skipped Count : 2</div></td><td><div>Total Count : 3075</div><div>Failed Count : 1</div><div>Skipped Count : 2</div></td></tr><tr><td>Result</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"><div><li>org.apache.oozie.test.TestWorkflow.testWorkflowWithStartAndEndCompletesSuccessfully</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.oozie.command.coord.TestCoordActionsKillXCommand.testActionKillCommandActionNumbers</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.oozie.command.coord.TestCoordActionsKillXCommand.testActionKillCommandActionNumbers</li></div><div><li>org.apache.oozie.event.TestEventGeneration.testCoordinatorActionEvent</li></div><div><li>org.apache.oozie.util.TestMetricsInstrumentation.testJMXInstrumentation</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.oozie.command.coord.TestCoordActionInputCheckXCommand.testNone</li></div><div><li>org.apache.oozie.service.TestCallableQueueService.testQueueSizeWithDelayedElements</li></div><div><li>org.apache.oozie.util.TestMetricsInstrumentation.testJMXInstrumentation</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.oozie.TestCoordinatorEngineStreamLog.testCoordLogStreaming</li></div><div><li>org.apache.oozie.TestDagELFunctions.testLastErrorNodeWithRetryFail</li></div><div><li>org.apache.oozie.TestDagELFunctions.testFunctions</li></div><div><li>org.apache.oozie.TestDagELFunctions.testLastErrorNodeWithRetrySucceed</li></div><div><li>org.apache.oozie.TestLocalOozieClientCoord.testHeaderMethods</li></div><div><li>org.apache.oozie.TestLocalOozieClientCoord.testJobMethods</li></div><div><li>org.apache.oozie.TestLocalOozieClientCoord.testJobsOperations</li></div><div><li>org.apache.oozie.TestLocalOozieClientCoord.testGetJobsInfo</li></div><div><li>org.apache.oozie.TestLocalOozieClientCoord.testValidateWSVersion</li></div><div><li>org.apache.oozie.TestLocalOozieClientCoord.testGetOozieUrl</li></div><div><li>org.apache.oozie.TestLocalOozieClientCoord.testReRun2</li></div><div><li>org.apache.oozie.TestLocalOozieClientCoord.testGetProtocolUrl</li></div><div><li>org.apache.oozie.TestV1JobsServletBundleEngine.testGetBundleJobs</li></div><div><li>org.apache.oozie.action.TestActionFailover.testFsFailover</li></div><div><li>org.apache.oozie.action.decision.TestDecisionActionExecutor.testDecision</li></div><div><li>org.apache.oozie.action.email.TestEmailActionExecutor.testDoAuthEmail</li></div><div><li>org.apache.oozie.action.email.TestEmailActionExecutor.testContentType</li></div><div><li>org.apache.oozie.action.email.TestEmailActionExecutor.testValidation</li></div><div><li>org.apache.oozie.action.email.TestEmailActionExecutor.testSetupMethods</li></div><div><li>org.apache.oozie.action.email.TestEmailActionExecutor.testContentTypeDefault</li></div><div><li>org.apache.oozie.action.email.TestEmailActionExecutor.testServerTimeouts</li></div><div><li>org.apache.oozie.action.email.TestEmailActionExecutor.testDoNormalEmail</li></div><div><li>org.apache.oozie.action.email.TestEmailActionExecutor.testHDFSFileAttachment</li></div><div><li>org.apache.oozie.action.email.TestEmailActionExecutor.testLocalFileAttachmentError</li></div><div><li>org.apache.oozie.action.hadoop.TestCredentials.testHbaseCredentials</li></div><div><li>org.apache.oozie.action.hadoop.TestDistCpActionExecutor.testSetupMethods</li></div><div><li>org.apache.oozie.action.hadoop.TestDistCpActionExecutor.testDistCpFile</li></div><div><li>org.apache.oozie.action.hadoop.TestDistcpMain.testJobIDPattern</li></div><div><li>org.apache.oozie.action.hadoop.TestDistcpMain.testMain</li></div><div><li>org.apache.oozie.action.hadoop.TestFSPrepareActions.testMkdir</li></div><div><li>org.apache.oozie.action.hadoop.TestFSPrepareActions.testDeleteWithGlob</li></div><div><li>org.apache.oozie.action.hadoop.TestFSPrepareActions.testForNullScheme</li></div><div><li>org.apache.oozie.action.hadoop.TestFSPrepareActions.testForInvalidScheme</li></div><div><li>org.apache.oozie.action.hadoop.TestFSPrepareActions.testDelete</li></div><div><li>org.apache.oozie.action.hadoop.TestFsActionExecutor.testChmodRelativePath</li></div><div><li>org.apache.oozie.action.hadoop.TestFsActionExecutor.testChgrp</li></div><div><li>org.apache.oozie.action.hadoop.TestFsActionExecutor.testChmod</li></div><div><li>org.apache.oozie.action.hadoop.TestFsActionExecutor.testMkdir</li></div><div><li>org.apache.oozie.action.hadoop.TestFsActionExecutor.testMove</li></div><div><li>org.apache.oozie.action.hadoop.TestFsActionExecutor.testChmodWithGlob</li></div><div><li>org.apache.oozie.action.hadoop.TestFsActionExecutor.testvalidateSameNN</li></div><div><li>org.apache.oozie.action.hadoop.TestFsActionExecutor.testChmodRecursive</li></div><div><li>org.apache.oozie.action.hadoop.TestFsActionExecutor.testSetupMethods</li></div><div><li>org.apache.oozie.action.hadoop.TestFsActionExecutor.testMoveWithGlob</li></div><div><li>org.apache.oozie.action.hadoop.TestFsActionExecutor.testRecovery</li></div><div><li>org.apache.oozie.action.hadoop.TestFsActionExecutor.testDeleteWithGlob</li></div><div><li>org.apache.oozie.action.hadoop.TestFsActionExecutor.testSubmitWithNameNode</li></div><div><li>org.apache.oozie.action.hadoop.TestFsActionExecutor.testChgrpWithGlob</li></div><div><li>org.apache.oozie.action.hadoop.TestFsActionExecutor.testFileSchemeWildcard</li></div><div><li>org.apache.oozie.action.hadoop.TestFsActionExecutor.testMovetoTrash</li></div><div><li>org.apache.oozie.action.hadoop.TestFsActionExecutor.testDeleteHcat</li></div><div><li>org.apache.oozie.action.hadoop.TestFsActionExecutor.testDoOperations</li></div><div><li>org.apache.oozie.action.hadoop.TestFsActionExecutor.testDoOperationsWithNameNodeElement</li></div><div><li>org.apache.oozie.action.hadoop.TestFsActionExecutor.testDelete</li></div><div><li>org.apache.oozie.action.hadoop.TestFsActionExecutor.testResolveToFullPath</li></div><div><li>org.apache.oozie.action.hadoop.TestFsActionExecutor.testPermissionMask</li></div><div><li>org.apache.oozie.action.hadoop.TestFsActionExecutor.testDeleteHcatTable</li></div><div><li>org.apache.oozie.action.hadoop.TestFsActionExecutor.testSetRep</li></div><div><li>org.apache.oozie.action.hadoop.TestFsActionExecutor.testSubmit</li></div><div><li>org.apache.oozie.action.hadoop.TestFsActionExecutor.testTouchz</li></div><div><li>org.apache.oozie.action.hadoop.TestFsActionExecutor.testChgrpRelativePath</li></div><div><li>org.apache.oozie.action.hadoop.TestFsActionExecutor.testRetryOnAccessControlError</li></div><div><li>org.apache.oozie.action.hadoop.TestFsActionExecutor.testValidatePath</li></div><div><li>org.apache.oozie.action.hadoop.TestFsELFunctions.testFunctions</li></div><div><li>org.apache.oozie.action.hadoop.TestHCatPrepareActions.testDelete</li></div><div><li>org.apache.oozie.action.hadoop.TestHadoopELFunctions.testELFunctionsReturningPigStats</li></div><div><li>org.apache.oozie.action.hadoop.TestHadoopELFunctions.testELFunctionsReturningMapReduceStats</li></div><div><li>org.apache.oozie.action.hadoop.TestHadoopELFunctions.testHadoopConfFunctions</li></div><div><li>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testDefaultConfigurationInActionConf</li></div><div><li>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testParseJobXmlAndConfiguration</li></div><div><li>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testCannotKillActionWhenACLSpecified</li></div><div><li>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testOutputSubmitOK</li></div><div><li>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testSubmitLauncherConfigurationOverridesLauncherMapperProperties</li></div><div><li>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testJobXmlWithOozieLauncher</li></div><div><li>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testCredentialsSkip</li></div><div><li>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testACLDefaults_noFalseChange</li></div><div><li>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testJobSubmissionWithoutYarnKill</li></div><div><li>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testFilesystemScheme</li></div><div><li>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testExceptionSubmitException</li></div><div><li>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testKill</li></div><div><li>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testDefaultConfigurationInLauncher</li></div><div><li>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testCommaSeparatedFilesAndArchives</li></div><div><li>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testSubmitOKWithLauncherEnvVars</li></div><div><li>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testSetRootLoggerLevel</li></div><div><li>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testSubmitOKWithLauncherJavaOptsExhaustingHeap</li></div><div><li>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testSubmitOKWithLauncherJavaOpts</li></div><div><li>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testPrepare</li></div><div><li>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testAddActionShareLib</li></div><div><li>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testJobXmlAndNonDefaultNamenode</li></div><div><li>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testSubmitFailsWithInvalidLauncherEnvVars</li></div><div><li>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testActionLibsPath</li></div><div><li>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testSubmitOKWithVcoresAndMemory</li></div><div><li>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testSetupMethods</li></div><div><li>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testEmptyArgsWithNullArgsNotAllowed</li></div><div><li>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testRecovery</li></div><div><li>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testAddToCache</li></div><div><li>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testSubmitWithLauncherQueue</li></div><div><li>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testEnvVarsPropagatedFromLauncherConfig</li></div><div><li>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testJavaOpts</li></div><div><li>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testActionSharelibResolution</li></div><div><li>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testGlobalConfigurationWithActionDefaults</li></div><div><li>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testExceptionSubmitThrowable</li></div><div><li>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testAdditionalJarSubmitOK</li></div><div><li>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testChildKill</li></div><div><li>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testLibFileArchives</li></div><div><li>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testCredentialsModule</li></div><div><li>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testIdSwapSubmitOK</li></div><div><li>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testParseJobXmlAndConfigurationWithELExpression</li></div><div><li>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testUpdateConfForTimeLineServiceEnabled</li></div><div><li>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testAddShareLibSchemeAndAuthority</li></div><div><li>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testEmptyArgsWithNullArgsAllowed</li></div><div><li>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testExit0SubmitOK</li></div><div><li>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testSimplestSubmitWithResourceManagerOK</li></div><div><li>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testCredentialsWithoutCredTag</li></div><div><li>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testResourceManagerInGlobalConfigurationCanBeOverridenWithJobTrackerInAction</li></div><div><li>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testACLDefaults_launcherACLsSetToDefault</li></div><div><li>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testMaxOutputDataSetByUser</li></div><div><li>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testActionShareLibWithNonDefaultNamenode</li></div><div><li>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testSubmitFailsWithNegativeMemory</li></div><div><li>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testCredentialsInvalid</li></div><div><li>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testACLDefaults_explicitLauncherAndActionSettings</li></div><div><li>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testSimpestSleSubmitOK</li></div><div><li>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testExit1SubmitError</li></div><div><li>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testSubmitFailsWithNegativeVcores</li></div><div><li>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testFileWithSpaces</li></div><div><li>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testInvalidMaxOutputDataSetByUser</li></div><div><li>org.apache.oozie.action.hadoop.TestJavaMain.testMain</li></div><div><li>org.apache.oozie.action.hadoop.TestLauncher.testSetupLauncherInfoHadoop2_0_2_alphaWorkaround</li></div><div><li>org.apache.oozie.action.hadoop.TestLauncher.testSetupLauncherInfoWithNonEmptyPrepareXML</li></div><div><li>org.apache.oozie.action.hadoop.TestLauncher.testSetupMainClass</li></div><div><li>org.apache.oozie.action.hadoop.TestLauncher.testCopyFileMultiplex</li></div><div><li>org.apache.oozie.action.hadoop.TestLauncher.testSetupLauncherInfoWithEmptyPrepareXML</li></div><div><li>org.apache.oozie.action.hadoop.TestLauncherFSURIHandler.testCreate</li></div><div><li>org.apache.oozie.action.hadoop.TestLauncherFSURIHandler.testDelete</li></div><div><li>org.apache.oozie.action.hadoop.TestLauncherHCatURIHandler.testDeleteTable</li></div><div><li>org.apache.oozie.action.hadoop.TestLauncherHCatURIHandler.testDelete</li></div><div><li>org.apache.oozie.action.hadoop.TestMapReduceMain.testMain</li></div><div><li>org.apache.oozie.action.hadoop.TestOozieJobInfo.testInfoWithBundle</li></div><div><li>org.apache.oozie.action.hadoop.TestPipesMain.testMain</li></div><div><li>org.apache.oozie.action.hadoop.TestPrepareActionsDriver.testDoOperationsWithValidXML</li></div><div><li>org.apache.oozie.action.hadoop.TestPrepareActionsDriver.testDoOperationsWithInvalidXML</li></div><div><li>org.apache.oozie.action.hadoop.TestRerun.testRerun</li></div><div><li>org.apache.oozie.action.hadoop.TestSharelibConfigs.testActionSharelibConfigPropagation</li></div><div><li>org.apache.oozie.action.hadoop.TestShellActionExecutor.testShellScriptHadoopConfDir</li></div><div><li>org.apache.oozie.action.hadoop.TestShellActionExecutor.testShellScriptError</li></div><div><li>org.apache.oozie.action.hadoop.TestShellActionExecutor.testSetupMethods</li></div><div><li>org.apache.oozie.action.hadoop.TestShellActionExecutor.testShellScript</li></div><div><li>org.apache.oozie.action.hadoop.TestShellActionExecutor.testEnvVar</li></div><div><li>org.apache.oozie.action.hadoop.TestShellActionExecutor.testShellScriptHadoopConfDirWithNoL4J</li></div><div><li>org.apache.oozie.action.hadoop.TestShellActionExecutor.testPerlScript</li></div><div><li>org.apache.oozie.action.hadoop.TestShellMain.testShellScriptSuccess</li></div><div><li>org.apache.oozie.action.hadoop.TestShellMain.testShellScriptFailure</li></div><div><li>org.apache.oozie.action.oozie.TestSubWorkflowActionExecutor.testSubworkflowLib</li></div><div><li>org.apache.oozie.action.oozie.TestSubWorkflowActionExecutor.testParentGlobalConfWithConfigDefault</li></div><div><li>org.apache.oozie.action.oozie.TestSubWorkflowActionExecutor.testType</li></div><div><li>org.apache.oozie.action.oozie.TestSubWorkflowActionExecutor.testSubWorkflowRerunTermination</li></div><div><li>org.apache.oozie.action.oozie.TestSubWorkflowActionExecutor.testConfigPropagation</li></div><div><li>org.apache.oozie.action.oozie.TestSubWorkflowActionExecutor.testSubworkflowDepth</li></div><div><li>org.apache.oozie.action.oozie.TestSubWorkflowActionExecutor.testSubWorkflowSuspend</li></div><div><li>org.apache.oozie.action.oozie.TestSubWorkflowActionExecutor.testSubWorkflowRerun</li></div><div><li>org.apache.oozie.action.oozie.TestSubWorkflowActionExecutor.testSubWorkflowStart</li></div><div><li>org.apache.oozie.action.oozie.TestSubWorkflowActionExecutor.testSubWorkflowRecovery</li></div><div><li>org.apache.oozie.action.oozie.TestSubWorkflowActionExecutor.testConfigNotPropagation</li></div><div><li>org.apache.oozie.action.oozie.TestSubWorkflowActionExecutor.testParentGlobalConf</li></div><div><li>org.apache.oozie.action.oozie.TestSubWorkflowActionExecutor.testSubWorkflowKillExternalChild</li></div><div><li>org.apache.oozie.action.oozie.TestSubWorkflowActionExecutor.testGetGroupFromParent</li></div><div><li>org.apache.oozie.action.oozie.TestSubWorkflowActionExecutor.testSubWorkflowConfCreation</li></div><div><li>org.apache.oozie.coord.TestCoordUtils.testGetCoordActionsFromDate</li></div><div><li>org.apache.oozie.coord.TestCoordUtils.testGetCoordActionsFromDateRange</li></div><div><li>org.apache.oozie.coord.TestCoordUtils.testGetWhereClause</li></div><div><li>org.apache.oozie.coord.TestCoordUtils.testGetCoordActionsFromIds</li></div><div><li>org.apache.oozie.coord.TestCoordUtils.testGetCoordActionsFromIdsRange</li></div><div><li>org.apache.oozie.coord.TestHCatELFunctions.testDataOutPartitionsPh1</li></div><div><li>org.apache.oozie.coord.TestHCatELFunctions.testDataOutPartitionValuePh1</li></div><div><li>org.apache.oozie.coord.TestHCatELFunctions.testDataInPartitionMaxPh1</li></div><div><li>org.apache.oozie.coord.TestHCatELFunctions.testDataInPartitionMinPh1</li></div><div><li>org.apache.oozie.coord.TestHCatELFunctions.testHCatPartitionExists</li></div><div><li>org.apache.oozie.coord.TestHCatELFunctions.testDatabase</li></div><div><li>org.apache.oozie.coord.TestHCatELFunctions.testDataOutPartitionValue</li></div><div><li>org.apache.oozie.coord.TestHCatELFunctions.testTable</li></div><div><li>org.apache.oozie.coord.TestHCatELFunctions.testHCatTableExists</li></div><div><li>org.apache.oozie.coord.TestHCatELFunctions.testDataOutPartitions</li></div><div><li>org.apache.oozie.coord.TestHCatELFunctions.testDataInPartitionsPh1</li></div><div><li>org.apache.oozie.coord.TestHCatELFunctions.testTablePh1</li></div><div><li>org.apache.oozie.coord.TestHCatELFunctions.testdataInPartitionFilterPh1</li></div><div><li>org.apache.oozie.coord.TestHCatELFunctions.testDatabasePh1</li></div><div><li>org.apache.oozie.coord.TestHCatELFunctions.testdataInPartitionFilter</li></div><div><li>org.apache.oozie.coord.TestHCatELFunctions.testDataInPartitionMax</li></div><div><li>org.apache.oozie.coord.TestHCatELFunctions.testDataInPartitionMin</li></div><div><li>org.apache.oozie.coord.TestHCatELFunctions.testDataInPartitions</li></div><div><li>org.apache.oozie.coord.input.logic.TestCoordInputLogicPush.testNestedConditionWithRange</li></div><div><li>org.apache.oozie.coord.input.logic.TestCoordInputLogicPush.testCurrentLatest</li></div><div><li>org.apache.oozie.coord.input.logic.TestCoordInputLogicPush.testLatestRange</li></div><div><li>org.apache.oozie.coord.input.logic.TestCoordInputLogicPush.testNestedCondition3</li></div><div><li>org.apache.oozie.coord.input.logic.TestCoordInputLogicPush.testExists</li></div><div><li>org.apache.oozie.coord.input.logic.TestCoordInputLogicPush.testLatestRangeComplex</li></div><div><li>org.apache.oozie.coord.input.logic.TestCoordInputLogicPush.testHcatHdfsLatest</li></div><div><li>org.apache.oozie.coord.input.logic.TestCoordInputLogicPush.testHcatHdfs</li></div><div><li>org.apache.oozie.coord.input.logic.TestCoordinatorInputLogic.testSimpleOr1</li></div><div><li>org.apache.oozie.coord.input.logic.TestCoordinatorInputLogic.testAnd</li></div><div><li>org.apache.oozie.coord.input.logic.TestCoordinatorInputLogic.testWait</li></div><div><li>org.apache.oozie.coord.input.logic.TestCoordinatorInputLogic.testNestedCondition</li></div><div><li>org.apache.oozie.coord.input.logic.TestCoordinatorInputLogic.testSingeSetWithMin</li></div><div><li>org.apache.oozie.coord.input.logic.TestCoordinatorInputLogic.testCoordWithoutInputCheck</li></div><div><li>org.apache.oozie.coord.input.logic.TestCoordinatorInputLogic.testCombineNegative</li></div><div><li>org.apache.oozie.coord.input.logic.TestCoordinatorInputLogic.testLatestRange</li></div><div><li>org.apache.oozie.coord.input.logic.TestCoordinatorInputLogic.testNestedCondition1</li></div><div><li>org.apache.oozie.coord.input.logic.TestCoordinatorInputLogic.testNestedCondition2</li></div><div><li>org.apache.oozie.coord.input.logic.TestCoordinatorInputLogic.testNestedCondition3</li></div><div><li>org.apache.oozie.coord.input.logic.TestCoordinatorInputLogic.testCombine</li></div><div><li>org.apache.oozie.coord.input.logic.TestCoordinatorInputLogic.testMinWait</li></div><div><li>org.apache.oozie.coord.input.logic.TestCoordinatorInputLogic.testWaitFail</li></div><div><li>org.apache.oozie.coord.input.logic.TestCoordinatorInputLogic.testMultipleInstance</li></div><div><li>org.apache.oozie.coord.input.logic.TestCoordinatorInputLogic.testValidateRange</li></div><div><li>org.apache.oozie.coord.input.logic.TestCoordinatorInputLogic.testSimpleOr</li></div><div><li>org.apache.oozie.coord.input.logic.TestCoordinatorInputLogic.testDryRun</li></div><div><li>org.apache.oozie.coord.input.logic.TestCoordinatorInputLogic.testCombineWithMin</li></div><div><li>org.apache.oozie.coord.input.logic.TestCoordinatorInputLogic.testLatest</li></div><div><li>org.apache.oozie.coord.input.logic.TestCoordinatorInputLogic.testAndWithMin</li></div><div><li>org.apache.oozie.coord.input.logic.TestCoordinatorInputLogic.testOrWithMin</li></div><div><li>org.apache.oozie.dependency.TestFSURIHandler.testExists</li></div><div><li>org.apache.oozie.dependency.TestHCatURIHandler.testDeleteTable</li></div><div><li>org.apache.oozie.dependency.TestHCatURIHandler.testExists</li></div><div><li>org.apache.oozie.event.TestEventGeneration.testInvalidXMLCoordinatorFailsForNoDuplicates</li></div><div><li>org.apache.oozie.event.TestEventGeneration.testCoordinatorActionEventDependencies</li></div><div><li>org.apache.oozie.event.TestEventGeneration.testWorkflowJobEvent</li></div><div><li>org.apache.oozie.event.TestEventGeneration.testCoordinatorActionEvent</li></div><div><li>org.apache.oozie.event.TestEventGeneration.testWorkflowJobEventError</li></div><div><li>org.apache.oozie.event.TestEventGeneration.testForNoDuplicatesWorkflowEvents</li></div><div><li>org.apache.oozie.event.TestEventGeneration.testWorkflowActionEvent</li></div><div><li>org.apache.oozie.event.TestEventGeneration.testForNoDuplicatesCoordinatorActionEvents</li></div><div><li>org.apache.oozie.event.TestEventQueue.testQueueOperations</li></div><div><li>org.apache.oozie.event.TestEventQueue.testMemoryEventQueueBasic</li></div><div><li>org.apache.oozie.executor.jpa.TestBatchQueryExecutor.testExecuteBatchUpdateInsertDelete</li></div><div><li>org.apache.oozie.executor.jpa.TestBatchQueryExecutor.testExecuteBatchUpdateInsertDeleteRollBack</li></div><div><li>org.apache.oozie.executor.jpa.TestBulkMonitorJPAExecutor.testMultipleCoordinators</li></div><div><li>org.apache.oozie.executor.jpa.TestBulkMonitorJPAExecutor.testSingleRecord</li></div><div><li>org.apache.oozie.executor.jpa.TestBulkMonitorJPAExecutor.testDefaultStatus</li></div><div><li>org.apache.oozie.executor.jpa.TestBulkMonitorJPAExecutor.testMultipleRecords</li></div><div><li>org.apache.oozie.executor.jpa.TestBulkMonitorJPAExecutor.testBundleIdWithCoordId</li></div><div><li>org.apache.oozie.executor.jpa.TestBulkMonitorJPAExecutor.testBundleId</li></div><div><li>org.apache.oozie.executor.jpa.TestBulkMonitorJPAExecutor.testJavaNoRecords</li></div><div><li>org.apache.oozie.executor.jpa.TestBulkMonitorJPAExecutor.testMultipleBundleIdsForName</li></div><div><li>org.apache.oozie.executor.jpa.TestBundleActionQueryExecutor.testGetSelectQuery</li></div><div><li>org.apache.oozie.executor.jpa.TestBundleActionQueryExecutor.testExecuteUpdate</li></div><div><li>org.apache.oozie.executor.jpa.TestBundleActionQueryExecutor.testGet</li></div><div><li>org.apache.oozie.executor.jpa.TestBundleActionQueryExecutor.testGetList</li></div><div><li>org.apache.oozie.executor.jpa.TestBundleActionQueryExecutor.testInsert</li></div><div><li>org.apache.oozie.executor.jpa.TestBundleActionQueryExecutor.testGetUpdateQuery</li></div><div><li>org.apache.oozie.executor.jpa.TestBundleActionsCountForJobGetJPAExecutor.testBundleActionsForJobCountGet</li></div><div><li>org.apache.oozie.executor.jpa.TestBundleJobInfoGetJPAExecutor.testBundleJobInfoGet</li></div><div><li>org.apache.oozie.executor.jpa.TestBundleJobInfoGetJPAExecutor.testGetJobInfoForText</li></div><div><li>org.apache.oozie.executor.jpa.TestBundleJobInfoGetJPAExecutor.testGetJobInfoForEndCreatedTime</li></div><div><li>org.apache.oozie.executor.jpa.TestBundleJobInfoGetJPAExecutor.testBundleJobsSortBy</li></div><div><li>org.apache.oozie.executor.jpa.TestBundleJobInfoGetJPAExecutor.testGetJobInfoForStartCreatedTime</li></div><div><li>org.apache.oozie.executor.jpa.TestBundleJobInfoGetJPAExecutor.testGetJobInfoForTextAndStatus</li></div><div><li>org.apache.oozie.executor.jpa.TestBundleJobQueryExecutor.testBundleIDsForStatusTransit</li></div><div><li>org.apache.oozie.executor.jpa.TestBundleJobQueryExecutor.testExecuteUpdate</li></div><div><li>org.apache.oozie.executor.jpa.TestBundleJobQueryExecutor.testGet</li></div><div><li>org.apache.oozie.executor.jpa.TestBundleJobQueryExecutor.testGetList</li></div><div><li>org.apache.oozie.executor.jpa.TestBundleJobQueryExecutor.testInsert</li></div><div><li>org.apache.oozie.executor.jpa.TestBundleJobQueryExecutor.testGetUpdateQuery</li></div><div><li>org.apache.oozie.executor.jpa.TestBundleJobsDeleteJPAExecutor.testDeleteBundlesRollback</li></div><div><li>org.apache.oozie.executor.jpa.TestBundleJobsDeleteJPAExecutor.testDeleteBundles</li></div><div><li>org.apache.oozie.executor.jpa.TestBundleJobsGetForPurgeJPAExecutor.testBundleJobsGetForPurgeJPAExecutorTooMany</li></div><div><li>org.apache.oozie.executor.jpa.TestCoordActionGetForCheckJPAExecutor.testCoordActionGet</li></div><div><li>org.apache.oozie.executor.jpa.TestCoordActionGetForExternalIdJPAExecutor.testCoordActionGet</li></div><div><li>org.apache.oozie.executor.jpa.TestCoordActionGetForInfoJPAExecutor.testCoordActionGetAllColumns</li></div><div><li>org.apache.oozie.executor.jpa.TestCoordActionGetForInfoJPAExecutor.testCoordActionGet</li></div><div><li>org.apache.oozie.executor.jpa.TestCoordActionGetForInputCheckJPAExecutor.testCoordActionGet</li></div><div><li>org.apache.oozie.executor.jpa.TestCoordActionGetForStartJPAExecutor.testCoordActionGet</li></div><div><li>org.apache.oozie.executor.jpa.TestCoordActionGetForTimeoutJPAExecutor.testCoordActionGet</li></div><div><li>org.apache.oozie.executor.jpa.TestCoordActionGetJPAExecutor.testCoordActionGet</li></div><div><li>org.apache.oozie.executor.jpa.TestCoordActionQueryExecutor.testGetTerminatedActionForDates</li></div><div><li>org.apache.oozie.executor.jpa.TestCoordActionQueryExecutor.testGetTerminatedActionIdsForDates</li></div><div><li>org.apache.oozie.executor.jpa.TestCoordActionRemoveJPAExecutor.testCoordActionRemove</li></div><div><li>org.apache.oozie.executor.jpa.TestCoordActionRemoveJPAExecutor.testRunningActionDelete</li></div><div><li>org.apache.oozie.executor.jpa.TestCoordActionsActiveCountJPAExecutor.testCoordActionGet</li></div><div><li>org.apache.oozie.executor.jpa.TestCoordActionsCountForJobIdJPAExecutor.testGetActionsCount</li></div><div><li>org.apache.oozie.executor.jpa.TestCoordActionsDeleteJPAExecutor.testDeleteCoordActionsRollback</li></div><div><li>org.apache.oozie.executor.jpa.TestCoordActionsDeleteJPAExecutor.testDeleteCoordActions</li></div><div><li>org.apache.oozie.executor.jpa.TestCoordActionsPendingFalseCountGetJPAExecutor.testCoordActionsPendingFalseCountGet</li></div><div><li>org.apache.oozie.executor.jpa.TestCoordActionsPendingFalseStatusCountGetJPAExecutor.testCoordActionPendingFalseStatusCountGet</li></div><div><li>org.apache.oozie.executor.jpa.TestCoordActionsRunningGetJPAExecutor.testCoordActionRunningGet</li></div><div><li>org.apache.oozie.executor.jpa.TestCoordJobGetActionByActionNumberJPAExecutor.testCoordActionsGetByActionNumber</li></div><div><li>org.apache.oozie.executor.jpa.TestCoordJobGetActionForNominalTimeJPAExecutor.testCoordActionGet</li></div><div><li>org.apache.oozie.executor.jpa.TestCoordJobGetActionsJPAExecutor.testCoordActionGet</li></div><div><li>org.apache.oozie.executor.jpa.TestCoordJobGetActionsNotCompletedJPAExecutor.testCoordActionsNotCompletetedForSize</li></div><div><li>org.apache.oozie.executor.jpa.TestCoordJobGetActionsNotCompletedJPAExecutor.testCoordActionsNotCompletetedForColumnValues</li></div><div><li>org.apache.oozie.executor.jpa.TestCoordJobGetActionsRunningJPAExecutor.testCoordActionsRunningForColumnValues</li></div><div><li>org.apache.oozie.executor.jpa.TestCoordJobGetActionsRunningJPAExecutor.testCoordActionsRunningForSize</li></div><div><li>org.apache.oozie.executor.jpa.TestCoordJobGetActionsSubsetJPAExecutor.testCoordActionOrderByDesc</li></div><div><li>org.apache.oozie.executor.jpa.TestCoordJobGetActionsSubsetJPAExecutor.testCoordActionOrderBy</li></div><div><li>org.apache.oozie.executor.jpa.TestCoordJobGetActionsSubsetJPAExecutor.testGetActionAllColumns</li></div><div><li>org.apache.oozie.executor.jpa.TestCoordJobGetActionsSubsetJPAExecutor.testCoordActionGet</li></div><div><li>org.apache.oozie.executor.jpa.TestCoordJobGetActionsSubsetJPAExecutor.testGetActionsWithNominalTimeFilter</li></div><div><li>org.apache.oozie.executor.jpa.TestCoordJobGetActionsSubsetJPAExecutor.testCoordActionFilter</li></div><div><li>org.apache.oozie.executor.jpa.TestCoordJobGetActionsSuspendedJPAExecutor.testCoordActionsSuspendedForColumnValues</li></div><div><li>org.apache.oozie.executor.jpa.TestCoordJobGetActionsSuspendedJPAExecutor.testCoordActionsSuspendedForSize</li></div><div><li>org.apache.oozie.executor.jpa.TestCoordJobGetJPAExecutor.testCoordJobGet</li></div><div><li>org.apache.oozie.executor.jpa.TestCoordJobGetReadyActionsJPAExecutor.testCoordActionGetFIFO</li></div><div><li>org.apache.oozie.executor.jpa.TestCoordJobGetReadyActionsJPAExecutor.testCoordActionGetLIFO</li></div><div><li>org.apache.oozie.executor.jpa.TestCoordJobGetReadyActionsJPAExecutor.testCoordActionGetNONE</li></div><div><li>org.apache.oozie.executor.jpa.TestCoordJobGetReadyActionsJPAExecutor.testCoordActionGetLAST_ONLY</li></div><div><li>org.apache.oozie.executor.jpa.TestCoordJobGetRunningActionsCountJPAExecutor.testCoordActionGet</li></div><div><li>org.apache.oozie.executor.jpa.TestCoordJobInfoGetJPAExecutor.testGetJobInfoForWrongTimeFormat</li></div><div><li>org.apache.oozie.executor.jpa.TestCoordJobInfoGetJPAExecutor.testGetJobInfoForText</li></div><div><li>org.apache.oozie.executor.jpa.TestCoordJobInfoGetJPAExecutor.testCoordJobGet</li></div><div><li>org.apache.oozie.executor.jpa.TestCoordJobInfoGetJPAExecutor.testGetJobInfoForBundleId</li></div><div><li>org.apache.oozie.executor.jpa.TestCoordJobInfoGetJPAExecutor.testGetJobInfoForEndCreatedTime</li></div><div><li>org.apache.oozie.executor.jpa.TestCoordJobInfoGetJPAExecutor.testCoordGetJobsSortBy</li></div><div><li>org.apache.oozie.executor.jpa.TestCoordJobInfoGetJPAExecutor.testGetJobInfoForStartCreatedTime</li></div><div><li>org.apache.oozie.executor.jpa.TestCoordJobInfoGetJPAExecutor.testGetJobInfoForTextAndStatus</li></div><div><li>org.apache.oozie.executor.jpa.TestCoordJobQueryExecutor.testExecuteUpdate</li></div><div><li>org.apache.oozie.executor.jpa.TestCoordJobQueryExecutor.testGet</li></div><div><li>org.apache.oozie.executor.jpa.TestCoordJobQueryExecutor.testGetList</li></div><div><li>org.apache.oozie.executor.jpa.TestCoordJobQueryExecutor.testInsert</li></div><div><li>org.apache.oozie.executor.jpa.TestCoordJobQueryExecutor.testGetUpdateQuery</li></div><div><li>org.apache.oozie.executor.jpa.TestCoordJobsCountNotForPurgeFromParentIdJPAExecutor.testCount</li></div><div><li>org.apache.oozie.executor.jpa.TestCoordJobsDeleteJPAExecutor.testDeleteCoordsRollback</li></div><div><li>org.apache.oozie.executor.jpa.TestCoordJobsDeleteJPAExecutor.testDeleteCoords</li></div><div><li>org.apache.oozie.executor.jpa.TestCoordJobsGetForPurgeJPAExecutor.testCoordJobsGetForPurgeJPAExecutorWithParent</li></div><div><li>org.apache.oozie.executor.jpa.TestCoordJobsGetForPurgeJPAExecutor.testCoordJobsGetForPurgeJPAExecutorTooMany</li></div><div><li>org.apache.oozie.executor.jpa.TestCoordJobsGetFromParentIdJPAExecutor.testGetBundleParentTooMany</li></div><div><li>org.apache.oozie.executor.jpa.TestCoordJobsGetFromParentIdJPAExecutor.testGetBundleParent</li></div><div><li>org.apache.oozie.executor.jpa.TestCoordJobsToBeMaterializedJPAExecutor.testCoordJobsToBeMaterializedCommand</li></div><div><li>org.apache.oozie.executor.jpa.TestSLAEventsGetForFilterJPAExecutor.testGetSLAEventsForCombinedWithRange</li></div><div><li>org.apache.oozie.executor.jpa.TestSLAEventsGetForFilterJPAExecutor.testGetSLAEventsForCoordJobId</li></div><div><li>org.apache.oozie.executor.jpa.TestSLAEventsGetForFilterJPAExecutor.testGetSLAEventsForCoordActionId</li></div><div><li>org.apache.oozie.executor.jpa.TestSLAEventsGetForFilterJPAExecutor.testGetSLAEventsForCombined</li></div><div><li>org.apache.oozie.executor.jpa.TestSLAEventsGetForFilterJPAExecutor.testGetSLAEventsForOR</li></div><div><li>org.apache.oozie.executor.jpa.TestSLAEventsGetForFilterJPAExecutor.testGetSLAEventsWithRange</li></div><div><li>org.apache.oozie.executor.jpa.TestSLAEventsGetForFilterJPAExecutor.testGetSLAEventsForAppName</li></div><div><li>org.apache.oozie.executor.jpa.TestSLAEventsGetForSeqIdJPAExecutor.testSLAEventsGetForSeqId</li></div><div><li>org.apache.oozie.executor.jpa.TestSLAEventsGetJPAExecutor.testSLAEventsGetForSeqId</li></div><div><li>org.apache.oozie.executor.jpa.TestSLARegistrationQueryExecutor.testGetSelectQuery</li></div><div><li>org.apache.oozie.executor.jpa.TestSLARegistrationQueryExecutor.testExecuteUpdate</li></div><div><li>org.apache.oozie.executor.jpa.TestSLARegistrationQueryExecutor.testGet</li></div><div><li>org.apache.oozie.executor.jpa.TestSLARegistrationQueryExecutor.testGetList</li></div><div><li>org.apache.oozie.executor.jpa.TestSLARegistrationQueryExecutor.testInsert</li></div><div><li>org.apache.oozie.executor.jpa.TestSLARegistrationQueryExecutor.testGetUpdateQuery</li></div><div><li>org.apache.oozie.executor.jpa.TestSLASummaryQueryExecutor.testGetQuery</li></div><div><li>org.apache.oozie.executor.jpa.TestSLASummaryQueryExecutor.testGetValue</li></div><div><li>org.apache.oozie.executor.jpa.TestSLASummaryQueryExecutor.testExecuteUpdate</li></div><div><li>org.apache.oozie.executor.jpa.TestSLASummaryQueryExecutor.testGet</li></div><div><li>org.apache.oozie.executor.jpa.TestSLASummaryQueryExecutor.testUpdateQuery</li></div><div><li>org.apache.oozie.executor.jpa.TestSLASummaryQueryExecutor.testGetList</li></div><div><li>org.apache.oozie.executor.jpa.TestSLASummaryQueryExecutor.testInsert</li></div><div><li>org.apache.oozie.executor.jpa.TestWorkflowActionDeleteJPAExecutor.testWfActionDelete</li></div><div><li>org.apache.oozie.executor.jpa.TestWorkflowActionGetJPAExecutor.testWfActionGetWithExecPath</li></div><div><li>org.apache.oozie.executor.jpa.TestWorkflowActionGetJPAExecutor.testWfActionGet</li></div><div><li>org.apache.oozie.executor.jpa.TestWorkflowActionQueryExecutor.testExecuteUpdate</li></div><div><li>org.apache.oozie.executor.jpa.TestWorkflowActionQueryExecutor.testGet</li></div><div><li>org.apache.oozie.executor.jpa.TestWorkflowActionQueryExecutor.testGetList</li></div><div><li>org.apache.oozie.executor.jpa.TestWorkflowActionQueryExecutor.testInsert</li></div><div><li>org.apache.oozie.executor.jpa.TestWorkflowActionQueryExecutor.testGetUpdateQuery</li></div><div><li>org.apache.oozie.executor.jpa.TestWorkflowActionRetryManualGetJPAExecutor.testWfActionRetryManualGet</li></div><div><li>org.apache.oozie.executor.jpa.TestWorkflowActionSubsetGetJPAExecutor.testWfActionSubsetGet</li></div><div><li>org.apache.oozie.executor.jpa.TestWorkflowActionsGetForJobJPAExecutor.testWfActionsGet</li></div><div><li>org.apache.oozie.executor.jpa.TestWorkflowIdGetForExternalIdJPAExecutor.testWfJobIdForExternalId</li></div><div><li>org.apache.oozie.executor.jpa.TestWorkflowInfoWithActionsSubsetGetJPAExecutor.testWfInfoWithActionSubsetGet</li></div><div><li>org.apache.oozie.executor.jpa.TestWorkflowJobDeleteJPAExecutor.testWorkflowJobDelete</li></div><div><li>org.apache.oozie.executor.jpa.TestWorkflowJobGetActionsJPAExecutor.testWfActionsGet</li></div><div><li>org.apache.oozie.executor.jpa.TestWorkflowJobGetJPAExecutor.testWfJobGet</li></div><div><li>org.apache.oozie.executor.jpa.TestWorkflowJobQueryExecutor.testGetSelectQuery</li></div><div><li>org.apache.oozie.executor.jpa.TestWorkflowJobQueryExecutor.testExecuteUpdate</li></div><div><li>org.apache.oozie.executor.jpa.TestWorkflowJobQueryExecutor.testGet</li></div><div><li>org.apache.oozie.executor.jpa.TestWorkflowJobQueryExecutor.testGetList</li></div><div><li>org.apache.oozie.executor.jpa.TestWorkflowJobQueryExecutor.testInsert</li></div><div><li>org.apache.oozie.executor.jpa.TestWorkflowJobQueryExecutor.testGetUpdateQuery</li></div><div><li>org.apache.oozie.executor.jpa.TestWorkflowJobsBasicInfoFromCoordParentIdJPAExecutor.testGetCoordinatorParent</li></div><div><li>org.apache.oozie.executor.jpa.TestWorkflowJobsBasicInfoFromCoordParentIdJPAExecutor.testGetWorkflowParentTooMany</li></div><div><li>org.apache.oozie.executor.jpa.TestWorkflowJobsBasicInfoFromWorkflowParentIdJPAExecutor.testGetWorkflowParent</li></div><div><li>org.apache.oozie.executor.jpa.TestWorkflowJobsBasicInfoFromWorkflowParentIdJPAExecutor.testGetCoordinatorParentTooMany</li></div><div><li>org.apache.oozie.executor.jpa.TestWorkflowJobsDeleteJPAExecutor.testDeleteWorkflowsRollback</li></div><div><li>org.apache.oozie.executor.jpa.TestWorkflowJobsDeleteJPAExecutor.testDeleteWorkflows</li></div><div><li>org.apache.oozie.executor.jpa.TestWorkflowJobsGetForPurgeJPAExecutor.testWfJobsGetForPurgeTooMany</li></div><div><li>org.apache.oozie.executor.jpa.TestWorkflowJobsGetForPurgeJPAExecutor.testWfJobsGetForPurgeWithParent</li></div><div><li>org.apache.oozie.executor.jpa.TestWorkflowJobsGetFromCoordParentIdJPAExecutor.testGetCoordinatorParent</li></div><div><li>org.apache.oozie.executor.jpa.TestWorkflowJobsGetFromCoordParentIdJPAExecutor.testGetWorkflowParentTooMany</li></div><div><li>org.apache.oozie.executor.jpa.TestWorkflowJobsGetFromWorkflowParentIdJPAExecutor.testGetWorkflowParent</li></div><div><li>org.apache.oozie.executor.jpa.TestWorkflowJobsGetFromWorkflowParentIdJPAExecutor.testGetCoordinatorParentTooMany</li></div><div><li>org.apache.oozie.executor.jpa.TestWorkflowsJobGetJPAExecutor.testWfJobsGet</li></div><div><li>org.apache.oozie.executor.jpa.TestWorkflowsJobGetJPAExecutor.testWfJobsSortBy</li></div><div><li>org.apache.oozie.executor.jpa.TestWorkflowsJobGetJPAExecutor.testGetWFInfoForText</li></div><div><li>org.apache.oozie.executor.jpa.TestWorkflowsJobGetJPAExecutor.testWfJobsGetWithCreatedTime</li></div><div><li>org.apache.oozie.executor.jpa.TestWorkflowsJobGetJPAExecutor.testGetWFInfoForTextAndStatus</li></div><div><li>org.apache.oozie.jms.TestHCatMessageHandler.testDropEventTypeMessage</li></div><div><li>org.apache.oozie.jms.TestHCatMessageHandler.testCacheUpdateByMessage</li></div><div><li>org.apache.oozie.service.TestActionCheckerService.testActionCheckerService</li></div><div><li>org.apache.oozie.service.TestActionCheckerService.testActionCheckerServiceCoord</li></div><div><li>org.apache.oozie.service.TestActionCheckerService.testActionCheckerServiceDelay</li></div><div><li>org.apache.oozie.service.TestAuthorizationService.testAuthorizationServiceForBundle</li></div><div><li>org.apache.oozie.service.TestAuthorizationService.testAdminUsersWithAdminFile</li></div><div><li>org.apache.oozie.service.TestAuthorizationService.testAdminUsersWithAdminGroup</li></div><div><li>org.apache.oozie.service.TestAuthorizationService.testAuthorizationServiceForCoord</li></div><div><li>org.apache.oozie.service.TestAuthorizationService.testAuthorizedSystemInfoSuccess</li></div><div><li>org.apache.oozie.service.TestAuthorizationService.testAuthorizationServiceUseDefaultGroup</li></div><div><li>org.apache.oozie.service.TestAuthorizationService.testDefaultGroup</li></div><div><li>org.apache.oozie.service.TestAuthorizationService.testAuthorizedSystemInfoFailure</li></div><div><li>org.apache.oozie.service.TestAuthorizationService.testWhenDefinedInAdminFileAndConfigurationThenAllowBothAdmins</li></div><div><li>org.apache.oozie.service.TestAuthorizationService.testAuthorizedSystemInfoDefaultSuccess</li></div><div><li>org.apache.oozie.service.TestAuthorizationService.testWhenDefinedInConfigurationThenAdminPrivilegesAllowed</li></div><div><li>org.apache.oozie.service.TestAuthorizationService.testErrors</li></div><div><li>org.apache.oozie.service.TestAuthorizationService.testAuthorizationServiceUseACLs</li></div><div><li>org.apache.oozie.service.TestCoordMaterializeTriggerService.testMaxMatThrottleNotPickedMultipleJobs</li></div><div><li>org.apache.oozie.service.TestCoordMaterializeTriggerService.testCoordMaterializeTriggerService1</li></div><div><li>org.apache.oozie.service.TestCoordMaterializeTriggerService.testCoordMaterializeTriggerService2</li></div><div><li>org.apache.oozie.service.TestCoordMaterializeTriggerService.testCoordMaterializeTriggerService3</li></div><div><li>org.apache.oozie.service.TestCoordMaterializeTriggerService.testMaxMatThrottleNotPicked</li></div><div><li>org.apache.oozie.service.TestEventHandlerService.testEventLogging</li></div><div><li>org.apache.oozie.service.TestEventHandlerService.testEventListener</li></div><div><li>org.apache.oozie.service.TestEventHandlerService.testService</li></div><div><li>org.apache.oozie.service.TestHAPartitionDependencyManagerEhCache.testPurgeMissingDependencies</li></div><div><li>org.apache.oozie.service.TestHAPartitionDependencyManagerEhCache.testDependencyCacheWithHA</li></div><div><li>org.apache.oozie.service.TestHAPartitionDependencyManagerEhCache.testCheckAfterActionDelete</li></div><div><li>org.apache.oozie.service.TestHAPartitionDependencyManagerService.testPurgeMissingDependencies</li></div><div><li>org.apache.oozie.service.TestHAPartitionDependencyManagerService.testDependencyCacheWithHA</li></div><div><li>org.apache.oozie.service.TestHAPartitionDependencyManagerService.testCheckAfterActionDelete</li></div><div><li>org.apache.oozie.service.TestHASLAService.testSLAUpdateWithHA</li></div><div><li>org.apache.oozie.service.TestHASLAService.testSLAAlertCommandWithHA</li></div><div><li>org.apache.oozie.service.TestHASLAService.testNoDuplicateEventsInHA</li></div><div><li>org.apache.oozie.service.TestHASLAService.testSLAFailOverWithHA</li></div><div><li>org.apache.oozie.service.TestHAShareLibService.testShareLibWithHA</li></div><div><li>org.apache.oozie.service.TestHCatAccessorService.testGetHCatConfLocal</li></div><div><li>org.apache.oozie.service.TestHCatAccessorService.testGetHCatConfHDFS</li></div><div><li>org.apache.oozie.service.TestHCatAccessorService.testGetJMSConnectionInfoNoDefault</li></div><div><li>org.apache.oozie.service.TestHCatAccessorService.testGetJMSConnectionInfo</li></div><div><li>org.apache.oozie.service.TestHadoopAccessorService.testCreateFileSystem</li></div><div><li>org.apache.oozie.service.TestHadoopAccessorService.testCheckSupportedFilesystem</li></div><div><li>org.apache.oozie.service.TestHadoopAccessorService.testCreateJobClient</li></div><div><li>org.apache.oozie.service.TestHadoopAccessorService.testValidateNameNode</li></div><div><li>org.apache.oozie.service.TestHadoopAccessorService.testCreateLocalResourceForConfigurationFile</li></div><div><li>org.apache.oozie.service.TestHadoopAccessorService.testActionConfigurations</li></div><div><li>org.apache.oozie.service.TestHadoopAccessorService.testCreateYarnClient</li></div><div><li>org.apache.oozie.service.TestHadoopAccessorService.testService</li></div><div><li>org.apache.oozie.service.TestHadoopAccessorService.testValidateJobTracker</li></div><div><li>org.apache.oozie.service.TestJMSTopicService.testIncorrectConfigurationDefault</li></div><div><li>org.apache.oozie.service.TestJMSTopicService.testTopicAsUser</li></div><div><li>org.apache.oozie.service.TestJMSTopicService.testIncorrectConfigurationJobType</li></div><div><li>org.apache.oozie.service.TestJMSTopicService.testMixedTopic1</li></div><div><li>org.apache.oozie.service.TestJMSTopicService.testMixedTopic2</li></div><div><li>org.apache.oozie.service.TestJMSTopicService.testTopicAsFixedString</li></div><div><li>org.apache.oozie.service.TestJMSTopicService.testTopicProperties1</li></div><div><li>org.apache.oozie.service.TestJMSTopicService.testTopicProperties2</li></div><div><li>org.apache.oozie.service.TestJMSTopicService.testTopicAsJobId</li></div><div><li>org.apache.oozie.service.TestPartitionDependencyManagerEhcache.testPartitionDependency</li></div><div><li>org.apache.oozie.service.TestPartitionDependencyManagerEhcache.testMaxElementsInMemory</li></div><div><li>org.apache.oozie.service.TestPartitionDependencyManagerEhcache.testEvictionOnTimeToIdle</li></div><div><li>org.apache.oozie.service.TestPartitionDependencyManagerEhcache.testEvictionOnTimeToLive</li></div><div><li>org.apache.oozie.service.TestPartitionDependencyManagerEhcache.testHCatCanonicalHostName</li></div><div><li>org.apache.oozie.service.TestPartitionDependencyManagerEhcache.testMemoryUsageAndSpeed</li></div><div><li>org.apache.oozie.service.TestPartitionDependencyManagerService.testPartitionDependency</li></div><div><li>org.apache.oozie.service.TestPartitionDependencyManagerService.testHCatCanonicalHostName</li></div><div><li>org.apache.oozie.service.TestPartitionDependencyManagerService.testMemoryUsageAndSpeed</li></div><div><li>org.apache.oozie.service.TestPauseTransitService.testPauseCoordinatorForBackwardSupport</li></div><div><li>org.apache.oozie.service.TestPauseTransitService.testUnpauseBundleAndCoordinator</li></div><div><li>org.apache.oozie.service.TestPauseTransitService.testPauseUnpause1</li></div><div><li>org.apache.oozie.service.TestPauseTransitService.testPauseUnpause2</li></div><div><li>org.apache.oozie.service.TestPauseTransitService.testPauseBundleAndCoordinator</li></div><div><li>org.apache.oozie.service.TestPauseTransitService.testStart1</li></div><div><li>org.apache.oozie.service.TestPauseTransitService.testStart2</li></div><div><li>org.apache.oozie.service.TestPurgeService.testPurgeServiceForBundle</li></div><div><li>org.apache.oozie.service.TestPurgeService.testPurgeServiceForCoordinator</li></div><div><li>org.apache.oozie.service.TestPurgeService.testPurgeServiceForWorkflow</li></div><div><li>org.apache.oozie.service.TestRecoveryService.testCoordActionRecoveryServiceForSuspended</li></div><div><li>org.apache.oozie.service.TestRecoveryService.testCoordCreateNotifyParentFailed</li></div><div><li>org.apache.oozie.service.TestRecoveryService.testWorkflowActionRecoveryService</li></div><div><li>org.apache.oozie.service.TestRecoveryService.testCoordActionRecoveryServiceForWaitingRegisterPartition</li></div><div><li>org.apache.oozie.service.TestRecoveryService.testBundleRecoveryCoordCreate</li></div><div><li>org.apache.oozie.service.TestRecoveryService.testCoordActionRecoveryServiceForKilled</li></div><div><li>org.apache.oozie.service.TestRecoveryService.testBundleRecoveryCoordExists</li></div><div><li>org.apache.oozie.service.TestRecoveryService.testCoordActionRecoveryServiceForResume</li></div><div><li>org.apache.oozie.service.TestRecoveryService.testCoordActionRecoveryServiceForSubmitted</li></div><div><li>org.apache.oozie.service.TestRecoveryService.testCoordActionRecoveryServiceForWaiting</li></div><div><li>org.apache.oozie.service.TestRecoveryService.testWorkflowActionRecoveryUserRetry</li></div><div><li>org.apache.oozie.service.TestShareLibService.testDeterminingLatestSharelibPathOn10Threads</li></div><div><li>org.apache.oozie.service.TestShareLibService.testPurgeJar</li></div><div><li>org.apache.oozie.service.TestShareLibService.testDeterminingLatestSharelibPathOn5Threads</li></div><div><li>org.apache.oozie.service.TestShareLibService.testCreateLauncherLibPath</li></div><div><li>org.apache.oozie.service.TestShareLibService.testShareLibLoadFilesFromHDFS</li></div><div><li>org.apache.oozie.service.TestShareLibService.testDuplicateJarsInDistributedCache</li></div><div><li>org.apache.oozie.service.TestShareLibService.testLoadMappingFilesFromDFSandLocalFs</li></div><div><li>org.apache.oozie.service.TestShareLibService.testShareLib</li></div><div><li>org.apache.oozie.service.TestShareLibService.testDeterminingLatestSharelibPathOn1Thread</li></div><div><li>org.apache.oozie.service.TestShareLibService.testGetShareLibCompatible</li></div><div><li>org.apache.oozie.service.TestShareLibService.testRetentionOverflow</li></div><div><li>org.apache.oozie.service.TestShareLibService.testParsingALotOfShareLibsParallel</li></div><div><li>org.apache.oozie.service.TestShareLibService.testShareLibLoadFilesFromLocalFs</li></div><div><li>org.apache.oozie.service.TestShareLibService.testLoadfromDFS</li></div><div><li>org.apache.oozie.service.TestShareLibService.testPurgeShareLib</li></div><div><li>org.apache.oozie.service.TestShareLibService.testConfFileAddedToDistributedCache</li></div><div><li>org.apache.oozie.service.TestShareLibService.testGetShareLibPath</li></div><div><li>org.apache.oozie.service.TestShareLibService.testfailFast</li></div><div><li>org.apache.oozie.service.TestShareLibService.testShareLibLoadFileMultipleFile</li></div><div><li>org.apache.oozie.service.TestShareLibService.testMetafileSymlink</li></div><div><li>org.apache.oozie.service.TestShareLibService.testConfFileAddedToActionConf</li></div><div><li>org.apache.oozie.service.TestShareLibService.testMultipleLauncherCall</li></div><div><li>org.apache.oozie.service.TestShareLibService.testAddShareLib_pig</li></div><div><li>org.apache.oozie.service.TestShareLibService.testAddShareLibDistributedCache</li></div><div><li>org.apache.oozie.service.TestShareLibService.testPurgeLauncherJar</li></div><div><li>org.apache.oozie.service.TestShareLibService.testAddShareLib_pig_withVersion</li></div><div><li>org.apache.oozie.service.TestStatusTransitService.testBundleStatusTransitRunningWithError</li></div><div><li>org.apache.oozie.service.TestStatusTransitService.testCoordStatusTransitServiceSuspendAndResume</li></div><div><li>org.apache.oozie.service.TestStatusTransitService.testCoordStatusTransitServiceSucceeded</li></div><div><li>org.apache.oozie.service.TestStatusTransitService.testBundleStatusTransitServiceSucceeded1</li></div><div><li>org.apache.oozie.service.TestStatusTransitService.testBundleStatusTransitServiceSucceeded2</li></div><div><li>org.apache.oozie.service.TestStatusTransitService.testBundleStatusTransitServiceSucceeded3</li></div><div><li>org.apache.oozie.service.TestStatusTransitService.testCoordStatusTransitServiceStaleCoordActions</li></div><div><li>org.apache.oozie.service.TestStatusTransitService.testFoo</li></div><div><li>org.apache.oozie.service.TestStatusTransitService.testCoordStatusTransitServiceSuspendedWithError</li></div><div><li>org.apache.oozie.service.TestStatusTransitService.testBundleStatusTransitServiceKilled</li></div><div><li>org.apache.oozie.service.TestStatusTransitService.testBundleStatusTransitWithLock</li></div><div><li>org.apache.oozie.service.TestStatusTransitService.testBundleStatusTransitServicePaused</li></div><div><li>org.apache.oozie.service.TestStatusTransitService.testBundleRunningAfterCoordResume</li></div><div><li>org.apache.oozie.service.TestStatusTransitService.testBundleStatusTransitServiceSuspended</li></div><div><li>org.apache.oozie.service.TestStatusTransitService.testCoordStatusTransitServiceUpdateLastModifiedTime</li></div><div><li>org.apache.oozie.service.TestStatusTransitService.testCoordStatusTransitServiceKilledByUser1</li></div><div><li>org.apache.oozie.service.TestStatusTransitService.testCoordStatusTransitServiceKilledByUser2</li></div><div><li>org.apache.oozie.service.TestStatusTransitService.testBundleStatusTransitServiceRunningWithError</li></div><div><li>org.apache.oozie.service.TestStatusTransitService.testCoordStatusTransitServicePaused</li></div><div><li>org.apache.oozie.service.TestStatusTransitService.testCoordStatusTransitServiceBackwardSupport</li></div><div><li>org.apache.oozie.service.TestStatusTransitService.testCoordStatusTransitServiceForTimeout</li></div><div><li>org.apache.oozie.service.TestStatusTransitService.testBundleStatusTransitRunningFromKilled</li></div><div><li>org.apache.oozie.service.TestStatusTransitService.testCoordStatusTransitServiceRunning1</li></div><div><li>org.apache.oozie.service.TestStatusTransitService.testCoordStatusTransitServiceRunning2</li></div><div><li>org.apache.oozie.service.TestStatusTransitService.testCoordStatusTransitServiceRunning3</li></div><div><li>org.apache.oozie.service.TestStatusTransitService.testCoordStatusTransitServiceSuspendedByUser</li></div><div><li>org.apache.oozie.service.TestStatusTransitService.testBundleStatusTransitServiceSuspendedWithError</li></div><div><li>org.apache.oozie.service.TestStatusTransitService.testBundleStatusCoordSubmitFails</li></div><div><li>org.apache.oozie.service.TestStatusTransitService.testCoordStatusTransitServiceDoneWithError</li></div><div><li>org.apache.oozie.service.TestStatusTransitService.testBundleStatusTransitServicePausedWithError</li></div><div><li>org.apache.oozie.service.TestStatusTransitService.testCoordStatusTransitServiceNoDoneWithErrorForBackwardSupport</li></div><div><li>org.apache.oozie.service.TestStatusTransitService.testBundleStatusTransitServiceForTerminalStates</li></div><div><li>org.apache.oozie.service.TestStatusTransitService.testCoordStatusTransitRunningFromKilled</li></div><div><li>org.apache.oozie.service.TestStatusTransitService.testBundleStatusTransitServiceKilled2</li></div><div><li>org.apache.oozie.service.TestStatusTransitService.testCoordStatusTransitServiceSuspendedBottomUp</li></div><div><li>org.apache.oozie.service.TestStatusTransitService.testCoordStatusTransitServicePausedWithError</li></div><div><li>org.apache.oozie.service.TestStatusTransitService.testBundleStatusNotTransitionFromKilled</li></div><div><li>org.apache.oozie.service.TestStatusTransitService.testCoordStatusTransitWithLock</li></div><div><li>org.apache.oozie.service.TestStatusTransitService.testCoordNotTransitionfromKilled</li></div><div><li>org.apache.oozie.service.TestZKJobsConcurrencyService.testGetJobIdsForThisServer</li></div><div><li>org.apache.oozie.service.TestZKJobsConcurrencyService.testGetServerUrls</li></div><div><li>org.apache.oozie.service.TestZKJobsConcurrencyService.testIsLeader</li></div><div><li>org.apache.oozie.service.TestZKJobsConcurrencyService.testInstrumentation</li></div><div><li>org.apache.oozie.service.TestZKJobsConcurrencyService.testRegisterUnregister</li></div><div><li>org.apache.oozie.service.TestZKJobsConcurrencyService.testisAllServerRequest</li></div><div><li>org.apache.oozie.service.TestZKJobsConcurrencyService.testIsJobIdForThisServer</li></div><div><li>org.apache.oozie.service.TestZKLocksService.testReentrantMultipleThread</li></div><div><li>org.apache.oozie.service.TestZKLocksService.testTimeoutWaitingWriteLockOozies</li></div><div><li>org.apache.oozie.service.TestZKLocksService.testWriteReadLockOozies</li></div><div><li>org.apache.oozie.service.TestZKLocksService.testTimeoutWaitingWriteLockThreads</li></div><div><li>org.apache.oozie.service.TestZKLocksService.testTimeoutTimingOutWriteLockOozies</li></div><div><li>org.apache.oozie.service.TestZKLocksService.testLocksAreReused</li></div><div><li>org.apache.oozie.service.TestZKLocksService.testWaitWriteLockOozies</li></div><div><li>org.apache.oozie.service.TestZKLocksService.testWaitWriteLockThreads</li></div><div><li>org.apache.oozie.service.TestZKLocksService.testReentrantMultipleCall</li></div><div><li>org.apache.oozie.service.TestZKLocksService.testLocksAreGarbageCollected</li></div><div><li>org.apache.oozie.service.TestZKLocksService.testLockRelease</li></div><div><li>org.apache.oozie.service.TestZKLocksService.testLockReaper</li></div><div><li>org.apache.oozie.service.TestZKLocksService.testRegisterUnregister</li></div><div><li>org.apache.oozie.service.TestZKLocksService.testReadLockOozies</li></div><div><li>org.apache.oozie.service.TestZKLocksService.testReadWriteLockOozies</li></div><div><li>org.apache.oozie.service.TestZKLocksService.testWriteReadLockThreads</li></div><div><li>org.apache.oozie.service.TestZKLocksService.testReadWriteLockThreads</li></div><div><li>org.apache.oozie.service.TestZKLocksService.testNoWaitWriteLockOozies</li></div><div><li>org.apache.oozie.service.TestZKLocksService.testTimeoutTimingOutWriteLockThreads</li></div><div><li>org.apache.oozie.service.TestZKLocksService.testReadLockThreads</li></div><div><li>org.apache.oozie.service.TestZKLocksService.testNoWaitWriteLockThreads</li></div><div><li>org.apache.oozie.service.TestZKLocksService.testRetriableRelease</li></div><div><li>org.apache.oozie.service.TestZKUUIDService.testRegisterUnregister</li></div><div><li>org.apache.oozie.service.TestZKUUIDService.testBulkJobForZKUUIDService</li></div><div><li>org.apache.oozie.service.TestZKUUIDService.testMultipleIDGeneration</li></div><div><li>org.apache.oozie.service.TestZKUUIDService.testFallback</li></div><div><li>org.apache.oozie.service.TestZKUUIDService.testMultipleIDGeneration_withMultiThread</li></div><div><li>org.apache.oozie.service.TestZKUUIDService.testResetSequence_withMultiThread</li></div><div><li>org.apache.oozie.service.TestZKUUIDService.testResetSequence</li></div><div><li>org.apache.oozie.service.TestZKUUIDService.testIDGeneration</li></div><div><li>org.apache.oozie.service.TestZKXLogStreamingService.testDisableLogOverWS</li></div><div><li>org.apache.oozie.service.TestZKXLogStreamingService.testStreamingWithMultipleOozieServers_coordActionList</li></div><div><li>org.apache.oozie.service.TestZKXLogStreamingService.testNoDashInConversionPattern</li></div><div><li>org.apache.oozie.service.TestZKXLogStreamingService.testStreamingWithMultipleOozieServers</li></div><div><li>org.apache.oozie.service.TestZKXLogStreamingService.testTuncateLog</li></div><div><li>org.apache.oozie.service.TestZKXLogStreamingService.testRegisterUnregister</li></div><div><li>org.apache.oozie.service.TestZKXLogStreamingService.testStreamingWithMultipleOozieServers_errorLog</li></div><div><li>org.apache.oozie.sla.TestSLACalculationJPAExecutor.testRollback</li></div><div><li>org.apache.oozie.sla.TestSLACalculationJPAExecutor.testInsert</li></div><div><li>org.apache.oozie.sla.TestSLACalculationJPAExecutor.testInsertUpdate</li></div><div><li>org.apache.oozie.sla.TestSLACalculatorMemory.testSLAHistorySet</li></div><div><li>org.apache.oozie.sla.TestSLACalculatorMemory.testEventOutOfOrder</li></div><div><li>org.apache.oozie.sla.TestSLACalculatorMemory.testWorkflowActionSLAStatusOnRestart</li></div><div><li>org.apache.oozie.sla.TestSLACalculatorMemory.testHistoryPurge</li></div><div><li>org.apache.oozie.sla.TestSLACalculatorMemory.testWFEndNotCoord</li></div><div><li>org.apache.oozie.sla.TestSLACalculatorMemory.testWorkflowJobSLAStatusOnRestart</li></div><div><li>org.apache.oozie.sla.TestSLACalculatorMemory.testEventMissOnRestart</li></div><div><li>org.apache.oozie.sla.TestSLACalculatorMemory.testDuplicateEndMiss</li></div><div><li>org.apache.oozie.sla.TestSLACalculatorMemory.testWhenSLARegistrationIsAddedAndAllDBCallsAreDisruptedBeanIsNotStored</li></div><div><li>org.apache.oozie.sla.TestSLACalculatorMemory.testSingleAddUpdateRemoveInstrumentedCorrectly</li></div><div><li>org.apache.oozie.sla.TestSLACalculatorMemory.testSLAEvents1</li></div><div><li>org.apache.oozie.sla.TestSLACalculatorMemory.testSLAEvents2</li></div><div><li>org.apache.oozie.sla.TestSLACalculatorMemory.testAddMultipleRestartRemoveMultipleInstrumentedCorrectly</li></div><div><li>org.apache.oozie.sla.TestSLACalculatorMemory.testWhenSLARegistrationIsUpdatedBeanIsStoredCorrectly</li></div><div><li>org.apache.oozie.sla.TestSLACalculatorMemory.testErrorLoggingWithJobIdPrefix</li></div><div><li>org.apache.oozie.sla.TestSLACalculatorMemory.testDisablingAlertsEvents</li></div><div><li>org.apache.oozie.sla.TestSLACalculatorMemory.testWhenSLARegistrationIsUpdatedAndAllDBCallsAreDisruptedBeanIsNotStored</li></div><div><li>org.apache.oozie.sla.TestSLACalculatorMemory.testLoadOnRestart</li></div><div><li>org.apache.oozie.sla.TestSLACalculatorMemory.testCoordinatorActionSLAStatusOnRestart</li></div><div><li>org.apache.oozie.sla.TestSLACalculatorMemory.testDuplicateStartMiss</li></div><div><li>org.apache.oozie.sla.TestSLACalculatorMemory.testWhenSLARegistrationExistsWithoutSLASummaryUpdateSLARetries</li></div><div><li>org.apache.oozie.sla.TestSLACalculatorMemory.testWhenSLARegistrationIsAddedBeanIsStoredCorrectly</li></div><div><li>org.apache.oozie.sla.TestSLAEventGeneration.testCoordRerunNoSLA</li></div><div><li>org.apache.oozie.sla.TestSLAEventGeneration.testWorkflowActionSLARerun</li></div><div><li>org.apache.oozie.sla.TestSLAEventGeneration.testSLASchema1BackwardCompatibilitySubmitStart</li></div><div><li>org.apache.oozie.sla.TestSLAEventGeneration.testWorkflowJobSLANewSubmitStart</li></div><div><li>org.apache.oozie.sla.TestSLAEventGeneration.testSLASchema1BackwardCompatibilityKill</li></div><div><li>org.apache.oozie.sla.TestSLAEventGeneration.testWorkflowJobSLANewKill</li></div><div><li>org.apache.oozie.sla.TestSLAEventGeneration.testFailureAndMissEventsOnKill</li></div><div><li>org.apache.oozie.sla.TestSLAEventGeneration.testWorkflowJobSLARerun</li></div><div><li>org.apache.oozie.sla.TestSLAEventGeneration.testCoordinatorActionCommandsSubmitAndStart</li></div><div><li>org.apache.oozie.sla.TestSLARegistrationGetJPAExecutor.testSLARegistrationGet</li></div><div><li>org.apache.oozie.sla.TestSLARegistrationGetJPAExecutor.testSLARegistrationBulkConfigMap</li></div><div><li>org.apache.oozie.sla.TestSLARegistrationGetJPAExecutor.testSlaConfigStringToMap</li></div><div><li>org.apache.oozie.sla.TestSLAService.testUpdateSLA</li></div><div><li>org.apache.oozie.sla.TestSLAService.testEndMissDBConfirm</li></div><div><li>org.apache.oozie.sla.TestSLAService.testSLAOperations</li></div><div><li>org.apache.oozie.sla.TestSLAService.testBasicService</li></div><div><li>org.apache.oozie.sla.TestSLASummaryGetOnRestartJPAExecutor.testSLARegistrationGet</li></div><div><li>org.apache.oozie.store.TestDBWorkflowStore.testDBWorkflowStore</li></div><div><li>org.apache.oozie.action.hadoop.TestHive2ActionExecutor.testHive2Action</li></div><div><li>org.apache.oozie.action.hadoop.TestPigActionExecutor.testExecutionStatsWithRetrieveStatsFalse</li></div><div><li>org.apache.oozie.action.hadoop.TestPigActionExecutor.testExternalChildIds</li></div><div><li>org.apache.oozie.action.hadoop.TestPigActionExecutor.testExecutionStats</li></div><div><li>org.apache.oozie.action.hadoop.TestPyspark.testPyspark</li></div><div><li>org.apache.oozie.action.hadoop.TestSparkActionExecutor.testSparkAction</li></div><div><li>org.apache.oozie.action.hadoop.TestSqoopActionExecutor.testSqoopActionWithRedundantArgsAndFreeFormQuery</li></div><div><li>org.apache.oozie.action.hadoop.TestSqoopActionExecutor.testSqoopActionWithArgsAndFreeFormQuery</li></div><div><li>org.apache.oozie.action.hadoop.TestSqoopActionExecutor.testSqoopActionWithRedundantPrefix</li></div><div><li>org.apache.oozie.action.hadoop.TestSqoopActionExecutor.testSqoopAction</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.oozie.command.coord.TestCoordActionInputCheckXCommandNonUTC.testNone</li></div><div><li>org.apache.oozie.service.TestCallableQueueService.testQueueSizeWithDelayedElements</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.oozie.command.coord.TestCoordActionsKillXCommand.testActionKillCommandActionNumbers</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;RUNNING&gt; but was:&lt;SUCCEEDED&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;RUNNING&gt; but was:&lt;RUNNINGWITHERROR&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;RUNNING&gt; but was:&lt;RUNNINGWITHERROR&gt;</li></div><div><li>E1018: Coord Job Rerun Error: part or all actions are not eligible to rerun!</li></div><div><li>Could not find own virtual machine</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;SKIPPED&gt; but was:&lt;WAITING&gt;</li></div><div><li>Queue size after execution expected:&lt;0&gt; but was:&lt;1&gt;</li></div><div><li>Unable to open socket file: target process not responding or HotSpot VM not loaded</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>E0712: Could not create lib paths list for application [testPath], Call From 24942bafbc3f/172.17.0.2 to localhost:44583 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>YARN App state for app application_1543553889041_0001 expected:&lt;FINISHED&gt; but was:&lt;RUNNING&gt;</li></div><div><li>YARN App state for app application_1543551807126_0007 expected:&lt;FINISHED&gt; but was:&lt;RUNNING&gt;</li></div><div><li>YARN App state for app application_1543551807126_0009 expected:&lt;FINISHED&gt; but was:&lt;RUNNING&gt;</li></div><div><li>YARN App state for app application_1543551807126_0011 expected:&lt;FINISHED&gt; but was:&lt;RUNNING&gt;</li></div><div><li>YARN App state for app application_1543554855173_0004 expected:&lt;FINISHED&gt; but was:&lt;RUNNING&gt;</li></div><div><li>YARN App state for app application_1543554855173_0001 expected:&lt;FINISHED&gt; but was:&lt;ACCEPTED&gt;</li></div><div><li>YARN App state for app application_1543554181882_0001 expected:&lt;FINISHED&gt; but was:&lt;RUNNING&gt;</li></div><div><li>YARN App state for app application_1543554181882_0003 expected:&lt;FINISHED&gt; but was:&lt;RUNNING&gt;</li></div><div><li>YARN App state for app application_1543554181882_0008 expected:&lt;FINISHED&gt; but was:&lt;RUNNING&gt;</li></div><div><li>YARN App state for app application_1543554181882_0010 expected:&lt;FINISHED&gt; but was:&lt;RUNNING&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;SKIPPED&gt; but was:&lt;WAITING&gt;</li></div><div><li>Queue size after execution expected:&lt;0&gt; but was:&lt;1&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;RUNNING&gt; but was:&lt;RUNNINGWITHERROR&gt;</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.oozie.test.TestWorkflow.testWorkflowWithStartAndEndCompletesSuccessfully</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.oozie.command.coord.TestCoordActionsKillXCommand.testActionKillCommandActionNumbers</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.oozie.command.coord.TestCoordActionsKillXCommand.testActionKillCommandActionNumbers</div></li><li><div>org.apache.oozie.event.TestEventGeneration.testCoordinatorActionEvent</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.oozie.command.coord.TestCoordActionInputCheckXCommand.testNone</div></li><li><div>org.apache.oozie.service.TestCallableQueueService.testQueueSizeWithDelayedElements</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.oozie.TestCoordinatorEngineStreamLog.testCoordLogStreaming</div></li><li><div>org.apache.oozie.TestDagELFunctions.testLastErrorNodeWithRetryFail</div></li><li><div>org.apache.oozie.TestDagELFunctions.testFunctions</div></li><li><div>org.apache.oozie.TestDagELFunctions.testLastErrorNodeWithRetrySucceed</div></li><li><div>org.apache.oozie.TestLocalOozieClientCoord.testHeaderMethods</div></li><li><div>org.apache.oozie.TestLocalOozieClientCoord.testJobMethods</div></li><li><div>org.apache.oozie.TestLocalOozieClientCoord.testJobsOperations</div></li><li><div>org.apache.oozie.TestLocalOozieClientCoord.testGetJobsInfo</div></li><li><div>org.apache.oozie.TestLocalOozieClientCoord.testValidateWSVersion</div></li><li><div>org.apache.oozie.TestLocalOozieClientCoord.testGetOozieUrl</div></li><li><div>org.apache.oozie.TestLocalOozieClientCoord.testReRun2</div></li><li><div>org.apache.oozie.TestLocalOozieClientCoord.testGetProtocolUrl</div></li><li><div>org.apache.oozie.TestV1JobsServletBundleEngine.testGetBundleJobs</div></li><li><div>org.apache.oozie.action.TestActionFailover.testFsFailover</div></li><li><div>org.apache.oozie.action.decision.TestDecisionActionExecutor.testDecision</div></li><li><div>org.apache.oozie.action.email.TestEmailActionExecutor.testDoAuthEmail</div></li><li><div>org.apache.oozie.action.email.TestEmailActionExecutor.testContentType</div></li><li><div>org.apache.oozie.action.email.TestEmailActionExecutor.testValidation</div></li><li><div>org.apache.oozie.action.email.TestEmailActionExecutor.testSetupMethods</div></li><li><div>org.apache.oozie.action.email.TestEmailActionExecutor.testContentTypeDefault</div></li><li><div>org.apache.oozie.action.email.TestEmailActionExecutor.testServerTimeouts</div></li><li><div>org.apache.oozie.action.email.TestEmailActionExecutor.testDoNormalEmail</div></li><li><div>org.apache.oozie.action.email.TestEmailActionExecutor.testHDFSFileAttachment</div></li><li><div>org.apache.oozie.action.email.TestEmailActionExecutor.testLocalFileAttachmentError</div></li><li><div>org.apache.oozie.action.hadoop.TestCredentials.testHbaseCredentials</div></li><li><div>org.apache.oozie.action.hadoop.TestDistCpActionExecutor.testSetupMethods</div></li><li><div>org.apache.oozie.action.hadoop.TestDistCpActionExecutor.testDistCpFile</div></li><li><div>org.apache.oozie.action.hadoop.TestDistcpMain.testJobIDPattern</div></li><li><div>org.apache.oozie.action.hadoop.TestDistcpMain.testMain</div></li><li><div>org.apache.oozie.action.hadoop.TestFSPrepareActions.testMkdir</div></li><li><div>org.apache.oozie.action.hadoop.TestFSPrepareActions.testDeleteWithGlob</div></li><li><div>org.apache.oozie.action.hadoop.TestFSPrepareActions.testForNullScheme</div></li><li><div>org.apache.oozie.action.hadoop.TestFSPrepareActions.testForInvalidScheme</div></li><li><div>org.apache.oozie.action.hadoop.TestFSPrepareActions.testDelete</div></li><li><div>org.apache.oozie.action.hadoop.TestFsActionExecutor.testChmodRelativePath</div></li><li><div>org.apache.oozie.action.hadoop.TestFsActionExecutor.testChgrp</div></li><li><div>org.apache.oozie.action.hadoop.TestFsActionExecutor.testChmod</div></li><li><div>org.apache.oozie.action.hadoop.TestFsActionExecutor.testMkdir</div></li><li><div>org.apache.oozie.action.hadoop.TestFsActionExecutor.testMove</div></li><li><div>org.apache.oozie.action.hadoop.TestFsActionExecutor.testChmodWithGlob</div></li><li><div>org.apache.oozie.action.hadoop.TestFsActionExecutor.testvalidateSameNN</div></li><li><div>org.apache.oozie.action.hadoop.TestFsActionExecutor.testChmodRecursive</div></li><li><div>org.apache.oozie.action.hadoop.TestFsActionExecutor.testSetupMethods</div></li><li><div>org.apache.oozie.action.hadoop.TestFsActionExecutor.testMoveWithGlob</div></li><li><div>org.apache.oozie.action.hadoop.TestFsActionExecutor.testRecovery</div></li><li><div>org.apache.oozie.action.hadoop.TestFsActionExecutor.testDeleteWithGlob</div></li><li><div>org.apache.oozie.action.hadoop.TestFsActionExecutor.testSubmitWithNameNode</div></li><li><div>org.apache.oozie.action.hadoop.TestFsActionExecutor.testChgrpWithGlob</div></li><li><div>org.apache.oozie.action.hadoop.TestFsActionExecutor.testFileSchemeWildcard</div></li><li><div>org.apache.oozie.action.hadoop.TestFsActionExecutor.testMovetoTrash</div></li><li><div>org.apache.oozie.action.hadoop.TestFsActionExecutor.testDeleteHcat</div></li><li><div>org.apache.oozie.action.hadoop.TestFsActionExecutor.testDoOperations</div></li><li><div>org.apache.oozie.action.hadoop.TestFsActionExecutor.testDoOperationsWithNameNodeElement</div></li><li><div>org.apache.oozie.action.hadoop.TestFsActionExecutor.testDelete</div></li><li><div>org.apache.oozie.action.hadoop.TestFsActionExecutor.testResolveToFullPath</div></li><li><div>org.apache.oozie.action.hadoop.TestFsActionExecutor.testPermissionMask</div></li><li><div>org.apache.oozie.action.hadoop.TestFsActionExecutor.testDeleteHcatTable</div></li><li><div>org.apache.oozie.action.hadoop.TestFsActionExecutor.testSetRep</div></li><li><div>org.apache.oozie.action.hadoop.TestFsActionExecutor.testSubmit</div></li><li><div>org.apache.oozie.action.hadoop.TestFsActionExecutor.testTouchz</div></li><li><div>org.apache.oozie.action.hadoop.TestFsActionExecutor.testChgrpRelativePath</div></li><li><div>org.apache.oozie.action.hadoop.TestFsActionExecutor.testRetryOnAccessControlError</div></li><li><div>org.apache.oozie.action.hadoop.TestFsActionExecutor.testValidatePath</div></li><li><div>org.apache.oozie.action.hadoop.TestFsELFunctions.testFunctions</div></li><li><div>org.apache.oozie.action.hadoop.TestHCatPrepareActions.testDelete</div></li><li><div>org.apache.oozie.action.hadoop.TestHadoopELFunctions.testELFunctionsReturningPigStats</div></li><li><div>org.apache.oozie.action.hadoop.TestHadoopELFunctions.testELFunctionsReturningMapReduceStats</div></li><li><div>org.apache.oozie.action.hadoop.TestHadoopELFunctions.testHadoopConfFunctions</div></li><li><div>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testDefaultConfigurationInActionConf</div></li><li><div>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testParseJobXmlAndConfiguration</div></li><li><div>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testCannotKillActionWhenACLSpecified</div></li><li><div>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testOutputSubmitOK</div></li><li><div>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testSubmitLauncherConfigurationOverridesLauncherMapperProperties</div></li><li><div>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testJobXmlWithOozieLauncher</div></li><li><div>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testCredentialsSkip</div></li><li><div>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testACLDefaults_noFalseChange</div></li><li><div>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testJobSubmissionWithoutYarnKill</div></li><li><div>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testFilesystemScheme</div></li><li><div>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testExceptionSubmitException</div></li><li><div>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testKill</div></li><li><div>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testDefaultConfigurationInLauncher</div></li><li><div>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testCommaSeparatedFilesAndArchives</div></li><li><div>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testSubmitOKWithLauncherEnvVars</div></li><li><div>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testSetRootLoggerLevel</div></li><li><div>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testSubmitOKWithLauncherJavaOptsExhaustingHeap</div></li><li><div>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testSubmitOKWithLauncherJavaOpts</div></li><li><div>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testPrepare</div></li><li><div>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testAddActionShareLib</div></li><li><div>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testJobXmlAndNonDefaultNamenode</div></li><li><div>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testSubmitFailsWithInvalidLauncherEnvVars</div></li><li><div>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testActionLibsPath</div></li><li><div>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testSubmitOKWithVcoresAndMemory</div></li><li><div>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testSetupMethods</div></li><li><div>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testEmptyArgsWithNullArgsNotAllowed</div></li><li><div>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testRecovery</div></li><li><div>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testAddToCache</div></li><li><div>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testSubmitWithLauncherQueue</div></li><li><div>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testEnvVarsPropagatedFromLauncherConfig</div></li><li><div>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testJavaOpts</div></li><li><div>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testActionSharelibResolution</div></li><li><div>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testGlobalConfigurationWithActionDefaults</div></li><li><div>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testExceptionSubmitThrowable</div></li><li><div>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testAdditionalJarSubmitOK</div></li><li><div>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testChildKill</div></li><li><div>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testLibFileArchives</div></li><li><div>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testCredentialsModule</div></li><li><div>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testIdSwapSubmitOK</div></li><li><div>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testParseJobXmlAndConfigurationWithELExpression</div></li><li><div>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testUpdateConfForTimeLineServiceEnabled</div></li><li><div>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testAddShareLibSchemeAndAuthority</div></li><li><div>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testEmptyArgsWithNullArgsAllowed</div></li><li><div>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testExit0SubmitOK</div></li><li><div>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testSimplestSubmitWithResourceManagerOK</div></li><li><div>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testCredentialsWithoutCredTag</div></li><li><div>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testResourceManagerInGlobalConfigurationCanBeOverridenWithJobTrackerInAction</div></li><li><div>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testACLDefaults_launcherACLsSetToDefault</div></li><li><div>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testMaxOutputDataSetByUser</div></li><li><div>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testActionShareLibWithNonDefaultNamenode</div></li><li><div>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testSubmitFailsWithNegativeMemory</div></li><li><div>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testCredentialsInvalid</div></li><li><div>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testACLDefaults_explicitLauncherAndActionSettings</div></li><li><div>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testSimpestSleSubmitOK</div></li><li><div>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testExit1SubmitError</div></li><li><div>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testSubmitFailsWithNegativeVcores</div></li><li><div>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testFileWithSpaces</div></li><li><div>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testInvalidMaxOutputDataSetByUser</div></li><li><div>org.apache.oozie.action.hadoop.TestJavaMain.testMain</div></li><li><div>org.apache.oozie.action.hadoop.TestLauncher.testSetupLauncherInfoHadoop2_0_2_alphaWorkaround</div></li><li><div>org.apache.oozie.action.hadoop.TestLauncher.testSetupLauncherInfoWithNonEmptyPrepareXML</div></li><li><div>org.apache.oozie.action.hadoop.TestLauncher.testSetupMainClass</div></li><li><div>org.apache.oozie.action.hadoop.TestLauncher.testCopyFileMultiplex</div></li><li><div>org.apache.oozie.action.hadoop.TestLauncher.testSetupLauncherInfoWithEmptyPrepareXML</div></li><li><div>org.apache.oozie.action.hadoop.TestLauncherFSURIHandler.testCreate</div></li><li><div>org.apache.oozie.action.hadoop.TestLauncherFSURIHandler.testDelete</div></li><li><div>org.apache.oozie.action.hadoop.TestLauncherHCatURIHandler.testDeleteTable</div></li><li><div>org.apache.oozie.action.hadoop.TestLauncherHCatURIHandler.testDelete</div></li><li><div>org.apache.oozie.action.hadoop.TestMapReduceMain.testMain</div></li><li><div>org.apache.oozie.action.hadoop.TestOozieJobInfo.testInfoWithBundle</div></li><li><div>org.apache.oozie.action.hadoop.TestPipesMain.testMain</div></li><li><div>org.apache.oozie.action.hadoop.TestPrepareActionsDriver.testDoOperationsWithValidXML</div></li><li><div>org.apache.oozie.action.hadoop.TestPrepareActionsDriver.testDoOperationsWithInvalidXML</div></li><li><div>org.apache.oozie.action.hadoop.TestRerun.testRerun</div></li><li><div>org.apache.oozie.action.hadoop.TestSharelibConfigs.testActionSharelibConfigPropagation</div></li><li><div>org.apache.oozie.action.hadoop.TestShellActionExecutor.testShellScriptHadoopConfDir</div></li><li><div>org.apache.oozie.action.hadoop.TestShellActionExecutor.testShellScriptError</div></li><li><div>org.apache.oozie.action.hadoop.TestShellActionExecutor.testSetupMethods</div></li><li><div>org.apache.oozie.action.hadoop.TestShellActionExecutor.testShellScript</div></li><li><div>org.apache.oozie.action.hadoop.TestShellActionExecutor.testEnvVar</div></li><li><div>org.apache.oozie.action.hadoop.TestShellActionExecutor.testShellScriptHadoopConfDirWithNoL4J</div></li><li><div>org.apache.oozie.action.hadoop.TestShellActionExecutor.testPerlScript</div></li><li><div>org.apache.oozie.action.hadoop.TestShellMain.testShellScriptSuccess</div></li><li><div>org.apache.oozie.action.hadoop.TestShellMain.testShellScriptFailure</div></li><li><div>org.apache.oozie.action.oozie.TestSubWorkflowActionExecutor.testSubworkflowLib</div></li><li><div>org.apache.oozie.action.oozie.TestSubWorkflowActionExecutor.testParentGlobalConfWithConfigDefault</div></li><li><div>org.apache.oozie.action.oozie.TestSubWorkflowActionExecutor.testType</div></li><li><div>org.apache.oozie.action.oozie.TestSubWorkflowActionExecutor.testSubWorkflowRerunTermination</div></li><li><div>org.apache.oozie.action.oozie.TestSubWorkflowActionExecutor.testConfigPropagation</div></li><li><div>org.apache.oozie.action.oozie.TestSubWorkflowActionExecutor.testSubworkflowDepth</div></li><li><div>org.apache.oozie.action.oozie.TestSubWorkflowActionExecutor.testSubWorkflowSuspend</div></li><li><div>org.apache.oozie.action.oozie.TestSubWorkflowActionExecutor.testSubWorkflowRerun</div></li><li><div>org.apache.oozie.action.oozie.TestSubWorkflowActionExecutor.testSubWorkflowStart</div></li><li><div>org.apache.oozie.action.oozie.TestSubWorkflowActionExecutor.testSubWorkflowRecovery</div></li><li><div>org.apache.oozie.action.oozie.TestSubWorkflowActionExecutor.testConfigNotPropagation</div></li><li><div>org.apache.oozie.action.oozie.TestSubWorkflowActionExecutor.testParentGlobalConf</div></li><li><div>org.apache.oozie.action.oozie.TestSubWorkflowActionExecutor.testSubWorkflowKillExternalChild</div></li><li><div>org.apache.oozie.action.oozie.TestSubWorkflowActionExecutor.testGetGroupFromParent</div></li><li><div>org.apache.oozie.action.oozie.TestSubWorkflowActionExecutor.testSubWorkflowConfCreation</div></li><li><div>org.apache.oozie.coord.TestCoordUtils.testGetCoordActionsFromDate</div></li><li><div>org.apache.oozie.coord.TestCoordUtils.testGetCoordActionsFromDateRange</div></li><li><div>org.apache.oozie.coord.TestCoordUtils.testGetWhereClause</div></li><li><div>org.apache.oozie.coord.TestCoordUtils.testGetCoordActionsFromIds</div></li><li><div>org.apache.oozie.coord.TestCoordUtils.testGetCoordActionsFromIdsRange</div></li><li><div>org.apache.oozie.coord.TestHCatELFunctions.testDataOutPartitionsPh1</div></li><li><div>org.apache.oozie.coord.TestHCatELFunctions.testDataOutPartitionValuePh1</div></li><li><div>org.apache.oozie.coord.TestHCatELFunctions.testDataInPartitionMaxPh1</div></li><li><div>org.apache.oozie.coord.TestHCatELFunctions.testDataInPartitionMinPh1</div></li><li><div>org.apache.oozie.coord.TestHCatELFunctions.testHCatPartitionExists</div></li><li><div>org.apache.oozie.coord.TestHCatELFunctions.testDatabase</div></li><li><div>org.apache.oozie.coord.TestHCatELFunctions.testDataOutPartitionValue</div></li><li><div>org.apache.oozie.coord.TestHCatELFunctions.testTable</div></li><li><div>org.apache.oozie.coord.TestHCatELFunctions.testHCatTableExists</div></li><li><div>org.apache.oozie.coord.TestHCatELFunctions.testDataOutPartitions</div></li><li><div>org.apache.oozie.coord.TestHCatELFunctions.testDataInPartitionsPh1</div></li><li><div>org.apache.oozie.coord.TestHCatELFunctions.testTablePh1</div></li><li><div>org.apache.oozie.coord.TestHCatELFunctions.testdataInPartitionFilterPh1</div></li><li><div>org.apache.oozie.coord.TestHCatELFunctions.testDatabasePh1</div></li><li><div>org.apache.oozie.coord.TestHCatELFunctions.testdataInPartitionFilter</div></li><li><div>org.apache.oozie.coord.TestHCatELFunctions.testDataInPartitionMax</div></li><li><div>org.apache.oozie.coord.TestHCatELFunctions.testDataInPartitionMin</div></li><li><div>org.apache.oozie.coord.TestHCatELFunctions.testDataInPartitions</div></li><li><div>org.apache.oozie.coord.input.logic.TestCoordInputLogicPush.testNestedConditionWithRange</div></li><li><div>org.apache.oozie.coord.input.logic.TestCoordInputLogicPush.testCurrentLatest</div></li><li><div>org.apache.oozie.coord.input.logic.TestCoordInputLogicPush.testLatestRange</div></li><li><div>org.apache.oozie.coord.input.logic.TestCoordInputLogicPush.testNestedCondition3</div></li><li><div>org.apache.oozie.coord.input.logic.TestCoordInputLogicPush.testExists</div></li><li><div>org.apache.oozie.coord.input.logic.TestCoordInputLogicPush.testLatestRangeComplex</div></li><li><div>org.apache.oozie.coord.input.logic.TestCoordInputLogicPush.testHcatHdfsLatest</div></li><li><div>org.apache.oozie.coord.input.logic.TestCoordInputLogicPush.testHcatHdfs</div></li><li><div>org.apache.oozie.coord.input.logic.TestCoordinatorInputLogic.testSimpleOr1</div></li><li><div>org.apache.oozie.coord.input.logic.TestCoordinatorInputLogic.testAnd</div></li><li><div>org.apache.oozie.coord.input.logic.TestCoordinatorInputLogic.testWait</div></li><li><div>org.apache.oozie.coord.input.logic.TestCoordinatorInputLogic.testNestedCondition</div></li><li><div>org.apache.oozie.coord.input.logic.TestCoordinatorInputLogic.testSingeSetWithMin</div></li><li><div>org.apache.oozie.coord.input.logic.TestCoordinatorInputLogic.testCoordWithoutInputCheck</div></li><li><div>org.apache.oozie.coord.input.logic.TestCoordinatorInputLogic.testCombineNegative</div></li><li><div>org.apache.oozie.coord.input.logic.TestCoordinatorInputLogic.testLatestRange</div></li><li><div>org.apache.oozie.coord.input.logic.TestCoordinatorInputLogic.testNestedCondition1</div></li><li><div>org.apache.oozie.coord.input.logic.TestCoordinatorInputLogic.testNestedCondition2</div></li><li><div>org.apache.oozie.coord.input.logic.TestCoordinatorInputLogic.testNestedCondition3</div></li><li><div>org.apache.oozie.coord.input.logic.TestCoordinatorInputLogic.testCombine</div></li><li><div>org.apache.oozie.coord.input.logic.TestCoordinatorInputLogic.testMinWait</div></li><li><div>org.apache.oozie.coord.input.logic.TestCoordinatorInputLogic.testWaitFail</div></li><li><div>org.apache.oozie.coord.input.logic.TestCoordinatorInputLogic.testMultipleInstance</div></li><li><div>org.apache.oozie.coord.input.logic.TestCoordinatorInputLogic.testValidateRange</div></li><li><div>org.apache.oozie.coord.input.logic.TestCoordinatorInputLogic.testSimpleOr</div></li><li><div>org.apache.oozie.coord.input.logic.TestCoordinatorInputLogic.testDryRun</div></li><li><div>org.apache.oozie.coord.input.logic.TestCoordinatorInputLogic.testCombineWithMin</div></li><li><div>org.apache.oozie.coord.input.logic.TestCoordinatorInputLogic.testLatest</div></li><li><div>org.apache.oozie.coord.input.logic.TestCoordinatorInputLogic.testAndWithMin</div></li><li><div>org.apache.oozie.coord.input.logic.TestCoordinatorInputLogic.testOrWithMin</div></li><li><div>org.apache.oozie.dependency.TestFSURIHandler.testExists</div></li><li><div>org.apache.oozie.dependency.TestHCatURIHandler.testDeleteTable</div></li><li><div>org.apache.oozie.dependency.TestHCatURIHandler.testExists</div></li><li><div>org.apache.oozie.event.TestEventGeneration.testInvalidXMLCoordinatorFailsForNoDuplicates</div></li><li><div>org.apache.oozie.event.TestEventGeneration.testCoordinatorActionEventDependencies</div></li><li><div>org.apache.oozie.event.TestEventGeneration.testWorkflowJobEvent</div></li><li><div>org.apache.oozie.event.TestEventGeneration.testCoordinatorActionEvent</div></li><li><div>org.apache.oozie.event.TestEventGeneration.testWorkflowJobEventError</div></li><li><div>org.apache.oozie.event.TestEventGeneration.testForNoDuplicatesWorkflowEvents</div></li><li><div>org.apache.oozie.event.TestEventGeneration.testWorkflowActionEvent</div></li><li><div>org.apache.oozie.event.TestEventGeneration.testForNoDuplicatesCoordinatorActionEvents</div></li><li><div>org.apache.oozie.event.TestEventQueue.testQueueOperations</div></li><li><div>org.apache.oozie.event.TestEventQueue.testMemoryEventQueueBasic</div></li><li><div>org.apache.oozie.executor.jpa.TestBatchQueryExecutor.testExecuteBatchUpdateInsertDelete</div></li><li><div>org.apache.oozie.executor.jpa.TestBatchQueryExecutor.testExecuteBatchUpdateInsertDeleteRollBack</div></li><li><div>org.apache.oozie.executor.jpa.TestBulkMonitorJPAExecutor.testMultipleCoordinators</div></li><li><div>org.apache.oozie.executor.jpa.TestBulkMonitorJPAExecutor.testSingleRecord</div></li><li><div>org.apache.oozie.executor.jpa.TestBulkMonitorJPAExecutor.testDefaultStatus</div></li><li><div>org.apache.oozie.executor.jpa.TestBulkMonitorJPAExecutor.testMultipleRecords</div></li><li><div>org.apache.oozie.executor.jpa.TestBulkMonitorJPAExecutor.testBundleIdWithCoordId</div></li><li><div>org.apache.oozie.executor.jpa.TestBulkMonitorJPAExecutor.testBundleId</div></li><li><div>org.apache.oozie.executor.jpa.TestBulkMonitorJPAExecutor.testJavaNoRecords</div></li><li><div>org.apache.oozie.executor.jpa.TestBulkMonitorJPAExecutor.testMultipleBundleIdsForName</div></li><li><div>org.apache.oozie.executor.jpa.TestBundleActionQueryExecutor.testGetSelectQuery</div></li><li><div>org.apache.oozie.executor.jpa.TestBundleActionQueryExecutor.testExecuteUpdate</div></li><li><div>org.apache.oozie.executor.jpa.TestBundleActionQueryExecutor.testGet</div></li><li><div>org.apache.oozie.executor.jpa.TestBundleActionQueryExecutor.testGetList</div></li><li><div>org.apache.oozie.executor.jpa.TestBundleActionQueryExecutor.testInsert</div></li><li><div>org.apache.oozie.executor.jpa.TestBundleActionQueryExecutor.testGetUpdateQuery</div></li><li><div>org.apache.oozie.executor.jpa.TestBundleActionsCountForJobGetJPAExecutor.testBundleActionsForJobCountGet</div></li><li><div>org.apache.oozie.executor.jpa.TestBundleJobInfoGetJPAExecutor.testBundleJobInfoGet</div></li><li><div>org.apache.oozie.executor.jpa.TestBundleJobInfoGetJPAExecutor.testGetJobInfoForText</div></li><li><div>org.apache.oozie.executor.jpa.TestBundleJobInfoGetJPAExecutor.testGetJobInfoForEndCreatedTime</div></li><li><div>org.apache.oozie.executor.jpa.TestBundleJobInfoGetJPAExecutor.testBundleJobsSortBy</div></li><li><div>org.apache.oozie.executor.jpa.TestBundleJobInfoGetJPAExecutor.testGetJobInfoForStartCreatedTime</div></li><li><div>org.apache.oozie.executor.jpa.TestBundleJobInfoGetJPAExecutor.testGetJobInfoForTextAndStatus</div></li><li><div>org.apache.oozie.executor.jpa.TestBundleJobQueryExecutor.testBundleIDsForStatusTransit</div></li><li><div>org.apache.oozie.executor.jpa.TestBundleJobQueryExecutor.testExecuteUpdate</div></li><li><div>org.apache.oozie.executor.jpa.TestBundleJobQueryExecutor.testGet</div></li><li><div>org.apache.oozie.executor.jpa.TestBundleJobQueryExecutor.testGetList</div></li><li><div>org.apache.oozie.executor.jpa.TestBundleJobQueryExecutor.testInsert</div></li><li><div>org.apache.oozie.executor.jpa.TestBundleJobQueryExecutor.testGetUpdateQuery</div></li><li><div>org.apache.oozie.executor.jpa.TestBundleJobsDeleteJPAExecutor.testDeleteBundlesRollback</div></li><li><div>org.apache.oozie.executor.jpa.TestBundleJobsDeleteJPAExecutor.testDeleteBundles</div></li><li><div>org.apache.oozie.executor.jpa.TestBundleJobsGetForPurgeJPAExecutor.testBundleJobsGetForPurgeJPAExecutorTooMany</div></li><li><div>org.apache.oozie.executor.jpa.TestCoordActionGetForCheckJPAExecutor.testCoordActionGet</div></li><li><div>org.apache.oozie.executor.jpa.TestCoordActionGetForExternalIdJPAExecutor.testCoordActionGet</div></li><li><div>org.apache.oozie.executor.jpa.TestCoordActionGetForInfoJPAExecutor.testCoordActionGetAllColumns</div></li><li><div>org.apache.oozie.executor.jpa.TestCoordActionGetForInfoJPAExecutor.testCoordActionGet</div></li><li><div>org.apache.oozie.executor.jpa.TestCoordActionGetForInputCheckJPAExecutor.testCoordActionGet</div></li><li><div>org.apache.oozie.executor.jpa.TestCoordActionGetForStartJPAExecutor.testCoordActionGet</div></li><li><div>org.apache.oozie.executor.jpa.TestCoordActionGetForTimeoutJPAExecutor.testCoordActionGet</div></li><li><div>org.apache.oozie.executor.jpa.TestCoordActionGetJPAExecutor.testCoordActionGet</div></li><li><div>org.apache.oozie.executor.jpa.TestCoordActionQueryExecutor.testGetTerminatedActionForDates</div></li><li><div>org.apache.oozie.executor.jpa.TestCoordActionQueryExecutor.testGetTerminatedActionIdsForDates</div></li><li><div>org.apache.oozie.executor.jpa.TestCoordActionRemoveJPAExecutor.testCoordActionRemove</div></li><li><div>org.apache.oozie.executor.jpa.TestCoordActionRemoveJPAExecutor.testRunningActionDelete</div></li><li><div>org.apache.oozie.executor.jpa.TestCoordActionsActiveCountJPAExecutor.testCoordActionGet</div></li><li><div>org.apache.oozie.executor.jpa.TestCoordActionsCountForJobIdJPAExecutor.testGetActionsCount</div></li><li><div>org.apache.oozie.executor.jpa.TestCoordActionsDeleteJPAExecutor.testDeleteCoordActionsRollback</div></li><li><div>org.apache.oozie.executor.jpa.TestCoordActionsDeleteJPAExecutor.testDeleteCoordActions</div></li><li><div>org.apache.oozie.executor.jpa.TestCoordActionsPendingFalseCountGetJPAExecutor.testCoordActionsPendingFalseCountGet</div></li><li><div>org.apache.oozie.executor.jpa.TestCoordActionsPendingFalseStatusCountGetJPAExecutor.testCoordActionPendingFalseStatusCountGet</div></li><li><div>org.apache.oozie.executor.jpa.TestCoordActionsRunningGetJPAExecutor.testCoordActionRunningGet</div></li><li><div>org.apache.oozie.executor.jpa.TestCoordJobGetActionByActionNumberJPAExecutor.testCoordActionsGetByActionNumber</div></li><li><div>org.apache.oozie.executor.jpa.TestCoordJobGetActionForNominalTimeJPAExecutor.testCoordActionGet</div></li><li><div>org.apache.oozie.executor.jpa.TestCoordJobGetActionsJPAExecutor.testCoordActionGet</div></li><li><div>org.apache.oozie.executor.jpa.TestCoordJobGetActionsNotCompletedJPAExecutor.testCoordActionsNotCompletetedForSize</div></li><li><div>org.apache.oozie.executor.jpa.TestCoordJobGetActionsNotCompletedJPAExecutor.testCoordActionsNotCompletetedForColumnValues</div></li><li><div>org.apache.oozie.executor.jpa.TestCoordJobGetActionsRunningJPAExecutor.testCoordActionsRunningForColumnValues</div></li><li><div>org.apache.oozie.executor.jpa.TestCoordJobGetActionsRunningJPAExecutor.testCoordActionsRunningForSize</div></li><li><div>org.apache.oozie.executor.jpa.TestCoordJobGetActionsSubsetJPAExecutor.testCoordActionOrderByDesc</div></li><li><div>org.apache.oozie.executor.jpa.TestCoordJobGetActionsSubsetJPAExecutor.testCoordActionOrderBy</div></li><li><div>org.apache.oozie.executor.jpa.TestCoordJobGetActionsSubsetJPAExecutor.testGetActionAllColumns</div></li><li><div>org.apache.oozie.executor.jpa.TestCoordJobGetActionsSubsetJPAExecutor.testCoordActionGet</div></li><li><div>org.apache.oozie.executor.jpa.TestCoordJobGetActionsSubsetJPAExecutor.testGetActionsWithNominalTimeFilter</div></li><li><div>org.apache.oozie.executor.jpa.TestCoordJobGetActionsSubsetJPAExecutor.testCoordActionFilter</div></li><li><div>org.apache.oozie.executor.jpa.TestCoordJobGetActionsSuspendedJPAExecutor.testCoordActionsSuspendedForColumnValues</div></li><li><div>org.apache.oozie.executor.jpa.TestCoordJobGetActionsSuspendedJPAExecutor.testCoordActionsSuspendedForSize</div></li><li><div>org.apache.oozie.executor.jpa.TestCoordJobGetJPAExecutor.testCoordJobGet</div></li><li><div>org.apache.oozie.executor.jpa.TestCoordJobGetReadyActionsJPAExecutor.testCoordActionGetFIFO</div></li><li><div>org.apache.oozie.executor.jpa.TestCoordJobGetReadyActionsJPAExecutor.testCoordActionGetLIFO</div></li><li><div>org.apache.oozie.executor.jpa.TestCoordJobGetReadyActionsJPAExecutor.testCoordActionGetNONE</div></li><li><div>org.apache.oozie.executor.jpa.TestCoordJobGetReadyActionsJPAExecutor.testCoordActionGetLAST_ONLY</div></li><li><div>org.apache.oozie.executor.jpa.TestCoordJobGetRunningActionsCountJPAExecutor.testCoordActionGet</div></li><li><div>org.apache.oozie.executor.jpa.TestCoordJobInfoGetJPAExecutor.testGetJobInfoForWrongTimeFormat</div></li><li><div>org.apache.oozie.executor.jpa.TestCoordJobInfoGetJPAExecutor.testGetJobInfoForText</div></li><li><div>org.apache.oozie.executor.jpa.TestCoordJobInfoGetJPAExecutor.testCoordJobGet</div></li><li><div>org.apache.oozie.executor.jpa.TestCoordJobInfoGetJPAExecutor.testGetJobInfoForBundleId</div></li><li><div>org.apache.oozie.executor.jpa.TestCoordJobInfoGetJPAExecutor.testGetJobInfoForEndCreatedTime</div></li><li><div>org.apache.oozie.executor.jpa.TestCoordJobInfoGetJPAExecutor.testCoordGetJobsSortBy</div></li><li><div>org.apache.oozie.executor.jpa.TestCoordJobInfoGetJPAExecutor.testGetJobInfoForStartCreatedTime</div></li><li><div>org.apache.oozie.executor.jpa.TestCoordJobInfoGetJPAExecutor.testGetJobInfoForTextAndStatus</div></li><li><div>org.apache.oozie.executor.jpa.TestCoordJobQueryExecutor.testExecuteUpdate</div></li><li><div>org.apache.oozie.executor.jpa.TestCoordJobQueryExecutor.testGet</div></li><li><div>org.apache.oozie.executor.jpa.TestCoordJobQueryExecutor.testGetList</div></li><li><div>org.apache.oozie.executor.jpa.TestCoordJobQueryExecutor.testInsert</div></li><li><div>org.apache.oozie.executor.jpa.TestCoordJobQueryExecutor.testGetUpdateQuery</div></li><li><div>org.apache.oozie.executor.jpa.TestCoordJobsCountNotForPurgeFromParentIdJPAExecutor.testCount</div></li><li><div>org.apache.oozie.executor.jpa.TestCoordJobsDeleteJPAExecutor.testDeleteCoordsRollback</div></li><li><div>org.apache.oozie.executor.jpa.TestCoordJobsDeleteJPAExecutor.testDeleteCoords</div></li><li><div>org.apache.oozie.executor.jpa.TestCoordJobsGetForPurgeJPAExecutor.testCoordJobsGetForPurgeJPAExecutorWithParent</div></li><li><div>org.apache.oozie.executor.jpa.TestCoordJobsGetForPurgeJPAExecutor.testCoordJobsGetForPurgeJPAExecutorTooMany</div></li><li><div>org.apache.oozie.executor.jpa.TestCoordJobsGetFromParentIdJPAExecutor.testGetBundleParentTooMany</div></li><li><div>org.apache.oozie.executor.jpa.TestCoordJobsGetFromParentIdJPAExecutor.testGetBundleParent</div></li><li><div>org.apache.oozie.executor.jpa.TestCoordJobsToBeMaterializedJPAExecutor.testCoordJobsToBeMaterializedCommand</div></li><li><div>org.apache.oozie.executor.jpa.TestSLAEventsGetForFilterJPAExecutor.testGetSLAEventsForCombinedWithRange</div></li><li><div>org.apache.oozie.executor.jpa.TestSLAEventsGetForFilterJPAExecutor.testGetSLAEventsForCoordJobId</div></li><li><div>org.apache.oozie.executor.jpa.TestSLAEventsGetForFilterJPAExecutor.testGetSLAEventsForCoordActionId</div></li><li><div>org.apache.oozie.executor.jpa.TestSLAEventsGetForFilterJPAExecutor.testGetSLAEventsForCombined</div></li><li><div>org.apache.oozie.executor.jpa.TestSLAEventsGetForFilterJPAExecutor.testGetSLAEventsForOR</div></li><li><div>org.apache.oozie.executor.jpa.TestSLAEventsGetForFilterJPAExecutor.testGetSLAEventsWithRange</div></li><li><div>org.apache.oozie.executor.jpa.TestSLAEventsGetForFilterJPAExecutor.testGetSLAEventsForAppName</div></li><li><div>org.apache.oozie.executor.jpa.TestSLAEventsGetForSeqIdJPAExecutor.testSLAEventsGetForSeqId</div></li><li><div>org.apache.oozie.executor.jpa.TestSLAEventsGetJPAExecutor.testSLAEventsGetForSeqId</div></li><li><div>org.apache.oozie.executor.jpa.TestSLARegistrationQueryExecutor.testGetSelectQuery</div></li><li><div>org.apache.oozie.executor.jpa.TestSLARegistrationQueryExecutor.testExecuteUpdate</div></li><li><div>org.apache.oozie.executor.jpa.TestSLARegistrationQueryExecutor.testGet</div></li><li><div>org.apache.oozie.executor.jpa.TestSLARegistrationQueryExecutor.testGetList</div></li><li><div>org.apache.oozie.executor.jpa.TestSLARegistrationQueryExecutor.testInsert</div></li><li><div>org.apache.oozie.executor.jpa.TestSLARegistrationQueryExecutor.testGetUpdateQuery</div></li><li><div>org.apache.oozie.executor.jpa.TestSLASummaryQueryExecutor.testGetQuery</div></li><li><div>org.apache.oozie.executor.jpa.TestSLASummaryQueryExecutor.testGetValue</div></li><li><div>org.apache.oozie.executor.jpa.TestSLASummaryQueryExecutor.testExecuteUpdate</div></li><li><div>org.apache.oozie.executor.jpa.TestSLASummaryQueryExecutor.testGet</div></li><li><div>org.apache.oozie.executor.jpa.TestSLASummaryQueryExecutor.testUpdateQuery</div></li><li><div>org.apache.oozie.executor.jpa.TestSLASummaryQueryExecutor.testGetList</div></li><li><div>org.apache.oozie.executor.jpa.TestSLASummaryQueryExecutor.testInsert</div></li><li><div>org.apache.oozie.executor.jpa.TestWorkflowActionDeleteJPAExecutor.testWfActionDelete</div></li><li><div>org.apache.oozie.executor.jpa.TestWorkflowActionGetJPAExecutor.testWfActionGetWithExecPath</div></li><li><div>org.apache.oozie.executor.jpa.TestWorkflowActionGetJPAExecutor.testWfActionGet</div></li><li><div>org.apache.oozie.executor.jpa.TestWorkflowActionQueryExecutor.testExecuteUpdate</div></li><li><div>org.apache.oozie.executor.jpa.TestWorkflowActionQueryExecutor.testGet</div></li><li><div>org.apache.oozie.executor.jpa.TestWorkflowActionQueryExecutor.testGetList</div></li><li><div>org.apache.oozie.executor.jpa.TestWorkflowActionQueryExecutor.testInsert</div></li><li><div>org.apache.oozie.executor.jpa.TestWorkflowActionQueryExecutor.testGetUpdateQuery</div></li><li><div>org.apache.oozie.executor.jpa.TestWorkflowActionRetryManualGetJPAExecutor.testWfActionRetryManualGet</div></li><li><div>org.apache.oozie.executor.jpa.TestWorkflowActionSubsetGetJPAExecutor.testWfActionSubsetGet</div></li><li><div>org.apache.oozie.executor.jpa.TestWorkflowActionsGetForJobJPAExecutor.testWfActionsGet</div></li><li><div>org.apache.oozie.executor.jpa.TestWorkflowIdGetForExternalIdJPAExecutor.testWfJobIdForExternalId</div></li><li><div>org.apache.oozie.executor.jpa.TestWorkflowInfoWithActionsSubsetGetJPAExecutor.testWfInfoWithActionSubsetGet</div></li><li><div>org.apache.oozie.executor.jpa.TestWorkflowJobDeleteJPAExecutor.testWorkflowJobDelete</div></li><li><div>org.apache.oozie.executor.jpa.TestWorkflowJobGetActionsJPAExecutor.testWfActionsGet</div></li><li><div>org.apache.oozie.executor.jpa.TestWorkflowJobGetJPAExecutor.testWfJobGet</div></li><li><div>org.apache.oozie.executor.jpa.TestWorkflowJobQueryExecutor.testGetSelectQuery</div></li><li><div>org.apache.oozie.executor.jpa.TestWorkflowJobQueryExecutor.testExecuteUpdate</div></li><li><div>org.apache.oozie.executor.jpa.TestWorkflowJobQueryExecutor.testGet</div></li><li><div>org.apache.oozie.executor.jpa.TestWorkflowJobQueryExecutor.testGetList</div></li><li><div>org.apache.oozie.executor.jpa.TestWorkflowJobQueryExecutor.testInsert</div></li><li><div>org.apache.oozie.executor.jpa.TestWorkflowJobQueryExecutor.testGetUpdateQuery</div></li><li><div>org.apache.oozie.executor.jpa.TestWorkflowJobsBasicInfoFromCoordParentIdJPAExecutor.testGetCoordinatorParent</div></li><li><div>org.apache.oozie.executor.jpa.TestWorkflowJobsBasicInfoFromCoordParentIdJPAExecutor.testGetWorkflowParentTooMany</div></li><li><div>org.apache.oozie.executor.jpa.TestWorkflowJobsBasicInfoFromWorkflowParentIdJPAExecutor.testGetWorkflowParent</div></li><li><div>org.apache.oozie.executor.jpa.TestWorkflowJobsBasicInfoFromWorkflowParentIdJPAExecutor.testGetCoordinatorParentTooMany</div></li><li><div>org.apache.oozie.executor.jpa.TestWorkflowJobsDeleteJPAExecutor.testDeleteWorkflowsRollback</div></li><li><div>org.apache.oozie.executor.jpa.TestWorkflowJobsDeleteJPAExecutor.testDeleteWorkflows</div></li><li><div>org.apache.oozie.executor.jpa.TestWorkflowJobsGetForPurgeJPAExecutor.testWfJobsGetForPurgeTooMany</div></li><li><div>org.apache.oozie.executor.jpa.TestWorkflowJobsGetForPurgeJPAExecutor.testWfJobsGetForPurgeWithParent</div></li><li><div>org.apache.oozie.executor.jpa.TestWorkflowJobsGetFromCoordParentIdJPAExecutor.testGetCoordinatorParent</div></li><li><div>org.apache.oozie.executor.jpa.TestWorkflowJobsGetFromCoordParentIdJPAExecutor.testGetWorkflowParentTooMany</div></li><li><div>org.apache.oozie.executor.jpa.TestWorkflowJobsGetFromWorkflowParentIdJPAExecutor.testGetWorkflowParent</div></li><li><div>org.apache.oozie.executor.jpa.TestWorkflowJobsGetFromWorkflowParentIdJPAExecutor.testGetCoordinatorParentTooMany</div></li><li><div>org.apache.oozie.executor.jpa.TestWorkflowsJobGetJPAExecutor.testWfJobsGet</div></li><li><div>org.apache.oozie.executor.jpa.TestWorkflowsJobGetJPAExecutor.testWfJobsSortBy</div></li><li><div>org.apache.oozie.executor.jpa.TestWorkflowsJobGetJPAExecutor.testGetWFInfoForText</div></li><li><div>org.apache.oozie.executor.jpa.TestWorkflowsJobGetJPAExecutor.testWfJobsGetWithCreatedTime</div></li><li><div>org.apache.oozie.executor.jpa.TestWorkflowsJobGetJPAExecutor.testGetWFInfoForTextAndStatus</div></li><li><div>org.apache.oozie.jms.TestHCatMessageHandler.testDropEventTypeMessage</div></li><li><div>org.apache.oozie.jms.TestHCatMessageHandler.testCacheUpdateByMessage</div></li><li><div>org.apache.oozie.service.TestActionCheckerService.testActionCheckerService</div></li><li><div>org.apache.oozie.service.TestActionCheckerService.testActionCheckerServiceCoord</div></li><li><div>org.apache.oozie.service.TestActionCheckerService.testActionCheckerServiceDelay</div></li><li><div>org.apache.oozie.service.TestAuthorizationService.testAuthorizationServiceForBundle</div></li><li><div>org.apache.oozie.service.TestAuthorizationService.testAdminUsersWithAdminFile</div></li><li><div>org.apache.oozie.service.TestAuthorizationService.testAdminUsersWithAdminGroup</div></li><li><div>org.apache.oozie.service.TestAuthorizationService.testAuthorizationServiceForCoord</div></li><li><div>org.apache.oozie.service.TestAuthorizationService.testAuthorizedSystemInfoSuccess</div></li><li><div>org.apache.oozie.service.TestAuthorizationService.testAuthorizationServiceUseDefaultGroup</div></li><li><div>org.apache.oozie.service.TestAuthorizationService.testDefaultGroup</div></li><li><div>org.apache.oozie.service.TestAuthorizationService.testAuthorizedSystemInfoFailure</div></li><li><div>org.apache.oozie.service.TestAuthorizationService.testWhenDefinedInAdminFileAndConfigurationThenAllowBothAdmins</div></li><li><div>org.apache.oozie.service.TestAuthorizationService.testAuthorizedSystemInfoDefaultSuccess</div></li><li><div>org.apache.oozie.service.TestAuthorizationService.testWhenDefinedInConfigurationThenAdminPrivilegesAllowed</div></li><li><div>org.apache.oozie.service.TestAuthorizationService.testErrors</div></li><li><div>org.apache.oozie.service.TestAuthorizationService.testAuthorizationServiceUseACLs</div></li><li><div>org.apache.oozie.service.TestCoordMaterializeTriggerService.testMaxMatThrottleNotPickedMultipleJobs</div></li><li><div>org.apache.oozie.service.TestCoordMaterializeTriggerService.testCoordMaterializeTriggerService1</div></li><li><div>org.apache.oozie.service.TestCoordMaterializeTriggerService.testCoordMaterializeTriggerService2</div></li><li><div>org.apache.oozie.service.TestCoordMaterializeTriggerService.testCoordMaterializeTriggerService3</div></li><li><div>org.apache.oozie.service.TestCoordMaterializeTriggerService.testMaxMatThrottleNotPicked</div></li><li><div>org.apache.oozie.service.TestEventHandlerService.testEventLogging</div></li><li><div>org.apache.oozie.service.TestEventHandlerService.testEventListener</div></li><li><div>org.apache.oozie.service.TestEventHandlerService.testService</div></li><li><div>org.apache.oozie.service.TestHAPartitionDependencyManagerEhCache.testPurgeMissingDependencies</div></li><li><div>org.apache.oozie.service.TestHAPartitionDependencyManagerEhCache.testDependencyCacheWithHA</div></li><li><div>org.apache.oozie.service.TestHAPartitionDependencyManagerEhCache.testCheckAfterActionDelete</div></li><li><div>org.apache.oozie.service.TestHAPartitionDependencyManagerService.testPurgeMissingDependencies</div></li><li><div>org.apache.oozie.service.TestHAPartitionDependencyManagerService.testDependencyCacheWithHA</div></li><li><div>org.apache.oozie.service.TestHAPartitionDependencyManagerService.testCheckAfterActionDelete</div></li><li><div>org.apache.oozie.service.TestHASLAService.testSLAUpdateWithHA</div></li><li><div>org.apache.oozie.service.TestHASLAService.testSLAAlertCommandWithHA</div></li><li><div>org.apache.oozie.service.TestHASLAService.testNoDuplicateEventsInHA</div></li><li><div>org.apache.oozie.service.TestHASLAService.testSLAFailOverWithHA</div></li><li><div>org.apache.oozie.service.TestHAShareLibService.testShareLibWithHA</div></li><li><div>org.apache.oozie.service.TestHCatAccessorService.testGetHCatConfLocal</div></li><li><div>org.apache.oozie.service.TestHCatAccessorService.testGetHCatConfHDFS</div></li><li><div>org.apache.oozie.service.TestHCatAccessorService.testGetJMSConnectionInfoNoDefault</div></li><li><div>org.apache.oozie.service.TestHCatAccessorService.testGetJMSConnectionInfo</div></li><li><div>org.apache.oozie.service.TestHadoopAccessorService.testCreateFileSystem</div></li><li><div>org.apache.oozie.service.TestHadoopAccessorService.testCheckSupportedFilesystem</div></li><li><div>org.apache.oozie.service.TestHadoopAccessorService.testCreateJobClient</div></li><li><div>org.apache.oozie.service.TestHadoopAccessorService.testValidateNameNode</div></li><li><div>org.apache.oozie.service.TestHadoopAccessorService.testCreateLocalResourceForConfigurationFile</div></li><li><div>org.apache.oozie.service.TestHadoopAccessorService.testActionConfigurations</div></li><li><div>org.apache.oozie.service.TestHadoopAccessorService.testCreateYarnClient</div></li><li><div>org.apache.oozie.service.TestHadoopAccessorService.testService</div></li><li><div>org.apache.oozie.service.TestHadoopAccessorService.testValidateJobTracker</div></li><li><div>org.apache.oozie.service.TestJMSTopicService.testIncorrectConfigurationDefault</div></li><li><div>org.apache.oozie.service.TestJMSTopicService.testTopicAsUser</div></li><li><div>org.apache.oozie.service.TestJMSTopicService.testIncorrectConfigurationJobType</div></li><li><div>org.apache.oozie.service.TestJMSTopicService.testMixedTopic1</div></li><li><div>org.apache.oozie.service.TestJMSTopicService.testMixedTopic2</div></li><li><div>org.apache.oozie.service.TestJMSTopicService.testTopicAsFixedString</div></li><li><div>org.apache.oozie.service.TestJMSTopicService.testTopicProperties1</div></li><li><div>org.apache.oozie.service.TestJMSTopicService.testTopicProperties2</div></li><li><div>org.apache.oozie.service.TestJMSTopicService.testTopicAsJobId</div></li><li><div>org.apache.oozie.service.TestPartitionDependencyManagerEhcache.testPartitionDependency</div></li><li><div>org.apache.oozie.service.TestPartitionDependencyManagerEhcache.testMaxElementsInMemory</div></li><li><div>org.apache.oozie.service.TestPartitionDependencyManagerEhcache.testEvictionOnTimeToIdle</div></li><li><div>org.apache.oozie.service.TestPartitionDependencyManagerEhcache.testEvictionOnTimeToLive</div></li><li><div>org.apache.oozie.service.TestPartitionDependencyManagerEhcache.testHCatCanonicalHostName</div></li><li><div>org.apache.oozie.service.TestPartitionDependencyManagerEhcache.testMemoryUsageAndSpeed</div></li><li><div>org.apache.oozie.service.TestPartitionDependencyManagerService.testPartitionDependency</div></li><li><div>org.apache.oozie.service.TestPartitionDependencyManagerService.testHCatCanonicalHostName</div></li><li><div>org.apache.oozie.service.TestPartitionDependencyManagerService.testMemoryUsageAndSpeed</div></li><li><div>org.apache.oozie.service.TestPauseTransitService.testPauseCoordinatorForBackwardSupport</div></li><li><div>org.apache.oozie.service.TestPauseTransitService.testUnpauseBundleAndCoordinator</div></li><li><div>org.apache.oozie.service.TestPauseTransitService.testPauseUnpause1</div></li><li><div>org.apache.oozie.service.TestPauseTransitService.testPauseUnpause2</div></li><li><div>org.apache.oozie.service.TestPauseTransitService.testPauseBundleAndCoordinator</div></li><li><div>org.apache.oozie.service.TestPauseTransitService.testStart1</div></li><li><div>org.apache.oozie.service.TestPauseTransitService.testStart2</div></li><li><div>org.apache.oozie.service.TestPurgeService.testPurgeServiceForBundle</div></li><li><div>org.apache.oozie.service.TestPurgeService.testPurgeServiceForCoordinator</div></li><li><div>org.apache.oozie.service.TestPurgeService.testPurgeServiceForWorkflow</div></li><li><div>org.apache.oozie.service.TestRecoveryService.testCoordActionRecoveryServiceForSuspended</div></li><li><div>org.apache.oozie.service.TestRecoveryService.testCoordCreateNotifyParentFailed</div></li><li><div>org.apache.oozie.service.TestRecoveryService.testWorkflowActionRecoveryService</div></li><li><div>org.apache.oozie.service.TestRecoveryService.testCoordActionRecoveryServiceForWaitingRegisterPartition</div></li><li><div>org.apache.oozie.service.TestRecoveryService.testBundleRecoveryCoordCreate</div></li><li><div>org.apache.oozie.service.TestRecoveryService.testCoordActionRecoveryServiceForKilled</div></li><li><div>org.apache.oozie.service.TestRecoveryService.testBundleRecoveryCoordExists</div></li><li><div>org.apache.oozie.service.TestRecoveryService.testCoordActionRecoveryServiceForResume</div></li><li><div>org.apache.oozie.service.TestRecoveryService.testCoordActionRecoveryServiceForSubmitted</div></li><li><div>org.apache.oozie.service.TestRecoveryService.testCoordActionRecoveryServiceForWaiting</div></li><li><div>org.apache.oozie.service.TestRecoveryService.testWorkflowActionRecoveryUserRetry</div></li><li><div>org.apache.oozie.service.TestShareLibService.testDeterminingLatestSharelibPathOn10Threads</div></li><li><div>org.apache.oozie.service.TestShareLibService.testPurgeJar</div></li><li><div>org.apache.oozie.service.TestShareLibService.testDeterminingLatestSharelibPathOn5Threads</div></li><li><div>org.apache.oozie.service.TestShareLibService.testCreateLauncherLibPath</div></li><li><div>org.apache.oozie.service.TestShareLibService.testShareLibLoadFilesFromHDFS</div></li><li><div>org.apache.oozie.service.TestShareLibService.testDuplicateJarsInDistributedCache</div></li><li><div>org.apache.oozie.service.TestShareLibService.testLoadMappingFilesFromDFSandLocalFs</div></li><li><div>org.apache.oozie.service.TestShareLibService.testShareLib</div></li><li><div>org.apache.oozie.service.TestShareLibService.testDeterminingLatestSharelibPathOn1Thread</div></li><li><div>org.apache.oozie.service.TestShareLibService.testGetShareLibCompatible</div></li><li><div>org.apache.oozie.service.TestShareLibService.testRetentionOverflow</div></li><li><div>org.apache.oozie.service.TestShareLibService.testParsingALotOfShareLibsParallel</div></li><li><div>org.apache.oozie.service.TestShareLibService.testShareLibLoadFilesFromLocalFs</div></li><li><div>org.apache.oozie.service.TestShareLibService.testLoadfromDFS</div></li><li><div>org.apache.oozie.service.TestShareLibService.testPurgeShareLib</div></li><li><div>org.apache.oozie.service.TestShareLibService.testConfFileAddedToDistributedCache</div></li><li><div>org.apache.oozie.service.TestShareLibService.testGetShareLibPath</div></li><li><div>org.apache.oozie.service.TestShareLibService.testfailFast</div></li><li><div>org.apache.oozie.service.TestShareLibService.testShareLibLoadFileMultipleFile</div></li><li><div>org.apache.oozie.service.TestShareLibService.testMetafileSymlink</div></li><li><div>org.apache.oozie.service.TestShareLibService.testConfFileAddedToActionConf</div></li><li><div>org.apache.oozie.service.TestShareLibService.testMultipleLauncherCall</div></li><li><div>org.apache.oozie.service.TestShareLibService.testAddShareLib_pig</div></li><li><div>org.apache.oozie.service.TestShareLibService.testAddShareLibDistributedCache</div></li><li><div>org.apache.oozie.service.TestShareLibService.testPurgeLauncherJar</div></li><li><div>org.apache.oozie.service.TestShareLibService.testAddShareLib_pig_withVersion</div></li><li><div>org.apache.oozie.service.TestStatusTransitService.testBundleStatusTransitRunningWithError</div></li><li><div>org.apache.oozie.service.TestStatusTransitService.testCoordStatusTransitServiceSuspendAndResume</div></li><li><div>org.apache.oozie.service.TestStatusTransitService.testCoordStatusTransitServiceSucceeded</div></li><li><div>org.apache.oozie.service.TestStatusTransitService.testBundleStatusTransitServiceSucceeded1</div></li><li><div>org.apache.oozie.service.TestStatusTransitService.testBundleStatusTransitServiceSucceeded2</div></li><li><div>org.apache.oozie.service.TestStatusTransitService.testBundleStatusTransitServiceSucceeded3</div></li><li><div>org.apache.oozie.service.TestStatusTransitService.testCoordStatusTransitServiceStaleCoordActions</div></li><li><div>org.apache.oozie.service.TestStatusTransitService.testFoo</div></li><li><div>org.apache.oozie.service.TestStatusTransitService.testCoordStatusTransitServiceSuspendedWithError</div></li><li><div>org.apache.oozie.service.TestStatusTransitService.testBundleStatusTransitServiceKilled</div></li><li><div>org.apache.oozie.service.TestStatusTransitService.testBundleStatusTransitWithLock</div></li><li><div>org.apache.oozie.service.TestStatusTransitService.testBundleStatusTransitServicePaused</div></li><li><div>org.apache.oozie.service.TestStatusTransitService.testBundleRunningAfterCoordResume</div></li><li><div>org.apache.oozie.service.TestStatusTransitService.testBundleStatusTransitServiceSuspended</div></li><li><div>org.apache.oozie.service.TestStatusTransitService.testCoordStatusTransitServiceUpdateLastModifiedTime</div></li><li><div>org.apache.oozie.service.TestStatusTransitService.testCoordStatusTransitServiceKilledByUser1</div></li><li><div>org.apache.oozie.service.TestStatusTransitService.testCoordStatusTransitServiceKilledByUser2</div></li><li><div>org.apache.oozie.service.TestStatusTransitService.testBundleStatusTransitServiceRunningWithError</div></li><li><div>org.apache.oozie.service.TestStatusTransitService.testCoordStatusTransitServicePaused</div></li><li><div>org.apache.oozie.service.TestStatusTransitService.testCoordStatusTransitServiceBackwardSupport</div></li><li><div>org.apache.oozie.service.TestStatusTransitService.testCoordStatusTransitServiceForTimeout</div></li><li><div>org.apache.oozie.service.TestStatusTransitService.testBundleStatusTransitRunningFromKilled</div></li><li><div>org.apache.oozie.service.TestStatusTransitService.testCoordStatusTransitServiceRunning1</div></li><li><div>org.apache.oozie.service.TestStatusTransitService.testCoordStatusTransitServiceRunning2</div></li><li><div>org.apache.oozie.service.TestStatusTransitService.testCoordStatusTransitServiceRunning3</div></li><li><div>org.apache.oozie.service.TestStatusTransitService.testCoordStatusTransitServiceSuspendedByUser</div></li><li><div>org.apache.oozie.service.TestStatusTransitService.testBundleStatusTransitServiceSuspendedWithError</div></li><li><div>org.apache.oozie.service.TestStatusTransitService.testBundleStatusCoordSubmitFails</div></li><li><div>org.apache.oozie.service.TestStatusTransitService.testCoordStatusTransitServiceDoneWithError</div></li><li><div>org.apache.oozie.service.TestStatusTransitService.testBundleStatusTransitServicePausedWithError</div></li><li><div>org.apache.oozie.service.TestStatusTransitService.testCoordStatusTransitServiceNoDoneWithErrorForBackwardSupport</div></li><li><div>org.apache.oozie.service.TestStatusTransitService.testBundleStatusTransitServiceForTerminalStates</div></li><li><div>org.apache.oozie.service.TestStatusTransitService.testCoordStatusTransitRunningFromKilled</div></li><li><div>org.apache.oozie.service.TestStatusTransitService.testBundleStatusTransitServiceKilled2</div></li><li><div>org.apache.oozie.service.TestStatusTransitService.testCoordStatusTransitServiceSuspendedBottomUp</div></li><li><div>org.apache.oozie.service.TestStatusTransitService.testCoordStatusTransitServicePausedWithError</div></li><li><div>org.apache.oozie.service.TestStatusTransitService.testBundleStatusNotTransitionFromKilled</div></li><li><div>org.apache.oozie.service.TestStatusTransitService.testCoordStatusTransitWithLock</div></li><li><div>org.apache.oozie.service.TestStatusTransitService.testCoordNotTransitionfromKilled</div></li><li><div>org.apache.oozie.service.TestZKJobsConcurrencyService.testGetJobIdsForThisServer</div></li><li><div>org.apache.oozie.service.TestZKJobsConcurrencyService.testGetServerUrls</div></li><li><div>org.apache.oozie.service.TestZKJobsConcurrencyService.testIsLeader</div></li><li><div>org.apache.oozie.service.TestZKJobsConcurrencyService.testInstrumentation</div></li><li><div>org.apache.oozie.service.TestZKJobsConcurrencyService.testRegisterUnregister</div></li><li><div>org.apache.oozie.service.TestZKJobsConcurrencyService.testisAllServerRequest</div></li><li><div>org.apache.oozie.service.TestZKJobsConcurrencyService.testIsJobIdForThisServer</div></li><li><div>org.apache.oozie.service.TestZKLocksService.testReentrantMultipleThread</div></li><li><div>org.apache.oozie.service.TestZKLocksService.testTimeoutWaitingWriteLockOozies</div></li><li><div>org.apache.oozie.service.TestZKLocksService.testWriteReadLockOozies</div></li><li><div>org.apache.oozie.service.TestZKLocksService.testTimeoutWaitingWriteLockThreads</div></li><li><div>org.apache.oozie.service.TestZKLocksService.testTimeoutTimingOutWriteLockOozies</div></li><li><div>org.apache.oozie.service.TestZKLocksService.testLocksAreReused</div></li><li><div>org.apache.oozie.service.TestZKLocksService.testWaitWriteLockOozies</div></li><li><div>org.apache.oozie.service.TestZKLocksService.testWaitWriteLockThreads</div></li><li><div>org.apache.oozie.service.TestZKLocksService.testReentrantMultipleCall</div></li><li><div>org.apache.oozie.service.TestZKLocksService.testLocksAreGarbageCollected</div></li><li><div>org.apache.oozie.service.TestZKLocksService.testLockRelease</div></li><li><div>org.apache.oozie.service.TestZKLocksService.testLockReaper</div></li><li><div>org.apache.oozie.service.TestZKLocksService.testRegisterUnregister</div></li><li><div>org.apache.oozie.service.TestZKLocksService.testReadLockOozies</div></li><li><div>org.apache.oozie.service.TestZKLocksService.testReadWriteLockOozies</div></li><li><div>org.apache.oozie.service.TestZKLocksService.testWriteReadLockThreads</div></li><li><div>org.apache.oozie.service.TestZKLocksService.testReadWriteLockThreads</div></li><li><div>org.apache.oozie.service.TestZKLocksService.testNoWaitWriteLockOozies</div></li><li><div>org.apache.oozie.service.TestZKLocksService.testTimeoutTimingOutWriteLockThreads</div></li><li><div>org.apache.oozie.service.TestZKLocksService.testReadLockThreads</div></li><li><div>org.apache.oozie.service.TestZKLocksService.testNoWaitWriteLockThreads</div></li><li><div>org.apache.oozie.service.TestZKLocksService.testRetriableRelease</div></li><li><div>org.apache.oozie.service.TestZKUUIDService.testRegisterUnregister</div></li><li><div>org.apache.oozie.service.TestZKUUIDService.testBulkJobForZKUUIDService</div></li><li><div>org.apache.oozie.service.TestZKUUIDService.testMultipleIDGeneration</div></li><li><div>org.apache.oozie.service.TestZKUUIDService.testFallback</div></li><li><div>org.apache.oozie.service.TestZKUUIDService.testMultipleIDGeneration_withMultiThread</div></li><li><div>org.apache.oozie.service.TestZKUUIDService.testResetSequence_withMultiThread</div></li><li><div>org.apache.oozie.service.TestZKUUIDService.testResetSequence</div></li><li><div>org.apache.oozie.service.TestZKUUIDService.testIDGeneration</div></li><li><div>org.apache.oozie.service.TestZKXLogStreamingService.testDisableLogOverWS</div></li><li><div>org.apache.oozie.service.TestZKXLogStreamingService.testStreamingWithMultipleOozieServers_coordActionList</div></li><li><div>org.apache.oozie.service.TestZKXLogStreamingService.testNoDashInConversionPattern</div></li><li><div>org.apache.oozie.service.TestZKXLogStreamingService.testStreamingWithMultipleOozieServers</div></li><li><div>org.apache.oozie.service.TestZKXLogStreamingService.testTuncateLog</div></li><li><div>org.apache.oozie.service.TestZKXLogStreamingService.testRegisterUnregister</div></li><li><div>org.apache.oozie.service.TestZKXLogStreamingService.testStreamingWithMultipleOozieServers_errorLog</div></li><li><div>org.apache.oozie.sla.TestSLACalculationJPAExecutor.testRollback</div></li><li><div>org.apache.oozie.sla.TestSLACalculationJPAExecutor.testInsert</div></li><li><div>org.apache.oozie.sla.TestSLACalculationJPAExecutor.testInsertUpdate</div></li><li><div>org.apache.oozie.sla.TestSLACalculatorMemory.testSLAHistorySet</div></li><li><div>org.apache.oozie.sla.TestSLACalculatorMemory.testEventOutOfOrder</div></li><li><div>org.apache.oozie.sla.TestSLACalculatorMemory.testWorkflowActionSLAStatusOnRestart</div></li><li><div>org.apache.oozie.sla.TestSLACalculatorMemory.testHistoryPurge</div></li><li><div>org.apache.oozie.sla.TestSLACalculatorMemory.testWFEndNotCoord</div></li><li><div>org.apache.oozie.sla.TestSLACalculatorMemory.testWorkflowJobSLAStatusOnRestart</div></li><li><div>org.apache.oozie.sla.TestSLACalculatorMemory.testEventMissOnRestart</div></li><li><div>org.apache.oozie.sla.TestSLACalculatorMemory.testDuplicateEndMiss</div></li><li><div>org.apache.oozie.sla.TestSLACalculatorMemory.testWhenSLARegistrationIsAddedAndAllDBCallsAreDisruptedBeanIsNotStored</div></li><li><div>org.apache.oozie.sla.TestSLACalculatorMemory.testSingleAddUpdateRemoveInstrumentedCorrectly</div></li><li><div>org.apache.oozie.sla.TestSLACalculatorMemory.testSLAEvents1</div></li><li><div>org.apache.oozie.sla.TestSLACalculatorMemory.testSLAEvents2</div></li><li><div>org.apache.oozie.sla.TestSLACalculatorMemory.testAddMultipleRestartRemoveMultipleInstrumentedCorrectly</div></li><li><div>org.apache.oozie.sla.TestSLACalculatorMemory.testWhenSLARegistrationIsUpdatedBeanIsStoredCorrectly</div></li><li><div>org.apache.oozie.sla.TestSLACalculatorMemory.testErrorLoggingWithJobIdPrefix</div></li><li><div>org.apache.oozie.sla.TestSLACalculatorMemory.testDisablingAlertsEvents</div></li><li><div>org.apache.oozie.sla.TestSLACalculatorMemory.testWhenSLARegistrationIsUpdatedAndAllDBCallsAreDisruptedBeanIsNotStored</div></li><li><div>org.apache.oozie.sla.TestSLACalculatorMemory.testLoadOnRestart</div></li><li><div>org.apache.oozie.sla.TestSLACalculatorMemory.testCoordinatorActionSLAStatusOnRestart</div></li><li><div>org.apache.oozie.sla.TestSLACalculatorMemory.testDuplicateStartMiss</div></li><li><div>org.apache.oozie.sla.TestSLACalculatorMemory.testWhenSLARegistrationExistsWithoutSLASummaryUpdateSLARetries</div></li><li><div>org.apache.oozie.sla.TestSLACalculatorMemory.testWhenSLARegistrationIsAddedBeanIsStoredCorrectly</div></li><li><div>org.apache.oozie.sla.TestSLAEventGeneration.testCoordRerunNoSLA</div></li><li><div>org.apache.oozie.sla.TestSLAEventGeneration.testWorkflowActionSLARerun</div></li><li><div>org.apache.oozie.sla.TestSLAEventGeneration.testSLASchema1BackwardCompatibilitySubmitStart</div></li><li><div>org.apache.oozie.sla.TestSLAEventGeneration.testWorkflowJobSLANewSubmitStart</div></li><li><div>org.apache.oozie.sla.TestSLAEventGeneration.testSLASchema1BackwardCompatibilityKill</div></li><li><div>org.apache.oozie.sla.TestSLAEventGeneration.testWorkflowJobSLANewKill</div></li><li><div>org.apache.oozie.sla.TestSLAEventGeneration.testFailureAndMissEventsOnKill</div></li><li><div>org.apache.oozie.sla.TestSLAEventGeneration.testWorkflowJobSLARerun</div></li><li><div>org.apache.oozie.sla.TestSLAEventGeneration.testCoordinatorActionCommandsSubmitAndStart</div></li><li><div>org.apache.oozie.sla.TestSLARegistrationGetJPAExecutor.testSLARegistrationGet</div></li><li><div>org.apache.oozie.sla.TestSLARegistrationGetJPAExecutor.testSLARegistrationBulkConfigMap</div></li><li><div>org.apache.oozie.sla.TestSLARegistrationGetJPAExecutor.testSlaConfigStringToMap</div></li><li><div>org.apache.oozie.sla.TestSLAService.testUpdateSLA</div></li><li><div>org.apache.oozie.sla.TestSLAService.testEndMissDBConfirm</div></li><li><div>org.apache.oozie.sla.TestSLAService.testSLAOperations</div></li><li><div>org.apache.oozie.sla.TestSLAService.testBasicService</div></li><li><div>org.apache.oozie.sla.TestSLASummaryGetOnRestartJPAExecutor.testSLARegistrationGet</div></li><li><div>org.apache.oozie.store.TestDBWorkflowStore.testDBWorkflowStore</div></li><li><div>org.apache.oozie.action.hadoop.TestHive2ActionExecutor.testHive2Action</div></li><li><div>org.apache.oozie.action.hadoop.TestPigActionExecutor.testExecutionStatsWithRetrieveStatsFalse</div></li><li><div>org.apache.oozie.action.hadoop.TestPigActionExecutor.testExternalChildIds</div></li><li><div>org.apache.oozie.action.hadoop.TestPigActionExecutor.testExecutionStats</div></li><li><div>org.apache.oozie.action.hadoop.TestPyspark.testPyspark</div></li><li><div>org.apache.oozie.action.hadoop.TestSparkActionExecutor.testSparkAction</div></li><li><div>org.apache.oozie.action.hadoop.TestSqoopActionExecutor.testSqoopActionWithRedundantArgsAndFreeFormQuery</div></li><li><div>org.apache.oozie.action.hadoop.TestSqoopActionExecutor.testSqoopActionWithArgsAndFreeFormQuery</div></li><li><div>org.apache.oozie.action.hadoop.TestSqoopActionExecutor.testSqoopActionWithRedundantPrefix</div></li><li><div>org.apache.oozie.action.hadoop.TestSqoopActionExecutor.testSqoopAction</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.oozie.command.coord.TestCoordActionInputCheckXCommandNonUTC.testNone</div></li><li><div>org.apache.oozie.service.TestCallableQueueService.testQueueSizeWithDelayedElements</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.oozie.command.coord.TestCoordActionsKillXCommand.testActionKillCommandActionNumbers</div></li></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="phoenix"><div style="font-weight:bold;" class="panel-heading">PHOENIX<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Prajyot)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>aa276bfd2ef856f9da24c32710c616c9d195463b</div><div><b>Last Run: </b>06-12-2018 23:45 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC UBUNTU18</th><th>X86 UBUNTU18</th><th>PPC RHEL72</th><th>X86 RHEL72</th><th>PPC RHEL75</th><th>X86 RHEL75</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 1719</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 1719</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 1719</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 1719</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 1719</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 1719</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 1719</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 1719</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td></tr><tr><td>Result</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="pig"><div style="font-weight:bold;" class="panel-heading">PIG<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Yussuf)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/trunk</div><div><b>Last Revision: </b>80bf29b75bc5b20c50cf06cef7d71c92a990266d</div><div><b>Last Run: </b>06-12-2018 02:58 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC UBUNTU18</th><th>X86 UBUNTU18</th><th>PPC RHEL72</th><th>X86 RHEL72</th><th>PPC RHEL75</th><th>X86 RHEL75</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 895</div><div>Failed Count : 9</div><div>Skipped Count : 0</div></td><td><div>Total Count : 895</div><div>Failed Count : 9</div><div>Skipped Count : 0</div></td><td><div>Total Count : 895</div><div>Failed Count : 9</div><div>Skipped Count : 0</div></td><td><div>Total Count : 895</div><div>Failed Count : 9</div><div>Skipped Count : 0</div></td><td><div>Total Count : 895</div><div>Failed Count : 9</div><div>Skipped Count : 0</div></td><td><div>Total Count : 895</div><div>Failed Count : 10</div><div>Skipped Count : 0</div></td><td><div>Total Count : 895</div><div>Failed Count : 9</div><div>Skipped Count : 0</div></td><td><div>Total Count : 895</div><div>Failed Count : 9</div><div>Skipped Count : 0</div></td></tr><tr><td>Result</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileDeclareCombo</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testSameParamInMultipleFiles</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileComboDuplicate</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileCombo</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testMultipleParamsinSingleLine</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileDeclareDefaultComboDuplicates</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testFileParamsFromMultipleFiles</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testMultipleDeclareScope</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileDeclareComboDuplicates</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileDeclareCombo</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testSameParamInMultipleFiles</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileComboDuplicate</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileCombo</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testMultipleParamsinSingleLine</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileDeclareDefaultComboDuplicates</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testFileParamsFromMultipleFiles</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testMultipleDeclareScope</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileDeclareComboDuplicates</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileDeclareCombo</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testSameParamInMultipleFiles</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileComboDuplicate</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileCombo</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testMultipleParamsinSingleLine</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileDeclareDefaultComboDuplicates</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testFileParamsFromMultipleFiles</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testMultipleDeclareScope</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileDeclareComboDuplicates</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileDeclareCombo</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testSameParamInMultipleFiles</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileComboDuplicate</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileCombo</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testMultipleParamsinSingleLine</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileDeclareDefaultComboDuplicates</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testFileParamsFromMultipleFiles</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testMultipleDeclareScope</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileDeclareComboDuplicates</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileDeclareCombo</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testSameParamInMultipleFiles</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileComboDuplicate</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileCombo</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testMultipleParamsinSingleLine</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileDeclareDefaultComboDuplicates</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testFileParamsFromMultipleFiles</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testMultipleDeclareScope</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileDeclareComboDuplicates</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.pig.test.TestBuiltin.testRANDOMWithJob</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileDeclareCombo</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testSameParamInMultipleFiles</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileComboDuplicate</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileCombo</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testMultipleParamsinSingleLine</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileDeclareDefaultComboDuplicates</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testFileParamsFromMultipleFiles</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testMultipleDeclareScope</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileDeclareComboDuplicates</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileDeclareCombo</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testSameParamInMultipleFiles</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileComboDuplicate</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileCombo</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testMultipleParamsinSingleLine</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileDeclareDefaultComboDuplicates</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testFileParamsFromMultipleFiles</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testMultipleDeclareScope</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileDeclareComboDuplicates</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileDeclareCombo</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testSameParamInMultipleFiles</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileComboDuplicate</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileCombo</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testMultipleParamsinSingleLine</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileDeclareDefaultComboDuplicates</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testFileParamsFromMultipleFiles</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testMultipleDeclareScope</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileDeclareComboDuplicates</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 27 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 27 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 27 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 27 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 27 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Values should be different. Actual: 0.5559819908695893</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 27 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 27 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 27 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.pig.test.TestBuiltin.testRANDOMWithJob</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="ranger"><div style="font-weight:bold;" class="panel-heading">RANGER<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Yussuf)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>beaa563f11a932a63743a5a59a0a17870a08d3e9</div><div><b>Last Run: </b>13-12-2018 01:58 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC UBUNTU18</th><th>X86 UBUNTU18</th><th>PPC RHEL72</th><th>X86 RHEL72</th><th>PPC RHEL75</th><th>X86 RHEL75</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 1315</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1315</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1315</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1315</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1315</div><div>Failed Count : 1</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1315</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1315</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1315</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td></tr><tr><td>Result</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.ranger.audit.TestAuditQueue.testAuditSummaryByInfra</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;9&gt; but was:&lt;0&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.ranger.audit.TestAuditQueue.testAuditSummaryByInfra</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="slider"><div style="font-weight:bold;" class="panel-heading">SLIDER<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Yussuf)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/develop</div><div><b>Last Revision: </b>1d4f519d763210f46e327338be72efa99e65cb5d</div><div><b>Last Run: </b>06-12-2018 20:54 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC UBUNTU18</th><th>X86 UBUNTU18</th><th>PPC RHEL72</th><th>X86 RHEL72</th><th>PPC RHEL75</th><th>X86 RHEL75</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 607</div><div>Failed Count : 0</div><div>Skipped Count : 13</div></td><td><div>Total Count : 607</div><div>Failed Count : 0</div><div>Skipped Count : 13</div></td><td><div>Total Count : 607</div><div>Failed Count : 0</div><div>Skipped Count : 13</div></td><td><div>Total Count : 607</div><div>Failed Count : 0</div><div>Skipped Count : 13</div></td><td><div>Total Count : 607</div><div>Failed Count : 0</div><div>Skipped Count : 13</div></td><td><div>Total Count : 607</div><div>Failed Count : 0</div><div>Skipped Count : 13</div></td><td><div>Total Count : 607</div><div>Failed Count : 0</div><div>Skipped Count : 13</div></td><td><div>Total Count : 607</div><div>Failed Count : 0</div><div>Skipped Count : 13</div></td></tr><tr><td>Result</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="spark"><div style="font-weight:bold;" class="panel-heading">SPARK<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Prajyot)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>07a700b3711057553dfbb7b047216565726509c7</div><div><b>Last Run: </b>21-11-2018 18:59 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC UBUNTU18</th><th>X86 UBUNTU18</th><th>PPC RHEL72</th><th>X86 RHEL72</th><th>PPC RHEL75</th><th>X86 RHEL75</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 15087</div><div>Failed Count : 3</div><div>Skipped Count : 652</div></td><td><div>Total Count : 2828</div><div>Failed Count : 890</div><div>Skipped Count : 26</div></td><td><div>Total Count : 14754</div><div>Failed Count : 48</div><div>Skipped Count : 651</div></td><td><div>Total Count : 16880</div><div>Failed Count : 3</div><div>Skipped Count : 666</div></td><td><div>Total Count : 14754</div><div>Failed Count : 49</div><div>Skipped Count : 651</div></td><td><div>Total Count : 2811</div><div>Failed Count : 871</div><div>Skipped Count : 26</div></td><td><div>Total Count : 15087</div><div>Failed Count : 2</div><div>Skipped Count : 652</div></td><td><div>Total Count : 2779</div><div>Failed Count : 842</div><div>Skipped Count : 26</div></td></tr><tr><td>Result</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"><div><li>org.apache.spark.streaming.kafka010.JavaDirectKafkaStreamSuite.testKafkaStream</li></div><div><li>org.apache.spark.streaming.kafka010.JavaKafkaRDDSuite.testKafkaRDD</li></div><div><li>org.apache.spark.sql.execution.ui.SQLAppStatusListenerSuite.driver side SQL metrics</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.spark.network.ChunkFetchIntegrationSuite.fetchBothChunks</li></div><div><li>org.apache.spark.network.sasl.SparkSaslSuite.testFileRegionEncryption</li></div><div><li>org.apache.spark.ExecutorPluginSuite.testPluginShutdownWithException</li></div><div><li>org.apache.spark.JavaJdbcRDDSuite.testJavaJdbcRDD</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.leftOuterJoin</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.foldReduce</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.mapsFromPairsToPairs</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.mapsFromPairsToPairs</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.flatMap</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.flatMap</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.foreach</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.foreach</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.map</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.map</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.zip</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.zip</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.keyBy</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.keyBy</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.groupBy</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.groupBy</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.mapPartitions</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.mapPartitions</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.foldByKey</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.foldByKey</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.mapOnPairRDD</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.mapOnPairRDD</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.sequenceFile</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.sequenceFile</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.collectPartitions</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.collectPartitions</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.reduceByKey</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.reduceByKey</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.foreachWithAnonymousClass</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.foreachWithAnonymousClass</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.collectAsMapWithIntArrayValues</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.collectAsMapWithIntArrayValues</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.zipPartitions</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.zipPartitions</li></div><div><li>test.org.apache.spark.JavaAPISuite.groupByOnPairRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.groupByOnPairRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.binaryFilesCaching</li></div><div><li>test.org.apache.spark.JavaAPISuite.binaryFilesCaching</li></div><div><li>test.org.apache.spark.JavaAPISuite.sparkContextUnion</li></div><div><li>test.org.apache.spark.JavaAPISuite.sparkContextUnion</li></div><div><li>test.org.apache.spark.JavaAPISuite.checkpointAndComputation</li></div><div><li>test.org.apache.spark.JavaAPISuite.checkpointAndComputation</li></div><div><li>test.org.apache.spark.JavaAPISuite.leftOuterJoin</li></div><div><li>test.org.apache.spark.JavaAPISuite.leftOuterJoin</li></div><div><li>test.org.apache.spark.JavaAPISuite.keyByOnPairRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.keyByOnPairRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.getNumPartitions</li></div><div><li>test.org.apache.spark.JavaAPISuite.getNumPartitions</li></div><div><li>test.org.apache.spark.JavaAPISuite.wholeTextFiles</li></div><div><li>test.org.apache.spark.JavaAPISuite.wholeTextFiles</li></div><div><li>test.org.apache.spark.JavaAPISuite.binaryFiles</li></div><div><li>test.org.apache.spark.JavaAPISuite.binaryFiles</li></div><div><li>test.org.apache.spark.JavaAPISuite.foldReduce</li></div><div><li>test.org.apache.spark.JavaAPISuite.foldReduce</li></div><div><li>test.org.apache.spark.JavaAPISuite.writeWithNewAPIHadoopFile</li></div><div><li>test.org.apache.spark.JavaAPISuite.writeWithNewAPIHadoopFile</li></div><div><li>test.org.apache.spark.JavaAPISuite.hadoopFile</li></div><div><li>test.org.apache.spark.JavaAPISuite.hadoopFile</li></div><div><li>test.org.apache.spark.JavaAPISuite.lookup</li></div><div><li>test.org.apache.spark.JavaAPISuite.lookup</li></div><div><li>test.org.apache.spark.JavaAPISuite.countAsync</li></div><div><li>test.org.apache.spark.JavaAPISuite.countAsync</li></div><div><li>test.org.apache.spark.JavaAPISuite.textFiles</li></div><div><li>test.org.apache.spark.JavaAPISuite.textFiles</li></div><div><li>test.org.apache.spark.JavaAPISuite.binaryRecords</li></div><div><li>test.org.apache.spark.JavaAPISuite.toLocalIterator</li></div><div><li>test.org.apache.spark.JavaAPISuite.repartitionAndSortWithinPartitions</li></div><div><li>test.org.apache.spark.JavaAPISuite.repartitionAndSortWithinPartitions</li></div><div><li>test.org.apache.spark.JavaAPISuite.reduce</li></div><div><li>test.org.apache.spark.JavaAPISuite.reduce</li></div><div><li>test.org.apache.spark.JavaAPISuite.sample</li></div><div><li>test.org.apache.spark.JavaAPISuite.sample</li></div><div><li>test.org.apache.spark.JavaAPISuite.sortBy</li></div><div><li>test.org.apache.spark.JavaAPISuite.sortBy</li></div><div><li>test.org.apache.spark.JavaAPISuite.mapsFromPairsToPairs</li></div><div><li>test.org.apache.spark.JavaAPISuite.mapsFromPairsToPairs</li></div><div><li>test.org.apache.spark.JavaAPISuite.flatMap</li></div><div><li>test.org.apache.spark.JavaAPISuite.flatMap</li></div><div><li>test.org.apache.spark.JavaAPISuite.cogroup3</li></div><div><li>test.org.apache.spark.JavaAPISuite.cogroup3</li></div><div><li>test.org.apache.spark.JavaAPISuite.cogroup4</li></div><div><li>test.org.apache.spark.JavaAPISuite.cogroup4</li></div><div><li>test.org.apache.spark.JavaAPISuite.randomSplit</li></div><div><li>test.org.apache.spark.JavaAPISuite.randomSplit</li></div><div><li>test.org.apache.spark.JavaAPISuite.persist</li></div><div><li>test.org.apache.spark.JavaAPISuite.persist</li></div><div><li>test.org.apache.spark.JavaAPISuite.foreach</li></div><div><li>test.org.apache.spark.JavaAPISuite.foreach</li></div><div><li>test.org.apache.spark.JavaAPISuite.hadoopFileCompressed</li></div><div><li>test.org.apache.spark.JavaAPISuite.hadoopFileCompressed</li></div><div><li>test.org.apache.spark.JavaAPISuite.textFilesCompressed</li></div><div><li>test.org.apache.spark.JavaAPISuite.textFilesCompressed</li></div><div><li>test.org.apache.spark.JavaAPISuite.testAsyncActionCancellation</li></div><div><li>test.org.apache.spark.JavaAPISuite.testAsyncActionCancellation</li></div><div><li>test.org.apache.spark.JavaAPISuite.checkpointAndRestore</li></div><div><li>test.org.apache.spark.JavaAPISuite.checkpointAndRestore</li></div><div><li>test.org.apache.spark.JavaAPISuite.sortByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.sortByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.aggregateByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.aggregateByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.map</li></div><div><li>test.org.apache.spark.JavaAPISuite.map</li></div><div><li>test.org.apache.spark.JavaAPISuite.max</li></div><div><li>test.org.apache.spark.JavaAPISuite.max</li></div><div><li>test.org.apache.spark.JavaAPISuite.min</li></div><div><li>test.org.apache.spark.JavaAPISuite.min</li></div><div><li>test.org.apache.spark.JavaAPISuite.top</li></div><div><li>test.org.apache.spark.JavaAPISuite.top</li></div><div><li>test.org.apache.spark.JavaAPISuite.zip</li></div><div><li>test.org.apache.spark.JavaAPISuite.zip</li></div><div><li>test.org.apache.spark.JavaAPISuite.fold</li></div><div><li>test.org.apache.spark.JavaAPISuite.fold</li></div><div><li>test.org.apache.spark.JavaAPISuite.glom</li></div><div><li>test.org.apache.spark.JavaAPISuite.glom</li></div><div><li>test.org.apache.spark.JavaAPISuite.take</li></div><div><li>test.org.apache.spark.JavaAPISuite.take</li></div><div><li>test.org.apache.spark.JavaAPISuite.javaDoubleRDDHistoGram</li></div><div><li>test.org.apache.spark.JavaAPISuite.javaDoubleRDDHistoGram</li></div><div><li>test.org.apache.spark.JavaAPISuite.collectUnderlyingScalaRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.collectUnderlyingScalaRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.keyBy</li></div><div><li>test.org.apache.spark.JavaAPISuite.mapPartitionsWithIndex</li></div><div><li>test.org.apache.spark.JavaAPISuite.mapPartitionsWithIndex</li></div><div><li>test.org.apache.spark.JavaAPISuite.sampleByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.sampleByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.intersection</li></div><div><li>test.org.apache.spark.JavaAPISuite.intersection</li></div><div><li>test.org.apache.spark.JavaAPISuite.aggregate</li></div><div><li>test.org.apache.spark.JavaAPISuite.aggregate</li></div><div><li>test.org.apache.spark.JavaAPISuite.cartesian</li></div><div><li>test.org.apache.spark.JavaAPISuite.cartesian</li></div><div><li>test.org.apache.spark.JavaAPISuite.countApproxDistinctByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.countApproxDistinctByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.readWithNewAPIHadoopFile</li></div><div><li>test.org.apache.spark.JavaAPISuite.readWithNewAPIHadoopFile</li></div><div><li>test.org.apache.spark.JavaAPISuite.testRegisterKryoClasses</li></div><div><li>test.org.apache.spark.JavaAPISuite.testRegisterKryoClasses</li></div><div><li>test.org.apache.spark.JavaAPISuite.groupBy</li></div><div><li>test.org.apache.spark.JavaAPISuite.groupBy</li></div><div><li>test.org.apache.spark.JavaAPISuite.sampleByKeyExact</li></div><div><li>test.org.apache.spark.JavaAPISuite.sampleByKeyExact</li></div><div><li>test.org.apache.spark.JavaAPISuite.mapPartitions</li></div><div><li>test.org.apache.spark.JavaAPISuite.mapPartitions</li></div><div><li>test.org.apache.spark.JavaAPISuite.takeOrdered</li></div><div><li>test.org.apache.spark.JavaAPISuite.takeOrdered</li></div><div><li>test.org.apache.spark.JavaAPISuite.foldByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.foldByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.objectFilesOfInts</li></div><div><li>test.org.apache.spark.JavaAPISuite.objectFilesOfInts</li></div><div><li>test.org.apache.spark.JavaAPISuite.treeAggregate</li></div><div><li>test.org.apache.spark.JavaAPISuite.treeAggregate</li></div><div><li>test.org.apache.spark.JavaAPISuite.testGetPersistentRDDs</li></div><div><li>test.org.apache.spark.JavaAPISuite.testGetPersistentRDDs</li></div><div><li>test.org.apache.spark.JavaAPISuite.approximateResults</li></div><div><li>test.org.apache.spark.JavaAPISuite.approximateResults</li></div><div><li>test.org.apache.spark.JavaAPISuite.treeReduce</li></div><div><li>test.org.apache.spark.JavaAPISuite.treeReduce</li></div><div><li>test.org.apache.spark.JavaAPISuite.collectAsMapAndSerialize</li></div><div><li>test.org.apache.spark.JavaAPISuite.collectAsMapAndSerialize</li></div><div><li>test.org.apache.spark.JavaAPISuite.countApproxDistinct</li></div><div><li>test.org.apache.spark.JavaAPISuite.countApproxDistinct</li></div><div><li>test.org.apache.spark.JavaAPISuite.javaDoubleRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.javaDoubleRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.mapOnPairRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.mapOnPairRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.testAsyncActionErrorWrapping</li></div><div><li>test.org.apache.spark.JavaAPISuite.testAsyncActionErrorWrapping</li></div><div><li>test.org.apache.spark.JavaAPISuite.naturalMax</li></div><div><li>test.org.apache.spark.JavaAPISuite.naturalMax</li></div><div><li>test.org.apache.spark.JavaAPISuite.naturalMin</li></div><div><li>test.org.apache.spark.JavaAPISuite.naturalMin</li></div><div><li>test.org.apache.spark.JavaAPISuite.sequenceFile</li></div><div><li>test.org.apache.spark.JavaAPISuite.collectPartitions</li></div><div><li>test.org.apache.spark.JavaAPISuite.collectPartitions</li></div><div><li>test.org.apache.spark.JavaAPISuite.cogroup</li></div><div><li>test.org.apache.spark.JavaAPISuite.cogroup</li></div><div><li>test.org.apache.spark.JavaAPISuite.reduceByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.reduceByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.repartition</li></div><div><li>test.org.apache.spark.JavaAPISuite.repartition</li></div><div><li>test.org.apache.spark.JavaAPISuite.iterator</li></div><div><li>test.org.apache.spark.JavaAPISuite.iterator</li></div><div><li>test.org.apache.spark.JavaAPISuite.emptyRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.emptyRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.zipWithIndex</li></div><div><li>test.org.apache.spark.JavaAPISuite.zipWithIndex</li></div><div><li>test.org.apache.spark.JavaAPISuite.foreachPartition</li></div><div><li>test.org.apache.spark.JavaAPISuite.foreachPartition</li></div><div><li>test.org.apache.spark.JavaAPISuite.combineByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.combineByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.takeAsync</li></div><div><li>test.org.apache.spark.JavaAPISuite.takeAsync</li></div><div><li>test.org.apache.spark.JavaAPISuite.collectAsMapWithIntArrayValues</li></div><div><li>test.org.apache.spark.JavaAPISuite.collectAsMapWithIntArrayValues</li></div><div><li>test.org.apache.spark.JavaAPISuite.objectFilesOfComplexTypes</li></div><div><li>test.org.apache.spark.JavaAPISuite.objectFilesOfComplexTypes</li></div><div><li>test.org.apache.spark.JavaAPISuite.zipWithUniqueId</li></div><div><li>test.org.apache.spark.JavaAPISuite.zipWithUniqueId</li></div><div><li>test.org.apache.spark.JavaAPISuite.collectAsync</li></div><div><li>test.org.apache.spark.JavaAPISuite.collectAsync</li></div><div><li>test.org.apache.spark.JavaAPISuite.foreachAsync</li></div><div><li>test.org.apache.spark.JavaAPISuite.foreachAsync</li></div><div><li>test.org.apache.spark.JavaAPISuite.zipPartitions</li></div><div><li>test.org.apache.spark.JavaAPISuite.zipPartitions</li></div><div><li>test.org.apache.spark.JavaAPISuite.reduceOnJavaDoubleRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.reduceOnJavaDoubleRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.isEmpty</li></div><div><li>test.org.apache.spark.JavaAPISuite.isEmpty</li></div><div><li>test.org.apache.spark.JavaSparkContextSuite.javaSparkContext</li></div><div><li>test.org.apache.spark.JavaSparkContextSuite.scalaSparkContext</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single BooleanType with seed -1709147623185220531</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single ByteType with seed -3919555143023375607</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single ShortType with seed 8561384662913348374</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single IntegerType with seed 304026297574910763</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single LongType with seed -580739969019513000</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single FloatType with seed 2279282329064842619</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single DoubleType with seed 4674024592659163352</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single DecimalType(8,0) with seed 3767101550863559382</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single DecimalType(8,4) with seed 9909692906609911</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single DecimalType(16,0) with seed -8576487407135221635</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single DecimalType(16,11) with seed -4104648621375032742</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single DecimalType(38,0) with seed 2909629238540037729</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single DecimalType(38,38) with seed -3501252253047424307</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single StringType with seed 2041645561532327328</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single BinaryType with seed 4653446803325427556</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.flat schema struct&lt;col_0:decimal(8,4),col_1:bigint,col_2:double,col_3:binary,col_4:decimal(38,38)&gt; with seed -7205048209896592561</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.flat schema struct&lt;col_0:decimal(16,11),col_1:decimal(16,0),col_2:int,col_3:binary,col_4:decimal(8,0)&gt; with seed -6213368686859328797</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.flat schema struct&lt;col_0:decimal(16,11),col_1:decimal(8,0),col_2:string,col_3:int,col_4:decimal(16,0)&gt; with seed 4087558098434540409</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.flat schema struct&lt;col_0:binary,col_1:decimal(8,0),col_2:boolean,col_3:decimal(38,38),col_4:bigint&gt; with seed 8296462836656940639</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.flat schema struct&lt;col_0:decimal(38,0),col_1:decimal(16,0),col_2:boolean,col_3:decimal(8,4),col_4:decimal(38,0)&gt; with seed 3192922614101055592</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.nested schema struct&lt;col_0:tinyint,col_1:decimal(38,0),col_2:binary,col_3:struct&lt;col_0:struct&lt;col_0:struct&lt;col_0:struct&lt;col_0:smallint&gt;&gt;,col_1:bigint,col_2:struct&lt;col_0:struct&lt;col_0:array&lt;bigint&gt;&gt;&gt;,col_3:decimal(8,4)&gt;,col_1:tinyint,col_2:array&lt;smallint&gt;&gt;,col_4:struct&lt;col_0:float&gt;&gt; with seed 6838662997250839589</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.nested schema struct&lt;col_0:struct&lt;col_0:struct&lt;col_0:array&lt;decimal(8,0)&gt;&gt;,col_1:boolean,col_2:array&lt;decimal(38,0)&gt;,col_3:struct&lt;col_0:array&lt;decimal(8,4)&gt;&gt;,col_4:struct&lt;col_0:binary&gt;&gt;,col_1:double,col_2:array&lt;decimal(16,11)&gt;,col_3:array&lt;decimal(16,0)&gt;,col_4:struct&lt;col_0:array&lt;bigint&gt;&gt;,col_5:struct&lt;col_0:array&lt;bigint&gt;&gt;&gt; with seed -8949425111688185310</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.nested schema struct&lt;col_0:decimal(16,0),col_1:struct&lt;col_0:array&lt;decimal(38,38)&gt;&gt;,col_2:array&lt;decimal(8,0)&gt;,col_3:array&lt;decimal(38,38)&gt;,col_4:array&lt;decimal(38,0)&gt;,col_5:struct&lt;col_0:double&gt;,col_6:struct&lt;col_0:array&lt;binary&gt;&gt;,col_7:decimal(38,38),col_8:struct&lt;col_0:struct&lt;col_0:string&gt;&gt;,col_9:decimal(16,0)&gt; with seed 5045342886160651481</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.nested schema struct&lt;col_0:struct&lt;col_0:struct&lt;col_0:decimal(38,38),col_1:string,col_2:decimal(8,0)&gt;,col_1:struct&lt;col_0:double&gt;&gt;,col_1:struct&lt;col_0:smallint&gt;,col_2:struct&lt;col_0:struct&lt;col_0:array&lt;decimal(16,11)&gt;,col_1:array&lt;smallint&gt;&gt;,col_1:tinyint&gt;,col_3:struct&lt;col_0:decimal(8,0)&gt;,col_4:struct&lt;col_0:struct&lt;col_0:struct&lt;col_0:struct&lt;col_0:decimal(16,0)&gt;&gt;&gt;&gt;&gt; with seed 8617731073184928310</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.nested schema struct&lt;col_0:decimal(8,4),col_1:array&lt;float&gt;,col_2:decimal(8,0),col_3:array&lt;smallint&gt;,col_4:array&lt;smallint&gt;,col_5:struct&lt;col_0:struct&lt;col_0:array&lt;decimal(16,0)&gt;&gt;&gt;,col_6:struct&lt;col_0:struct&lt;col_0:array&lt;tinyint&gt;&gt;&gt;,col_7:double,col_8:boolean,col_9:array&lt;decimal(8,4)&gt;&gt; with seed -1895918017264487710</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.read int as string</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.read string as int</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.read float as double</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.read double as float</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.Handle unsupported input of record type</li></div><div><li>org.apache.spark.sql.avro.AvroFunctionsSuite.roundtrip in to_avro and from_avro - int and string</li></div><div><li>org.apache.spark.sql.avro.AvroFunctionsSuite.roundtrip in to_avro and from_avro - struct</li></div><div><li>org.apache.spark.sql.avro.AvroFunctionsSuite.handle invalid input in from_avro</li></div><div><li>org.apache.spark.sql.avro.AvroFunctionsSuite.roundtrip in to_avro and from_avro - array with null</li></div><div><li>org.apache.spark.streaming.kafka010.JavaDirectKafkaStreamSuite.testKafkaStream</li></div><div><li>org.apache.spark.streaming.kafka010.JavaKafkaRDDSuite.testKafkaRDD</li></div><div><li>org.apache.spark.graphx.EdgeRDDSuite.cache, getStorageLevel</li></div><div><li>org.apache.spark.graphx.EdgeRDDSuite.checkpointing</li></div><div><li>org.apache.spark.graphx.GraphLoaderSuite.GraphLoader.edgeListFile</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.joinVertices</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.collectNeighborIds</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.removeSelfEdges</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.filter</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.convertToCanonicalEdges</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.collectEdgesCycleDirectionOut</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.collectEdgesCycleDirectionIn</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.collectEdgesCycleDirectionEither</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.collectEdgesChainDirectionOut</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.collectEdgesChainDirectionIn</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.collectEdgesChainDirectionEither</li></div><div><li>org.apache.spark.graphx.GraphSuite.Graph.fromEdgeTuples</li></div><div><li>org.apache.spark.graphx.GraphSuite.Graph.fromEdges</li></div><div><li>org.apache.spark.graphx.GraphSuite.Graph.apply</li></div><div><li>org.apache.spark.graphx.GraphSuite.triplets</li></div><div><li>org.apache.spark.graphx.GraphSuite.partitionBy</li></div><div><li>org.apache.spark.graphx.GraphSuite.mapVertices</li></div><div><li>org.apache.spark.graphx.GraphSuite.mapVertices changing type with same erased type</li></div><div><li>org.apache.spark.graphx.GraphSuite.mapEdges</li></div><div><li>org.apache.spark.graphx.GraphSuite.mapTriplets</li></div><div><li>org.apache.spark.graphx.GraphSuite.reverse</li></div><div><li>org.apache.spark.graphx.GraphSuite.reverse with join elimination</li></div><div><li>org.apache.spark.graphx.GraphSuite.subgraph</li></div><div><li>org.apache.spark.graphx.GraphSuite.mask</li></div><div><li>org.apache.spark.graphx.GraphSuite.groupEdges</li></div><div><li>org.apache.spark.graphx.GraphSuite.aggregateMessages</li></div><div><li>org.apache.spark.graphx.GraphSuite.outerJoinVertices</li></div><div><li>org.apache.spark.graphx.GraphSuite.more edge partitions than vertex partitions</li></div><div><li>org.apache.spark.graphx.GraphSuite.checkpoint</li></div><div><li>org.apache.spark.graphx.GraphSuite.cache, getStorageLevel</li></div><div><li>org.apache.spark.graphx.GraphSuite.non-default number of edge partitions</li></div><div><li>org.apache.spark.graphx.GraphSuite.unpersist graph RDD</li></div><div><li>org.apache.spark.graphx.GraphSuite.SPARK-14219: pickRandomVertex</li></div><div><li>org.apache.spark.graphx.PregelSuite.1 iteration</li></div><div><li>org.apache.spark.graphx.PregelSuite.chain propagation</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.filter</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.mapValues</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.minus</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.minus with RDD[(VertexId, VD)]</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.minus with non-equal number of partitions</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.diff</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.diff with RDD[(VertexId, VD)]</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.diff vertices with non-equal number of partitions</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.leftJoin</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.leftJoin vertices with non-equal number of partitions</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.innerJoin</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.innerJoin vertices with the non-equal number of partitions</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.aggregateUsingIndex</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.mergeFunc</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.cache, getStorageLevel</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.checkpoint</li></div><div><li>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Grid Connected Components</li></div><div><li>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Reverse Grid Connected Components</li></div><div><li>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Chain Connected Components</li></div><div><li>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Reverse Chain Connected Components</li></div><div><li>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Connected Components on a Toy Connected Graph</li></div><div><li>org.apache.spark.graphx.lib.LabelPropagationSuite.Label Propagation</li></div><div><li>org.apache.spark.graphx.lib.PageRankSuite.Star PageRank</li></div><div><li>org.apache.spark.graphx.lib.PageRankSuite.Star PersonalPageRank</li></div><div><li>org.apache.spark.graphx.lib.PageRankSuite.Grid PageRank</li></div><div><li>org.apache.spark.graphx.lib.PageRankSuite.Chain PageRank</li></div><div><li>org.apache.spark.graphx.lib.PageRankSuite.Chain PersonalizedPageRank</li></div><div><li>org.apache.spark.graphx.lib.PageRankSuite.Loop with source PageRank</li></div><div><li>org.apache.spark.graphx.lib.PageRankSuite.Loop with sink PageRank</li></div><div><li>org.apache.spark.graphx.lib.SVDPlusPlusSuite.Test SVD++ with mean square error on training set</li></div><div><li>org.apache.spark.graphx.lib.ShortestPathsSuite.Shortest Path Computations</li></div><div><li>org.apache.spark.graphx.lib.StronglyConnectedComponentsSuite.Island Strongly Connected Components</li></div><div><li>org.apache.spark.graphx.lib.StronglyConnectedComponentsSuite.Cycle Strongly Connected Components</li></div><div><li>org.apache.spark.graphx.lib.StronglyConnectedComponentsSuite.2 Cycle Strongly Connected Components</li></div><div><li>org.apache.spark.graphx.lib.TriangleCountSuite.Count a single triangle</li></div><div><li>org.apache.spark.graphx.lib.TriangleCountSuite.Count two triangles</li></div><div><li>org.apache.spark.graphx.lib.TriangleCountSuite.Count two triangles with bi-directed edges</li></div><div><li>org.apache.spark.graphx.lib.TriangleCountSuite.Count a single triangle with duplicate edges</li></div><div><li>org.apache.spark.graphx.util.GraphGeneratorsSuite.GraphGenerators.logNormalGraph</li></div><div><li>org.apache.spark.graphx.util.GraphGeneratorsSuite.SPARK-5064 GraphGenerators.rmatGraph numEdges upper bound</li></div><div><li>org.apache.spark.graphx.util.PeriodicGraphCheckpointerSuite.Persisting</li></div><div><li>org.apache.spark.graphx.util.PeriodicGraphCheckpointerSuite.Checkpointing</li></div><div><li>org.apache.spark.ml.JavaPipelineSuite.pipeline</li></div><div><li>org.apache.spark.ml.JavaPipelineSuite.pipeline</li></div><div><li>org.apache.spark.ml.classification.JavaDecisionTreeClassifierSuite.runDT</li></div><div><li>org.apache.spark.ml.classification.JavaDecisionTreeClassifierSuite.runDT</li></div><div><li>org.apache.spark.ml.classification.JavaGBTClassifierSuite.runDT</li></div><div><li>org.apache.spark.ml.classification.JavaGBTClassifierSuite.runDT</li></div><div><li>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionWithSetters</li></div><div><li>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionWithSetters</li></div><div><li>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionTrainingSummary</li></div><div><li>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionTrainingSummary</li></div><div><li>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionPredictorClassifierMethods</li></div><div><li>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionPredictorClassifierMethods</li></div><div><li>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionDefaultParams</li></div><div><li>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionDefaultParams</li></div><div><li>org.apache.spark.ml.classification.JavaMultilayerPerceptronClassifierSuite.testMLPC</li></div><div><li>org.apache.spark.ml.classification.JavaMultilayerPerceptronClassifierSuite.testMLPC</li></div><div><li>org.apache.spark.ml.classification.JavaNaiveBayesSuite.testNaiveBayes</li></div><div><li>org.apache.spark.ml.classification.JavaNaiveBayesSuite.testNaiveBayes</li></div><div><li>org.apache.spark.ml.classification.JavaNaiveBayesSuite.naiveBayesDefaultParams</li></div><div><li>org.apache.spark.ml.classification.JavaNaiveBayesSuite.naiveBayesDefaultParams</li></div><div><li>org.apache.spark.ml.classification.JavaOneVsRestSuite.oneVsRestDefaultParams</li></div><div><li>org.apache.spark.ml.classification.JavaOneVsRestSuite.oneVsRestDefaultParams</li></div><div><li>org.apache.spark.ml.classification.JavaRandomForestClassifierSuite.runDT</li></div><div><li>org.apache.spark.ml.classification.JavaRandomForestClassifierSuite.runDT</li></div><div><li>org.apache.spark.ml.clustering.JavaKMeansSuite.fitAndTransform</li></div><div><li>org.apache.spark.ml.clustering.JavaKMeansSuite.fitAndTransform</li></div><div><li>org.apache.spark.ml.feature.JavaBucketizerSuite.bucketizerTest</li></div><div><li>org.apache.spark.ml.feature.JavaBucketizerSuite.bucketizerTest</li></div><div><li>org.apache.spark.ml.feature.JavaBucketizerSuite.bucketizerMultipleColumnsTest</li></div><div><li>org.apache.spark.ml.feature.JavaBucketizerSuite.bucketizerMultipleColumnsTest</li></div><div><li>org.apache.spark.ml.feature.JavaDCTSuite.javaCompatibilityTest</li></div><div><li>org.apache.spark.ml.feature.JavaDCTSuite.javaCompatibilityTest</li></div><div><li>org.apache.spark.ml.feature.JavaHashingTFSuite.hashingTF</li></div><div><li>org.apache.spark.ml.feature.JavaHashingTFSuite.hashingTF</li></div><div><li>org.apache.spark.ml.feature.JavaNormalizerSuite.normalizer</li></div><div><li>org.apache.spark.ml.feature.JavaNormalizerSuite.normalizer</li></div><div><li>org.apache.spark.ml.feature.JavaPCASuite.testPCA</li></div><div><li>org.apache.spark.ml.feature.JavaPCASuite.testPCA</li></div><div><li>org.apache.spark.ml.feature.JavaPolynomialExpansionSuite.polynomialExpansionTest</li></div><div><li>org.apache.spark.ml.feature.JavaPolynomialExpansionSuite.polynomialExpansionTest</li></div><div><li>org.apache.spark.ml.feature.JavaStandardScalerSuite.standardScaler</li></div><div><li>org.apache.spark.ml.feature.JavaStandardScalerSuite.standardScaler</li></div><div><li>org.apache.spark.ml.feature.JavaStopWordsRemoverSuite.javaCompatibilityTest</li></div><div><li>org.apache.spark.ml.feature.JavaStopWordsRemoverSuite.javaCompatibilityTest</li></div><div><li>org.apache.spark.ml.feature.JavaStringIndexerSuite.testStringIndexer</li></div><div><li>org.apache.spark.ml.feature.JavaStringIndexerSuite.testStringIndexer</li></div><div><li>org.apache.spark.ml.feature.JavaTokenizerSuite.regexTokenizer</li></div><div><li>org.apache.spark.ml.feature.JavaTokenizerSuite.regexTokenizer</li></div><div><li>org.apache.spark.ml.feature.JavaVectorAssemblerSuite.testVectorAssembler</li></div><div><li>org.apache.spark.ml.feature.JavaVectorAssemblerSuite.testVectorAssembler</li></div><div><li>org.apache.spark.ml.feature.JavaVectorIndexerSuite.vectorIndexerAPI</li></div><div><li>org.apache.spark.ml.feature.JavaVectorIndexerSuite.vectorIndexerAPI</li></div><div><li>org.apache.spark.ml.feature.JavaVectorSlicerSuite.vectorSlice</li></div><div><li>org.apache.spark.ml.feature.JavaVectorSlicerSuite.vectorSlice</li></div><div><li>org.apache.spark.ml.feature.JavaWord2VecSuite.testJavaWord2Vec</li></div><div><li>org.apache.spark.ml.feature.JavaWord2VecSuite.testJavaWord2Vec</li></div><div><li>org.apache.spark.ml.regression.JavaDecisionTreeRegressorSuite.runDT</li></div><div><li>org.apache.spark.ml.regression.JavaDecisionTreeRegressorSuite.runDT</li></div><div><li>org.apache.spark.ml.regression.JavaGBTRegressorSuite.runDT</li></div><div><li>org.apache.spark.ml.regression.JavaGBTRegressorSuite.runDT</li></div><div><li>org.apache.spark.ml.regression.JavaLinearRegressionSuite.linearRegressionDefaultParams</li></div><div><li>org.apache.spark.ml.regression.JavaLinearRegressionSuite.linearRegressionDefaultParams</li></div><div><li>org.apache.spark.ml.regression.JavaLinearRegressionSuite.linearRegressionWithSetters</li></div><div><li>org.apache.spark.ml.regression.JavaLinearRegressionSuite.linearRegressionWithSetters</li></div><div><li>org.apache.spark.ml.regression.JavaRandomForestRegressorSuite.runDT</li></div><div><li>org.apache.spark.ml.regression.JavaRandomForestRegressorSuite.runDT</li></div><div><li>org.apache.spark.ml.source.libsvm.JavaLibSVMRelationSuite.verifyLibSVMDF</li></div><div><li>org.apache.spark.ml.source.libsvm.JavaLibSVMRelationSuite.verifyLibSVMDF</li></div><div><li>org.apache.spark.ml.stat.JavaKolmogorovSmirnovTestSuite.testKSTestNamedDistribution</li></div><div><li>org.apache.spark.ml.stat.JavaKolmogorovSmirnovTestSuite.testKSTestNamedDistribution</li></div><div><li>org.apache.spark.ml.stat.JavaKolmogorovSmirnovTestSuite.testKSTestCDF</li></div><div><li>org.apache.spark.ml.stat.JavaKolmogorovSmirnovTestSuite.testKSTestCDF</li></div><div><li>org.apache.spark.ml.stat.JavaSummarizerSuite.testSummarizer</li></div><div><li>org.apache.spark.ml.stat.JavaSummarizerSuite.testSummarizer</li></div><div><li>org.apache.spark.ml.tuning.JavaCrossValidatorSuite.crossValidationWithLogisticRegression</li></div><div><li>org.apache.spark.ml.tuning.JavaCrossValidatorSuite.crossValidationWithLogisticRegression</li></div><div><li>org.apache.spark.ml.util.JavaDefaultReadWriteSuite.testDefaultReadWrite</li></div><div><li>org.apache.spark.ml.util.JavaDefaultReadWriteSuite.testDefaultReadWrite</li></div><div><li>org.apache.spark.mllib.classification.JavaLogisticRegressionSuite.runLRUsingConstructor</li></div><div><li>org.apache.spark.mllib.classification.JavaLogisticRegressionSuite.runLRUsingConstructor</li></div><div><li>org.apache.spark.mllib.classification.JavaLogisticRegressionSuite.runLRUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.classification.JavaLogisticRegressionSuite.runLRUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.testPredictJavaRDD</li></div><div><li>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.testPredictJavaRDD</li></div><div><li>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.runUsingConstructor</li></div><div><li>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.runUsingConstructor</li></div><div><li>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.runUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.runUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.testModelTypeSetters</li></div><div><li>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.testModelTypeSetters</li></div><div><li>org.apache.spark.mllib.classification.JavaSVMSuite.runSVMUsingConstructor</li></div><div><li>org.apache.spark.mllib.classification.JavaSVMSuite.runSVMUsingConstructor</li></div><div><li>org.apache.spark.mllib.classification.JavaSVMSuite.runSVMUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.classification.JavaSVMSuite.runSVMUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.classification.JavaStreamingLogisticRegressionSuite.javaAPI</li></div><div><li>org.apache.spark.mllib.classification.JavaStreamingLogisticRegressionSuite.javaAPI</li></div><div><li>org.apache.spark.mllib.clustering.JavaBisectingKMeansSuite.twoDimensionalData</li></div><div><li>org.apache.spark.mllib.clustering.JavaBisectingKMeansSuite.twoDimensionalData</li></div><div><li>org.apache.spark.mllib.clustering.JavaGaussianMixtureSuite.runGaussianMixture</li></div><div><li>org.apache.spark.mllib.clustering.JavaGaussianMixtureSuite.runGaussianMixture</li></div><div><li>org.apache.spark.mllib.clustering.JavaKMeansSuite.testPredictJavaRDD</li></div><div><li>org.apache.spark.mllib.clustering.JavaKMeansSuite.testPredictJavaRDD</li></div><div><li>org.apache.spark.mllib.clustering.JavaKMeansSuite.runKMeansUsingConstructor</li></div><div><li>org.apache.spark.mllib.clustering.JavaKMeansSuite.runKMeansUsingConstructor</li></div><div><li>org.apache.spark.mllib.clustering.JavaKMeansSuite.runKMeansUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.clustering.JavaKMeansSuite.runKMeansUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.clustering.JavaLDASuite.onlineOptimizerCompatibility</li></div><div><li>org.apache.spark.mllib.clustering.JavaLDASuite.onlineOptimizerCompatibility</li></div><div><li>org.apache.spark.mllib.clustering.JavaLDASuite.distributedLDAModel</li></div><div><li>org.apache.spark.mllib.clustering.JavaLDASuite.distributedLDAModel</li></div><div><li>org.apache.spark.mllib.clustering.JavaLDASuite.localLDAModel</li></div><div><li>org.apache.spark.mllib.clustering.JavaLDASuite.localLDAModel</li></div><div><li>org.apache.spark.mllib.clustering.JavaLDASuite.localLdaMethods</li></div><div><li>org.apache.spark.mllib.clustering.JavaLDASuite.localLdaMethods</li></div><div><li>org.apache.spark.mllib.clustering.JavaStreamingKMeansSuite.javaAPI</li></div><div><li>org.apache.spark.mllib.clustering.JavaStreamingKMeansSuite.javaAPI</li></div><div><li>org.apache.spark.mllib.evaluation.JavaRankingMetricsSuite.rankingMetrics</li></div><div><li>org.apache.spark.mllib.evaluation.JavaRankingMetricsSuite.rankingMetrics</li></div><div><li>org.apache.spark.mllib.feature.JavaTfIdfSuite.tfIdfMinimumDocumentFrequency</li></div><div><li>org.apache.spark.mllib.feature.JavaTfIdfSuite.tfIdfMinimumDocumentFrequency</li></div><div><li>org.apache.spark.mllib.feature.JavaTfIdfSuite.tfIdf</li></div><div><li>org.apache.spark.mllib.feature.JavaTfIdfSuite.tfIdf</li></div><div><li>org.apache.spark.mllib.feature.JavaWord2VecSuite.word2Vec</li></div><div><li>org.apache.spark.mllib.feature.JavaWord2VecSuite.word2Vec</li></div><div><li>org.apache.spark.mllib.fpm.JavaAssociationRulesSuite.runAssociationRules</li></div><div><li>org.apache.spark.mllib.fpm.JavaFPGrowthSuite.runFPGrowthSaveLoad</li></div><div><li>org.apache.spark.mllib.fpm.JavaFPGrowthSuite.runFPGrowthSaveLoad</li></div><div><li>org.apache.spark.mllib.fpm.JavaFPGrowthSuite.runFPGrowth</li></div><div><li>org.apache.spark.mllib.fpm.JavaFPGrowthSuite.runFPGrowth</li></div><div><li>org.apache.spark.mllib.fpm.JavaPrefixSpanSuite.runPrefixSpan</li></div><div><li>org.apache.spark.mllib.fpm.JavaPrefixSpanSuite.runPrefixSpan</li></div><div><li>org.apache.spark.mllib.fpm.JavaPrefixSpanSuite.runPrefixSpanSaveLoad</li></div><div><li>org.apache.spark.mllib.fpm.JavaPrefixSpanSuite.runPrefixSpanSaveLoad</li></div><div><li>org.apache.spark.mllib.linalg.distributed.JavaRowMatrixSuite.rowMatrixQRDecomposition</li></div><div><li>org.apache.spark.mllib.linalg.distributed.JavaRowMatrixSuite.rowMatrixQRDecomposition</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testNormalVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testNormalVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testArbitrary</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testArbitrary</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testLogNormalVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testLogNormalVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testExponentialVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testExponentialVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testUniformRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testUniformRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testRandomVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testRandomVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testGammaRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testGammaRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testUniformVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testUniformVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testPoissonRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testPoissonRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testNormalRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testNormalRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testPoissonVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testPoissonVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testGammaVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testGammaVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testExponentialRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testExponentialRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testLNormalRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testLNormalRDD</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runALSUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runALSUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSUsingConstructor</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSUsingConstructor</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runRecommend</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runRecommend</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSWithNegativeWeight</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSWithNegativeWeight</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runALSUsingConstructor</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runALSUsingConstructor</li></div><div><li>org.apache.spark.mllib.regression.JavaIsotonicRegressionSuite.testIsotonicRegressionJavaRDD</li></div><div><li>org.apache.spark.mllib.regression.JavaIsotonicRegressionSuite.testIsotonicRegressionJavaRDD</li></div><div><li>org.apache.spark.mllib.regression.JavaIsotonicRegressionSuite.testIsotonicRegressionPredictionsJavaRDD</li></div><div><li>org.apache.spark.mllib.regression.JavaIsotonicRegressionSuite.testIsotonicRegressionPredictionsJavaRDD</li></div><div><li>org.apache.spark.mllib.regression.JavaLassoSuite.runLassoUsingConstructor</li></div><div><li>org.apache.spark.mllib.regression.JavaLassoSuite.runLassoUsingConstructor</li></div><div><li>org.apache.spark.mllib.regression.JavaLassoSuite.runLassoUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.regression.JavaLassoSuite.runLassoUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.testPredictJavaRDD</li></div><div><li>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.testPredictJavaRDD</li></div><div><li>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.runLinearRegressionUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.runLinearRegressionUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.runLinearRegressionUsingConstructor</li></div><div><li>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.runLinearRegressionUsingConstructor</li></div><div><li>org.apache.spark.mllib.regression.JavaRidgeRegressionSuite.runRidgeRegressionUsingConstructor</li></div><div><li>org.apache.spark.mllib.regression.JavaRidgeRegressionSuite.runRidgeRegressionUsingConstructor</li></div><div><li>org.apache.spark.mllib.regression.JavaRidgeRegressionSuite.runRidgeRegressionUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.regression.JavaRidgeRegressionSuite.runRidgeRegressionUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.regression.JavaStreamingLinearRegressionSuite.javaAPI</li></div><div><li>org.apache.spark.mllib.regression.JavaStreamingLinearRegressionSuite.javaAPI</li></div><div><li>org.apache.spark.mllib.stat.JavaStatisticsSuite.testCorr</li></div><div><li>org.apache.spark.mllib.stat.JavaStatisticsSuite.testCorr</li></div><div><li>org.apache.spark.mllib.stat.JavaStatisticsSuite.chiSqTest</li></div><div><li>org.apache.spark.mllib.stat.JavaStatisticsSuite.chiSqTest</li></div><div><li>org.apache.spark.mllib.stat.JavaStatisticsSuite.streamingTest</li></div><div><li>org.apache.spark.mllib.stat.JavaStatisticsSuite.kolmogorovSmirnovTest</li></div><div><li>org.apache.spark.mllib.stat.JavaStatisticsSuite.kolmogorovSmirnovTest</li></div><div><li>org.apache.spark.mllib.tree.JavaDecisionTreeSuite.runDTUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.tree.JavaDecisionTreeSuite.runDTUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.tree.JavaDecisionTreeSuite.runDTUsingConstructor</li></div><div><li>org.apache.spark.mllib.tree.JavaDecisionTreeSuite.runDTUsingConstructor</li></div><div><li>org.apache.spark.mllib.util.JavaMLUtilsSuite.testConvertMatrixColumnsToAndFromML</li></div><div><li>org.apache.spark.mllib.util.JavaMLUtilsSuite.testConvertMatrixColumnsToAndFromML</li></div><div><li>org.apache.spark.mllib.util.JavaMLUtilsSuite.testConvertVectorColumnsToAndFromML</li></div><div><li>org.apache.spark.mllib.util.JavaMLUtilsSuite.testConvertVectorColumnsToAndFromML</li></div><div><li>org.apache.spark.repl.ReplSuite.SPARK-15236: use Hive catalog</li></div><div><li>org.apache.spark.repl.ReplSuite.SPARK-15236: use in-memory catalog</li></div><div><li>org.apache.spark.repl.ReplSuite.broadcast vars</li></div><div><li>org.apache.spark.repl.ReplSuite.line wrapper only initialized once when used as encoder outer scope</li></div><div><li>org.apache.spark.repl.ReplSuite.define case class and create Dataset together with paste mode</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.simple foreach with accumulator</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.external vars</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.external classes</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.external functions</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.external functions that access vars</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.broadcast vars</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.interacting with files</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.local-cluster mode</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.SPARK-2576 importing implicits</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.Datasets and encoders</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.SPARK-2632 importing a method from non serializable class and not using it.</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.collecting objects of class defined in repl</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.collecting objects of class defined in repl - shuffling</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.replicating blocks of object with class defined in repl</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.newProductSeqEncoder with REPL defined class</li></div><div><li>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationCount</li></div><div><li>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationCount</li></div><div><li>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationSumDouble</li></div><div><li>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationSumDouble</li></div><div><li>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationSumLong</li></div><div><li>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationSumLong</li></div><div><li>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationAverage</li></div><div><li>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationAverage</li></div><div><li>test.org.apache.spark.sql.JavaApplySchemaSuite.applySchema</li></div><div><li>test.org.apache.spark.sql.JavaApplySchemaSuite.applySchema</li></div><div><li>test.org.apache.spark.sql.JavaApplySchemaSuite.dataFrameRDDOperations</li></div><div><li>test.org.apache.spark.sql.JavaApplySchemaSuite.dataFrameRDDOperations</li></div><div><li>test.org.apache.spark.sql.JavaApplySchemaSuite.applySchemaToJSON</li></div><div><li>test.org.apache.spark.sql.JavaApplySchemaSuite.applySchemaToJSON</li></div><div><li>test.org.apache.spark.sql.JavaBeanDeserializationSuite.testBeanWithArrayFieldDeserialization</li></div><div><li>test.org.apache.spark.sql.JavaBeanDeserializationSuite.testBeanWithArrayFieldDeserialization</li></div><div><li>test.org.apache.spark.sql.JavaBeanDeserializationSuite.testBeanWithMapFieldsDeserialization</li></div><div><li>test.org.apache.spark.sql.JavaBeanDeserializationSuite.testBeanWithMapFieldsDeserialization</li></div><div><li>test.org.apache.spark.sql.JavaColumnExpressionSuite.isInCollectionCheckExceptionMessage</li></div><div><li>test.org.apache.spark.sql.JavaColumnExpressionSuite.isInCollectionCheckExceptionMessage</li></div><div><li>test.org.apache.spark.sql.JavaColumnExpressionSuite.isInCollectionWorksCorrectlyOnJava</li></div><div><li>test.org.apache.spark.sql.JavaColumnExpressionSuite.isInCollectionWorksCorrectlyOnJava</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testFormatAPI</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testTextAPI</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testJsonAPI</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testLoadAPI</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testOptionsAPI</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testSaveModeAPI</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testCsvAPI</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testParquetAPI</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testTextFileAPI</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCollectAndTake</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCollectAndTake</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testJsonRDDToDataFrame</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testJsonRDDToDataFrame</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testVarargMethods</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testVarargMethods</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testBeanWithoutGetter</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testBeanWithoutGetter</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateStructTypeFromList</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateStructTypeFromList</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testSampleBy</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testSampleBy</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCrosstab</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCrosstab</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testUDF</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testUDF</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFromFromList</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFromFromList</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCircularReferenceBean</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCircularReferenceBean</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testFrequentItems</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testFrequentItems</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testSampleByColumn</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testSampleByColumn</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testExecution</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testExecution</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testTextLoad</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testTextLoad</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.pivot</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.pivot</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testGenericLoad</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testGenericLoad</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCountMinSketch</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCountMinSketch</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.pivotColumnValues</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.pivotColumnValues</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFrameFromJavaBeans</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFrameFromJavaBeans</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCorrelation</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCorrelation</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testBloomFilter</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testBloomFilter</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCovariance</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCovariance</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFrameFromLocalJavaBeans</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFrameFromLocalJavaBeans</li></div><div><li>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationCount</li></div><div><li>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationCount</li></div><div><li>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationSumDouble</li></div><div><li>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationSumDouble</li></div><div><li>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationSumLong</li></div><div><li>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationSumLong</li></div><div><li>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationAnonClass</li></div><div><li>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationAnonClass</li></div><div><li>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationAverage</li></div><div><li>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationAverage</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testRuntimeNullabilityCheck</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testRuntimeNullabilityCheck</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean1</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean1</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean2</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean2</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean3</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean3</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testSerializeNull</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testSerializeNull</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testRandomSplit</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testRandomSplit</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testTypedFilterPreservingSchema</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testTypedFilterPreservingSchema</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJoin</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJoin</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testTake</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testTake</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testToLocalIterator</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testToLocalIterator</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testSpecificLists</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testSpecificLists</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testForeach</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testForeach</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJavaEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJavaEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testPrimitiveEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testPrimitiveEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testEmptyBean</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testEmptyBean</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testCommonOperation</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testCommonOperation</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testNullInTopLevelBean</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testNullInTopLevelBean</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testGroupBy</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testGroupBy</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testSetOperation</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testSetOperation</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testBeanWithEnum</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testBeanWithEnum</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testKryoEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testKryoEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.test</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.test</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJavaBeanEncoder2</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJavaBeanEncoder2</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testCollect</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testCollect</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testKryoEncoderErrorMessageForPrivateClass</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testKryoEncoderErrorMessageForPrivateClass</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJavaBeanEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJavaBeanEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testTupleEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testTupleEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testNestedTupleEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testNestedTupleEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testTupleEncoderSchema</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testTupleEncoderSchema</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testReduce</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testReduce</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testSelect</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testSelect</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJavaEncoderErrorMessageForPrivateClass</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJavaEncoderErrorMessageForPrivateClass</li></div><div><li>test.org.apache.spark.sql.JavaSaveLoadSuite.saveAndLoadWithSchema</li></div><div><li>test.org.apache.spark.sql.JavaSaveLoadSuite.saveAndLoadWithSchema</li></div><div><li>test.org.apache.spark.sql.JavaSaveLoadSuite.saveAndLoad</li></div><div><li>test.org.apache.spark.sql.JavaSaveLoadSuite.saveAndLoad</li></div><div><li>test.org.apache.spark.sql.JavaUDAFSuite.udf1Test</li></div><div><li>test.org.apache.spark.sql.JavaUDAFSuite.udf1Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf1Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf1Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf2Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf2Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf3Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf3Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf4Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf4Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf5Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf5Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf6Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf6Test</li></div><div><li>test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.testForeachBatchAPI</li></div><div><li>test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.testForeachBatchAPI</li></div><div><li>test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.testForeachAPI</li></div><div><li>test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.testForeachAPI</li></div><div><li>org.apache.spark.sql.hive.HiveMetastoreLazyInitializationSuite.lazily initialize Hive client</li></div><div><li>org.apache.spark.sql.hive.JavaDataFrameSuite.testUDAF</li></div><div><li>org.apache.spark.sql.hive.JavaDataFrameSuite.saveTableAndQueryIt</li></div><div><li>org.apache.spark.sql.hive.JavaMetastoreDataSourcesSuite.saveExternalTableAndQueryIt</li></div><div><li>org.apache.spark.sql.hive.JavaMetastoreDataSourcesSuite.saveTableAndQueryIt</li></div><div><li>org.apache.spark.sql.hive.JavaMetastoreDataSourcesSuite.saveExternalTableWithSchemaAndQueryIt</li></div><div><li>org.apache.spark.streaming.JavaMapWithStateSuite.testBasicFunction</li></div><div><li>org.apache.spark.streaming.JavaReceiverAPISuite.testReceiver</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testStreamingContextTransform</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testStreamingContextTransform</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testFlatMapValues</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testFlatMapValues</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testMapPartitions</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testMapPartitions</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairFilter</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairFilter</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testCombineByKey</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testCombineByKey</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testMap</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testMap</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairToNormalRDDTransform</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairToNormalRDDTransform</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairReduceByKey</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairReduceByKey</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairMap</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairMap</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testFlatMap</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testFlatMap</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testReduceByKeyAndWindowWithInverse</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testReduceByKeyAndWindowWithInverse</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testReduceByWindow</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testReduceByWindow</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairFlatMap</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairFlatMap</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairToPairFlatMapWithChangingTypes</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairToPairFlatMapWithChangingTypes</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairMapPartitions</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairMapPartitions</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testVariousTransform</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testVariousTransform</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testTransformWith</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testTransformWith</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testVariousTransformWith</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testVariousTransformWith</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testReduceByKeyAndWindow</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testReduceByKeyAndWindow</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairTransform</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairTransform</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testFilter</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testFilter</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairMap2</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairMap2</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testMapValues</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testMapValues</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testReduce</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testReduce</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testUpdateStateByKey</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testUpdateStateByKey</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testTransform</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testTransform</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testStreamingContextTransform</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testFlatMapValues</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testReduceByWindowWithInverse</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testMapPartitions</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairFilter</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairFilter</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testRepartitionFewerPartitions</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testRepartitionFewerPartitions</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCombineByKey</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCombineByKey</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testContextGetOrCreate</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testContextGetOrCreate</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testWindowWithSlideDuration</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testWindowWithSlideDuration</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testQueueStream</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testQueueStream</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCountByValue</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCountByValue</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testMap</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testMap</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairToNormalRDDTransform</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairToNormalRDDTransform</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairReduceByKey</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCount</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCount</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCheckpointMasterRecovery</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCheckpointMasterRecovery</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairMap</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairMap</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testUnion</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testUnion</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testFlatMap</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testFlatMap</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testReduceByKeyAndWindowWithInverse</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testReduceByKeyAndWindowWithInverse</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testGlom</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testGlom</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testJoin</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testJoin</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairFlatMap</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairFlatMap</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairToPairFlatMapWithChangingTypes</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairToPairFlatMapWithChangingTypes</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairMapPartitions</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairMapPartitions</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testRepartitionMorePartitions</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testRepartitionMorePartitions</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testReduceByWindowWithoutInverse</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testReduceByWindowWithoutInverse</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testLeftOuterJoin</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testLeftOuterJoin</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testVariousTransform</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testVariousTransform</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testTransformWith</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testTransformWith</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testVariousTransformWith</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testVariousTransformWith</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testTextFileStream</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testTextFileStream</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairGroupByKey</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairGroupByKey</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCoGroup</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCoGroup</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testInitialization</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testInitialization</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testGroupByKeyAndWindow</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testGroupByKeyAndWindow</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testReduceByKeyAndWindow</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testReduceByKeyAndWindow</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testForeachRDD</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testForeachRDD</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testFileStream</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testFileStream</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairTransform</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairTransform</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testFilter</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testFilter</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairMap2</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairMap2</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testMapValues</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testMapValues</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testReduce</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testReduce</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testUpdateStateByKey</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testUpdateStateByKey</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testTransform</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testTransform</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testWindow</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testWindow</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCountByValueAndWindow</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCountByValueAndWindow</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testRawSocketStream</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testRawSocketStream</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testUpdateStateByKeyWithInitial</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testUpdateStateByKeyWithInitial</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testContextState</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testContextState</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexDescendingWithStart</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexWithStart</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndexDescendingWithStart</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexDescending</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexWithStart</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexWithLast</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexWithSkip</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexWithMax</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexDescending</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndexDescendingWithLast</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexDescending</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexDescendingWithLast</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndex</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndexWithLast</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexWithStart</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexDescendingWithStart</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexWithLast</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexWithSkip</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndexDescending</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.testRefWithIntNaturalKey</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexDescending</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexDescendingWithStart</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexWithMax</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndex</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexWithLast</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexWithSkip</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexWithMax</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexDescendingWithLast</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexDescendingWithLast</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexDescendingWithStart</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndex</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexWithLast</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexWithSkip</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexWithStart</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndex</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexDescendingWithLast</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndexWithStart</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndex</li></div><div><li>org.apache.spark.util.kvstore.LevelDBSuite.testMultipleTypesWriteReadDelete</li></div><div><li>org.apache.spark.util.kvstore.LevelDBSuite.testObjectWriteReadDelete</li></div><div><li>org.apache.spark.util.kvstore.LevelDBSuite.testSkip</li></div><div><li>org.apache.spark.util.kvstore.LevelDBSuite.testMultipleObjectWriteReadDelete</li></div><div><li>org.apache.spark.util.kvstore.LevelDBSuite.testReopenAndVersionCheckDb</li></div><div><li>org.apache.spark.util.kvstore.LevelDBSuite.testMetadata</li></div><div><li>org.apache.spark.util.kvstore.LevelDBSuite.testUpdate</li></div><div><li>org.apache.spark.util.kvstore.LevelDBSuite.testNegativeIndexValues</li></div><div><li>org.apache.spark.streaming.kafka010.JavaDirectKafkaStreamSuite.testKafkaStream</li></div><div><li>org.apache.spark.streaming.kafka010.JavaKafkaRDDSuite.testKafkaRDD</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.spark.streaming.kafka010.JavaDirectKafkaStreamSuite.testKafkaStream</li></div><div><li>org.apache.spark.streaming.kafka010.JavaKafkaRDDSuite.testKafkaRDD</li></div><div><li>org.apache.spark.sql.hive.thriftserver.HiveThriftHttpServerSuite.Checks Hive version</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexDescendingWithStart</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexWithStart</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndexDescendingWithStart</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexDescending</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexWithStart</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexWithLast</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexWithSkip</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexWithMax</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexDescending</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndexDescendingWithLast</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexDescending</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexDescendingWithLast</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndex</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndexWithLast</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexWithStart</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexDescendingWithStart</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexWithLast</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexWithSkip</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndexDescending</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.testRefWithIntNaturalKey</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexDescending</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexDescendingWithStart</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexWithMax</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndex</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexWithLast</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexWithSkip</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexWithMax</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexDescendingWithLast</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexDescendingWithLast</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexDescendingWithStart</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndex</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexWithLast</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexWithSkip</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexWithStart</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndex</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexDescendingWithLast</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndexWithStart</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndex</li></div><div><li>org.apache.spark.util.kvstore.LevelDBSuite.testMultipleTypesWriteReadDelete</li></div><div><li>org.apache.spark.util.kvstore.LevelDBSuite.testObjectWriteReadDelete</li></div><div><li>org.apache.spark.util.kvstore.LevelDBSuite.testSkip</li></div><div><li>org.apache.spark.util.kvstore.LevelDBSuite.testMultipleObjectWriteReadDelete</li></div><div><li>org.apache.spark.util.kvstore.LevelDBSuite.testReopenAndVersionCheckDb</li></div><div><li>org.apache.spark.util.kvstore.LevelDBSuite.testMetadata</li></div><div><li>org.apache.spark.util.kvstore.LevelDBSuite.testUpdate</li></div><div><li>org.apache.spark.util.kvstore.LevelDBSuite.testNegativeIndexValues</li></div><div><li>org.apache.spark.streaming.kafka010.JavaDirectKafkaStreamSuite.testKafkaStream</li></div><div><li>org.apache.spark.streaming.kafka010.JavaKafkaRDDSuite.testKafkaRDD</li></div><div><li>org.apache.spark.sql.hive.HiveSparkSubmitSuite.SPARK-8020: set sql conf in spark conf</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.spark.network.ChunkFetchIntegrationSuite.fetchFileChunk</li></div><div><li>org.apache.spark.JavaJdbcRDDSuite.testJavaJdbcRDD</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.leftOuterJoin</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.foldReduce</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.foldReduce</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.mapsFromPairsToPairs</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.mapsFromPairsToPairs</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.flatMap</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.flatMap</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.foreach</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.map</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.zip</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.zip</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.keyBy</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.keyBy</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.groupBy</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.groupBy</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.mapPartitions</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.mapPartitions</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.foldByKey</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.foldByKey</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.mapOnPairRDD</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.mapOnPairRDD</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.sequenceFile</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.sequenceFile</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.collectPartitions</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.collectPartitions</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.reduceByKey</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.reduceByKey</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.foreachWithAnonymousClass</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.foreachWithAnonymousClass</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.collectAsMapWithIntArrayValues</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.collectAsMapWithIntArrayValues</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.zipPartitions</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.zipPartitions</li></div><div><li>test.org.apache.spark.JavaAPISuite.groupByOnPairRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.groupByOnPairRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.binaryFilesCaching</li></div><div><li>test.org.apache.spark.JavaAPISuite.binaryFilesCaching</li></div><div><li>test.org.apache.spark.JavaAPISuite.sparkContextUnion</li></div><div><li>test.org.apache.spark.JavaAPISuite.sparkContextUnion</li></div><div><li>test.org.apache.spark.JavaAPISuite.checkpointAndComputation</li></div><div><li>test.org.apache.spark.JavaAPISuite.checkpointAndComputation</li></div><div><li>test.org.apache.spark.JavaAPISuite.leftOuterJoin</li></div><div><li>test.org.apache.spark.JavaAPISuite.leftOuterJoin</li></div><div><li>test.org.apache.spark.JavaAPISuite.keyByOnPairRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.keyByOnPairRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.getNumPartitions</li></div><div><li>test.org.apache.spark.JavaAPISuite.getNumPartitions</li></div><div><li>test.org.apache.spark.JavaAPISuite.wholeTextFiles</li></div><div><li>test.org.apache.spark.JavaAPISuite.wholeTextFiles</li></div><div><li>test.org.apache.spark.JavaAPISuite.binaryFiles</li></div><div><li>test.org.apache.spark.JavaAPISuite.binaryFiles</li></div><div><li>test.org.apache.spark.JavaAPISuite.foldReduce</li></div><div><li>test.org.apache.spark.JavaAPISuite.foldReduce</li></div><div><li>test.org.apache.spark.JavaAPISuite.writeWithNewAPIHadoopFile</li></div><div><li>test.org.apache.spark.JavaAPISuite.writeWithNewAPIHadoopFile</li></div><div><li>test.org.apache.spark.JavaAPISuite.hadoopFile</li></div><div><li>test.org.apache.spark.JavaAPISuite.hadoopFile</li></div><div><li>test.org.apache.spark.JavaAPISuite.lookup</li></div><div><li>test.org.apache.spark.JavaAPISuite.lookup</li></div><div><li>test.org.apache.spark.JavaAPISuite.countAsync</li></div><div><li>test.org.apache.spark.JavaAPISuite.countAsync</li></div><div><li>test.org.apache.spark.JavaAPISuite.textFiles</li></div><div><li>test.org.apache.spark.JavaAPISuite.textFiles</li></div><div><li>test.org.apache.spark.JavaAPISuite.binaryRecords</li></div><div><li>test.org.apache.spark.JavaAPISuite.binaryRecords</li></div><div><li>test.org.apache.spark.JavaAPISuite.toLocalIterator</li></div><div><li>test.org.apache.spark.JavaAPISuite.toLocalIterator</li></div><div><li>test.org.apache.spark.JavaAPISuite.repartitionAndSortWithinPartitions</li></div><div><li>test.org.apache.spark.JavaAPISuite.repartitionAndSortWithinPartitions</li></div><div><li>test.org.apache.spark.JavaAPISuite.reduce</li></div><div><li>test.org.apache.spark.JavaAPISuite.reduce</li></div><div><li>test.org.apache.spark.JavaAPISuite.sample</li></div><div><li>test.org.apache.spark.JavaAPISuite.sample</li></div><div><li>test.org.apache.spark.JavaAPISuite.sortBy</li></div><div><li>test.org.apache.spark.JavaAPISuite.sortBy</li></div><div><li>test.org.apache.spark.JavaAPISuite.mapsFromPairsToPairs</li></div><div><li>test.org.apache.spark.JavaAPISuite.mapsFromPairsToPairs</li></div><div><li>test.org.apache.spark.JavaAPISuite.flatMap</li></div><div><li>test.org.apache.spark.JavaAPISuite.flatMap</li></div><div><li>test.org.apache.spark.JavaAPISuite.cogroup3</li></div><div><li>test.org.apache.spark.JavaAPISuite.cogroup3</li></div><div><li>test.org.apache.spark.JavaAPISuite.cogroup4</li></div><div><li>test.org.apache.spark.JavaAPISuite.cogroup4</li></div><div><li>test.org.apache.spark.JavaAPISuite.randomSplit</li></div><div><li>test.org.apache.spark.JavaAPISuite.randomSplit</li></div><div><li>test.org.apache.spark.JavaAPISuite.persist</li></div><div><li>test.org.apache.spark.JavaAPISuite.persist</li></div><div><li>test.org.apache.spark.JavaAPISuite.foreach</li></div><div><li>test.org.apache.spark.JavaAPISuite.hadoopFileCompressed</li></div><div><li>test.org.apache.spark.JavaAPISuite.hadoopFileCompressed</li></div><div><li>test.org.apache.spark.JavaAPISuite.textFilesCompressed</li></div><div><li>test.org.apache.spark.JavaAPISuite.textFilesCompressed</li></div><div><li>test.org.apache.spark.JavaAPISuite.testAsyncActionCancellation</li></div><div><li>test.org.apache.spark.JavaAPISuite.testAsyncActionCancellation</li></div><div><li>test.org.apache.spark.JavaAPISuite.checkpointAndRestore</li></div><div><li>test.org.apache.spark.JavaAPISuite.checkpointAndRestore</li></div><div><li>test.org.apache.spark.JavaAPISuite.sortByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.sortByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.aggregateByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.aggregateByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.map</li></div><div><li>test.org.apache.spark.JavaAPISuite.map</li></div><div><li>test.org.apache.spark.JavaAPISuite.max</li></div><div><li>test.org.apache.spark.JavaAPISuite.max</li></div><div><li>test.org.apache.spark.JavaAPISuite.min</li></div><div><li>test.org.apache.spark.JavaAPISuite.min</li></div><div><li>test.org.apache.spark.JavaAPISuite.top</li></div><div><li>test.org.apache.spark.JavaAPISuite.top</li></div><div><li>test.org.apache.spark.JavaAPISuite.zip</li></div><div><li>test.org.apache.spark.JavaAPISuite.zip</li></div><div><li>test.org.apache.spark.JavaAPISuite.fold</li></div><div><li>test.org.apache.spark.JavaAPISuite.fold</li></div><div><li>test.org.apache.spark.JavaAPISuite.glom</li></div><div><li>test.org.apache.spark.JavaAPISuite.glom</li></div><div><li>test.org.apache.spark.JavaAPISuite.take</li></div><div><li>test.org.apache.spark.JavaAPISuite.take</li></div><div><li>test.org.apache.spark.JavaAPISuite.javaDoubleRDDHistoGram</li></div><div><li>test.org.apache.spark.JavaAPISuite.javaDoubleRDDHistoGram</li></div><div><li>test.org.apache.spark.JavaAPISuite.collectUnderlyingScalaRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.keyBy</li></div><div><li>test.org.apache.spark.JavaAPISuite.mapPartitionsWithIndex</li></div><div><li>test.org.apache.spark.JavaAPISuite.sampleByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.intersection</li></div><div><li>test.org.apache.spark.JavaAPISuite.aggregate</li></div><div><li>test.org.apache.spark.JavaAPISuite.aggregate</li></div><div><li>test.org.apache.spark.JavaAPISuite.cartesian</li></div><div><li>test.org.apache.spark.JavaAPISuite.cartesian</li></div><div><li>test.org.apache.spark.JavaAPISuite.countApproxDistinctByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.countApproxDistinctByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.readWithNewAPIHadoopFile</li></div><div><li>test.org.apache.spark.JavaAPISuite.readWithNewAPIHadoopFile</li></div><div><li>test.org.apache.spark.JavaAPISuite.testRegisterKryoClasses</li></div><div><li>test.org.apache.spark.JavaAPISuite.testRegisterKryoClasses</li></div><div><li>test.org.apache.spark.JavaAPISuite.groupBy</li></div><div><li>test.org.apache.spark.JavaAPISuite.groupBy</li></div><div><li>test.org.apache.spark.JavaAPISuite.sampleByKeyExact</li></div><div><li>test.org.apache.spark.JavaAPISuite.sampleByKeyExact</li></div><div><li>test.org.apache.spark.JavaAPISuite.mapPartitions</li></div><div><li>test.org.apache.spark.JavaAPISuite.mapPartitions</li></div><div><li>test.org.apache.spark.JavaAPISuite.takeOrdered</li></div><div><li>test.org.apache.spark.JavaAPISuite.takeOrdered</li></div><div><li>test.org.apache.spark.JavaAPISuite.foldByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.foldByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.objectFilesOfInts</li></div><div><li>test.org.apache.spark.JavaAPISuite.objectFilesOfInts</li></div><div><li>test.org.apache.spark.JavaAPISuite.treeAggregate</li></div><div><li>test.org.apache.spark.JavaAPISuite.treeAggregate</li></div><div><li>test.org.apache.spark.JavaAPISuite.testGetPersistentRDDs</li></div><div><li>test.org.apache.spark.JavaAPISuite.testGetPersistentRDDs</li></div><div><li>test.org.apache.spark.JavaAPISuite.approximateResults</li></div><div><li>test.org.apache.spark.JavaAPISuite.approximateResults</li></div><div><li>test.org.apache.spark.JavaAPISuite.treeReduce</li></div><div><li>test.org.apache.spark.JavaAPISuite.treeReduce</li></div><div><li>test.org.apache.spark.JavaAPISuite.collectAsMapAndSerialize</li></div><div><li>test.org.apache.spark.JavaAPISuite.collectAsMapAndSerialize</li></div><div><li>test.org.apache.spark.JavaAPISuite.countApproxDistinct</li></div><div><li>test.org.apache.spark.JavaAPISuite.countApproxDistinct</li></div><div><li>test.org.apache.spark.JavaAPISuite.javaDoubleRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.javaDoubleRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.mapOnPairRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.mapOnPairRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.testAsyncActionErrorWrapping</li></div><div><li>test.org.apache.spark.JavaAPISuite.testAsyncActionErrorWrapping</li></div><div><li>test.org.apache.spark.JavaAPISuite.naturalMax</li></div><div><li>test.org.apache.spark.JavaAPISuite.naturalMax</li></div><div><li>test.org.apache.spark.JavaAPISuite.naturalMin</li></div><div><li>test.org.apache.spark.JavaAPISuite.naturalMin</li></div><div><li>test.org.apache.spark.JavaAPISuite.sequenceFile</li></div><div><li>test.org.apache.spark.JavaAPISuite.sequenceFile</li></div><div><li>test.org.apache.spark.JavaAPISuite.collectPartitions</li></div><div><li>test.org.apache.spark.JavaAPISuite.collectPartitions</li></div><div><li>test.org.apache.spark.JavaAPISuite.cogroup</li></div><div><li>test.org.apache.spark.JavaAPISuite.cogroup</li></div><div><li>test.org.apache.spark.JavaAPISuite.reduceByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.reduceByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.repartition</li></div><div><li>test.org.apache.spark.JavaAPISuite.repartition</li></div><div><li>test.org.apache.spark.JavaAPISuite.iterator</li></div><div><li>test.org.apache.spark.JavaAPISuite.iterator</li></div><div><li>test.org.apache.spark.JavaAPISuite.emptyRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.emptyRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.zipWithIndex</li></div><div><li>test.org.apache.spark.JavaAPISuite.foreachPartition</li></div><div><li>test.org.apache.spark.JavaAPISuite.foreachPartition</li></div><div><li>test.org.apache.spark.JavaAPISuite.combineByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.combineByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.takeAsync</li></div><div><li>test.org.apache.spark.JavaAPISuite.takeAsync</li></div><div><li>test.org.apache.spark.JavaAPISuite.collectAsMapWithIntArrayValues</li></div><div><li>test.org.apache.spark.JavaAPISuite.collectAsMapWithIntArrayValues</li></div><div><li>test.org.apache.spark.JavaAPISuite.objectFilesOfComplexTypes</li></div><div><li>test.org.apache.spark.JavaAPISuite.objectFilesOfComplexTypes</li></div><div><li>test.org.apache.spark.JavaAPISuite.zipWithUniqueId</li></div><div><li>test.org.apache.spark.JavaAPISuite.zipWithUniqueId</li></div><div><li>test.org.apache.spark.JavaAPISuite.collectAsync</li></div><div><li>test.org.apache.spark.JavaAPISuite.collectAsync</li></div><div><li>test.org.apache.spark.JavaAPISuite.foreachAsync</li></div><div><li>test.org.apache.spark.JavaAPISuite.foreachAsync</li></div><div><li>test.org.apache.spark.JavaAPISuite.zipPartitions</li></div><div><li>test.org.apache.spark.JavaAPISuite.zipPartitions</li></div><div><li>test.org.apache.spark.JavaAPISuite.reduceOnJavaDoubleRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.reduceOnJavaDoubleRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.isEmpty</li></div><div><li>test.org.apache.spark.JavaAPISuite.isEmpty</li></div><div><li>test.org.apache.spark.JavaSparkContextSuite.javaSparkContext</li></div><div><li>test.org.apache.spark.JavaSparkContextSuite.scalaSparkContext</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single BooleanType with seed 7020069396768296380</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single ByteType with seed -2399284702189095073</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single ShortType with seed -7719897024036539527</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single IntegerType with seed 3194364168583009346</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single LongType with seed -8233301204981990319</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single FloatType with seed 5142463758832068401</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single DoubleType with seed 8662749565340803335</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single DecimalType(8,0) with seed 5649588326852021390</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single DecimalType(8,4) with seed 3581673481165790187</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single DecimalType(16,0) with seed -9189186181081872341</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single DecimalType(16,11) with seed -316910867452416915</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single DecimalType(38,0) with seed -6869032807411776259</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single DecimalType(38,38) with seed -5492932143993552910</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single StringType with seed -2968675814663939058</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single BinaryType with seed 934282045452751850</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.flat schema struct&lt;col_0:double,col_1:decimal(38,38),col_2:boolean,col_3:bigint,col_4:decimal(16,0)&gt; with seed -5226596492428831048</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.flat schema struct&lt;col_0:bigint,col_1:boolean,col_2:tinyint,col_3:decimal(16,0),col_4:decimal(38,38)&gt; with seed -6991691242331003614</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.flat schema struct&lt;col_0:string,col_1:decimal(8,0),col_2:binary,col_3:tinyint,col_4:double&gt; with seed -7698775092196679665</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.flat schema struct&lt;col_0:float,col_1:decimal(8,0),col_2:string,col_3:boolean,col_4:smallint&gt; with seed 5184276442399998038</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.flat schema struct&lt;col_0:double,col_1:string,col_2:decimal(38,0),col_3:float,col_4:binary&gt; with seed -6346626587057858145</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.nested schema struct&lt;col_0:array&lt;decimal(38,38)&gt;,col_1:double,col_2:array&lt;bigint&gt;,col_3:struct&lt;col_0:struct&lt;col_0:array&lt;tinyint&gt;&gt;&gt;,col_4:bigint,col_5:struct&lt;col_0:struct&lt;col_0:binary&gt;&gt;,col_6:array&lt;int&gt;,col_7:decimal(8,4),col_8:array&lt;float&gt;,col_9:struct&lt;col_0:float&gt;&gt; with seed -6847694401803089910</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.nested schema struct&lt;col_0:struct&lt;col_0:array&lt;decimal(38,0)&gt;,col_1:struct&lt;col_0:array&lt;decimal(8,0)&gt;&gt;&gt;,col_1:struct&lt;col_0:struct&lt;col_0:decimal(8,4),col_1:bigint&gt;,col_1:struct&lt;col_0:struct&lt;col_0:array&lt;int&gt;&gt;&gt;&gt;,col_2:array&lt;decimal(38,0)&gt;,col_3:array&lt;tinyint&gt;,col_4:struct&lt;col_0:array&lt;decimal(8,0)&gt;,col_1:array&lt;decimal(38,0)&gt;&gt;,col_5:boolean&gt; with seed -4135230179903928490</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.nested schema struct&lt;col_0:decimal(38,0),col_1:struct&lt;col_0:decimal(16,11),col_1:struct&lt;col_0:struct&lt;col_0:struct&lt;col_0:array&lt;tinyint&gt;&gt;,col_1:struct&lt;col_0:struct&lt;col_0:array&lt;int&gt;&gt;&gt;&gt;,col_1:struct&lt;col_0:array&lt;string&gt;&gt;,col_2:array&lt;decimal(38,38)&gt;,col_3:array&lt;decimal(16,11)&gt;&gt;,col_2:array&lt;decimal(16,0)&gt;&gt;,col_2:boolean,col_3:array&lt;decimal(8,4)&gt;&gt; with seed 9011471362555659161</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.nested schema struct&lt;col_0:struct&lt;col_0:array&lt;string&gt;,col_1:struct&lt;col_0:struct&lt;col_0:string&gt;,col_1:bigint&gt;,col_2:struct&lt;col_0:tinyint&gt;,col_3:decimal(16,0),col_4:decimal(8,4),col_5:decimal(8,0)&gt;,col_1:struct&lt;col_0:string&gt;,col_2:struct&lt;col_0:array&lt;float&gt;&gt;,col_3:decimal(8,0)&gt; with seed -6084749057723529533</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.nested schema struct&lt;col_0:decimal(8,0),col_1:struct&lt;col_0:struct&lt;col_0:struct&lt;col_0:decimal(38,0)&gt;&gt;,col_1:double,col_2:decimal(8,4),col_3:struct&lt;col_0:float&gt;&gt;,col_2:struct&lt;col_0:binary,col_1:string,col_2:struct&lt;col_0:array&lt;double&gt;&gt;&gt;,col_3:int,col_4:bigint&gt; with seed -686107255735563379</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.read int as string</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.read string as int</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.read float as double</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.read double as float</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.Handle unsupported input of record type</li></div><div><li>org.apache.spark.sql.avro.AvroFunctionsSuite.roundtrip in to_avro and from_avro - int and string</li></div><div><li>org.apache.spark.sql.avro.AvroFunctionsSuite.roundtrip in to_avro and from_avro - struct</li></div><div><li>org.apache.spark.sql.avro.AvroFunctionsSuite.handle invalid input in from_avro</li></div><div><li>org.apache.spark.sql.avro.AvroFunctionsSuite.roundtrip in to_avro and from_avro - array with null</li></div><div><li>org.apache.spark.streaming.kafka010.JavaDirectKafkaStreamSuite.testKafkaStream</li></div><div><li>org.apache.spark.streaming.kafka010.JavaKafkaRDDSuite.testKafkaRDD</li></div><div><li>org.apache.spark.graphx.EdgeRDDSuite.checkpointing</li></div><div><li>org.apache.spark.graphx.GraphLoaderSuite.GraphLoader.edgeListFile</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.joinVertices</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.collectNeighborIds</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.removeSelfEdges</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.filter</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.convertToCanonicalEdges</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.collectEdgesCycleDirectionOut</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.collectEdgesCycleDirectionIn</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.collectEdgesCycleDirectionEither</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.collectEdgesChainDirectionOut</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.collectEdgesChainDirectionIn</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.collectEdgesChainDirectionEither</li></div><div><li>org.apache.spark.graphx.GraphSuite.Graph.fromEdgeTuples</li></div><div><li>org.apache.spark.graphx.GraphSuite.Graph.fromEdges</li></div><div><li>org.apache.spark.graphx.GraphSuite.Graph.apply</li></div><div><li>org.apache.spark.graphx.GraphSuite.triplets</li></div><div><li>org.apache.spark.graphx.GraphSuite.partitionBy</li></div><div><li>org.apache.spark.graphx.GraphSuite.mapVertices</li></div><div><li>org.apache.spark.graphx.GraphSuite.mapVertices changing type with same erased type</li></div><div><li>org.apache.spark.graphx.GraphSuite.mapEdges</li></div><div><li>org.apache.spark.graphx.GraphSuite.mapTriplets</li></div><div><li>org.apache.spark.graphx.GraphSuite.reverse</li></div><div><li>org.apache.spark.graphx.GraphSuite.reverse with join elimination</li></div><div><li>org.apache.spark.graphx.GraphSuite.subgraph</li></div><div><li>org.apache.spark.graphx.GraphSuite.mask</li></div><div><li>org.apache.spark.graphx.GraphSuite.groupEdges</li></div><div><li>org.apache.spark.graphx.GraphSuite.aggregateMessages</li></div><div><li>org.apache.spark.graphx.GraphSuite.outerJoinVertices</li></div><div><li>org.apache.spark.graphx.GraphSuite.more edge partitions than vertex partitions</li></div><div><li>org.apache.spark.graphx.GraphSuite.checkpoint</li></div><div><li>org.apache.spark.graphx.GraphSuite.cache, getStorageLevel</li></div><div><li>org.apache.spark.graphx.GraphSuite.non-default number of edge partitions</li></div><div><li>org.apache.spark.graphx.GraphSuite.unpersist graph RDD</li></div><div><li>org.apache.spark.graphx.GraphSuite.SPARK-14219: pickRandomVertex</li></div><div><li>org.apache.spark.graphx.PregelSuite.1 iteration</li></div><div><li>org.apache.spark.graphx.PregelSuite.chain propagation</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.filter</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.mapValues</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.minus</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.minus with RDD[(VertexId, VD)]</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.minus with non-equal number of partitions</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.diff</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.diff with RDD[(VertexId, VD)]</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.diff vertices with non-equal number of partitions</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.leftJoin</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.leftJoin vertices with non-equal number of partitions</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.innerJoin</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.innerJoin vertices with the non-equal number of partitions</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.aggregateUsingIndex</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.mergeFunc</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.cache, getStorageLevel</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.checkpoint</li></div><div><li>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Grid Connected Components</li></div><div><li>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Reverse Grid Connected Components</li></div><div><li>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Chain Connected Components</li></div><div><li>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Reverse Chain Connected Components</li></div><div><li>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Connected Components on a Toy Connected Graph</li></div><div><li>org.apache.spark.graphx.lib.LabelPropagationSuite.Label Propagation</li></div><div><li>org.apache.spark.graphx.lib.PageRankSuite.Star PageRank</li></div><div><li>org.apache.spark.graphx.lib.PageRankSuite.Star PersonalPageRank</li></div><div><li>org.apache.spark.graphx.lib.PageRankSuite.Grid PageRank</li></div><div><li>org.apache.spark.graphx.lib.PageRankSuite.Chain PageRank</li></div><div><li>org.apache.spark.graphx.lib.PageRankSuite.Chain PersonalizedPageRank</li></div><div><li>org.apache.spark.graphx.lib.PageRankSuite.Loop with source PageRank</li></div><div><li>org.apache.spark.graphx.lib.PageRankSuite.Loop with sink PageRank</li></div><div><li>org.apache.spark.graphx.lib.SVDPlusPlusSuite.Test SVD++ with mean square error on training set</li></div><div><li>org.apache.spark.graphx.lib.ShortestPathsSuite.Shortest Path Computations</li></div><div><li>org.apache.spark.graphx.lib.StronglyConnectedComponentsSuite.Island Strongly Connected Components</li></div><div><li>org.apache.spark.graphx.lib.StronglyConnectedComponentsSuite.Cycle Strongly Connected Components</li></div><div><li>org.apache.spark.graphx.lib.StronglyConnectedComponentsSuite.2 Cycle Strongly Connected Components</li></div><div><li>org.apache.spark.graphx.lib.TriangleCountSuite.Count a single triangle</li></div><div><li>org.apache.spark.graphx.lib.TriangleCountSuite.Count two triangles</li></div><div><li>org.apache.spark.graphx.lib.TriangleCountSuite.Count two triangles with bi-directed edges</li></div><div><li>org.apache.spark.graphx.lib.TriangleCountSuite.Count a single triangle with duplicate edges</li></div><div><li>org.apache.spark.graphx.util.GraphGeneratorsSuite.GraphGenerators.logNormalGraph</li></div><div><li>org.apache.spark.graphx.util.GraphGeneratorsSuite.SPARK-5064 GraphGenerators.rmatGraph numEdges upper bound</li></div><div><li>org.apache.spark.graphx.util.PeriodicGraphCheckpointerSuite.Persisting</li></div><div><li>org.apache.spark.graphx.util.PeriodicGraphCheckpointerSuite.Checkpointing</li></div><div><li>org.apache.spark.ml.JavaPipelineSuite.pipeline</li></div><div><li>org.apache.spark.ml.JavaPipelineSuite.pipeline</li></div><div><li>org.apache.spark.ml.classification.JavaDecisionTreeClassifierSuite.runDT</li></div><div><li>org.apache.spark.ml.classification.JavaDecisionTreeClassifierSuite.runDT</li></div><div><li>org.apache.spark.ml.classification.JavaGBTClassifierSuite.runDT</li></div><div><li>org.apache.spark.ml.classification.JavaGBTClassifierSuite.runDT</li></div><div><li>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionWithSetters</li></div><div><li>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionWithSetters</li></div><div><li>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionTrainingSummary</li></div><div><li>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionTrainingSummary</li></div><div><li>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionPredictorClassifierMethods</li></div><div><li>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionPredictorClassifierMethods</li></div><div><li>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionDefaultParams</li></div><div><li>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionDefaultParams</li></div><div><li>org.apache.spark.ml.classification.JavaMultilayerPerceptronClassifierSuite.testMLPC</li></div><div><li>org.apache.spark.ml.classification.JavaMultilayerPerceptronClassifierSuite.testMLPC</li></div><div><li>org.apache.spark.ml.classification.JavaNaiveBayesSuite.testNaiveBayes</li></div><div><li>org.apache.spark.ml.classification.JavaNaiveBayesSuite.testNaiveBayes</li></div><div><li>org.apache.spark.ml.classification.JavaNaiveBayesSuite.naiveBayesDefaultParams</li></div><div><li>org.apache.spark.ml.classification.JavaNaiveBayesSuite.naiveBayesDefaultParams</li></div><div><li>org.apache.spark.ml.classification.JavaOneVsRestSuite.oneVsRestDefaultParams</li></div><div><li>org.apache.spark.ml.classification.JavaOneVsRestSuite.oneVsRestDefaultParams</li></div><div><li>org.apache.spark.ml.classification.JavaRandomForestClassifierSuite.runDT</li></div><div><li>org.apache.spark.ml.classification.JavaRandomForestClassifierSuite.runDT</li></div><div><li>org.apache.spark.ml.clustering.JavaKMeansSuite.fitAndTransform</li></div><div><li>org.apache.spark.ml.clustering.JavaKMeansSuite.fitAndTransform</li></div><div><li>org.apache.spark.ml.feature.JavaBucketizerSuite.bucketizerTest</li></div><div><li>org.apache.spark.ml.feature.JavaBucketizerSuite.bucketizerTest</li></div><div><li>org.apache.spark.ml.feature.JavaBucketizerSuite.bucketizerMultipleColumnsTest</li></div><div><li>org.apache.spark.ml.feature.JavaBucketizerSuite.bucketizerMultipleColumnsTest</li></div><div><li>org.apache.spark.ml.feature.JavaDCTSuite.javaCompatibilityTest</li></div><div><li>org.apache.spark.ml.feature.JavaDCTSuite.javaCompatibilityTest</li></div><div><li>org.apache.spark.ml.feature.JavaHashingTFSuite.hashingTF</li></div><div><li>org.apache.spark.ml.feature.JavaNormalizerSuite.normalizer</li></div><div><li>org.apache.spark.ml.feature.JavaNormalizerSuite.normalizer</li></div><div><li>org.apache.spark.ml.feature.JavaPCASuite.testPCA</li></div><div><li>org.apache.spark.ml.feature.JavaPCASuite.testPCA</li></div><div><li>org.apache.spark.ml.feature.JavaPolynomialExpansionSuite.polynomialExpansionTest</li></div><div><li>org.apache.spark.ml.feature.JavaPolynomialExpansionSuite.polynomialExpansionTest</li></div><div><li>org.apache.spark.ml.feature.JavaStandardScalerSuite.standardScaler</li></div><div><li>org.apache.spark.ml.feature.JavaStandardScalerSuite.standardScaler</li></div><div><li>org.apache.spark.ml.feature.JavaStopWordsRemoverSuite.javaCompatibilityTest</li></div><div><li>org.apache.spark.ml.feature.JavaStopWordsRemoverSuite.javaCompatibilityTest</li></div><div><li>org.apache.spark.ml.feature.JavaStringIndexerSuite.testStringIndexer</li></div><div><li>org.apache.spark.ml.feature.JavaStringIndexerSuite.testStringIndexer</li></div><div><li>org.apache.spark.ml.feature.JavaTokenizerSuite.regexTokenizer</li></div><div><li>org.apache.spark.ml.feature.JavaTokenizerSuite.regexTokenizer</li></div><div><li>org.apache.spark.ml.feature.JavaVectorAssemblerSuite.testVectorAssembler</li></div><div><li>org.apache.spark.ml.feature.JavaVectorAssemblerSuite.testVectorAssembler</li></div><div><li>org.apache.spark.ml.feature.JavaVectorIndexerSuite.vectorIndexerAPI</li></div><div><li>org.apache.spark.ml.feature.JavaVectorIndexerSuite.vectorIndexerAPI</li></div><div><li>org.apache.spark.ml.feature.JavaVectorSlicerSuite.vectorSlice</li></div><div><li>org.apache.spark.ml.feature.JavaVectorSlicerSuite.vectorSlice</li></div><div><li>org.apache.spark.ml.feature.JavaWord2VecSuite.testJavaWord2Vec</li></div><div><li>org.apache.spark.ml.feature.JavaWord2VecSuite.testJavaWord2Vec</li></div><div><li>org.apache.spark.ml.regression.JavaDecisionTreeRegressorSuite.runDT</li></div><div><li>org.apache.spark.ml.regression.JavaDecisionTreeRegressorSuite.runDT</li></div><div><li>org.apache.spark.ml.regression.JavaGBTRegressorSuite.runDT</li></div><div><li>org.apache.spark.ml.regression.JavaGBTRegressorSuite.runDT</li></div><div><li>org.apache.spark.ml.regression.JavaLinearRegressionSuite.linearRegressionDefaultParams</li></div><div><li>org.apache.spark.ml.regression.JavaLinearRegressionSuite.linearRegressionWithSetters</li></div><div><li>org.apache.spark.ml.regression.JavaRandomForestRegressorSuite.runDT</li></div><div><li>org.apache.spark.ml.regression.JavaRandomForestRegressorSuite.runDT</li></div><div><li>org.apache.spark.ml.source.libsvm.JavaLibSVMRelationSuite.verifyLibSVMDF</li></div><div><li>org.apache.spark.ml.source.libsvm.JavaLibSVMRelationSuite.verifyLibSVMDF</li></div><div><li>org.apache.spark.ml.stat.JavaKolmogorovSmirnovTestSuite.testKSTestNamedDistribution</li></div><div><li>org.apache.spark.ml.stat.JavaKolmogorovSmirnovTestSuite.testKSTestNamedDistribution</li></div><div><li>org.apache.spark.ml.stat.JavaKolmogorovSmirnovTestSuite.testKSTestCDF</li></div><div><li>org.apache.spark.ml.stat.JavaKolmogorovSmirnovTestSuite.testKSTestCDF</li></div><div><li>org.apache.spark.ml.stat.JavaSummarizerSuite.testSummarizer</li></div><div><li>org.apache.spark.ml.stat.JavaSummarizerSuite.testSummarizer</li></div><div><li>org.apache.spark.ml.tuning.JavaCrossValidatorSuite.crossValidationWithLogisticRegression</li></div><div><li>org.apache.spark.ml.tuning.JavaCrossValidatorSuite.crossValidationWithLogisticRegression</li></div><div><li>org.apache.spark.ml.util.JavaDefaultReadWriteSuite.testDefaultReadWrite</li></div><div><li>org.apache.spark.ml.util.JavaDefaultReadWriteSuite.testDefaultReadWrite</li></div><div><li>org.apache.spark.mllib.classification.JavaLogisticRegressionSuite.runLRUsingConstructor</li></div><div><li>org.apache.spark.mllib.classification.JavaLogisticRegressionSuite.runLRUsingConstructor</li></div><div><li>org.apache.spark.mllib.classification.JavaLogisticRegressionSuite.runLRUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.classification.JavaLogisticRegressionSuite.runLRUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.testPredictJavaRDD</li></div><div><li>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.testPredictJavaRDD</li></div><div><li>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.runUsingConstructor</li></div><div><li>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.runUsingConstructor</li></div><div><li>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.runUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.runUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.testModelTypeSetters</li></div><div><li>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.testModelTypeSetters</li></div><div><li>org.apache.spark.mllib.classification.JavaSVMSuite.runSVMUsingConstructor</li></div><div><li>org.apache.spark.mllib.classification.JavaSVMSuite.runSVMUsingConstructor</li></div><div><li>org.apache.spark.mllib.classification.JavaSVMSuite.runSVMUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.classification.JavaSVMSuite.runSVMUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.classification.JavaStreamingLogisticRegressionSuite.javaAPI</li></div><div><li>org.apache.spark.mllib.classification.JavaStreamingLogisticRegressionSuite.javaAPI</li></div><div><li>org.apache.spark.mllib.clustering.JavaBisectingKMeansSuite.twoDimensionalData</li></div><div><li>org.apache.spark.mllib.clustering.JavaBisectingKMeansSuite.twoDimensionalData</li></div><div><li>org.apache.spark.mllib.clustering.JavaGaussianMixtureSuite.runGaussianMixture</li></div><div><li>org.apache.spark.mllib.clustering.JavaGaussianMixtureSuite.runGaussianMixture</li></div><div><li>org.apache.spark.mllib.clustering.JavaKMeansSuite.testPredictJavaRDD</li></div><div><li>org.apache.spark.mllib.clustering.JavaKMeansSuite.runKMeansUsingConstructor</li></div><div><li>org.apache.spark.mllib.clustering.JavaKMeansSuite.runKMeansUsingConstructor</li></div><div><li>org.apache.spark.mllib.clustering.JavaKMeansSuite.runKMeansUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.clustering.JavaKMeansSuite.runKMeansUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.clustering.JavaLDASuite.onlineOptimizerCompatibility</li></div><div><li>org.apache.spark.mllib.clustering.JavaLDASuite.onlineOptimizerCompatibility</li></div><div><li>org.apache.spark.mllib.clustering.JavaLDASuite.distributedLDAModel</li></div><div><li>org.apache.spark.mllib.clustering.JavaLDASuite.distributedLDAModel</li></div><div><li>org.apache.spark.mllib.clustering.JavaLDASuite.localLDAModel</li></div><div><li>org.apache.spark.mllib.clustering.JavaLDASuite.localLDAModel</li></div><div><li>org.apache.spark.mllib.clustering.JavaLDASuite.localLdaMethods</li></div><div><li>org.apache.spark.mllib.clustering.JavaStreamingKMeansSuite.javaAPI</li></div><div><li>org.apache.spark.mllib.evaluation.JavaRankingMetricsSuite.rankingMetrics</li></div><div><li>org.apache.spark.mllib.evaluation.JavaRankingMetricsSuite.rankingMetrics</li></div><div><li>org.apache.spark.mllib.feature.JavaTfIdfSuite.tfIdfMinimumDocumentFrequency</li></div><div><li>org.apache.spark.mllib.feature.JavaTfIdfSuite.tfIdfMinimumDocumentFrequency</li></div><div><li>org.apache.spark.mllib.feature.JavaTfIdfSuite.tfIdf</li></div><div><li>org.apache.spark.mllib.feature.JavaTfIdfSuite.tfIdf</li></div><div><li>org.apache.spark.mllib.feature.JavaWord2VecSuite.word2Vec</li></div><div><li>org.apache.spark.mllib.feature.JavaWord2VecSuite.word2Vec</li></div><div><li>org.apache.spark.mllib.fpm.JavaAssociationRulesSuite.runAssociationRules</li></div><div><li>org.apache.spark.mllib.fpm.JavaAssociationRulesSuite.runAssociationRules</li></div><div><li>org.apache.spark.mllib.fpm.JavaFPGrowthSuite.runFPGrowthSaveLoad</li></div><div><li>org.apache.spark.mllib.fpm.JavaFPGrowthSuite.runFPGrowthSaveLoad</li></div><div><li>org.apache.spark.mllib.fpm.JavaFPGrowthSuite.runFPGrowth</li></div><div><li>org.apache.spark.mllib.fpm.JavaFPGrowthSuite.runFPGrowth</li></div><div><li>org.apache.spark.mllib.fpm.JavaPrefixSpanSuite.runPrefixSpan</li></div><div><li>org.apache.spark.mllib.fpm.JavaPrefixSpanSuite.runPrefixSpan</li></div><div><li>org.apache.spark.mllib.fpm.JavaPrefixSpanSuite.runPrefixSpanSaveLoad</li></div><div><li>org.apache.spark.mllib.fpm.JavaPrefixSpanSuite.runPrefixSpanSaveLoad</li></div><div><li>org.apache.spark.mllib.linalg.distributed.JavaRowMatrixSuite.rowMatrixQRDecomposition</li></div><div><li>org.apache.spark.mllib.linalg.distributed.JavaRowMatrixSuite.rowMatrixQRDecomposition</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testNormalVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testNormalVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testArbitrary</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testArbitrary</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testLogNormalVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testLogNormalVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testExponentialVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testExponentialVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testUniformRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testUniformRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testRandomVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testRandomVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testGammaRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testGammaRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testUniformVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testUniformVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testPoissonRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testPoissonRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testNormalRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testNormalRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testPoissonVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testPoissonVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testGammaVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testGammaVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testExponentialRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testExponentialRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testLNormalRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testLNormalRDD</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runALSUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runALSUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSUsingConstructor</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSUsingConstructor</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runRecommend</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runRecommend</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSWithNegativeWeight</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSWithNegativeWeight</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runALSUsingConstructor</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runALSUsingConstructor</li></div><div><li>org.apache.spark.mllib.regression.JavaIsotonicRegressionSuite.testIsotonicRegressionJavaRDD</li></div><div><li>org.apache.spark.mllib.regression.JavaIsotonicRegressionSuite.testIsotonicRegressionJavaRDD</li></div><div><li>org.apache.spark.mllib.regression.JavaIsotonicRegressionSuite.testIsotonicRegressionPredictionsJavaRDD</li></div><div><li>org.apache.spark.mllib.regression.JavaIsotonicRegressionSuite.testIsotonicRegressionPredictionsJavaRDD</li></div><div><li>org.apache.spark.mllib.regression.JavaLassoSuite.runLassoUsingConstructor</li></div><div><li>org.apache.spark.mllib.regression.JavaLassoSuite.runLassoUsingConstructor</li></div><div><li>org.apache.spark.mllib.regression.JavaLassoSuite.runLassoUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.regression.JavaLassoSuite.runLassoUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.testPredictJavaRDD</li></div><div><li>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.testPredictJavaRDD</li></div><div><li>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.runLinearRegressionUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.runLinearRegressionUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.runLinearRegressionUsingConstructor</li></div><div><li>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.runLinearRegressionUsingConstructor</li></div><div><li>org.apache.spark.mllib.regression.JavaRidgeRegressionSuite.runRidgeRegressionUsingConstructor</li></div><div><li>org.apache.spark.mllib.regression.JavaRidgeRegressionSuite.runRidgeRegressionUsingConstructor</li></div><div><li>org.apache.spark.mllib.regression.JavaRidgeRegressionSuite.runRidgeRegressionUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.regression.JavaRidgeRegressionSuite.runRidgeRegressionUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.regression.JavaStreamingLinearRegressionSuite.javaAPI</li></div><div><li>org.apache.spark.mllib.regression.JavaStreamingLinearRegressionSuite.javaAPI</li></div><div><li>org.apache.spark.mllib.stat.JavaStatisticsSuite.testCorr</li></div><div><li>org.apache.spark.mllib.stat.JavaStatisticsSuite.testCorr</li></div><div><li>org.apache.spark.mllib.stat.JavaStatisticsSuite.chiSqTest</li></div><div><li>org.apache.spark.mllib.stat.JavaStatisticsSuite.chiSqTest</li></div><div><li>org.apache.spark.mllib.stat.JavaStatisticsSuite.streamingTest</li></div><div><li>org.apache.spark.mllib.stat.JavaStatisticsSuite.streamingTest</li></div><div><li>org.apache.spark.mllib.stat.JavaStatisticsSuite.kolmogorovSmirnovTest</li></div><div><li>org.apache.spark.mllib.stat.JavaStatisticsSuite.kolmogorovSmirnovTest</li></div><div><li>org.apache.spark.mllib.tree.JavaDecisionTreeSuite.runDTUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.tree.JavaDecisionTreeSuite.runDTUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.tree.JavaDecisionTreeSuite.runDTUsingConstructor</li></div><div><li>org.apache.spark.mllib.tree.JavaDecisionTreeSuite.runDTUsingConstructor</li></div><div><li>org.apache.spark.mllib.util.JavaMLUtilsSuite.testConvertMatrixColumnsToAndFromML</li></div><div><li>org.apache.spark.mllib.util.JavaMLUtilsSuite.testConvertMatrixColumnsToAndFromML</li></div><div><li>org.apache.spark.mllib.util.JavaMLUtilsSuite.testConvertVectorColumnsToAndFromML</li></div><div><li>org.apache.spark.mllib.util.JavaMLUtilsSuite.testConvertVectorColumnsToAndFromML</li></div><div><li>org.apache.spark.repl.ReplSuite.SPARK-15236: use Hive catalog</li></div><div><li>org.apache.spark.repl.ReplSuite.SPARK-15236: use in-memory catalog</li></div><div><li>org.apache.spark.repl.ReplSuite.broadcast vars</li></div><div><li>org.apache.spark.repl.ReplSuite.line wrapper only initialized once when used as encoder outer scope</li></div><div><li>org.apache.spark.repl.ReplSuite.define case class and create Dataset together with paste mode</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.simple foreach with accumulator</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.external vars</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.external classes</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.external functions</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.external functions that access vars</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.broadcast vars</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.interacting with files</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.local-cluster mode</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.SPARK-2576 importing implicits</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.Datasets and encoders</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.SPARK-2632 importing a method from non serializable class and not using it.</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.collecting objects of class defined in repl</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.collecting objects of class defined in repl - shuffling</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.replicating blocks of object with class defined in repl</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.newProductSeqEncoder with REPL defined class</li></div><div><li>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationCount</li></div><div><li>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationCount</li></div><div><li>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationSumDouble</li></div><div><li>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationSumDouble</li></div><div><li>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationSumLong</li></div><div><li>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationSumLong</li></div><div><li>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationAverage</li></div><div><li>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationAverage</li></div><div><li>test.org.apache.spark.sql.JavaApplySchemaSuite.applySchema</li></div><div><li>test.org.apache.spark.sql.JavaApplySchemaSuite.applySchema</li></div><div><li>test.org.apache.spark.sql.JavaApplySchemaSuite.dataFrameRDDOperations</li></div><div><li>test.org.apache.spark.sql.JavaApplySchemaSuite.dataFrameRDDOperations</li></div><div><li>test.org.apache.spark.sql.JavaApplySchemaSuite.applySchemaToJSON</li></div><div><li>test.org.apache.spark.sql.JavaApplySchemaSuite.applySchemaToJSON</li></div><div><li>test.org.apache.spark.sql.JavaBeanDeserializationSuite.testBeanWithArrayFieldDeserialization</li></div><div><li>test.org.apache.spark.sql.JavaBeanDeserializationSuite.testBeanWithArrayFieldDeserialization</li></div><div><li>test.org.apache.spark.sql.JavaBeanDeserializationSuite.testBeanWithMapFieldsDeserialization</li></div><div><li>test.org.apache.spark.sql.JavaBeanDeserializationSuite.testBeanWithMapFieldsDeserialization</li></div><div><li>test.org.apache.spark.sql.JavaColumnExpressionSuite.isInCollectionCheckExceptionMessage</li></div><div><li>test.org.apache.spark.sql.JavaColumnExpressionSuite.isInCollectionCheckExceptionMessage</li></div><div><li>test.org.apache.spark.sql.JavaColumnExpressionSuite.isInCollectionWorksCorrectlyOnJava</li></div><div><li>test.org.apache.spark.sql.JavaColumnExpressionSuite.isInCollectionWorksCorrectlyOnJava</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testFormatAPI</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testTextAPI</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testJsonAPI</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testLoadAPI</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testOptionsAPI</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testSaveModeAPI</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testCsvAPI</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testParquetAPI</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testTextFileAPI</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCollectAndTake</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCollectAndTake</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testJsonRDDToDataFrame</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testJsonRDDToDataFrame</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testVarargMethods</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testVarargMethods</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testBeanWithoutGetter</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testBeanWithoutGetter</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateStructTypeFromList</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateStructTypeFromList</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testSampleBy</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testSampleBy</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCrosstab</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCrosstab</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testUDF</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testUDF</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFromFromList</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFromFromList</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCircularReferenceBean</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCircularReferenceBean</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testFrequentItems</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testFrequentItems</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testSampleByColumn</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testSampleByColumn</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testExecution</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testExecution</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testTextLoad</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testTextLoad</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.pivot</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.pivot</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testGenericLoad</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testGenericLoad</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCountMinSketch</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCountMinSketch</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.pivotColumnValues</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.pivotColumnValues</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFrameFromJavaBeans</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFrameFromJavaBeans</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCorrelation</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCorrelation</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testBloomFilter</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testBloomFilter</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCovariance</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCovariance</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFrameFromLocalJavaBeans</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFrameFromLocalJavaBeans</li></div><div><li>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationCount</li></div><div><li>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationCount</li></div><div><li>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationSumDouble</li></div><div><li>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationSumDouble</li></div><div><li>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationSumLong</li></div><div><li>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationSumLong</li></div><div><li>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationAnonClass</li></div><div><li>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationAnonClass</li></div><div><li>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationAverage</li></div><div><li>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationAverage</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testRuntimeNullabilityCheck</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testRuntimeNullabilityCheck</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean1</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean1</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean2</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean2</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean3</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean3</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testSerializeNull</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testSerializeNull</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testRandomSplit</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testRandomSplit</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testTypedFilterPreservingSchema</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testTypedFilterPreservingSchema</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJoin</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJoin</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testTake</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testTake</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testToLocalIterator</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testToLocalIterator</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testSpecificLists</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testSpecificLists</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testForeach</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testForeach</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJavaEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJavaEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testPrimitiveEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testPrimitiveEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testEmptyBean</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testEmptyBean</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testCommonOperation</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testCommonOperation</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testNullInTopLevelBean</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testNullInTopLevelBean</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testGroupBy</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testGroupBy</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testSetOperation</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testSetOperation</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testBeanWithEnum</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testBeanWithEnum</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testKryoEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testKryoEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.test</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.test</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJavaBeanEncoder2</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJavaBeanEncoder2</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testCollect</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testCollect</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testKryoEncoderErrorMessageForPrivateClass</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testKryoEncoderErrorMessageForPrivateClass</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJavaBeanEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJavaBeanEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testTupleEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testTupleEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testNestedTupleEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testNestedTupleEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testTupleEncoderSchema</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testTupleEncoderSchema</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testReduce</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testReduce</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testSelect</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testSelect</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJavaEncoderErrorMessageForPrivateClass</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJavaEncoderErrorMessageForPrivateClass</li></div><div><li>test.org.apache.spark.sql.JavaSaveLoadSuite.saveAndLoadWithSchema</li></div><div><li>test.org.apache.spark.sql.JavaSaveLoadSuite.saveAndLoadWithSchema</li></div><div><li>test.org.apache.spark.sql.JavaSaveLoadSuite.saveAndLoad</li></div><div><li>test.org.apache.spark.sql.JavaSaveLoadSuite.saveAndLoad</li></div><div><li>test.org.apache.spark.sql.JavaUDAFSuite.udf1Test</li></div><div><li>test.org.apache.spark.sql.JavaUDAFSuite.udf1Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf1Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf1Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf2Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf2Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf3Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf3Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf4Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf4Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf5Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf5Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf6Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf6Test</li></div><div><li>test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.testForeachBatchAPI</li></div><div><li>test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.testForeachBatchAPI</li></div><div><li>test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.testForeachAPI</li></div><div><li>test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.testForeachAPI</li></div><div><li>org.apache.spark.sql.hive.HiveMetastoreLazyInitializationSuite.lazily initialize Hive client</li></div><div><li>org.apache.spark.sql.hive.JavaDataFrameSuite.testUDAF</li></div><div><li>org.apache.spark.sql.hive.JavaDataFrameSuite.saveTableAndQueryIt</li></div><div><li>org.apache.spark.sql.hive.JavaMetastoreDataSourcesSuite.saveExternalTableAndQueryIt</li></div><div><li>org.apache.spark.sql.hive.JavaMetastoreDataSourcesSuite.saveTableAndQueryIt</li></div><div><li>org.apache.spark.sql.hive.JavaMetastoreDataSourcesSuite.saveExternalTableWithSchemaAndQueryIt</li></div><div><li>org.apache.spark.streaming.JavaMapWithStateSuite.testBasicFunction</li></div><div><li>org.apache.spark.streaming.JavaReceiverAPISuite.testReceiver</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testStreamingContextTransform</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testFlatMapValues</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testFlatMapValues</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testMapPartitions</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testMapPartitions</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairFilter</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairFilter</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testCombineByKey</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testCombineByKey</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testMap</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testMap</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairToNormalRDDTransform</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairToNormalRDDTransform</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairReduceByKey</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairReduceByKey</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairMap</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairMap</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testFlatMap</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testReduceByKeyAndWindowWithInverse</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testReduceByWindow</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testReduceByWindow</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairFlatMap</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairFlatMap</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairToPairFlatMapWithChangingTypes</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairToPairFlatMapWithChangingTypes</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairMapPartitions</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairMapPartitions</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testVariousTransform</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testVariousTransform</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testTransformWith</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testTransformWith</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testVariousTransformWith</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testVariousTransformWith</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testReduceByKeyAndWindow</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testReduceByKeyAndWindow</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairTransform</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairTransform</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testFilter</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testFilter</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairMap2</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairMap2</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testMapValues</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testMapValues</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testReduce</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testReduce</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testUpdateStateByKey</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testUpdateStateByKey</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testTransform</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testTransform</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testStreamingContextTransform</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testFlatMapValues</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testReduceByWindowWithInverse</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testMapPartitions</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairFilter</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testRepartitionFewerPartitions</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testRepartitionFewerPartitions</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCombineByKey</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCombineByKey</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testContextGetOrCreate</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testWindowWithSlideDuration</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testQueueStream</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testQueueStream</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCountByValue</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCountByValue</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testMap</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testMap</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairToNormalRDDTransform</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairToNormalRDDTransform</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairReduceByKey</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairReduceByKey</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCount</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCount</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCheckpointMasterRecovery</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCheckpointMasterRecovery</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairMap</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testUnion</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testUnion</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testFlatMap</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testFlatMap</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testReduceByKeyAndWindowWithInverse</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testReduceByKeyAndWindowWithInverse</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testGlom</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testGlom</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testJoin</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairFlatMap</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairFlatMap</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairToPairFlatMapWithChangingTypes</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairToPairFlatMapWithChangingTypes</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairMapPartitions</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairMapPartitions</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testRepartitionMorePartitions</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testRepartitionMorePartitions</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testReduceByWindowWithoutInverse</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testReduceByWindowWithoutInverse</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testLeftOuterJoin</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testLeftOuterJoin</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testVariousTransform</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testVariousTransform</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testTransformWith</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testTransformWith</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testVariousTransformWith</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testVariousTransformWith</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testTextFileStream</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testTextFileStream</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairGroupByKey</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairGroupByKey</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCoGroup</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCoGroup</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testInitialization</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testInitialization</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testGroupByKeyAndWindow</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testGroupByKeyAndWindow</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testReduceByKeyAndWindow</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testReduceByKeyAndWindow</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testForeachRDD</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testForeachRDD</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testFileStream</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testFileStream</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairTransform</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairTransform</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testFilter</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testFilter</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairMap2</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairMap2</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testMapValues</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testReduce</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testUpdateStateByKey</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testTransform</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testTransform</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testWindow</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testWindow</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCountByValueAndWindow</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCountByValueAndWindow</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testRawSocketStream</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testRawSocketStream</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testSocketTextStream</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testSocketTextStream</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testUpdateStateByKeyWithInitial</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testUpdateStateByKeyWithInitial</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testContextState</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testContextState</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.spark.streaming.kafka010.JavaDirectKafkaStreamSuite.testKafkaStream</li></div><div><li>org.apache.spark.streaming.kafka010.JavaKafkaRDDSuite.testKafkaRDD</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.spark.network.ChunkFetchIntegrationSuite.fetchBothChunks</li></div><div><li>org.apache.spark.network.sasl.SparkSaslSuite.testFileRegionEncryption</li></div><div><li>org.apache.spark.ExecutorPluginSuite.testAddMultiplePlugins</li></div><div><li>org.apache.spark.ExecutorPluginSuite.testPluginClassDoesNotExist</li></div><div><li>org.apache.spark.ExecutorPluginSuite.testAddPlugin</li></div><div><li>org.apache.spark.ExecutorPluginSuite.testPluginShutdownWithException</li></div><div><li>org.apache.spark.JavaJdbcRDDSuite.testJavaJdbcRDD</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.leftOuterJoin</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.leftOuterJoin</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.foldReduce</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.foldReduce</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.mapsFromPairsToPairs</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.mapsFromPairsToPairs</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.flatMap</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.flatMap</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.foreach</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.foreach</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.map</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.map</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.zip</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.zip</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.keyBy</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.keyBy</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.groupBy</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.groupBy</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.mapPartitions</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.mapPartitions</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.foldByKey</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.foldByKey</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.mapOnPairRDD</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.mapOnPairRDD</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.sequenceFile</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.sequenceFile</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.collectPartitions</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.collectPartitions</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.reduceByKey</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.foreachWithAnonymousClass</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.foreachWithAnonymousClass</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.collectAsMapWithIntArrayValues</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.collectAsMapWithIntArrayValues</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.zipPartitions</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.zipPartitions</li></div><div><li>test.org.apache.spark.JavaAPISuite.groupByOnPairRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.groupByOnPairRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.binaryFilesCaching</li></div><div><li>test.org.apache.spark.JavaAPISuite.binaryFilesCaching</li></div><div><li>test.org.apache.spark.JavaAPISuite.sparkContextUnion</li></div><div><li>test.org.apache.spark.JavaAPISuite.sparkContextUnion</li></div><div><li>test.org.apache.spark.JavaAPISuite.checkpointAndComputation</li></div><div><li>test.org.apache.spark.JavaAPISuite.checkpointAndComputation</li></div><div><li>test.org.apache.spark.JavaAPISuite.leftOuterJoin</li></div><div><li>test.org.apache.spark.JavaAPISuite.leftOuterJoin</li></div><div><li>test.org.apache.spark.JavaAPISuite.keyByOnPairRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.keyByOnPairRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.getNumPartitions</li></div><div><li>test.org.apache.spark.JavaAPISuite.getNumPartitions</li></div><div><li>test.org.apache.spark.JavaAPISuite.wholeTextFiles</li></div><div><li>test.org.apache.spark.JavaAPISuite.wholeTextFiles</li></div><div><li>test.org.apache.spark.JavaAPISuite.binaryFiles</li></div><div><li>test.org.apache.spark.JavaAPISuite.binaryFiles</li></div><div><li>test.org.apache.spark.JavaAPISuite.foldReduce</li></div><div><li>test.org.apache.spark.JavaAPISuite.foldReduce</li></div><div><li>test.org.apache.spark.JavaAPISuite.writeWithNewAPIHadoopFile</li></div><div><li>test.org.apache.spark.JavaAPISuite.writeWithNewAPIHadoopFile</li></div><div><li>test.org.apache.spark.JavaAPISuite.hadoopFile</li></div><div><li>test.org.apache.spark.JavaAPISuite.hadoopFile</li></div><div><li>test.org.apache.spark.JavaAPISuite.lookup</li></div><div><li>test.org.apache.spark.JavaAPISuite.lookup</li></div><div><li>test.org.apache.spark.JavaAPISuite.countAsync</li></div><div><li>test.org.apache.spark.JavaAPISuite.countAsync</li></div><div><li>test.org.apache.spark.JavaAPISuite.textFiles</li></div><div><li>test.org.apache.spark.JavaAPISuite.textFiles</li></div><div><li>test.org.apache.spark.JavaAPISuite.binaryRecords</li></div><div><li>test.org.apache.spark.JavaAPISuite.binaryRecords</li></div><div><li>test.org.apache.spark.JavaAPISuite.toLocalIterator</li></div><div><li>test.org.apache.spark.JavaAPISuite.toLocalIterator</li></div><div><li>test.org.apache.spark.JavaAPISuite.repartitionAndSortWithinPartitions</li></div><div><li>test.org.apache.spark.JavaAPISuite.repartitionAndSortWithinPartitions</li></div><div><li>test.org.apache.spark.JavaAPISuite.reduce</li></div><div><li>test.org.apache.spark.JavaAPISuite.reduce</li></div><div><li>test.org.apache.spark.JavaAPISuite.sample</li></div><div><li>test.org.apache.spark.JavaAPISuite.sample</li></div><div><li>test.org.apache.spark.JavaAPISuite.sortBy</li></div><div><li>test.org.apache.spark.JavaAPISuite.sortBy</li></div><div><li>test.org.apache.spark.JavaAPISuite.mapsFromPairsToPairs</li></div><div><li>test.org.apache.spark.JavaAPISuite.mapsFromPairsToPairs</li></div><div><li>test.org.apache.spark.JavaAPISuite.flatMap</li></div><div><li>test.org.apache.spark.JavaAPISuite.flatMap</li></div><div><li>test.org.apache.spark.JavaAPISuite.cogroup3</li></div><div><li>test.org.apache.spark.JavaAPISuite.cogroup3</li></div><div><li>test.org.apache.spark.JavaAPISuite.cogroup4</li></div><div><li>test.org.apache.spark.JavaAPISuite.cogroup4</li></div><div><li>test.org.apache.spark.JavaAPISuite.randomSplit</li></div><div><li>test.org.apache.spark.JavaAPISuite.randomSplit</li></div><div><li>test.org.apache.spark.JavaAPISuite.persist</li></div><div><li>test.org.apache.spark.JavaAPISuite.persist</li></div><div><li>test.org.apache.spark.JavaAPISuite.foreach</li></div><div><li>test.org.apache.spark.JavaAPISuite.foreach</li></div><div><li>test.org.apache.spark.JavaAPISuite.hadoopFileCompressed</li></div><div><li>test.org.apache.spark.JavaAPISuite.hadoopFileCompressed</li></div><div><li>test.org.apache.spark.JavaAPISuite.textFilesCompressed</li></div><div><li>test.org.apache.spark.JavaAPISuite.textFilesCompressed</li></div><div><li>test.org.apache.spark.JavaAPISuite.testAsyncActionCancellation</li></div><div><li>test.org.apache.spark.JavaAPISuite.testAsyncActionCancellation</li></div><div><li>test.org.apache.spark.JavaAPISuite.checkpointAndRestore</li></div><div><li>test.org.apache.spark.JavaAPISuite.checkpointAndRestore</li></div><div><li>test.org.apache.spark.JavaAPISuite.sortByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.sortByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.aggregateByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.aggregateByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.map</li></div><div><li>test.org.apache.spark.JavaAPISuite.map</li></div><div><li>test.org.apache.spark.JavaAPISuite.max</li></div><div><li>test.org.apache.spark.JavaAPISuite.max</li></div><div><li>test.org.apache.spark.JavaAPISuite.min</li></div><div><li>test.org.apache.spark.JavaAPISuite.min</li></div><div><li>test.org.apache.spark.JavaAPISuite.top</li></div><div><li>test.org.apache.spark.JavaAPISuite.top</li></div><div><li>test.org.apache.spark.JavaAPISuite.zip</li></div><div><li>test.org.apache.spark.JavaAPISuite.fold</li></div><div><li>test.org.apache.spark.JavaAPISuite.fold</li></div><div><li>test.org.apache.spark.JavaAPISuite.glom</li></div><div><li>test.org.apache.spark.JavaAPISuite.glom</li></div><div><li>test.org.apache.spark.JavaAPISuite.take</li></div><div><li>test.org.apache.spark.JavaAPISuite.take</li></div><div><li>test.org.apache.spark.JavaAPISuite.javaDoubleRDDHistoGram</li></div><div><li>test.org.apache.spark.JavaAPISuite.javaDoubleRDDHistoGram</li></div><div><li>test.org.apache.spark.JavaAPISuite.collectUnderlyingScalaRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.collectUnderlyingScalaRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.keyBy</li></div><div><li>test.org.apache.spark.JavaAPISuite.keyBy</li></div><div><li>test.org.apache.spark.JavaAPISuite.mapPartitionsWithIndex</li></div><div><li>test.org.apache.spark.JavaAPISuite.mapPartitionsWithIndex</li></div><div><li>test.org.apache.spark.JavaAPISuite.sampleByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.sampleByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.intersection</li></div><div><li>test.org.apache.spark.JavaAPISuite.intersection</li></div><div><li>test.org.apache.spark.JavaAPISuite.aggregate</li></div><div><li>test.org.apache.spark.JavaAPISuite.aggregate</li></div><div><li>test.org.apache.spark.JavaAPISuite.cartesian</li></div><div><li>test.org.apache.spark.JavaAPISuite.cartesian</li></div><div><li>test.org.apache.spark.JavaAPISuite.countApproxDistinctByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.countApproxDistinctByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.readWithNewAPIHadoopFile</li></div><div><li>test.org.apache.spark.JavaAPISuite.readWithNewAPIHadoopFile</li></div><div><li>test.org.apache.spark.JavaAPISuite.testRegisterKryoClasses</li></div><div><li>test.org.apache.spark.JavaAPISuite.testRegisterKryoClasses</li></div><div><li>test.org.apache.spark.JavaAPISuite.groupBy</li></div><div><li>test.org.apache.spark.JavaAPISuite.groupBy</li></div><div><li>test.org.apache.spark.JavaAPISuite.sampleByKeyExact</li></div><div><li>test.org.apache.spark.JavaAPISuite.sampleByKeyExact</li></div><div><li>test.org.apache.spark.JavaAPISuite.mapPartitions</li></div><div><li>test.org.apache.spark.JavaAPISuite.mapPartitions</li></div><div><li>test.org.apache.spark.JavaAPISuite.takeOrdered</li></div><div><li>test.org.apache.spark.JavaAPISuite.takeOrdered</li></div><div><li>test.org.apache.spark.JavaAPISuite.foldByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.foldByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.objectFilesOfInts</li></div><div><li>test.org.apache.spark.JavaAPISuite.objectFilesOfInts</li></div><div><li>test.org.apache.spark.JavaAPISuite.treeAggregate</li></div><div><li>test.org.apache.spark.JavaAPISuite.treeAggregate</li></div><div><li>test.org.apache.spark.JavaAPISuite.testGetPersistentRDDs</li></div><div><li>test.org.apache.spark.JavaAPISuite.testGetPersistentRDDs</li></div><div><li>test.org.apache.spark.JavaAPISuite.approximateResults</li></div><div><li>test.org.apache.spark.JavaAPISuite.approximateResults</li></div><div><li>test.org.apache.spark.JavaAPISuite.treeReduce</li></div><div><li>test.org.apache.spark.JavaAPISuite.treeReduce</li></div><div><li>test.org.apache.spark.JavaAPISuite.collectAsMapAndSerialize</li></div><div><li>test.org.apache.spark.JavaAPISuite.collectAsMapAndSerialize</li></div><div><li>test.org.apache.spark.JavaAPISuite.countApproxDistinct</li></div><div><li>test.org.apache.spark.JavaAPISuite.countApproxDistinct</li></div><div><li>test.org.apache.spark.JavaAPISuite.javaDoubleRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.javaDoubleRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.mapOnPairRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.mapOnPairRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.testAsyncActionErrorWrapping</li></div><div><li>test.org.apache.spark.JavaAPISuite.naturalMax</li></div><div><li>test.org.apache.spark.JavaAPISuite.naturalMin</li></div><div><li>test.org.apache.spark.JavaAPISuite.sequenceFile</li></div><div><li>test.org.apache.spark.JavaAPISuite.sequenceFile</li></div><div><li>test.org.apache.spark.JavaAPISuite.collectPartitions</li></div><div><li>test.org.apache.spark.JavaAPISuite.collectPartitions</li></div><div><li>test.org.apache.spark.JavaAPISuite.cogroup</li></div><div><li>test.org.apache.spark.JavaAPISuite.cogroup</li></div><div><li>test.org.apache.spark.JavaAPISuite.reduceByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.reduceByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.repartition</li></div><div><li>test.org.apache.spark.JavaAPISuite.repartition</li></div><div><li>test.org.apache.spark.JavaAPISuite.iterator</li></div><div><li>test.org.apache.spark.JavaAPISuite.iterator</li></div><div><li>test.org.apache.spark.JavaAPISuite.emptyRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.emptyRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.zipWithIndex</li></div><div><li>test.org.apache.spark.JavaAPISuite.zipWithIndex</li></div><div><li>test.org.apache.spark.JavaAPISuite.foreachPartition</li></div><div><li>test.org.apache.spark.JavaAPISuite.foreachPartition</li></div><div><li>test.org.apache.spark.JavaAPISuite.combineByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.combineByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.takeAsync</li></div><div><li>test.org.apache.spark.JavaAPISuite.takeAsync</li></div><div><li>test.org.apache.spark.JavaAPISuite.collectAsMapWithIntArrayValues</li></div><div><li>test.org.apache.spark.JavaAPISuite.collectAsMapWithIntArrayValues</li></div><div><li>test.org.apache.spark.JavaAPISuite.objectFilesOfComplexTypes</li></div><div><li>test.org.apache.spark.JavaAPISuite.objectFilesOfComplexTypes</li></div><div><li>test.org.apache.spark.JavaAPISuite.zipWithUniqueId</li></div><div><li>test.org.apache.spark.JavaAPISuite.zipWithUniqueId</li></div><div><li>test.org.apache.spark.JavaAPISuite.collectAsync</li></div><div><li>test.org.apache.spark.JavaAPISuite.collectAsync</li></div><div><li>test.org.apache.spark.JavaAPISuite.foreachAsync</li></div><div><li>test.org.apache.spark.JavaAPISuite.foreachAsync</li></div><div><li>test.org.apache.spark.JavaAPISuite.zipPartitions</li></div><div><li>test.org.apache.spark.JavaAPISuite.zipPartitions</li></div><div><li>test.org.apache.spark.JavaAPISuite.reduceOnJavaDoubleRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.reduceOnJavaDoubleRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.isEmpty</li></div><div><li>test.org.apache.spark.JavaAPISuite.isEmpty</li></div><div><li>test.org.apache.spark.JavaSparkContextSuite.scalaSparkContext</li></div><div><li>org.apache.spark.streaming.kafka010.JavaDirectKafkaStreamSuite.testKafkaStream</li></div><div><li>org.apache.spark.streaming.kafka010.JavaKafkaRDDSuite.testKafkaRDD</li></div><div><li>org.apache.spark.graphx.EdgeRDDSuite.cache, getStorageLevel</li></div><div><li>org.apache.spark.graphx.EdgeRDDSuite.checkpointing</li></div><div><li>org.apache.spark.graphx.GraphLoaderSuite.GraphLoader.edgeListFile</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.joinVertices</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.collectNeighborIds</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.removeSelfEdges</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.filter</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.convertToCanonicalEdges</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.collectEdgesCycleDirectionOut</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.collectEdgesCycleDirectionIn</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.collectEdgesCycleDirectionEither</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.collectEdgesChainDirectionOut</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.collectEdgesChainDirectionIn</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.collectEdgesChainDirectionEither</li></div><div><li>org.apache.spark.graphx.GraphSuite.Graph.fromEdgeTuples</li></div><div><li>org.apache.spark.graphx.GraphSuite.Graph.fromEdges</li></div><div><li>org.apache.spark.graphx.GraphSuite.Graph.apply</li></div><div><li>org.apache.spark.graphx.GraphSuite.triplets</li></div><div><li>org.apache.spark.graphx.GraphSuite.partitionBy</li></div><div><li>org.apache.spark.graphx.GraphSuite.mapVertices</li></div><div><li>org.apache.spark.graphx.GraphSuite.mapVertices changing type with same erased type</li></div><div><li>org.apache.spark.graphx.GraphSuite.mapEdges</li></div><div><li>org.apache.spark.graphx.GraphSuite.mapTriplets</li></div><div><li>org.apache.spark.graphx.GraphSuite.reverse</li></div><div><li>org.apache.spark.graphx.GraphSuite.reverse with join elimination</li></div><div><li>org.apache.spark.graphx.GraphSuite.subgraph</li></div><div><li>org.apache.spark.graphx.GraphSuite.mask</li></div><div><li>org.apache.spark.graphx.GraphSuite.groupEdges</li></div><div><li>org.apache.spark.graphx.GraphSuite.aggregateMessages</li></div><div><li>org.apache.spark.graphx.GraphSuite.outerJoinVertices</li></div><div><li>org.apache.spark.graphx.GraphSuite.more edge partitions than vertex partitions</li></div><div><li>org.apache.spark.graphx.GraphSuite.checkpoint</li></div><div><li>org.apache.spark.graphx.GraphSuite.cache, getStorageLevel</li></div><div><li>org.apache.spark.graphx.GraphSuite.non-default number of edge partitions</li></div><div><li>org.apache.spark.graphx.GraphSuite.unpersist graph RDD</li></div><div><li>org.apache.spark.graphx.GraphSuite.SPARK-14219: pickRandomVertex</li></div><div><li>org.apache.spark.graphx.PregelSuite.1 iteration</li></div><div><li>org.apache.spark.graphx.PregelSuite.chain propagation</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.filter</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.mapValues</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.minus</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.minus with RDD[(VertexId, VD)]</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.minus with non-equal number of partitions</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.diff</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.diff with RDD[(VertexId, VD)]</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.diff vertices with non-equal number of partitions</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.leftJoin</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.leftJoin vertices with non-equal number of partitions</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.innerJoin</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.innerJoin vertices with the non-equal number of partitions</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.aggregateUsingIndex</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.mergeFunc</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.cache, getStorageLevel</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.checkpoint</li></div><div><li>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Grid Connected Components</li></div><div><li>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Reverse Grid Connected Components</li></div><div><li>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Chain Connected Components</li></div><div><li>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Reverse Chain Connected Components</li></div><div><li>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Connected Components on a Toy Connected Graph</li></div><div><li>org.apache.spark.graphx.lib.LabelPropagationSuite.Label Propagation</li></div><div><li>org.apache.spark.graphx.lib.PageRankSuite.Star PageRank</li></div><div><li>org.apache.spark.graphx.lib.PageRankSuite.Star PersonalPageRank</li></div><div><li>org.apache.spark.graphx.lib.PageRankSuite.Grid PageRank</li></div><div><li>org.apache.spark.graphx.lib.PageRankSuite.Chain PageRank</li></div><div><li>org.apache.spark.graphx.lib.PageRankSuite.Chain PersonalizedPageRank</li></div><div><li>org.apache.spark.graphx.lib.PageRankSuite.Loop with source PageRank</li></div><div><li>org.apache.spark.graphx.lib.PageRankSuite.Loop with sink PageRank</li></div><div><li>org.apache.spark.graphx.lib.SVDPlusPlusSuite.Test SVD++ with mean square error on training set</li></div><div><li>org.apache.spark.graphx.lib.ShortestPathsSuite.Shortest Path Computations</li></div><div><li>org.apache.spark.graphx.lib.StronglyConnectedComponentsSuite.Island Strongly Connected Components</li></div><div><li>org.apache.spark.graphx.lib.StronglyConnectedComponentsSuite.Cycle Strongly Connected Components</li></div><div><li>org.apache.spark.graphx.lib.StronglyConnectedComponentsSuite.2 Cycle Strongly Connected Components</li></div><div><li>org.apache.spark.graphx.lib.TriangleCountSuite.Count a single triangle</li></div><div><li>org.apache.spark.graphx.lib.TriangleCountSuite.Count two triangles</li></div><div><li>org.apache.spark.graphx.lib.TriangleCountSuite.Count two triangles with bi-directed edges</li></div><div><li>org.apache.spark.graphx.lib.TriangleCountSuite.Count a single triangle with duplicate edges</li></div><div><li>org.apache.spark.graphx.util.GraphGeneratorsSuite.GraphGenerators.logNormalGraph</li></div><div><li>org.apache.spark.graphx.util.GraphGeneratorsSuite.SPARK-5064 GraphGenerators.rmatGraph numEdges upper bound</li></div><div><li>org.apache.spark.graphx.util.PeriodicGraphCheckpointerSuite.Persisting</li></div><div><li>org.apache.spark.graphx.util.PeriodicGraphCheckpointerSuite.Checkpointing</li></div><div><li>org.apache.spark.ml.JavaPipelineSuite.pipeline</li></div><div><li>org.apache.spark.ml.JavaPipelineSuite.pipeline</li></div><div><li>org.apache.spark.ml.classification.JavaDecisionTreeClassifierSuite.runDT</li></div><div><li>org.apache.spark.ml.classification.JavaDecisionTreeClassifierSuite.runDT</li></div><div><li>org.apache.spark.ml.classification.JavaGBTClassifierSuite.runDT</li></div><div><li>org.apache.spark.ml.classification.JavaGBTClassifierSuite.runDT</li></div><div><li>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionWithSetters</li></div><div><li>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionWithSetters</li></div><div><li>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionTrainingSummary</li></div><div><li>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionTrainingSummary</li></div><div><li>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionPredictorClassifierMethods</li></div><div><li>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionPredictorClassifierMethods</li></div><div><li>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionDefaultParams</li></div><div><li>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionDefaultParams</li></div><div><li>org.apache.spark.ml.classification.JavaMultilayerPerceptronClassifierSuite.testMLPC</li></div><div><li>org.apache.spark.ml.classification.JavaMultilayerPerceptronClassifierSuite.testMLPC</li></div><div><li>org.apache.spark.ml.classification.JavaNaiveBayesSuite.testNaiveBayes</li></div><div><li>org.apache.spark.ml.classification.JavaNaiveBayesSuite.testNaiveBayes</li></div><div><li>org.apache.spark.ml.classification.JavaNaiveBayesSuite.naiveBayesDefaultParams</li></div><div><li>org.apache.spark.ml.classification.JavaNaiveBayesSuite.naiveBayesDefaultParams</li></div><div><li>org.apache.spark.ml.classification.JavaOneVsRestSuite.oneVsRestDefaultParams</li></div><div><li>org.apache.spark.ml.classification.JavaOneVsRestSuite.oneVsRestDefaultParams</li></div><div><li>org.apache.spark.ml.classification.JavaRandomForestClassifierSuite.runDT</li></div><div><li>org.apache.spark.ml.classification.JavaRandomForestClassifierSuite.runDT</li></div><div><li>org.apache.spark.ml.clustering.JavaKMeansSuite.fitAndTransform</li></div><div><li>org.apache.spark.ml.clustering.JavaKMeansSuite.fitAndTransform</li></div><div><li>org.apache.spark.ml.feature.JavaBucketizerSuite.bucketizerTest</li></div><div><li>org.apache.spark.ml.feature.JavaBucketizerSuite.bucketizerMultipleColumnsTest</li></div><div><li>org.apache.spark.ml.feature.JavaBucketizerSuite.bucketizerMultipleColumnsTest</li></div><div><li>org.apache.spark.ml.feature.JavaDCTSuite.javaCompatibilityTest</li></div><div><li>org.apache.spark.ml.feature.JavaDCTSuite.javaCompatibilityTest</li></div><div><li>org.apache.spark.ml.feature.JavaHashingTFSuite.hashingTF</li></div><div><li>org.apache.spark.ml.feature.JavaHashingTFSuite.hashingTF</li></div><div><li>org.apache.spark.ml.feature.JavaNormalizerSuite.normalizer</li></div><div><li>org.apache.spark.ml.feature.JavaNormalizerSuite.normalizer</li></div><div><li>org.apache.spark.ml.feature.JavaPCASuite.testPCA</li></div><div><li>org.apache.spark.ml.feature.JavaPCASuite.testPCA</li></div><div><li>org.apache.spark.ml.feature.JavaPolynomialExpansionSuite.polynomialExpansionTest</li></div><div><li>org.apache.spark.ml.feature.JavaPolynomialExpansionSuite.polynomialExpansionTest</li></div><div><li>org.apache.spark.ml.feature.JavaStandardScalerSuite.standardScaler</li></div><div><li>org.apache.spark.ml.feature.JavaStandardScalerSuite.standardScaler</li></div><div><li>org.apache.spark.ml.feature.JavaStopWordsRemoverSuite.javaCompatibilityTest</li></div><div><li>org.apache.spark.ml.feature.JavaStopWordsRemoverSuite.javaCompatibilityTest</li></div><div><li>org.apache.spark.ml.feature.JavaStringIndexerSuite.testStringIndexer</li></div><div><li>org.apache.spark.ml.feature.JavaStringIndexerSuite.testStringIndexer</li></div><div><li>org.apache.spark.ml.feature.JavaTokenizerSuite.regexTokenizer</li></div><div><li>org.apache.spark.ml.feature.JavaVectorAssemblerSuite.testVectorAssembler</li></div><div><li>org.apache.spark.ml.feature.JavaVectorAssemblerSuite.testVectorAssembler</li></div><div><li>org.apache.spark.ml.feature.JavaVectorIndexerSuite.vectorIndexerAPI</li></div><div><li>org.apache.spark.ml.feature.JavaVectorIndexerSuite.vectorIndexerAPI</li></div><div><li>org.apache.spark.ml.feature.JavaVectorSlicerSuite.vectorSlice</li></div><div><li>org.apache.spark.ml.feature.JavaVectorSlicerSuite.vectorSlice</li></div><div><li>org.apache.spark.ml.feature.JavaWord2VecSuite.testJavaWord2Vec</li></div><div><li>org.apache.spark.ml.feature.JavaWord2VecSuite.testJavaWord2Vec</li></div><div><li>org.apache.spark.ml.regression.JavaDecisionTreeRegressorSuite.runDT</li></div><div><li>org.apache.spark.ml.regression.JavaDecisionTreeRegressorSuite.runDT</li></div><div><li>org.apache.spark.ml.regression.JavaGBTRegressorSuite.runDT</li></div><div><li>org.apache.spark.ml.regression.JavaGBTRegressorSuite.runDT</li></div><div><li>org.apache.spark.ml.regression.JavaLinearRegressionSuite.linearRegressionDefaultParams</li></div><div><li>org.apache.spark.ml.regression.JavaLinearRegressionSuite.linearRegressionDefaultParams</li></div><div><li>org.apache.spark.ml.regression.JavaLinearRegressionSuite.linearRegressionWithSetters</li></div><div><li>org.apache.spark.ml.regression.JavaLinearRegressionSuite.linearRegressionWithSetters</li></div><div><li>org.apache.spark.ml.regression.JavaRandomForestRegressorSuite.runDT</li></div><div><li>org.apache.spark.ml.regression.JavaRandomForestRegressorSuite.runDT</li></div><div><li>org.apache.spark.ml.source.libsvm.JavaLibSVMRelationSuite.verifyLibSVMDF</li></div><div><li>org.apache.spark.ml.source.libsvm.JavaLibSVMRelationSuite.verifyLibSVMDF</li></div><div><li>org.apache.spark.ml.stat.JavaKolmogorovSmirnovTestSuite.testKSTestNamedDistribution</li></div><div><li>org.apache.spark.ml.stat.JavaKolmogorovSmirnovTestSuite.testKSTestCDF</li></div><div><li>org.apache.spark.ml.stat.JavaSummarizerSuite.testSummarizer</li></div><div><li>org.apache.spark.ml.tuning.JavaCrossValidatorSuite.crossValidationWithLogisticRegression</li></div><div><li>org.apache.spark.ml.tuning.JavaCrossValidatorSuite.crossValidationWithLogisticRegression</li></div><div><li>org.apache.spark.ml.util.JavaDefaultReadWriteSuite.testDefaultReadWrite</li></div><div><li>org.apache.spark.ml.util.JavaDefaultReadWriteSuite.testDefaultReadWrite</li></div><div><li>org.apache.spark.mllib.classification.JavaLogisticRegressionSuite.runLRUsingConstructor</li></div><div><li>org.apache.spark.mllib.classification.JavaLogisticRegressionSuite.runLRUsingConstructor</li></div><div><li>org.apache.spark.mllib.classification.JavaLogisticRegressionSuite.runLRUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.classification.JavaLogisticRegressionSuite.runLRUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.testPredictJavaRDD</li></div><div><li>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.testPredictJavaRDD</li></div><div><li>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.runUsingConstructor</li></div><div><li>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.runUsingConstructor</li></div><div><li>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.runUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.runUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.testModelTypeSetters</li></div><div><li>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.testModelTypeSetters</li></div><div><li>org.apache.spark.mllib.classification.JavaSVMSuite.runSVMUsingConstructor</li></div><div><li>org.apache.spark.mllib.classification.JavaSVMSuite.runSVMUsingConstructor</li></div><div><li>org.apache.spark.mllib.classification.JavaSVMSuite.runSVMUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.classification.JavaSVMSuite.runSVMUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.classification.JavaStreamingLogisticRegressionSuite.javaAPI</li></div><div><li>org.apache.spark.mllib.classification.JavaStreamingLogisticRegressionSuite.javaAPI</li></div><div><li>org.apache.spark.mllib.clustering.JavaBisectingKMeansSuite.twoDimensionalData</li></div><div><li>org.apache.spark.mllib.clustering.JavaBisectingKMeansSuite.twoDimensionalData</li></div><div><li>org.apache.spark.mllib.clustering.JavaGaussianMixtureSuite.runGaussianMixture</li></div><div><li>org.apache.spark.mllib.clustering.JavaGaussianMixtureSuite.runGaussianMixture</li></div><div><li>org.apache.spark.mllib.clustering.JavaKMeansSuite.testPredictJavaRDD</li></div><div><li>org.apache.spark.mllib.clustering.JavaKMeansSuite.testPredictJavaRDD</li></div><div><li>org.apache.spark.mllib.clustering.JavaKMeansSuite.runKMeansUsingConstructor</li></div><div><li>org.apache.spark.mllib.clustering.JavaKMeansSuite.runKMeansUsingConstructor</li></div><div><li>org.apache.spark.mllib.clustering.JavaKMeansSuite.runKMeansUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.clustering.JavaKMeansSuite.runKMeansUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.clustering.JavaLDASuite.onlineOptimizerCompatibility</li></div><div><li>org.apache.spark.mllib.clustering.JavaLDASuite.distributedLDAModel</li></div><div><li>org.apache.spark.mllib.clustering.JavaLDASuite.distributedLDAModel</li></div><div><li>org.apache.spark.mllib.clustering.JavaLDASuite.localLDAModel</li></div><div><li>org.apache.spark.mllib.clustering.JavaLDASuite.localLDAModel</li></div><div><li>org.apache.spark.mllib.clustering.JavaLDASuite.localLdaMethods</li></div><div><li>org.apache.spark.mllib.clustering.JavaLDASuite.localLdaMethods</li></div><div><li>org.apache.spark.mllib.clustering.JavaStreamingKMeansSuite.javaAPI</li></div><div><li>org.apache.spark.mllib.clustering.JavaStreamingKMeansSuite.javaAPI</li></div><div><li>org.apache.spark.mllib.evaluation.JavaRankingMetricsSuite.rankingMetrics</li></div><div><li>org.apache.spark.mllib.evaluation.JavaRankingMetricsSuite.rankingMetrics</li></div><div><li>org.apache.spark.mllib.feature.JavaTfIdfSuite.tfIdfMinimumDocumentFrequency</li></div><div><li>org.apache.spark.mllib.feature.JavaTfIdfSuite.tfIdfMinimumDocumentFrequency</li></div><div><li>org.apache.spark.mllib.feature.JavaTfIdfSuite.tfIdf</li></div><div><li>org.apache.spark.mllib.feature.JavaTfIdfSuite.tfIdf</li></div><div><li>org.apache.spark.mllib.feature.JavaWord2VecSuite.word2Vec</li></div><div><li>org.apache.spark.mllib.feature.JavaWord2VecSuite.word2Vec</li></div><div><li>org.apache.spark.mllib.fpm.JavaAssociationRulesSuite.runAssociationRules</li></div><div><li>org.apache.spark.mllib.fpm.JavaAssociationRulesSuite.runAssociationRules</li></div><div><li>org.apache.spark.mllib.fpm.JavaFPGrowthSuite.runFPGrowthSaveLoad</li></div><div><li>org.apache.spark.mllib.fpm.JavaFPGrowthSuite.runFPGrowthSaveLoad</li></div><div><li>org.apache.spark.mllib.fpm.JavaFPGrowthSuite.runFPGrowth</li></div><div><li>org.apache.spark.mllib.fpm.JavaFPGrowthSuite.runFPGrowth</li></div><div><li>org.apache.spark.mllib.fpm.JavaPrefixSpanSuite.runPrefixSpan</li></div><div><li>org.apache.spark.mllib.fpm.JavaPrefixSpanSuite.runPrefixSpanSaveLoad</li></div><div><li>org.apache.spark.mllib.fpm.JavaPrefixSpanSuite.runPrefixSpanSaveLoad</li></div><div><li>org.apache.spark.mllib.linalg.distributed.JavaRowMatrixSuite.rowMatrixQRDecomposition</li></div><div><li>org.apache.spark.mllib.linalg.distributed.JavaRowMatrixSuite.rowMatrixQRDecomposition</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testNormalVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testNormalVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testArbitrary</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testArbitrary</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testLogNormalVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testLogNormalVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testExponentialVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testExponentialVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testUniformRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testUniformRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testRandomVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testRandomVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testGammaRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testGammaRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testUniformVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testUniformVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testPoissonRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testPoissonRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testNormalRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testNormalRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testPoissonVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testPoissonVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testGammaVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testGammaVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testExponentialRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testExponentialRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testLNormalRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testLNormalRDD</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runALSUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runALSUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSUsingConstructor</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSUsingConstructor</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runRecommend</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runRecommend</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSWithNegativeWeight</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSWithNegativeWeight</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runALSUsingConstructor</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runALSUsingConstructor</li></div><div><li>org.apache.spark.mllib.regression.JavaIsotonicRegressionSuite.testIsotonicRegressionJavaRDD</li></div><div><li>org.apache.spark.mllib.regression.JavaIsotonicRegressionSuite.testIsotonicRegressionJavaRDD</li></div><div><li>org.apache.spark.mllib.regression.JavaIsotonicRegressionSuite.testIsotonicRegressionPredictionsJavaRDD</li></div><div><li>org.apache.spark.mllib.regression.JavaLassoSuite.runLassoUsingConstructor</li></div><div><li>org.apache.spark.mllib.regression.JavaLassoSuite.runLassoUsingConstructor</li></div><div><li>org.apache.spark.mllib.regression.JavaLassoSuite.runLassoUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.regression.JavaLassoSuite.runLassoUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.testPredictJavaRDD</li></div><div><li>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.runLinearRegressionUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.runLinearRegressionUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.runLinearRegressionUsingConstructor</li></div><div><li>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.runLinearRegressionUsingConstructor</li></div><div><li>org.apache.spark.mllib.regression.JavaRidgeRegressionSuite.runRidgeRegressionUsingConstructor</li></div><div><li>org.apache.spark.mllib.regression.JavaRidgeRegressionSuite.runRidgeRegressionUsingConstructor</li></div><div><li>org.apache.spark.mllib.regression.JavaRidgeRegressionSuite.runRidgeRegressionUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.regression.JavaRidgeRegressionSuite.runRidgeRegressionUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.regression.JavaStreamingLinearRegressionSuite.javaAPI</li></div><div><li>org.apache.spark.mllib.regression.JavaStreamingLinearRegressionSuite.javaAPI</li></div><div><li>org.apache.spark.mllib.stat.JavaStatisticsSuite.testCorr</li></div><div><li>org.apache.spark.mllib.stat.JavaStatisticsSuite.testCorr</li></div><div><li>org.apache.spark.mllib.stat.JavaStatisticsSuite.chiSqTest</li></div><div><li>org.apache.spark.mllib.stat.JavaStatisticsSuite.chiSqTest</li></div><div><li>org.apache.spark.mllib.stat.JavaStatisticsSuite.streamingTest</li></div><div><li>org.apache.spark.mllib.stat.JavaStatisticsSuite.streamingTest</li></div><div><li>org.apache.spark.mllib.stat.JavaStatisticsSuite.kolmogorovSmirnovTest</li></div><div><li>org.apache.spark.mllib.stat.JavaStatisticsSuite.kolmogorovSmirnovTest</li></div><div><li>org.apache.spark.mllib.tree.JavaDecisionTreeSuite.runDTUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.tree.JavaDecisionTreeSuite.runDTUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.tree.JavaDecisionTreeSuite.runDTUsingConstructor</li></div><div><li>org.apache.spark.mllib.tree.JavaDecisionTreeSuite.runDTUsingConstructor</li></div><div><li>org.apache.spark.mllib.util.JavaMLUtilsSuite.testConvertMatrixColumnsToAndFromML</li></div><div><li>org.apache.spark.mllib.util.JavaMLUtilsSuite.testConvertMatrixColumnsToAndFromML</li></div><div><li>org.apache.spark.mllib.util.JavaMLUtilsSuite.testConvertVectorColumnsToAndFromML</li></div><div><li>org.apache.spark.mllib.util.JavaMLUtilsSuite.testConvertVectorColumnsToAndFromML</li></div><div><li>org.apache.spark.repl.ReplSuite.SPARK-15236: use Hive catalog</li></div><div><li>org.apache.spark.repl.ReplSuite.SPARK-15236: use in-memory catalog</li></div><div><li>org.apache.spark.repl.ReplSuite.broadcast vars</li></div><div><li>org.apache.spark.repl.ReplSuite.line wrapper only initialized once when used as encoder outer scope</li></div><div><li>org.apache.spark.repl.ReplSuite.define case class and create Dataset together with paste mode</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.simple foreach with accumulator</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.external vars</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.external classes</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.external functions</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.external functions that access vars</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.broadcast vars</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.interacting with files</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.local-cluster mode</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.SPARK-2576 importing implicits</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.Datasets and encoders</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.SPARK-2632 importing a method from non serializable class and not using it.</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.collecting objects of class defined in repl</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.collecting objects of class defined in repl - shuffling</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.replicating blocks of object with class defined in repl</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.newProductSeqEncoder with REPL defined class</li></div><div><li>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationCount</li></div><div><li>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationCount</li></div><div><li>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationSumDouble</li></div><div><li>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationSumDouble</li></div><div><li>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationSumLong</li></div><div><li>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationSumLong</li></div><div><li>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationAverage</li></div><div><li>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationAverage</li></div><div><li>test.org.apache.spark.sql.JavaApplySchemaSuite.applySchema</li></div><div><li>test.org.apache.spark.sql.JavaApplySchemaSuite.applySchema</li></div><div><li>test.org.apache.spark.sql.JavaApplySchemaSuite.dataFrameRDDOperations</li></div><div><li>test.org.apache.spark.sql.JavaApplySchemaSuite.dataFrameRDDOperations</li></div><div><li>test.org.apache.spark.sql.JavaApplySchemaSuite.applySchemaToJSON</li></div><div><li>test.org.apache.spark.sql.JavaApplySchemaSuite.applySchemaToJSON</li></div><div><li>test.org.apache.spark.sql.JavaBeanDeserializationSuite.testBeanWithArrayFieldDeserialization</li></div><div><li>test.org.apache.spark.sql.JavaBeanDeserializationSuite.testBeanWithArrayFieldDeserialization</li></div><div><li>test.org.apache.spark.sql.JavaBeanDeserializationSuite.testBeanWithMapFieldsDeserialization</li></div><div><li>test.org.apache.spark.sql.JavaBeanDeserializationSuite.testBeanWithMapFieldsDeserialization</li></div><div><li>test.org.apache.spark.sql.JavaColumnExpressionSuite.isInCollectionCheckExceptionMessage</li></div><div><li>test.org.apache.spark.sql.JavaColumnExpressionSuite.isInCollectionCheckExceptionMessage</li></div><div><li>test.org.apache.spark.sql.JavaColumnExpressionSuite.isInCollectionWorksCorrectlyOnJava</li></div><div><li>test.org.apache.spark.sql.JavaColumnExpressionSuite.isInCollectionWorksCorrectlyOnJava</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testFormatAPI</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testTextAPI</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testJsonAPI</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testLoadAPI</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testOptionsAPI</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testSaveModeAPI</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testCsvAPI</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testParquetAPI</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testTextFileAPI</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCollectAndTake</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCollectAndTake</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testJsonRDDToDataFrame</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testJsonRDDToDataFrame</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testVarargMethods</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testVarargMethods</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testBeanWithoutGetter</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testBeanWithoutGetter</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateStructTypeFromList</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateStructTypeFromList</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testSampleBy</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testSampleBy</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCrosstab</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCrosstab</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testUDF</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testUDF</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFromFromList</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFromFromList</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCircularReferenceBean</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCircularReferenceBean</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testFrequentItems</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testFrequentItems</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testSampleByColumn</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testSampleByColumn</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testExecution</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testExecution</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testTextLoad</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testTextLoad</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.pivot</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.pivot</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testGenericLoad</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testGenericLoad</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCountMinSketch</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCountMinSketch</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.pivotColumnValues</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.pivotColumnValues</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFrameFromJavaBeans</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFrameFromJavaBeans</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCorrelation</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCorrelation</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testBloomFilter</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testBloomFilter</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCovariance</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCovariance</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFrameFromLocalJavaBeans</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFrameFromLocalJavaBeans</li></div><div><li>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationCount</li></div><div><li>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationCount</li></div><div><li>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationSumDouble</li></div><div><li>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationSumDouble</li></div><div><li>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationSumLong</li></div><div><li>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationSumLong</li></div><div><li>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationAnonClass</li></div><div><li>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationAnonClass</li></div><div><li>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationAverage</li></div><div><li>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationAverage</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testRuntimeNullabilityCheck</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testRuntimeNullabilityCheck</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean1</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean1</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean2</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean2</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean3</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean3</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testSerializeNull</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testSerializeNull</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testRandomSplit</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testRandomSplit</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testTypedFilterPreservingSchema</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testTypedFilterPreservingSchema</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJoin</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJoin</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testTake</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testTake</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testToLocalIterator</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testToLocalIterator</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testSpecificLists</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testSpecificLists</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testForeach</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testForeach</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJavaEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJavaEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testPrimitiveEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testPrimitiveEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testEmptyBean</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testEmptyBean</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testCommonOperation</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testCommonOperation</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testNullInTopLevelBean</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testNullInTopLevelBean</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testGroupBy</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testGroupBy</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testSetOperation</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testSetOperation</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testBeanWithEnum</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testBeanWithEnum</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testKryoEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testKryoEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.test</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.test</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJavaBeanEncoder2</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJavaBeanEncoder2</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testCollect</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testCollect</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testKryoEncoderErrorMessageForPrivateClass</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testKryoEncoderErrorMessageForPrivateClass</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJavaBeanEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJavaBeanEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testTupleEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testTupleEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testNestedTupleEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testNestedTupleEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testTupleEncoderSchema</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testTupleEncoderSchema</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testReduce</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testReduce</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testSelect</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testSelect</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJavaEncoderErrorMessageForPrivateClass</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJavaEncoderErrorMessageForPrivateClass</li></div><div><li>test.org.apache.spark.sql.JavaSaveLoadSuite.saveAndLoadWithSchema</li></div><div><li>test.org.apache.spark.sql.JavaSaveLoadSuite.saveAndLoadWithSchema</li></div><div><li>test.org.apache.spark.sql.JavaSaveLoadSuite.saveAndLoad</li></div><div><li>test.org.apache.spark.sql.JavaSaveLoadSuite.saveAndLoad</li></div><div><li>test.org.apache.spark.sql.JavaUDAFSuite.udf1Test</li></div><div><li>test.org.apache.spark.sql.JavaUDAFSuite.udf1Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf1Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf1Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf2Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf2Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf3Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf3Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf4Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf4Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf5Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf5Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf6Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf6Test</li></div><div><li>test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.testForeachBatchAPI</li></div><div><li>test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.testForeachBatchAPI</li></div><div><li>test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.testForeachAPI</li></div><div><li>test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.testForeachAPI</li></div><div><li>org.apache.spark.sql.hive.HiveMetastoreLazyInitializationSuite.lazily initialize Hive client</li></div><div><li>org.apache.spark.sql.hive.JavaDataFrameSuite.testUDAF</li></div><div><li>org.apache.spark.sql.hive.JavaDataFrameSuite.saveTableAndQueryIt</li></div><div><li>org.apache.spark.sql.hive.JavaMetastoreDataSourcesSuite.saveExternalTableAndQueryIt</li></div><div><li>org.apache.spark.sql.hive.JavaMetastoreDataSourcesSuite.saveTableAndQueryIt</li></div><div><li>org.apache.spark.sql.hive.JavaMetastoreDataSourcesSuite.saveExternalTableWithSchemaAndQueryIt</li></div><div><li>org.apache.spark.streaming.JavaMapWithStateSuite.testBasicFunction</li></div><div><li>org.apache.spark.streaming.JavaReceiverAPISuite.testReceiver</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testStreamingContextTransform</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testFlatMapValues</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testMapPartitions</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testMapPartitions</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairFilter</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairFilter</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testCombineByKey</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testCombineByKey</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testMap</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testMap</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairToNormalRDDTransform</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairToNormalRDDTransform</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairReduceByKey</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairReduceByKey</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairMap</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairMap</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testFlatMap</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testFlatMap</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testReduceByKeyAndWindowWithInverse</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testReduceByKeyAndWindowWithInverse</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testReduceByWindow</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testReduceByWindow</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairFlatMap</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairFlatMap</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairToPairFlatMapWithChangingTypes</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairToPairFlatMapWithChangingTypes</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairMapPartitions</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairMapPartitions</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testVariousTransform</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testVariousTransform</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testTransformWith</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testTransformWith</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testVariousTransformWith</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testVariousTransformWith</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testReduceByKeyAndWindow</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testReduceByKeyAndWindow</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairTransform</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairTransform</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testFilter</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testFilter</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairMap2</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairMap2</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testMapValues</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testMapValues</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testReduce</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testReduce</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testUpdateStateByKey</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testUpdateStateByKey</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testTransform</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testTransform</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testStreamingContextTransform</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testFlatMapValues</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testReduceByWindowWithInverse</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testMapPartitions</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairFilter</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairFilter</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testRepartitionFewerPartitions</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testRepartitionFewerPartitions</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCombineByKey</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCombineByKey</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testContextGetOrCreate</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testContextGetOrCreate</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testWindowWithSlideDuration</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testWindowWithSlideDuration</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testQueueStream</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testQueueStream</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCountByValue</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCountByValue</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testMap</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testMap</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairToNormalRDDTransform</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairToNormalRDDTransform</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairReduceByKey</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairReduceByKey</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCount</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCount</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCheckpointMasterRecovery</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairMap</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairMap</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testUnion</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testUnion</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testFlatMap</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testFlatMap</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testReduceByKeyAndWindowWithInverse</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testReduceByKeyAndWindowWithInverse</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testGlom</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testGlom</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testJoin</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testJoin</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairFlatMap</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairFlatMap</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairToPairFlatMapWithChangingTypes</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairToPairFlatMapWithChangingTypes</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairMapPartitions</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairMapPartitions</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testRepartitionMorePartitions</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testRepartitionMorePartitions</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testReduceByWindowWithoutInverse</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testReduceByWindowWithoutInverse</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testLeftOuterJoin</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testLeftOuterJoin</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testTransformWith</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testTextFileStream</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairGroupByKey</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCoGroup</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testInitialization</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testInitialization</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testGroupByKeyAndWindow</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testGroupByKeyAndWindow</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testReduceByKeyAndWindow</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testReduceByKeyAndWindow</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testForeachRDD</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testForeachRDD</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testFileStream</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testFileStream</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairTransform</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairTransform</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testFilter</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testFilter</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairMap2</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testMapValues</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testReduce</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testReduce</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testUpdateStateByKey</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testUpdateStateByKey</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testTransform</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testTransform</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testWindow</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testWindow</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCountByValueAndWindow</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCountByValueAndWindow</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testRawSocketStream</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testRawSocketStream</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testSocketTextStream</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testSocketTextStream</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testUpdateStateByKeyWithInitial</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testUpdateStateByKeyWithInitial</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testContextState</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testContextState</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"><div><li>org/apache/zookeeper/AsyncCallback$MultiCallback</li></div><div><li>org/apache/zookeeper/AsyncCallback$MultiCallback</li></div><div><li>java.util.NoSuchElementException was thrown.</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Timeout getting response from the server</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;2&gt; but was:&lt;0&gt;</li></div><div><li>SparkContext has been shutdown</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMe</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMe</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:44)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:44)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:44)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:44)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:44)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:44)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:44)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:44)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:44)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:44)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:44)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:44)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:44)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:44)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:44)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAcce</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Metho</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Metho</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Metho</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:44)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:44)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>Exception evaluating from_avro(to_avro(false), "boolean")</li></div><div><li>Exception evaluating from_avro(to_avro(-38), "int")</li></div><div><li>Exception evaluating from_avro(to_avro(14381), "int")</li></div><div><li>Exception evaluating from_avro(to_avro(2147483647), "int")</li></div><div><li>Exception evaluating from_avro(to_avro(-9223372036854775808), "long")</li></div><div><li>Exception evaluating from_avro(to_avro(0.0), "float")</li></div><div><li>Exception evaluating from_avro(to_avro(-1.7976931348623157E308), "double")</li></div><div><li>Exception evaluating from_avro(to_avro(96676580), {"type":"fixed","name":"fixed","namespace":"topLevelRecord","size":4,"logicalType":"decimal","precision":8,"scale":0})</li></div><div><li>Exception evaluating from_avro(to_avro(5293.0197), {"type":"fixed","name":"fixed","namespace":"topLevelRecord","size":4,"logicalType":"decimal","precision":8,"scale":4})</li></div><div><li>Exception evaluating from_avro(to_avro(8332264159836074), {"type":"fixed","name":"fixed","namespace":"topLevelRecord","size":7,"logicalType":"decimal","precision":16,"scale":0})</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>Exception evaluating from_avro(to_avro(-2205146533008348483), {"type":"fixed","name":"fixed","namespace":"topLevelRecord","size":16,"logicalType":"decimal","precision":38,"scale":0})</li></div><div><li>Exception evaluating from_avro(to_avro(-6.183507458775660419E-20), {"type":"fixed","name":"fixed","namespace":"topLevelRecord","size":16,"logicalType":"decimal","precision":38,"scale":38})</li></div><div><li>Exception evaluating from_avro(to_avro(怘껲叶䁌箆䶑ឯ⵹͍誨娊넂祋࿇칤連㑃킉饁沒뉗Ӏ뉵牽ꖠ鋄鶧⶛检⻬⩡迡㧊苎ꔽ᝸灔㘱剩㹵뽈뷰쪆᪎繤၊ᢐ₩厦⋞锌越㕁ㄱꤏ薧蕅忚౏騴뚭쌱屶鑹ឝߪ╴ം枃ʄ鲼鐇匭筯믅吀쫽쮇䠑伌ۭݍᾗ´વ蒀⨵浅銒싍叡ꀃ篾푅抪絔ㅕ㇢⢝㙴⁛઩쎽᳖㫺ꏤ姐᜙⊛➎棷┆붠襦㓰礚误ᮬ䲎⍏氒膭ዽ⨵ꋛ엻쉰븠Ḉ絅䌯Ჽ떍鐩ꎻ䵸姺官㜧⴨ዲ쎏鐢ሔ皎칔ᵝᘂ갖౗뼛틆꺿᜜幨臂ី靥縎酧⃮ᵼ椩欆歇욿Ȑ඘Ȗŕꕱ℅찷폴蠌覠㒟奉ß蒵Ᏽ孒梲죾觌ဒ▝썄싇쒄텄┺䡅囓੾묮旝䵐䢽쵣㤘璼ࠅ枃蒵ꇵ秙멡㠗♲⊾䷢ա鬸谣埂齱ዓ鉓虾覀똵ཪ糰콻쪨䈢㫛嫕鳇묰끌ᗱ⪖ꥌ蜄푎橖轗ꎠჄ슣諲嬿⺤褷軓羆艿֒䔌赢蔜抮趭唼䝂旨냿ꃋ㌣甋䎨秡娝皳ꮉ穖렛防㾼♛嫃俑茮页뫲់鈹铓黤␻乡鵧坝璂ᭉꛍ✜⹗ꑰꇯත㮶멖嘳唧吔狹颉엯緢镘濣鯶㶘ᮎ鳪뷑늩톏蕩䊀䃘퍇倄曻죌ኔ排⑃뒫킔魽찬넰锟똍铟筃鰴쳙섚뒴컶妁벪ª쬪껂ꊧ㋲쟊᷾枸쟣sⳫࣘ☮䮓샄峼囦攉ꄲ뤁䥛ꡁ䖝敇</li></div><div><li>Exception evaluating from_avro(to_avro(0xC380FEC6C29A78F8648C05197A224793A61BE58E939B5BD4DCFEE927CC60ABEAAA48AF7DA8ECF1F937FFFD56FDADDA828A9A74DC7540C3D60796855CA23CC51722D14BFECEACE96FC87B8930B3CEBFED0DE5EE22BDF78F37204714188180D13090A921A0955D47D968BB1DA8E7CE1E75B628C6864624ABF76AFBFD23C04C4232BB1D98B9F38F59D66CA49A5898BB76FBAD313B87C10CAFDB6D151C68A57B7DC450DEDDD5F2A1295FAE66), "bytes")</li></div><div><li>Exception evaluating from_avro(to_avro([1895.7694,-8380742785563524233,1.9854111045975293E89,[B@1693ff90,-6.804757579553782637E-20]), {"type":"record","name":"topLevelRecord","fields":[{"name":"col_0","type":[{"type":"fixed","name":"fixed","namespace":"topLevelRecord.col_0","size":4,"logicalType":"decimal","precision":8,"scale":4},"null"]},{"name":"col_1","type":"long"},{"name":"col_2","type":["do</li></div><div><li>Exception evaluating from_avro(to_avro([97445.61423152481,-6116399460857559,null,[B@570b2057,-94492077]), {"type":"record","name":"topLevelRecord","fields":[{"name":"col_0","type":{"type":"fixed","name":"fixed","namespace":"topLevelRecord.col_0","size":7,"logicalType":"decimal","precision":16,"scale":11}},{"name":"col_1","type":[{"type":"fixed","name":"fixed","namespace":"topLevelRecord.col_1","si</li></div><div><li>Exception evaluating from_avro(to_avro([-88724.59739522541,66670111,嶀裁臛꣐讕Ꝺ솂何廰뛍즸衋䋷풂㙁ב擁廇䌿漖亓堍ꩴ忇襊告׶获坜뵣鈒櫾䀤ㆣ輴㡤㯧꼺㑤ṥ還价믔꽈͚ሣ℥ヒ珍駱另撍꿷꧌봸籒ή꺧ږㇵ䇽㎝礨臈咐㔃ူ⥒꡾찒潧㦿鉲圸ⲝᦨ뗴Ⲹ軼ⴆ洨竁똌堖錀謏曊⫣㓼롾ᆷ퉱疛喬뎨ੂ㙊햬ܟ噩쌍魴㔆ꀁ彞⏙豅첩឴臚ῂὧ燳㜒迁貒蜍㗠싙ẃ훠ᙁ鞅什蓶鲟놗ꨚ뼲ꯟ郪ꭜ窑ᩥ⁥韽⽰巾亮멦Ṫ䈻숓貸띩櫿䞮↵鸿칖㊟ꤷ憙躐戲瑃䡙Ǎ钻ꕢ垗飑뮙՜秼㦲‒곷䰛ᜊ㠭쳴迃䉾騕兏㊦밧Ї㞷㯔텒Ꚃꏵ弞㯺꡽栆꩓瀽䗡᷻蘞闳䙟쓏쌋᛭犚ꂁ幾⫵蔸䢗植⮳⬡鎂ƋḞᬪ联ാ࿋⟟펂饂掸魏塖჋ᾤ휱鑼≹奈遯珽ັ嚒鯟泳ի尌礒創黯脔㭬厇뢆ꗑ㟈뿥៕⎝赾ⱴ肟⻌ ⏎躡뚼볮늸쐊峬吰袼웈屮餕䢟䠀ꛜ昝থ组㕧痙灖沏踌䎚팔섗ᕥ校р嚊㸻嘮齆옟䀀姂㜇ռ讎怀柴,-1583341008,-4562704024088944]), {"typ</li></div><div><li>Exception evaluating from_avro(to_avro([[B@47f0f414,-81972036,false,-6.305640986383761101E-20,9223372036854775807]), {"type":"record","name":"topLevelRecord","fields":[{"name":"col_0","type":["bytes","null"]},{"name":"col_1","type":[{"type":"fixed","name":"fixed","namespace":"topLevelRecord.col_1","size":4,"logicalType":"decimal","precision":8,"scale":0},"null"]},{"name":"col_2","type":"boolean"},</li></div><div><li>Exception evaluating from_avro(to_avro([-2452529266533136879,null,true,4716.8673,4350299279088611442]), {"type":"record","name":"topLevelRecord","fields":[{"name":"col_0","type":{"type":"fixed","name":"fixed","namespace":"topLevelRecord.col_0","size":16,"logicalType":"decimal","precision":38,"scale":0}},{"name":"col_1","type":[{"type":"fixed","name":"fixed","namespace":"topLevelRecord.col_1","size</li></div><div><li>Exception evaluating from_avro(to_avro([-72,5822724140865990638,[B@42066f0d,[[[[-16582]],-3771699072918538079,[[[9223372036854775807]]],5207.2360],127,[0]],[-1.24902445E-23]]), {"type":"record","name":"topLevelRecord","fields":[{"name":"col_0","type":["int","null"]},{"name":"col_1","type":{"type":"fixed","name":"fixed","namespace":"topLevelRecord.col_1","size":16,"logicalType":"decimal","precision</li></div><div><li>Exception evaluating from_avro(to_avro([[[[17981906]],false,[877140200413749566],[[2318.3064]],[null]],4.9E-324,[-90674.70506463282],[4090170720392034],[[9223372036854775807]],[[-5074343270575173813]]]), {"type":"record","name":"topLevelRecord","fields":[{"name":"col_0","type":{"type":"record","name":"col_0","namespace":"topLevelRecord","fields":[{"name":"col_0","type":[{"type":"record","name":"co</li></div><div><li>Exception evaluating from_avro(to_avro([-7226120902285881,[[5.521782425339991913E-20]],[-18213033],[-4.726809941963634040E-20],[166521111406313822],[0.0],[[[B@44a485bc]],-8.291001626993592069E-20,[[끫彝燢垟줄⪬ḍ䄢餻莪ᜣ埓軁ᰄ斵䕏賖誱虪铙름ꏣ迂ꎞ糜⦈ఙ삮ᑘ⧐渽膫軎觸റ댧㍢숕⯔賍⃸쓸ʏ玊윻超ᤴ똼꫔Èഉ貃Ἔḃ⫀첂櫣執䖻ᰝ컋㛍ဦ旚⁼邗锬ꂛ犙Ⳅ턮覝∟䧿蕵켆呄醰庺⻆㒺໣篒歿䍄᥅扂ꑥ븰蝄켋ꀟ嚺⩴ኝꪶ趄ꀽ竤起딷퀆쎈春邁໲癓襤彥뺥侵Ḧꑨ쌡ϋ쩖絣Ꞧ棺㚗꾉㢌똔⨸面䨖豤꽃⦢⿾옓恀족渆屐䄳橿뚗榍〱룄の@龵ᄐ잃덩睡䘝緛㉞蛠횝◼ᾳ尙붊鎙㼁롩笽ꁐ㈿雿왕팡ᯈ텄䁴䣫抇鬘ᮭ⼔촌惀ⷩȬಛ᜖샄原$뢱⾻篓࿒䩖翰ፍ埥잾曜ࡢꂏ甭撗齪ḿ䟛꓊엚</li></div><div><li>Exception evaluating from_avro(to_avro([[[6.188674441993588694E-20,㎚䇏⍑譖ݥ䯮쩛뾒乞곂颪融ꣃ媒ń棐❺㶻캬捼륳襱쟳됢ꢶᙤሥ맋挜崽퍫㖬㰽敵㪹ꥧꙄᤸꐡ䥣쉯亐겥쪇鄥䑶旓⯺䐗袮붰ꯦ៓ᦀ쁦᪨끅庐쉖뤒쌮䵼帷ꔰ㔊捴暙㺸ꇷ䗏쭉쀖毠ꮶ䣨첨틒ǃꭤⅬ춭绫饈ͦ鍈빢͠獠㥚덍蒂窑튕ᅠ敧꿊膘襄ၜ䞆槜滷☆䟽쨚朽硗흞Ẇ薳匸⤞蹑禇驵윂挢遧ᔓ鞔꾗ϛᎡ㱬륛㠔믥衼鬨⁻䠒盂渫阏闤崧清줗⠩ꁴ읏欔兇᫱諁ᫌ떆퀋̗䏲㼔ㄎ줎ᅌ㳱蛉˗릏飙◭⿗싩璷ꎼ玵삧傼좒튉ᓔ愩礍츢匀멝沉㶾훏䒺떛퐋⧖ࢱ㘃媙྇欌ᾋꛙ慲с쑮Ⓜ▐ᣩ晐㿞蘱፥㣔㿁᝘䎶젳인毠쳪뼯䍣称꿮荩嵨㸸廹篢䝼頜躞᮫癴좱୐显궈ꃪ䉆♍⡶庒ꐠ᷆㿲⇾鳢睵晌˖㻌ꣴ鞺ꨆ襑伧幬颜봕渳ᛷ鋶䍁␤勂ᘢ倄칪啁쉤ⷖ외⤿賟ྮ酹쪺鑹䂬쯮ை풄죃ΰ뚉䛴焳㣖鑃宭腜錩㕣典ᤏ⫝̸ഺ㻰髒脕뜢䰊츽⧨嚵兿๛袥ᕐ訌吩⹽䇹憼狌蚳璓盈ꕫ봤䨯⾤뭉钜슦㶻嬂䜾⌺ᚏ혚귃ฅ͕⛳큵踽厲ꪘ㩯萱ᦝ摴錃㳻</li></div><div><li>Exception evaluating from_avro(to_avro([-3272.9508,[-2971829.0],-95599459,[-32768],[5100],[[[2782052963060036]]],[[[-61]]],-4.804784178520982E46,false,[-2688.6318]]), {"type":"record","name":"topLevelRecord","fields":[{"name":"col_0","type":{"type":"fixed","name":"fixed","namespace":"topLevelRecord.col_0","size":4,"logicalType":"decimal","precision":8,"scale":4}},{"name":"col_1","type":{"type":"ar</li></div><div><li>Exception evaluating from_avro(to_avro(1), &amp;#010;{&amp;#010;  "type": "string",&amp;#010;  "name": "my_string"&amp;#010;}&amp;#010;       , (mode,PERMISSIVE))</li></div><div><li>Exception evaluating from_avro(to_avro(abc), &amp;#010;{&amp;#010;  "type": "int",&amp;#010;  "name": "my_int"&amp;#010;}&amp;#010;       )</li></div><div><li>Exception evaluating from_avro(to_avro(1.23), &amp;#010;{&amp;#010;  "type": "double",&amp;#010;  "name": "my_double"&amp;#010;}&amp;#010;       , (mode,PERMISSIVE))</li></div><div><li>Exception evaluating from_avro(to_avro(1.23), &amp;#010;{&amp;#010;  "type": "float",&amp;#010;  "name": "my_float"&amp;#010;}&amp;#010;       )</li></div><div><li>create random record with seed 1526456083416939841 Exception evaluating from_avro(to_avro([㪌ҵᖿ䦪琍戶鵶吼✚爴儂ₙἦẋꀢሉ遴ᡨ伈쓪粿☜ꄋ괸絔滟紟ᦖ鬹㷙睆퇃̷䈹崨뮃䪣翄퇏炽牲䈲閄扞ꇟ䐕眺√硠唏䗈ᚄ䪼儛냻勪옞晍鯝锧䵑丳♤ᗅ蘍있漵䕓⬔⩿彊쨌醿䉡脮凕誂뙡鰷掷䍜Ꚉᆒ멹챾∧ꃩ풔䶺趝᳘飘ꇎ鮘䵭藤礜Ꭴ㚃䲲倎ꧨ⪷쁻櫅펠躕ኒϽ磁蟍磕㈤밳Ռ㖷攬漂쑰瞫៧ấ꒟ᰴ呄豖죆찣轮殺릩ܚ㌋媪鼡젺娩㡳剬扠뼺ᙽʽ爇梔⚖咳誃䙖噗䝙忧橻ᥤ좛⁓泃忔钊鬄ꭞ뜩걂旤↞ᛋ鼭䓖›⼐Ɇ楉ꢛ憶䬽㉰⤤诫㲀훥辧츚賥輾⁬鈷븙퀱眣⏷炾慿럿밎찆ٸ큱丰않鮐隫렳ꮮ䢹빜圁ṹ鴳͖냧㓟誻ꉽꦺá削妌磏⦗鷕笤䳧貙ᚫ媮詭ꔣ멣ᛐ梞Ƅ菪㟸턪珉䁬횦侻烉뵲⇿罻᭵㠝ꯈ罂ᓵ擪퉜浴霮젼届鎦鹋龈誜눝俬쌰ꅞ쬤ᙇ挮⨁ꢲ㖇䩘뙹뮸狺㝨휝並㻪졿ཞ㷍썍瀩봩ᰢ㖔貣炕␻穖㭀伈쪙ꫧ榊먕梳뀅钔῵碴砒喎迓䉧뢨錈慨ꙻ㶝뽪밬</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.sql.avro.AvroFunctionsSuite.beforeAll(AvroFunctionsSuite.scala:27)&amp;#010;org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:212)&amp;#010;org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)&amp;#010;org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.s</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.sql.avro.AvroFunctionsSuite.beforeAll(AvroFunctionsSuite.scala:27)&amp;#010;org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:212)&amp;#010;org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)&amp;#010;org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.s</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>org/apache/zookeeper/AsyncCallback$MultiCallback</li></div><div><li>org/apache/zookeeper/AsyncCallback$MultiCallback</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(LocalSparkContext.scala:29)&amp;#010;org.apache.spark.graphx.GraphLoaderSuite.withSpark(GraphLo</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(LocalSparkContext.scala:29)&amp;#010;org.apache.spark.graphx.GraphOpsSuite.withSpark(GraphOpsSu</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>SparkContext has been shutdown</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(LocalSparkContext.scala:29)&amp;#010;org.apache.spark.graphx.GraphOpsSuite.withSpark(GraphOpsSu</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>SparkContext has been shutdown</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(LocalSparkContext.scala:29)&amp;#010;org.apache.spark.graphx.PregelSuite.withSpark(PregelSuite.</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(LocalSparkContext.scala:29)&amp;#010;org.apache.spark.graphx.PregelSuite.withSpark(PregelSuite.</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(LocalSparkContext.scala:29)&amp;#010;org.apache.spark.graphx.VertexRDDSuite.withSpark(VertexRDD</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(LocalSparkContext.scala:29)&amp;#010;org.apache.spark.graphx.lib.ConnectedComponentsSuite.withS</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(LocalSparkContext.scala:29)&amp;#010;org.apache.spark.graphx.lib.LabelPropagationSuite.withSpar</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(LocalSparkContext.scala:29)&amp;#010;org.apache.spark.graphx.lib.PageRankSuite.withSpark(PageRa</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(LocalSparkContext.scala:29)&amp;#010;org.apache.spark.graphx.lib.SVDPlusPlusSuite.withSpark(SVD</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(LocalSparkContext.scala:29)&amp;#010;org.apache.spark.graphx.lib.ShortestPathsSuite.withSpark(S</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>SparkContext has been shutdown</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.JavaPipelineSuite.setUp(JavaPipelineSuite.java:42)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Metho</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.JavaPipelineSuite.setUp(JavaPipelineSuite.java:42)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Metho</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.JavaPipelineSuite.setUp(JavaPipelineSuite.java:42)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Metho</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.JavaPipelineSuite.setUp(JavaPipelineSuite.java:42)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Metho</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.JavaPipelineSuite.setUp(JavaPipelineSuite.java:42)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Metho</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.JavaPipelineSuite.setUp(JavaPipelineSuite.java:42)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Metho</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.JavaPipelineSuite.setUp(JavaPipelineSuite.java:42)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Metho</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.JavaPipelineSuite.setUp(JavaPipelineSuite.java:42)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Metho</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.JavaPipelineSuite.setUp(JavaPipelineSuite.java:42)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Metho</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.JavaPipelineSuite.setUp(JavaPipelineSuite.java:42)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Metho</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.JavaPipelineSuite.setUp(JavaPipelineSuite.java:42)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Metho</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.JavaPipelineSuite.setUp(JavaPipelineSuite.java:42)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Metho</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.tuning.JavaCrossValidatorSuite.setUp(JavaCrossValidatorSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.tuning.JavaCrossValidatorSuite.setUp(JavaCrossValidatorSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.tuning.JavaCrossValidatorSuite.setUp(JavaCrossValidatorSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.tuning.JavaCrossValidatorSuite.setUp(JavaCrossValidatorSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.tuning.JavaCrossValidatorSuite.setUp(JavaCrossValidatorSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.tuning.JavaCrossValidatorSuite.setUp(JavaCrossValidatorSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.tuning.JavaCrossValidatorSuite.setUp(JavaCrossValidatorSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.JavaPipelineSuite.setUp(JavaPipelineSuite.java:42)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Metho</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.JavaPipelineSuite.setUp(JavaPipelineSuite.java:42)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Metho</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.JavaPipelineSuite.setUp(JavaPipelineSuite.java:42)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Metho</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.JavaPipelineSuite.setUp(JavaPipelineSuite.java:42)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Metho</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.JavaPipelineSuite.setUp(JavaPipelineSuite.java:42)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Metho</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.JavaPipelineSuite.setUp(JavaPipelineSuite.java:42)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Metho</li></div><div><li>java.lang.NullPointerException
	at org.apache.spark.ml.source.libsvm.JavaLibSVMRelationSuite.tearDown(JavaLibSVMRelationSuite.java:57)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.stat.JavaSummarizerSuite.setUp(JavaSummarizerSuite.java:42)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.refl</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.stat.JavaSummarizerSuite.setUp(JavaSummarizerSuite.java:42)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.refl</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.stat.JavaSummarizerSuite.setUp(JavaSummarizerSuite.java:42)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.refl</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.tuning.JavaCrossValidatorSuite.setUp(JavaCrossValidatorSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
	at org.apache.spark.ml.util.JavaDefaultReadWriteSuite.tearDown(JavaDefaultReadWriteSuite.java:41)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:4</li></div><div><li>java.lang.NullPointerException
	at org.apache.spark.mllib.classification.JavaStreamingLogisticRegressionSuite.tearDown(JavaStreamingLogisticRegressionSuite.java:55)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.mllib.clustering.JavaLDASuite.setUp(JavaLDASuite.java:41)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.M</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.mllib.clustering.JavaLDASuite.setUp(JavaLDASuite.java:41)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.M</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.mllib.clustering.JavaLDASuite.setUp(JavaLDASuite.java:41)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.M</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.mllib.clustering.JavaLDASuite.setUp(JavaLDASuite.java:41)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.M</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.mllib.clustering.JavaLDASuite.setUp(JavaLDASuite.java:41)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.M</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.mllib.clustering.JavaLDASuite.setUp(JavaLDASuite.java:41)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.M</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.mllib.clustering.JavaLDASuite.setUp(JavaLDASuite.java:41)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.M</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.mllib.clustering.JavaLDASuite.setUp(JavaLDASuite.java:41)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.M</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.mllib.clustering.JavaLDASuite.setUp(JavaLDASuite.java:41)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.M</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.mllib.clustering.JavaLDASuite.setUp(JavaLDASuite.java:41)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at org.apache.spark.mllib.clustering.JavaStreamingKMeansSuite.tearDown(JavaStreamingKMeansSuite.java:54)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.stat.JavaSummarizerSuite.setUp(JavaSummarizerSuite.java:42)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.refl</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.mllib.clustering.JavaLDASuite.setUp(JavaLDASuite.java:41)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.M</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.mllib.clustering.JavaLDASuite.setUp(JavaLDASuite.java:41)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.M</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.mllib.clustering.JavaLDASuite.setUp(JavaLDASuite.java:41)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.M</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.mllib.clustering.JavaLDASuite.setUp(JavaLDASuite.java:41)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.M</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.mllib.clustering.JavaLDASuite.setUp(JavaLDASuite.java:41)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.M</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.mllib.clustering.JavaLDASuite.setUp(JavaLDASuite.java:41)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.M</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.mllib.clustering.JavaLDASuite.setUp(JavaLDASuite.java:41)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.M</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.mllib.clustering.JavaLDASuite.setUp(JavaLDASuite.java:41)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.M</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.mllib.clustering.JavaLDASuite.setUp(JavaLDASuite.java:41)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.M</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.mllib.clustering.JavaLDASuite.setUp(JavaLDASuite.java:41)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.M</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.mllib.clustering.JavaLDASuite.setUp(JavaLDASuite.java:41)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.M</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.mllib.clustering.JavaLDASuite.setUp(JavaLDASuite.java:41)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.M</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.mllib.clustering.JavaLDASuite.setUp(JavaLDASuite.java:41)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.M</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.mllib.clustering.JavaLDASuite.setUp(JavaLDASuite.java:41)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.M</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.mllib.clustering.JavaLDASuite.setUp(JavaLDASuite.java:41)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.M</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.mllib.clustering.JavaLDASuite.setUp(JavaLDASuite.java:41)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.M</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.mllib.clustering.JavaLDASuite.setUp(JavaLDASuite.java:41)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.M</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.mllib.clustering.JavaLDASuite.setUp(JavaLDASuite.java:41)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.M</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.stat.JavaSummarizerSuite.setUp(JavaSummarizerSuite.java:42)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.refl</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.stat.JavaSummarizerSuite.setUp(JavaSummarizerSuite.java:42)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.refl</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.stat.JavaSummarizerSuite.setUp(JavaSummarizerSuite.java:42)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.refl</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.stat.JavaSummarizerSuite.setUp(JavaSummarizerSuite.java:42)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.refl</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.stat.JavaSummarizerSuite.setUp(JavaSummarizerSuite.java:42)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.refl</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.stat.JavaSummarizerSuite.setUp(JavaSummarizerSuite.java:42)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.refl</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:4</li></div><div><li>java.lang.NullPointerException
	at org.apache.spark.mllib.regression.JavaStreamingLinearRegressionSuite.tearDown(JavaStreamingLinearRegressionSuite.java:54)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
	at org.apache.spark.mllib.stat.JavaStatisticsSuite.tearDown(JavaStatisticsSuite.java:65)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
	at org.apache.spark.mllib.stat.JavaStatisticsSuite.tearDown(JavaStatisticsSuite.java:65)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
org.apache.spark.mllib.stat.JavaStatisticsSuite.setUp(JavaStatisticsSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
org.apache.spark.mllib.stat.JavaStatisticsSuite.setUp(JavaStatisticsSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect</li></div><div><li>java.lang.NullPointerException
	at org.apache.spark.mllib.stat.JavaStatisticsSuite.tearDown(JavaStatisticsSuite.java:65)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.mllib.clustering.JavaLDASuite.setUp(JavaLDASuite.java:41)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.M</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.mllib.clustering.JavaLDASuite.setUp(JavaLDASuite.java:41)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.M</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)&amp;#010;org.apache.spark.repl.Main$.createSparkSession(Main.scala:112)&amp;#010;&lt;init&gt;(&lt;console&gt;:15)&amp;#010;&lt;init&gt;(&lt;console&gt;:42)&amp;#</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)&amp;#010;org.apache.spark.repl.Main$.createSparkSession(Main.scala:118)&amp;#010;&lt;init&gt;(&lt;console&gt;:15)&amp;#010;&lt;init&gt;(&lt;console&gt;:42)&amp;#</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;Welcome to&amp;#010;      ____              __&amp;#010;     / __/__  ___ _____/ /__&amp;#010;    _\ \/ _ \/ _ `/ __/  '_/&amp;#010;   /___/ .__/\_,_/_/ /_/\_\   version 3.0.0-SNAPSHOT&amp;#010;      /_/&amp;#010;         &amp;#010;Using Scala version 2.12.7 (OpenJDK 64-Bit Server VM, Java 1.8.0_151)&amp;#010;Type in expressions to have them evaluated.&amp;#010;Type :hel</li></div><div><li>isContain was true Interpreter output contained 'Exception':&amp;#010;Welcome to&amp;#010;      ____              __&amp;#010;     / __/__  ___ _____/ /__&amp;#010;    _\ \/ _ \/ _ `/ __/  '_/&amp;#010;   /___/ .__/\_,_/_/ /_/\_\   version 3.0.0-SNAPSHOT&amp;#010;      /_/&amp;#010;         &amp;#010;Using Scala version 2.12.7 (OpenJDK 64-Bit Server VM, Java 1.8.0_151)&amp;#010;Type in expressions to have them evaluated.&amp;#010;Type :</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;java.lang.NoClassDefFoundError: org/spark_project/guava/cache/Weigher&amp;#010;  at org.apache.spark.network.shuffle.ExternalShuffleBlockHandler.&lt;init&gt;(ExternalShuffleBlockHandler.java:64)&amp;#010;  at org.apache.spark.deploy.ExternalShuffleService.newShuffleBlockHandler(ExternalShuffleService.scala:63)&amp;#010;  at org.apache.spark.deploy.Exter</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:17: error: not found: value sc&amp;#010;       val accum = sc.longAccumulator&amp;#010;                   ^&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:18: error: not found: value sc&amp;#010;       sc.parallelize(1 to 10).foreach(x =&gt; accum.add(x))&amp;#010;       ^&amp;#010;&lt;console&gt;:18: error: not found: value accum&amp;#010;       sc.parallelize(1 </li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; v: Int = 7&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:19: error: not found: value sc&amp;#010;       val res1 = sc.parallelize(1 to 10).map(x =&gt; v).collect().reduceLeft(_+_)&amp;#010;                  ^&amp;#010;&amp;#010;scala&gt; v: Int = 10&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:19: error: not found: value sc&amp;#010;       val res2 = sc.parallelize(1 to 10).map(x =&gt;</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt;      |      | defined class C&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:18: error: not found: value sc&amp;#010;       val res = sc.parallelize(1 to 10).map(x =&gt; (new C).foo).collect().reduceLeft(_+_)&amp;#010;                 ^&amp;#010;&amp;#010;scala&gt;      | _result_1542829238895: Int = 1&amp;#010;&amp;#010;scala&gt; </li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; double: (x: Int)Int&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:19: error: not found: value sc&amp;#010;       val res = sc.parallelize(1 to 10).map(x =&gt; double(x)).collect().reduceLeft(_+_)&amp;#010;                 ^&amp;#010;&amp;#010;scala&gt;      | _result_1542829239920: Int = 1&amp;#010;&amp;#010;scala&gt; </li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; v: Int = 7&amp;#010;&amp;#010;scala&gt; getV: ()Int&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:19: error: not found: value sc&amp;#010;       val res1 = sc.parallelize(1 to 10).map(x =&gt; getV()).collect().reduceLeft(_+_)&amp;#010;                  ^&amp;#010;&amp;#010;scala&gt; v: Int = 10&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:19: error: not found: value sc&amp;#010;       val res2</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; array: Array[Int] = Array(0, 0, 0, 0, 0)&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:19: error: not found: value sc&amp;#010;       val broadcastArray = sc.broadcast(array)&amp;#010;                            ^&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:17: error: not found: value sc&amp;#010;       val res1 = sc.parallelize(0 to 4).map(x =&gt; broadcastArray.value(x</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:17: error: not found: value sc&amp;#010;       var file = sc.textFile("/var/lib/jenkins/workspace/spark/repl/target/tmp/spark-75b2ce38-2a78-4d2d-9626-e5acec773787/input").cache()&amp;#010;                  ^&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:17: error: not found: value file&amp;#010;       val res1 = file.count()&amp;#010;            </li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; v: Int = 7&amp;#010;&amp;#010;scala&gt; getV: ()Int&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:19: error: not found: value sc&amp;#010;       val res1 = sc.parallelize(1 to 10).map(x =&gt; getV()).collect().reduceLeft(_+_)&amp;#010;                  ^&amp;#010;&amp;#010;scala&gt; v: Int = 10&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:19: error: not found: value sc&amp;#010;       val res2</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:17: error: not found: value spark&amp;#010;       import spark.implicits._&amp;#010;              ^&amp;#010;&amp;#010;scala&gt; defined class TestCaseClass&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:22: error: not found: value sc&amp;#010;       sc.parallelize(1 to 10).map(x =&gt; TestCaseClass(x)).toDF().collect()&amp;#010;       ^&amp;#010;&amp;#010;scala&gt; &amp;#010</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; import org.apache.spark.sql.functions._&amp;#010;&amp;#010;scala&gt; import org.apache.spark.sql.{Encoder, Encoders}&amp;#010;&amp;#010;scala&gt; import org.apache.spark.sql.expressions.Aggregator&amp;#010;&amp;#010;scala&gt; import org.apache.spark.sql.TypedColumn&amp;#010;&amp;#010;scala&gt;      |      |      |      |      |      |      | simpleSum: org.apache.sp</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; defined class TestClass&amp;#010;&amp;#010;scala&gt; t: TestClass = TestClass@53c70969&amp;#010;&amp;#010;scala&gt; import t.testMethod&amp;#010;&amp;#010;scala&gt; defined class TestCaseClass&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:31: error: not found: value sc&amp;#010;       sc.parallelize(1 to 10).map(x =&gt; TestCaseClass(x)).collect()&amp;#010;       ^&amp;#010;&amp;#010;scala&gt; </li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; defined class Foo&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:28: error: not found: value sc&amp;#010;       val res = sc.parallelize((1 to 100).map(Foo), 10).collect()&amp;#010;                 ^&amp;#010;&amp;#010;scala&gt;      | _result_1542829253456: Int = 1&amp;#010;&amp;#010;scala&gt; </li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; defined class Foo&amp;#010;&amp;#010;scala&gt; list: List[(Int, Foo)] = List((1,Foo(1)), (1,Foo(2)))&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:28: error: not found: value sc&amp;#010;       val res = sc.parallelize(list).groupByKey().collect()&amp;#010;                 ^&amp;#010;&amp;#010;scala&gt;      | _result_1542829254478: Int = 1&amp;#010;&amp;#010;scala&gt; </li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; timeout: Int = 60000&amp;#010;&amp;#010;scala&gt; start: Long = 1542829255927&amp;#010;&amp;#010;scala&gt;      |      |      | &lt;console&gt;:31: error: not found: value sc&amp;#010;       while(sc.statusTracker.getExecutorInfos.size != 3 &amp;&amp;&amp;#010;             ^&amp;#010;&amp;#010;scala&gt;      |      | &amp;#010;scala&gt; import org.apache.spark.storage.StorageLevel._&amp;</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; defined class Click&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:33: error: not found: value spark&amp;#010;       spark.implicits.newProductSeqEncoder[Click]&amp;#010;       ^&amp;#010;&amp;#010;scala&gt;      | _result_1542829258036: Int = 1&amp;#010;&amp;#010;scala&gt; </li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaApplySchemaSuite.setUp(JavaApplySchemaSuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native </li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaApplySchemaSuite.setUp(JavaApplySchemaSuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native </li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaApplySchemaSuite.setUp(JavaApplySchemaSuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native </li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaApplySchemaSuite.setUp(JavaApplySchemaSuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native </li></div><div><li>java.lang.NullPointerException
</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaApplySchemaSuite.tearDown(JavaApplySchemaSuite.java:60)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaApplySchemaSuite.setUp(JavaApplySchemaSuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaApplySchemaSuite.tearDown(JavaApplySchemaSuite.java:60)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaApplySchemaSuite.setUp(JavaApplySchemaSuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaApplySchemaSuite.tearDown(JavaApplySchemaSuite.java:60)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaBeanDeserializationSuite.tearDown(JavaBeanDeserializationSuite.java:41)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaBeanDeserializationSuite.tearDown(JavaBeanDeserializationSuite.java:41)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaApplySchemaSuite.setUp(JavaApplySchemaSuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native </li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaColumnExpressionSuite.tearDown(JavaColumnExpressionSuite.java:46)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaApplySchemaSuite.setUp(JavaApplySchemaSuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native </li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaColumnExpressionSuite.tearDown(JavaColumnExpressionSuite.java:46)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaApplySchemaSuite.setUp(JavaApplySchemaSuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native </li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaApplySchemaSuite.setUp(JavaApplySchemaSuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native </li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaApplySchemaSuite.setUp(JavaApplySchemaSuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native </li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaApplySchemaSuite.setUp(JavaApplySchemaSuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native </li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaApplySchemaSuite.setUp(JavaApplySchemaSuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native </li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaApplySchemaSuite.setUp(JavaApplySchemaSuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native </li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaApplySchemaSuite.setUp(JavaApplySchemaSuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native </li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaApplySchemaSuite.setUp(JavaApplySchemaSuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native </li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaApplySchemaSuite.setUp(JavaApplySchemaSuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native </li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMetho</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaSaveLoadSuite.tearDown(JavaSaveLoadSuite.java:76)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMetho</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaSaveLoadSuite.tearDown(JavaSaveLoadSuite.java:76)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMetho</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaUDAFSuite.tearDown(JavaUDAFSuite.java:42)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaUDFSuite.tearDown(JavaUDFSuite.java:50)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaUDFSuite.tearDown(JavaUDFSuite.java:50)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMetho</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaUDFSuite.tearDown(JavaUDFSuite.java:50)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMetho</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaUDFSuite.tearDown(JavaUDFSuite.java:50)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMetho</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaUDFSuite.tearDown(JavaUDFSuite.java:50)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMetho</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaUDFSuite.tearDown(JavaUDFSuite.java:50)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.tearDown(JavaDataStreamReaderWriterSuite.java:49)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.tearDown(JavaDataStreamReaderWriterSuite.java:49)
</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.sql.hive.HiveMetastoreLazyInitializationSuite.$anonfun$new$1(HiveMetastoreLazyInitializationSuite.scala:31)&amp;#010;org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)&amp;#010;org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)&amp;#010;org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.s</li></div><div><li>java.lang.ExceptionInInitializerError
	at org.apache.spark.sql.hive.JavaDataFrameSuite.setUp(JavaDataFrameSuite.java:50)
Caused by: java.lang.IllegalStateException: 
Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.hive.JavaDataFrameSuite.setUp(JavaDataFrameSuite.java:50)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>Could not initialize class org.apache.spark.sql.hive.test.TestHive$</li></div><div><li>Could not initialize class org.apache.spark.sql.hive.test.TestHive$</li></div><div><li>Could not initialize class org.apache.spark.sql.hive.test.TestHive$</li></div><div><li>Could not initialize class org.apache.spark.sql.hive.test.TestHive$</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(StreamingContext.scala:85)
org.apache.spark.streaming.api.java.JavaStreamingContext.&lt;init&gt;(JavaStreaming</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(StreamingContext.scala:85)
org.apache.spark.streaming.api.java.JavaStreamingContext.&lt;init&gt;(JavaStreaming</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Could not load library. Reasons: [no leveldbjni64-1.8 in java.library.path, no leveldbjni-1.8 in java.library.path, no leveldbjni in java.library.path, /var/lib/jenkins/workspace/spark/common/kvstore/target/tmp/libleveldbjni-64-1-942530653138087895.8: /var/lib/jenkins/workspace/spark/common/kvstore/target/tmp/libleveldbjni-64-1-942530653138087895.8: cannot open shared object file: No such file or </li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>org/apache/zookeeper/AsyncCallback$MultiCallback</li></div><div><li>org/apache/zookeeper/AsyncCallback$MultiCallback</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org/apache/zookeeper/AsyncCallback$MultiCallback</li></div><div><li>org/apache/zookeeper/AsyncCallback$MultiCallback</li></div><div><li>Error while cleaning up the server resources</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Could not load library. Reasons: [no leveldbjni64-1.8 in java.library.path, no leveldbjni-1.8 in java.library.path, no leveldbjni in java.library.path, /var/lib/jenkins/workspace/spark/common/kvstore/target/tmp/libleveldbjni-64-1-7468695008099956963.8: /var/lib/jenkins/workspace/spark/common/kvstore/target/tmp/libleveldbjni-64-1-7468695008099956963.8: cannot open shared object file: No such file o</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>org/apache/zookeeper/AsyncCallback$MultiCallback</li></div><div><li>org/apache/zookeeper/AsyncCallback$MultiCallback</li></div><div><li>Timeout of './bin/spark-submit' '--class' 'org.apache.spark.sql.hive.SparkSQLConfTest' '--name' 'SparkSQLConfTest' '--master' 'local-cluster[2,1,1024]' '--conf' 'spark.ui.enabled=false' '--conf' 'spark.master.rest.enabled=false' '--conf' 'spark.sql.hive.metastore.version=0.12' '--conf' 'spark.sql.hive.metastore.jars=maven' '--driver-java-options' '-Dderby.system.durability=test' 'file:/var/lib/jen</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Timeout getting response from the server</li></div><div><li>SparkContext has been shutdown</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMe</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMe</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMe</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>SparkContext has been shutdown</li></div><div><li>SparkContext has been shutdown</li></div><div><li>SparkContext has been shutdown</li></div><div><li>SparkContext has been shutdown</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:71)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:43)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:71)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:43)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>Exception evaluating from_avro(to_avro(true), "boolean")</li></div><div><li>Exception evaluating from_avro(to_avro(0), "int")</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>Exception evaluating from_avro(to_avro(0), "int")</li></div><div><li>Exception evaluating from_avro(to_avro(6182052057508334851), "long")</li></div><div><li>Exception evaluating from_avro(to_avro(0.0), "float")</li></div><div><li>Exception evaluating from_avro(to_avro(-3.7923602203505276E231), "double")</li></div><div><li>Exception evaluating from_avro(to_avro(-92462873), {"type":"fixed","name":"fixed","namespace":"topLevelRecord","size":4,"logicalType":"decimal","precision":8,"scale":0})</li></div><div><li>Exception evaluating from_avro(to_avro(-2600.3674), {"type":"fixed","name":"fixed","namespace":"topLevelRecord","size":4,"logicalType":"decimal","precision":8,"scale":4})</li></div><div><li>Exception evaluating from_avro(to_avro(-3143083915223305), {"type":"fixed","name":"fixed","namespace":"topLevelRecord","size":7,"logicalType":"decimal","precision":16,"scale":0})</li></div><div><li>Exception evaluating from_avro(to_avro(-48503.84005849837), {"type":"fixed","name":"fixed","namespace":"topLevelRecord","size":7,"logicalType":"decimal","precision":16,"scale":11})</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>Exception evaluating from_avro(to_avro(6.243924813832474714E-20), {"type":"fixed","name":"fixed","namespace":"topLevelRecord","size":16,"logicalType":"decimal","precision":38,"scale":38})</li></div><div><li>Exception evaluating from_avro(to_avro(輄깬睻䏝⺥쳳簝쯹⯄Ձ⽗쾂౗䨌봬詺Დ蛌ᦙᅈ૞㞠杻犤쨎䟌䷶齽꫄⊊究忹㶁쳙潛웤끙ⱇૺ齛艀⏒寅檺ᗢ쏻ㇶ늈糔䉍쒫悁ᦃ틦霺㲈嬾䥸ꊎӽ緂獮墨唙镕㈞⚨⑑蝺搩칋杽䗆騮鶛ᙲ䏍쨐⡼荧骙辆㍯Ʞ㷍킛䆒ⴤ㖇猔朡㛟螕違쇄䯩뤩핁訃䌀콌囅쵃⊄䋳噷犂鹋䆵勑ἷ蔘うࠕ狜ᖚ뇓흄㐤⑾猅繜⋓䉓졁褩ㄤ鎗䭳ꗿḑ퇳튅⾑견쵳簱躁ࡱ廏ꮏ큍야⠆㵀㙜蕳蹗戨ᜀ줧䜒㎑䁩麔릉贂힋뢅瘍깥泹됇塯腃뱗⼬垊鮬అ돐퀶⃻愝ꂄ뻰텈嗉鿁묟謆篓᯴숬䐤幱ጨᖎ蕤ꣴ琱漤䊼뾯ꤲ鴛聂遅髶水㥏왞퇤ፃ훝䆚ꁤ᪰㷶㓀퍿⧭稭逈틚냨갎咰ǜ➢࿂藿唓⁘ꅐ沎擿됊ꐌ偎Ꮙ컨✼ٌ횬Ⲋ⇯紂뗼硎ロቭ骿㍷Ǿ㤻Ḯ螆鍼秙뽾쬮⽷뻠絅醮魛ෂ礣獒䕲䘒厳곤쵲앂ߊⳋࢰ↴繂‼稂⎣ຑ⤅镕실対㎤칾鞩࣎œ僩कꐽ윥귳법䤚礧Ꞿ䊻葷ꦋ훭噡噀쪸㙺涣禶쁼ʔꊴ욆뙙쪆判㸼Ɔ㶺↳獏᪵쌥㡓焼ꉈ棍쥍阣뛾൸빐㚉᭩票깞㭼鹃꭫ౣ☴㔌撑봳ꢈ졮喷贑苤橅뢋刟ʜ⦙ᚍ堚判嗔즲죨㎀鸩枂혢ꔚ呫첑</li></div><div><li>Exception evaluating from_avro(to_avro(0xF21859166D02FE22CD2D72FF54D6972A78604D11FBEEEA69F0908E69179EC7E517540D67CE9F36952C1CA968264FF0B3FB157E99066AB9A084063420558B63B002C15B96FF78F06B67931C55E84C2596C6D7AE830DAB6B1955C3CB516C741A82DB1F7BD86905F0C4C8EAEEE3C4DF310B556D6A2F65F3B8BE5153F891825E1F608C225AFCE7C431BA004BA6527717E1AB2268DA2E142024660946E1FC8C33BD8445611AAF4B5AEFFAA3F62728D3E1EE5531E0ACE</li></div><div><li>Exception evaluating from_avro(to_avro([NaN,-1.14506342090033620E-21,true,-2745216667602944116,2786806862196414]), {"type":"record","name":"topLevelRecord","fields":[{"name":"col_0","type":"double"},{"name":"col_1","type":{"type":"fixed","name":"fixed","namespace":"topLevelRecord.col_1","size":16,"logicalType":"decimal","precision":38,"scale":38}},{"name":"col_2","type":"boolean"},{"name":"col_3",</li></div><div><li>Exception evaluating from_avro(to_avro([0,false,0,-4197940144310078,-5.960605159251756930E-20]), {"type":"record","name":"topLevelRecord","fields":[{"name":"col_0","type":["long","null"]},{"name":"col_1","type":["boolean","null"]},{"name":"col_2","type":"int"},{"name":"col_3","type":{"type":"fixed","name":"fixed","namespace":"topLevelRecord.col_3","size":7,"logicalType":"decimal","precision":16,"s</li></div><div><li>Exception evaluating from_avro(to_avro([ᙪꍗ팬렩끎῟⍆崘䉾巵਻죹芚塨ᨮ摳ꈤ駄쀃鳅礎咒䆢祷袘끰倈嵳긿쪠䀊꤆Ὀ㺃㠷⯭㽆ᢥ㯴俽슂༢酥ჩ⾪撺钉츰ጯ쟝慏ᰀచ噞偊䃺衁䗅샢砠翌㨖뮁冷졢⪬㆟牌筀ᇿë哊駨꼃틮埬몱口ꔙ㜷柣䲓눔喇鶠밠✇潭ᩗ醼჊ⴶᮜ侅黁쫬嘄掖枀ᵝꦀ巅쇘ያ쪀횛燏깙్윾缧඗쟫듊歫왉愓貆ƛ곚쇥쏧킵縃낮狳栄贄弌ಈ箎⠢䐰䍪䟉쭌潉◇䁟㑧ꈄ珏ᥪꌗ⎀ಇꦶ౳ꄘ䅰蕈濘켧⸻豈߰퉡쨞믏੡㌟⢋僌⫛㼥ẏㄟᲰ垄筘➥㧢믒䳊揤詑쨱橮涎苔痗㨇䖫굸㯈쇫羓寮犴ꆘ틏困靟듂䃣져뼏땄윀調搳ᦨ洛㬻梅擹稺穎㧁콷⤙晣噼䥗염蛚䍒獠渜옥혉╁疮ᚽ憯克꓃䗈艮썘ꁣ䗸색⊌懸㉩㜜롦촙ⱅ๘ꖯꪾꄅ횡鳙꺈屙蝻织⊸兀筤崲⳷໩᣹埾헝㰠甫낛፜犴㐽鎙짨벊꾚ꣾ℄弶⯂荶彁ꐞ䥖኶᪪걌㢏픻⾵犇柖㭊볲⇦륎束쀳堶蘀꼞恴ꕄ钬쵤쩍슂攄庿味훑罶얈輗䯳塅띱༥靦ꃗ䳠࿰荪춍匉땅䅔롚ឤ午룙㥽갩ꏯ倍⸌糇圩⅂쫣揆썠ਔ茾沙ఒ枇춐睶駱詨薜貏矡潧㗍ஓ츹송⠇匼歑샭騗蜐Ⱘ쐖</li></div><div><li>Exception evaluating from_avro(to_avro([NaN,-26027996,ત瓯㜬᭶Ⱔ䅭蒥⅐ԝ攉섣⛈擔姄垊鴡ⴾ⡺倀㕾礋얗葈샸ᔹ垓뀾퇜§᥂Ј癊刉邡ぶ곸繒犐혊⸧鶇縞蒉뿌鰊硥렀㬼䋤嬗䮧掋э䳲粞ƒ᫺絷궤ʊ؛ۛ尧ḫ㰤괉⃇㺬㫔约ㅂ죆懨⇰햴防캩Ꮺ뫩礀㝱ꮸ᫽ᰚ溃봝ꏸ߽曝음쌱㌙栉ꈋ㣥䕤ꈘ⁊샛貛엺ᶕ祴혮鬢傜ኣ첔汣捭뵼浏ᱢ饾掄쌯懋펐叅庳敋ރ舘ᜀ貄ṛ샋Ჲ涏ⲁ멫⪎꾪淲踒垅ḍ깣웨ꮩ읐ᳺ⬸ꃨ砶鍊ᯗ烥枻歖㑀햝4넗䲗拂믘㐂╕손ꚑ琲킃놚潷쎉悜晉⯁ⵄ䂬풿셢恚悅䮬찾앐㟜१⹦ꇌꢼ賌⸮멠謽㱠ֵ戯왵⸛衳㇩⓶컾Àᡉⲁ팳뭶騐竳軰韄衵耘锝ꠓ匚ℭ挥쮓⽉孤囧緌ੱ寴寃圶尻຀㠘ञ愑絲㌯㎏渇獦펞ꈹ嘑霋⇥㫲蕆瞦쨵퇆믲䰯㵗뢕鑯纂࡛颜⧎릣얐㮚Ι믥瑬Ꙏꁈ쭒㌈槣鬅鯖㩳ᰪ꜔ۚ속樣퀥䬱ꊦ瀡淇컹㐞茠㨹ᯍ츴袀䒡㬹郥쨀ℷ统齘瑈᠟땱쥃땉囶밓걣翆⴫㠇ꩦ촣꼿၂罆픷쎢ᛁ̪ᇿ旒䓗쯖寪䓖儻嚚늒㑵ಪ뒠筚ᇢ쏒⟲랷͖칡勭޿⫂높钏」ᆕ㍇皷䔞㖦㘢ꅉ㱷삤❠ꗻן聼</li></div><div><li>Exception evaluating from_avro(to_avro([1.7306941021248733E-50,婜级䳅ฑ⾿袗囘殻ᨋ궶㴼̎ɰ뷝푷␷ण이ᵠ皲졞鴇鐎勌춹霗潝歮桧=淽Ყ둍嘌萵㴍毬콡䶼쵧읶굷ࣕ蚛䋵벂퓧훹꾁턊螤휇鵰壡ڻ㴦㾷헡ƫ䢍뷽懪痁깑ᇯᵡ⹽끨̏ぽ뷤혠唁ㇹ닃앷ꮧ㲙ℑ┢徚ꖆ紱௾琱릻鈗晨辌첊㵆喨ᬥ悭灷启갵滌ℋ된䌞㢖贕膲当ᅂ㮝Თ⯙㱹穡䋑틑旴ꕘ躪뢦酅䓞♡䱕韌▜鉛뭨ⓗ新㩘惝跍䊌⢇㎏빐그豦䨣렞휨㗰칝眭Ω뱻咅仟Ȼ垿䍻ㇰ糛ͭ詡࡜豪㔙⍬䫥缫㱉횲녙鋺瞃됕쮘ᷘ琎槼討縏ឈ墶ꊡ賧ꚢ킱凤壚먀쑌檥ᗨ㑮苬砮乪⽄閼ꘞ᙮뎰澇츂믯쬑쑊憞੝딿處㩚咎벞쬪서ꯢ歺羮뾄肆ⶢ크᥃䀴歚ࣗ羳奩ꐻ歽횆ਏ햃驤㵰깍㖴븼샶椙㘨だ믻ꊤ趘㾰鴘뀼ᰣ퟇ꙕҲ瀓퍣ᅆ鴧躋㼰畷㊴儆銸ʜ㲃誚緊洶후饛ន蚀咫羘倒ꅁ犆ଵ猣椼Ј甜ꮦ⭕倾쿳⯭劽ꣽẙ귴랁沚왖볯቉⸤娕鿔픫璥⎦鰩쌵躡皳눘ᬙ+쒽⭓ꃱ㔸▵㙎ꏅ뼁쇦눔喩ೱ䔁㏕밧ꕳ흌棾䅅ܺ쎿㛻蜡⊇ꞿ艧Â떌 柔ꜷ簬ஊ虹㡠똳ⴛ䲽</li></div><div><li>Exception evaluating from_avro(to_avro([[-8.221108454336629230E-20],null,[0],[[[-103]]],-4829135515104516905,[[[B@391dfe7c]],[-1557705764],-3608.6819,[-70.70905],[null]]), {"type":"record","name":"topLevelRecord","fields":[{"name":"col_0","type":{"type":"array","items":[{"type":"fixed","name":"fixed","namespace":"topLevelRecord.col_0","size":16,"logicalType":"decimal","precision":38,"scale":38},"n</li></div><div><li>Exception evaluating from_avro(to_avro([[[2092832000875582393],[[-42169030]]],[[6441.4447,-7851214212309161041],[[[-2147483648]]]],null,[-128],[[90699043],[2338456050412981600]],true]), {"type":"record","name":"topLevelRecord","fields":[{"name":"col_0","type":[{"type":"record","name":"col_0","namespace":"topLevelRecord","fields":[{"name":"col_0","type":{"type":"array","items":[{"type":"fixed","nam</li></div><div><li>Exception evaluating from_avro(to_avro([1818710107665646138,[47433.38747846641,[[[[127]],[[[-2147483648]]]],[[⮮ᵓ罘됟뇝⧇끫嗻㼭૯ᰁ脛✠쌖鑲邠卦릔纍扸鸬瘃볳릔絢箅솩શ抴㢸妮蝖ଗ붧㷍幰薊꼙訓枙䉫ⱇ苽댚떸辘㏫狙멏뾚䂎珌蛉泚霾們˛긮Ҍ㥡揉볬酝쵹ě亍龏冻쑫緦齏䙸Ж⽺掷ⷕ庛䅮ꄬ霱ᯚ쮪狙嘚ֹಸ鲤瓽疾嗪컩嶈㎈㐤儖띸듐뼆瘵㖏焱窧掺ᐠ蒮⿓၂ᵗ䚜か♖⪉毨⯖꺰톤씟ᗢ櫊뭯৑ꮯ멾ꔄɍ넁ż㦕깋ܔମ䆦罺Ṁ郍藆帕肣薭ᡞ䐜⻫髈谗ⷯ晵쎟뵛寤엟ᥒ魷酗箻甪Ҵྼ늺骉駚ʘꮢ龉䕥⛬蚖Ⴒ끮మ鉮ቨ蒚垷짩೬會뎝뤇⻂訫魘傫죏ᴲ⪜枔㒙瞮쇚꽰슴輣嚧츟禌㳅ꚁ加粄㫔⺹ᧆ귉鞐瘠Ͻ⩟⬲⍑嗔멘넠묬았젾戗㣍枀㶕࿏඄㑓云쯈㏡粮䢏濶ꖿ⫽鑘ᤡᎻ耝辉꤄孄墿誗뇪ౄ⸡牏ꈛ쇺Ϻ淬秔斍㝒㿒峆赠岔䷧憛ᨿ닅฿쟗볮끹亱熰⍈ᇸᡄ⶟뗕휌姶甖찣曑꒕엏愵毜馄驍저岿┴鼶ጩ⤽藬㲊ꈻ</li></div><div><li>Exception evaluating from_avro(to_avro([[[뽒궹鋗䆻磨쒅ꭔᵅ結ॺ䰳ᖣ暵៶꣒艸쵽鄢琠骜圉ᴀ셍嵟蠜禘䁬ᨚܛᄃ伛褔ࣵ霭腆≽뢸痦პ흳㶻ຆ聃摠⌏鹀싲矘젪쑂ⷰ㶪펫쨇ᱭ눬窳쀤⸝쑅춿↛썝杗ᳩ嗹䄿饇औ쳧抾쬜犱爘Ԓ碬䍬戗鴙⅕姳㑺带恆䆬쯱뭔䣖댬薺ꥅ饦ⱖ鷸骰摿恹쭩팋多嘵ۃꦻ䧼죚Ҿ厼悤芜붸摇꾩㗼灸⪄猇Ƭ첫㯉⊦欭⢧᧪댌牳ⲏ쪷妸℘斬擛牠퓳睿췢Ꝑ鵃厀Œ㒽彖૾쒅蕭뜐嘇쨏乪싊㰻僂㱑ꎸ凝뀌ᨃ齷鿒筱陧萙⣚쥡됫䄉峏痌慯ꍶ貣僑ሃ 㶄殑],[[佛ᒍ㊁Զ犽䇘攻㚫㊶茭ᣩ糄縎蹑㢵伷疊䎏䱑ڝ밄쐾읏롯䆇톻痂〾鼔췼間䷧뛱똛⻍鹎䕏뗰⨿䡀蚥쿞榋쳴恈쮨홑捨寃蝛≰퇀絟螉繹諾ᥨㅵ꽛⢣첔팫賐䌰髵礒ዎ碟㈠侵쳷珸䃣㬲䴼옢㠣얡뙁ڏ䭈ㄼ㚬ٽ钩钓궭㕎㸠鳬㪀⢰찮◔쫓銜⌽얘蚊ถ쎆渻䣧࢞噪袴鰼˷辂夂ቴꥱ葱笸ಉ粆㱌䋲厹鹁௽梍詃푬랕磇Ῐ᳜ß䌚ᜯ⍣뫖孚䪯⪾섫᳐쑆璇瓚ᡸꝭ⚹ꥠ诱τ켮쵾囑乎릕⧥瑦䇣䕳鬬겎熒꼻鯔ꚧ䗬픎ꓤ←믇ᯨ䱧볗ጠꝆ偹Ẹ衭㜑秚푭ꑿ</li></div><div><li>Exception evaluating from_avro(to_avro([null,[[[-1468999699632294169]],-1.2083574959001583E-52,1398.8086,[-1.004353E-18]],[[B@46610fc9,雎ⶄᡏ윜↢餙鈲哮ꤹ翅駿㎾౒㯕息䩓飾ᣕꞵ긞෨絍家駧蛡䧏펜ٸ밇甡뿥਄Ǟ짉ዲ䐺뵷ꁉ║ࡖꇜ餋㈓턺䳀Ũ悒銫䈩帚㇞섺㑍쐟㠋렊꧟✜ᖚᴀ뵬돒憰ꢑ繴Ω酈ꠡ㥲쨕튃바뎎짏떘獨⨿롹췭裶嚥ᎈ᳞넏⯡옄쏰쉜䳫㚊嶜䔵졅⒰➶㳉뢉ꅣ㥥畅ꨒ씢栓⋣䤻즽ю䪇펷䟹惖㎣舍㹽齲䑠ꛥ狓챴몵आߝ餯旒浣Ҕ๛퉹먋툸溡楙呯騐Ữⵒ產甚烄놐髃郹ùᛶ‥俻阞㌙峆ꢮ欅玚晨犻催‍酡閬뿣䷰㻉礪⹃朶쳔蔧朦찔∾ឫ૓팹詺ѓඇ蟅∱ℱ㹔綑ꜽ❴⒍澗쵈ˍ⩁賤㋙닂⺱뵴곇끗戞酢華쟹裎夓ⷩ䈀㿷䤆⍀␨洑裞脝֦촴䍻펡乀皃商蒑⎅‼樮鈋꺫粀ㄗ⤇⺷鞕퉽켮걀씱쿴ẗ쌫䚯㎝큟⼰脹╯⎸쁟⼸䑖䮜믡왏銂섯ꃽ㜏㉀닭첓䒫蒁㖀挪㋔ᆅ䙁枏厊蘁꟞</li></div><div><li>Exception evaluating from_avro(to_avro(1), &amp;#010;{&amp;#010;  "type": "string",&amp;#010;  "name": "my_string"&amp;#010;}&amp;#010;       , (mode,PERMISSIVE))</li></div><div><li>Exception evaluating from_avro(to_avro(abc), &amp;#010;{&amp;#010;  "type": "int",&amp;#010;  "name": "my_int"&amp;#010;}&amp;#010;       )</li></div><div><li>Exception evaluating from_avro(to_avro(1.23), &amp;#010;{&amp;#010;  "type": "double",&amp;#010;  "name": "my_double"&amp;#010;}&amp;#010;       , (mode,PERMISSIVE))</li></div><div><li>Exception evaluating from_avro(to_avro(1.23), &amp;#010;{&amp;#010;  "type": "float",&amp;#010;  "name": "my_float"&amp;#010;}&amp;#010;       )</li></div><div><li>create random record with seed -5557646993725901181 Exception evaluating from_avro(to_avro([᭫揪㋫潔Ꙡ锾ΰ㸜➙㕷拞׾૏⦆矎둣礯ѫ턜ᵮ歹᭓釪暫ю냝髣ྋ҆Ҡ댧₡呂嵋騚뤉ᴅ쁷輧뤯눪鑛䨚ⲧʔ괤풑ꖽ榎提⎵欚賋᭡꼒確⫗뿈ꀥ蜛㕙໌଼꠾喝ሤ꬏䣋ꔟ쾍꽬䷒龸۫㘽ϯ锌䵙⿓됴곑͂᱔䤗༯壺뺇焨㓤黢勿ృ鱔衫뢟瘺诨ꑳ럕⒜厔魧嫛墧䩐롼ⱁ賦쨮Ꭺꖦ৓䄜㍄蹜픋贏퐿㝓㺭씌擾됿Ӥꦖ໸줛ᆝ䷛⌎䠼켯犣녽↔ଗ뀧ꃸᣯ鴰銈絧妑Ὀ瑣퍾㺇ᴛ鼌㱥䄠䛹㨎Ⴣ耨俬ͺ栉㙶枵抨俙ⶎ尡꜃䫁痁鿝⚭ϒყ愃ɠ캕⍑▲嘰詷秔规俟ퟃꚏ藳눦ᓿ髣῭ꌶ苉蔃閖쥞鲕ڏ,0,6542.1463,true,1.967932329760487540E-20]), {"type":"record","name":"topLevelRecord","fields":[{"name":"col_0",</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.sql.avro.AvroFunctionsSuite.beforeAll(AvroFunctionsSuite.scala:27)&amp;#010;org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:212)&amp;#010;org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)&amp;#010;org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.s</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.sql.avro.AvroFunctionsSuite.beforeAll(AvroFunctionsSuite.scala:27)&amp;#010;org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:212)&amp;#010;org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)&amp;#010;org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.s</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>org/apache/zookeeper/AsyncCallback$MultiCallback</li></div><div><li>org/apache/zookeeper/AsyncCallback$MultiCallback</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(LocalSparkContext.scala:29)&amp;#010;org.apache.spark.graphx.GraphOpsSuite.withSpark(GraphOpsSu</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(LocalSparkContext.scala:29)&amp;#010;org.apache.spark.graphx.GraphSuite.withSpark(GraphSuite.sc</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(LocalSparkContext.scala:29)&amp;#010;org.apache.spark.graphx.PregelSuite.withSpark(PregelSuite.</li></div><div><li>SparkContext has been shutdown</li></div><div><li>SparkContext has been shutdown</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>SparkContext has been shutdown</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(LocalSparkContext.scala:29)&amp;#010;org.apache.spark.graphx.lib.ConnectedComponentsSuite.withS</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(LocalSparkContext.scala:29)&amp;#010;org.apache.spark.graphx.lib.SVDPlusPlusSuite.withSpark(SVD</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(LocalSparkContext.scala:29)&amp;#010;org.apache.spark.graphx.lib.StronglyConnectedComponentsSui</li></div><div><li>SparkContext has been shutdown</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(LocalSparkContext.scala:29)&amp;#010;org.apache.spark.graphx.lib.TriangleCountSuite.withSpark(T</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>SparkContext has been shutdown</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.regression.JavaLinearRegressionSuite.setUp(JavaLinearRegressionSuite.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.regression.JavaLinearRegressionSuite.setUp(JavaLinearRegressionSuite.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.regression.JavaLinearRegressionSuite.setUp(JavaLinearRegressionSuite.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.regression.JavaLinearRegressionSuite.setUp(JavaLinearRegressionSuite.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.regression.JavaLinearRegressionSuite.setUp(JavaLinearRegressionSuite.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.regression.JavaLinearRegressionSuite.setUp(JavaLinearRegressionSuite.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.regression.JavaLinearRegressionSuite.setUp(JavaLinearRegressionSuite.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.regression.JavaLinearRegressionSuite.setUp(JavaLinearRegressionSuite.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.regression.JavaLinearRegressionSuite.setUp(JavaLinearRegressionSuite.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.regression.JavaLinearRegressionSuite.setUp(JavaLinearRegressionSuite.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.regression.JavaLinearRegressionSuite.setUp(JavaLinearRegressionSuite.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.tuning.JavaCrossValidatorSuite.setUp(JavaCrossValidatorSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.source.libsvm.JavaLibSVMRelationSuite.setUp(JavaLibSVMRelationSuite.java:47)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.r</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.source.libsvm.JavaLibSVMRelationSuite.setUp(JavaLibSVMRelationSuite.java:47)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.r</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.source.libsvm.JavaLibSVMRelationSuite.setUp(JavaLibSVMRelationSuite.java:47)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.r</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Error while instantiating 'org.apache.spark.sql.internal.SessionStateBuilder':</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.source.libsvm.JavaLibSVMRelationSuite.setUp(JavaLibSVMRelationSuite.java:47)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.r</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.source.libsvm.JavaLibSVMRelationSuite.setUp(JavaLibSVMRelationSuite.java:47)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.r</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.source.libsvm.JavaLibSVMRelationSuite.setUp(JavaLibSVMRelationSuite.java:47)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.r</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.source.libsvm.JavaLibSVMRelationSuite.setUp(JavaLibSVMRelationSuite.java:47)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.r</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.source.libsvm.JavaLibSVMRelationSuite.setUp(JavaLibSVMRelationSuite.java:47)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.r</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.source.libsvm.JavaLibSVMRelationSuite.setUp(JavaLibSVMRelationSuite.java:47)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.r</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.source.libsvm.JavaLibSVMRelationSuite.setUp(JavaLibSVMRelationSuite.java:47)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.r</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.source.libsvm.JavaLibSVMRelationSuite.setUp(JavaLibSVMRelationSuite.java:47)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.r</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.source.libsvm.JavaLibSVMRelationSuite.setUp(JavaLibSVMRelationSuite.java:47)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.r</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.source.libsvm.JavaLibSVMRelationSuite.setUp(JavaLibSVMRelationSuite.java:47)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.r</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.regression.JavaLinearRegressionSuite.setUp(JavaLinearRegressionSuite.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.regression.JavaLinearRegressionSuite.setUp(JavaLinearRegressionSuite.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.source.libsvm.JavaLibSVMRelationSuite.setUp(JavaLibSVMRelationSuite.java:47)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.r</li></div><div><li>java.lang.NullPointerException
	at org.apache.spark.ml.source.libsvm.JavaLibSVMRelationSuite.tearDown(JavaLibSVMRelationSuite.java:57)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.regression.JavaLinearRegressionSuite.setUp(JavaLinearRegressionSuite.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.regression.JavaLinearRegressionSuite.setUp(JavaLinearRegressionSuite.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.regression.JavaLinearRegressionSuite.setUp(JavaLinearRegressionSuite.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.tuning.JavaCrossValidatorSuite.setUp(JavaCrossValidatorSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.tuning.JavaCrossValidatorSuite.setUp(JavaCrossValidatorSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at org.apache.spark.ml.util.JavaDefaultReadWriteSuite.tearDown(JavaDefaultReadWriteSuite.java:41)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:4</li></div><div><li>java.lang.NullPointerException
	at org.apache.spark.mllib.classification.JavaStreamingLogisticRegressionSuite.tearDown(JavaStreamingLogisticRegressionSuite.java:55)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.mllib.clustering.JavaLDASuite.setUp(JavaLDASuite.java:41)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.M</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.mllib.clustering.JavaLDASuite.setUp(JavaLDASuite.java:41)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.M</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(StreamingContext.scala:85)
org.apache.spark.streaming.api.java.JavaStreamingContext.&lt;init&gt;(JavaStreaming</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.mllib.clustering.JavaLDASuite.setUp(JavaLDASuite.java:41)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.M</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.mllib.clustering.JavaLDASuite.setUp(JavaLDASuite.java:41)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.M</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.mllib.clustering.JavaLDASuite.setUp(JavaLDASuite.java:41)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.M</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.mllib.clustering.JavaLDASuite.setUp(JavaLDASuite.java:41)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.M</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.mllib.clustering.JavaLDASuite.setUp(JavaLDASuite.java:41)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.M</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.mllib.clustering.JavaLDASuite.setUp(JavaLDASuite.java:41)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.M</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.mllib.clustering.JavaLDASuite.setUp(JavaLDASuite.java:41)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.M</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.mllib.clustering.JavaLDASuite.setUp(JavaLDASuite.java:41)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.M</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.mllib.clustering.JavaLDASuite.setUp(JavaLDASuite.java:41)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.M</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.regression.JavaLinearRegressionSuite.setUp(JavaLinearRegressionSuite.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.regression.JavaLinearRegressionSuite.setUp(JavaLinearRegressionSuite.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.regression.JavaLinearRegressionSuite.setUp(JavaLinearRegressionSuite.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.regression.JavaLinearRegressionSuite.setUp(JavaLinearRegressionSuite.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.regression.JavaLinearRegressionSuite.setUp(JavaLinearRegressionSuite.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.regression.JavaLinearRegressionSuite.setUp(JavaLinearRegressionSuite.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:4</li></div><div><li>java.lang.NullPointerException
	at org.apache.spark.mllib.regression.JavaStreamingLinearRegressionSuite.tearDown(JavaStreamingLinearRegressionSuite.java:54)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
	at org.apache.spark.mllib.stat.JavaStatisticsSuite.tearDown(JavaStatisticsSuite.java:65)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
	at org.apache.spark.mllib.stat.JavaStatisticsSuite.tearDown(JavaStatisticsSuite.java:65)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
	at org.apache.spark.mllib.stat.JavaStatisticsSuite.tearDown(JavaStatisticsSuite.java:65)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
	at org.apache.spark.mllib.stat.JavaStatisticsSuite.tearDown(JavaStatisticsSuite.java:65)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.mllib.clustering.JavaLDASuite.setUp(JavaLDASuite.java:41)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.M</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.mllib.clustering.JavaLDASuite.setUp(JavaLDASuite.java:41)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.M</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.mllib.clustering.JavaLDASuite.setUp(JavaLDASuite.java:41)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.M</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.mllib.clustering.JavaLDASuite.setUp(JavaLDASuite.java:41)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.M</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)&amp;#010;org.apache.spark.repl.Main$.createSparkSession(Main.scala:112)&amp;#010;&lt;init&gt;(&lt;console&gt;:15)&amp;#010;&lt;init&gt;(&lt;console&gt;:42)&amp;#</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)&amp;#010;org.apache.spark.repl.Main$.createSparkSession(Main.scala:118)&amp;#010;&lt;init&gt;(&lt;console&gt;:15)&amp;#010;&lt;init&gt;(&lt;console&gt;:42)&amp;#</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;Welcome to&amp;#010;      ____              __&amp;#010;     / __/__  ___ _____/ /__&amp;#010;    _\ \/ _ \/ _ `/ __/  '_/&amp;#010;   /___/ .__/\_,_/_/ /_/\_\   version 3.0.0-SNAPSHOT&amp;#010;      /_/&amp;#010;         &amp;#010;Using Scala version 2.12.7 (OpenJDK 64-Bit Server VM, Java 1.8.0_161)&amp;#010;Type in expressions to have them evaluated.&amp;#010;Type :hel</li></div><div><li>isContain was true Interpreter output contained 'Exception':&amp;#010;Welcome to&amp;#010;      ____              __&amp;#010;     / __/__  ___ _____/ /__&amp;#010;    _\ \/ _ \/ _ `/ __/  '_/&amp;#010;   /___/ .__/\_,_/_/ /_/\_\   version 3.0.0-SNAPSHOT&amp;#010;      /_/&amp;#010;         &amp;#010;Using Scala version 2.12.7 (OpenJDK 64-Bit Server VM, Java 1.8.0_161)&amp;#010;Type in expressions to have them evaluated.&amp;#010;Type :</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;java.lang.NoClassDefFoundError: org/spark_project/guava/cache/Weigher&amp;#010;  at org.apache.spark.network.shuffle.ExternalShuffleBlockHandler.&lt;init&gt;(ExternalShuffleBlockHandler.java:64)&amp;#010;  at org.apache.spark.deploy.ExternalShuffleService.newShuffleBlockHandler(ExternalShuffleService.scala:63)&amp;#010;  at org.apache.spark.deploy.Exter</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:17: error: not found: value sc&amp;#010;       val accum = sc.longAccumulator&amp;#010;                   ^&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:18: error: not found: value sc&amp;#010;       sc.parallelize(1 to 10).foreach(x =&gt; accum.add(x))&amp;#010;       ^&amp;#010;&lt;console&gt;:18: error: not found: value accum&amp;#010;       sc.parallelize(1 </li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; v: Int = 7&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:19: error: not found: value sc&amp;#010;       val res1 = sc.parallelize(1 to 10).map(x =&gt; v).collect().reduceLeft(_+_)&amp;#010;                  ^&amp;#010;&amp;#010;scala&gt; v: Int = 10&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:19: error: not found: value sc&amp;#010;       val res2 = sc.parallelize(1 to 10).map(x =&gt;</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt;      |      | defined class C&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:18: error: not found: value sc&amp;#010;       val res = sc.parallelize(1 to 10).map(x =&gt; (new C).foo).collect().reduceLeft(_+_)&amp;#010;                 ^&amp;#010;&amp;#010;scala&gt;      | _result_1542830619476: Int = 1&amp;#010;&amp;#010;scala&gt; </li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; double: (x: Int)Int&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:19: error: not found: value sc&amp;#010;       val res = sc.parallelize(1 to 10).map(x =&gt; double(x)).collect().reduceLeft(_+_)&amp;#010;                 ^&amp;#010;&amp;#010;scala&gt;      | _result_1542830620484: Int = 1&amp;#010;&amp;#010;scala&gt; </li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; v: Int = 7&amp;#010;&amp;#010;scala&gt; getV: ()Int&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:19: error: not found: value sc&amp;#010;       val res1 = sc.parallelize(1 to 10).map(x =&gt; getV()).collect().reduceLeft(_+_)&amp;#010;                  ^&amp;#010;&amp;#010;scala&gt; v: Int = 10&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:19: error: not found: value sc&amp;#010;       val res2</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; array: Array[Int] = Array(0, 0, 0, 0, 0)&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:19: error: not found: value sc&amp;#010;       val broadcastArray = sc.broadcast(array)&amp;#010;                            ^&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:17: error: not found: value sc&amp;#010;       val res1 = sc.parallelize(0 to 4).map(x =&gt; broadcastArray.value(x</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:17: error: not found: value sc&amp;#010;       var file = sc.textFile("/var/lib/jenkins/workspace/spark/repl/target/tmp/spark-5875eb70-3ca1-473e-bf00-9be9fc4fc97b/input").cache()&amp;#010;                  ^&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:17: error: not found: value file&amp;#010;       val res1 = file.count()&amp;#010;            </li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; v: Int = 7&amp;#010;&amp;#010;scala&gt; getV: ()Int&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:19: error: not found: value sc&amp;#010;       val res1 = sc.parallelize(1 to 10).map(x =&gt; getV()).collect().reduceLeft(_+_)&amp;#010;                  ^&amp;#010;&amp;#010;scala&gt; v: Int = 10&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:19: error: not found: value sc&amp;#010;       val res2</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:17: error: not found: value spark&amp;#010;       import spark.implicits._&amp;#010;              ^&amp;#010;&amp;#010;scala&gt; defined class TestCaseClass&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:22: error: not found: value sc&amp;#010;       sc.parallelize(1 to 10).map(x =&gt; TestCaseClass(x)).toDF().collect()&amp;#010;       ^&amp;#010;&amp;#010;scala&gt; &amp;#010</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; import org.apache.spark.sql.functions._&amp;#010;&amp;#010;scala&gt; import org.apache.spark.sql.{Encoder, Encoders}&amp;#010;&amp;#010;scala&gt; import org.apache.spark.sql.expressions.Aggregator&amp;#010;&amp;#010;scala&gt; import org.apache.spark.sql.TypedColumn&amp;#010;&amp;#010;scala&gt;      |      |      |      |      |      |      | simpleSum: org.apache.sp</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; defined class TestClass&amp;#010;&amp;#010;scala&gt; t: TestClass = TestClass@41946622&amp;#010;&amp;#010;scala&gt; import t.testMethod&amp;#010;&amp;#010;scala&gt; defined class TestCaseClass&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:31: error: not found: value sc&amp;#010;       sc.parallelize(1 to 10).map(x =&gt; TestCaseClass(x)).collect()&amp;#010;       ^&amp;#010;&amp;#010;scala&gt; </li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; defined class Foo&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:28: error: not found: value sc&amp;#010;       val res = sc.parallelize((1 to 100).map(Foo), 10).collect()&amp;#010;                 ^&amp;#010;&amp;#010;scala&gt;      | _result_1542830633901: Int = 1&amp;#010;&amp;#010;scala&gt; </li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; defined class Foo&amp;#010;&amp;#010;scala&gt; list: List[(Int, Foo)] = List((1,Foo(1)), (1,Foo(2)))&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:28: error: not found: value sc&amp;#010;       val res = sc.parallelize(list).groupByKey().collect()&amp;#010;                 ^&amp;#010;&amp;#010;scala&gt;      | _result_1542830634911: Int = 1&amp;#010;&amp;#010;scala&gt; </li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; timeout: Int = 60000&amp;#010;&amp;#010;scala&gt; start: Long = 1542830636320&amp;#010;&amp;#010;scala&gt;      |      |      | &lt;console&gt;:31: error: not found: value sc&amp;#010;       while(sc.statusTracker.getExecutorInfos.size != 3 &amp;&amp;&amp;#010;             ^&amp;#010;&amp;#010;scala&gt;      |      | &amp;#010;scala&gt; import org.apache.spark.storage.StorageLevel._&amp;</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; defined class Click&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:33: error: not found: value spark&amp;#010;       spark.implicits.newProductSeqEncoder[Click]&amp;#010;       ^&amp;#010;&amp;#010;scala&gt;      | _result_1542830638439: Int = 1&amp;#010;&amp;#010;scala&gt; </li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMetho</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaApplySchemaSuite.tearDown(JavaApplySchemaSuite.java:60)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMetho</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaApplySchemaSuite.tearDown(JavaApplySchemaSuite.java:60)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMetho</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaApplySchemaSuite.tearDown(JavaApplySchemaSuite.java:60)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaBeanDeserializationSuite.tearDown(JavaBeanDeserializationSuite.java:41)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaBeanDeserializationSuite.tearDown(JavaBeanDeserializationSuite.java:41)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaColumnExpressionSuite.tearDown(JavaColumnExpressionSuite.java:46)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaColumnExpressionSuite.tearDown(JavaColumnExpressionSuite.java:46)
</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>RpcEnv has been stopped</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeC</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeC</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaSaveLoadSuite.tearDown(JavaSaveLoadSuite.java:76)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeC</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaSaveLoadSuite.tearDown(JavaSaveLoadSuite.java:76)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeC</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaUDAFSuite.tearDown(JavaUDAFSuite.java:42)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMetho</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaUDFSuite.tearDown(JavaUDFSuite.java:50)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMetho</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaUDFSuite.tearDown(JavaUDFSuite.java:50)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMetho</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaUDFSuite.tearDown(JavaUDFSuite.java:50)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMetho</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaUDFSuite.tearDown(JavaUDFSuite.java:50)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMetho</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaUDFSuite.tearDown(JavaUDFSuite.java:50)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMetho</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaUDFSuite.tearDown(JavaUDFSuite.java:50)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.tearDown(JavaDataStreamReaderWriterSuite.java:49)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.tearDown(JavaDataStreamReaderWriterSuite.java:49)
</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.sql.hive.HiveMetastoreLazyInitializationSuite.$anonfun$new$1(HiveMetastoreLazyInitializationSuite.scala:31)&amp;#010;org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)&amp;#010;org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)&amp;#010;org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.s</li></div><div><li>java.lang.ExceptionInInitializerError
	at org.apache.spark.sql.hive.JavaDataFrameSuite.setUp(JavaDataFrameSuite.java:50)
Caused by: java.lang.IllegalStateException: 
Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.hive.JavaDataFrameSuite.setUp(JavaDataFrameSuite.java:50)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>Could not initialize class org.apache.spark.sql.hive.test.TestHive$</li></div><div><li>Could not initialize class org.apache.spark.sql.hive.test.TestHive$</li></div><div><li>Could not initialize class org.apache.spark.sql.hive.test.TestHive$</li></div><div><li>Could not initialize class org.apache.spark.sql.hive.test.TestHive$</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(StreamingContext.scala:85)
org.apache.spark.streaming.api.java.JavaStreamingContext.&lt;init&gt;(JavaStreaming</li></div><div><li>java.lang.AssertionError
	at org.apache.spark.streaming.JavaReceiverAPISuite.testReceiver(JavaReceiverAPISuite.java:86)
</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(StreamingContext.scala:85)
org.apache.spark.streaming.api.java.JavaStreamingContext.&lt;init&gt;(JavaStreaming</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(StreamingContext.scala:85)
org.apache.spark.streaming.api.java.JavaStreamingContext.&lt;init&gt;(JavaStreaming</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(StreamingContext.scala:85)
org.apache.spark.streaming.api.java.JavaStreamingContext.&lt;init&gt;(JavaStreaming</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org/apache/zookeeper/AsyncCallback$MultiCallback</li></div><div><li>org/apache/zookeeper/AsyncCallback$MultiCallback</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Timeout getting response from the server</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;2&gt; but was:&lt;0&gt;</li></div><div><li>java.lang.AssertionError
	at org.apache.spark.ExecutorPluginSuite.testPluginClassDoesNotExist(ExecutorPluginSuite.java:72)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.ExecutorPluginSuite.testAddMultiplePlugins(ExecutorPluginSuite.java:91)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessor</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.ExecutorPluginSuite.testAddMultiplePlugins(ExecutorPluginSuite.java:91)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessor</li></div><div><li>SparkContext has been shutdown</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>SparkContext has been shutdown</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)
test.org.apache.spark.JavaSparkContextSuite.scalaSparkContext(JavaSparkContextSuite.java:56)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)
test.org.apache.spark.JavaSparkContextSuite.scalaSparkContext(JavaSparkContextSuite.java:56)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)
test.org.apache.spark.JavaSparkContextSuite.scalaSparkContext(JavaSparkContextSuite.java:56)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)
test.org.apache.spark.JavaSparkContextSuite.scalaSparkContext(JavaSparkContextSuite.java:56)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)
test.org.apache.spark.JavaSparkContextSuite.scalaSparkContext(JavaSparkContextSuite.java:56)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)
test.org.apache.spark.JavaSparkContextSuite.scalaSparkContext(JavaSparkContextSuite.java:56)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)
test.org.apache.spark.JavaSparkContextSuite.scalaSparkContext(JavaSparkContextSuite.java:56)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)
test.org.apache.spark.JavaSparkContextSuite.scalaSparkContext(JavaSparkContextSuite.java:56)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)
test.org.apache.spark.JavaSparkContextSuite.scalaSparkContext(JavaSparkContextSuite.java:56)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)
test.org.apache.spark.JavaSparkContextSuite.scalaSparkContext(JavaSparkContextSuite.java:56)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)
test.org.apache.spark.JavaSparkContextSuite.scalaSparkContext(JavaSparkContextSuite.java:56)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)
test.org.apache.spark.JavaSparkContextSuite.scalaSparkContext(JavaSparkContextSuite.java:56)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)
test.org.apache.spark.JavaSparkContextSuite.scalaSparkContext(JavaSparkContextSuite.java:56)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)
test.org.apache.spark.JavaSparkContextSuite.scalaSparkContext(JavaSparkContextSuite.java:56)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)
test.org.apache.spark.JavaSparkContextSuite.scalaSparkContext(JavaSparkContextSuite.java:56)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)
test.org.apache.spark.JavaSparkContextSuite.scalaSparkContext(JavaSparkContextSuite.java:56)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)
test.org.apache.spark.JavaSparkContextSuite.scalaSparkContext(JavaSparkContextSuite.java:56)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)
test.org.apache.spark.JavaSparkContextSuite.scalaSparkContext(JavaSparkContextSuite.java:56)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)
test.org.apache.spark.JavaSparkContextSuite.scalaSparkContext(JavaSparkContextSuite.java:56)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)
test.org.apache.spark.JavaSparkContextSuite.scalaSparkContext(JavaSparkContextSuite.java:56)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)
test.org.apache.spark.JavaSparkContextSuite.scalaSparkContext(JavaSparkContextSuite.java:56)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)
test.org.apache.spark.JavaSparkContextSuite.scalaSparkContext(JavaSparkContextSuite.java:56)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)
test.org.apache.spark.JavaSparkContextSuite.scalaSparkContext(JavaSparkContextSuite.java:56)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)
test.org.apache.spark.JavaSparkContextSuite.scalaSparkContext(JavaSparkContextSuite.java:56)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)
test.org.apache.spark.JavaSparkContextSuite.scalaSparkContext(JavaSparkContextSuite.java:56)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)
test.org.apache.spark.JavaSparkContextSuite.scalaSparkContext(JavaSparkContextSuite.java:56)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)
test.org.apache.spark.JavaSparkContextSuite.scalaSparkContext(JavaSparkContextSuite.java:56)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)
test.org.apache.spark.JavaSparkContextSuite.scalaSparkContext(JavaSparkContextSuite.java:56)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)
test.org.apache.spark.JavaSparkContextSuite.scalaSparkContext(JavaSparkContextSuite.java:56)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)
test.org.apache.spark.JavaSparkContextSuite.scalaSparkContext(JavaSparkContextSuite.java:56)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)
test.org.apache.spark.JavaSparkContextSuite.scalaSparkContext(JavaSparkContextSuite.java:56)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)
test.org.apache.spark.JavaSparkContextSuite.scalaSparkContext(JavaSparkContextSuite.java:56)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)
test.org.apache.spark.JavaSparkContextSuite.scalaSparkContext(JavaSparkContextSuite.java:56)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)
test.org.apache.spark.JavaSparkContextSuite.scalaSparkContext(JavaSparkContextSuite.java:56)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)
test.org.apache.spark.JavaSparkContextSuite.scalaSparkContext(JavaSparkContextSuite.java:56)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)
test.org.apache.spark.JavaSparkContextSuite.scalaSparkContext(JavaSparkContextSuite.java:56)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)
test.org.apache.spark.JavaSparkContextSuite.scalaSparkContext(JavaSparkContextSuite.java:56)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)
test.org.apache.spark.JavaSparkContextSuite.scalaSparkContext(JavaSparkContextSuite.java:56)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Metho</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Metho</li></div><div><li>SparkContext has been shutdown</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Metho</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)
test.org.apache.spark.JavaSparkContextSuite.scalaSparkContext(JavaSparkContextSuite.java:56)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</li></div><div><li>org/apache/zookeeper/AsyncCallback$MultiCallback</li></div><div><li>org/apache/zookeeper/AsyncCallback$MultiCallback</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>SparkContext has been shutdown</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(LocalSparkContext.scala:29)&amp;#010;org.apache.spark.graphx.GraphOpsSuite.withSpark(GraphOpsSu</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(LocalSparkContext.scala:29)&amp;#010;org.apache.spark.graphx.GraphOpsSuite.withSpark(GraphOpsSu</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>SparkContext has been shutdown</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(LocalSparkContext.scala:29)&amp;#010;org.apache.spark.graphx.PregelSuite.withSpark(PregelSuite.</li></div><div><li>SparkContext has been shutdown</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(LocalSparkContext.scala:29)&amp;#010;org.apache.spark.graphx.lib.PageRankSuite.withSpark(PageRa</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(LocalSparkContext.scala:29)&amp;#010;org.apache.spark.graphx.lib.SVDPlusPlusSuite.withSpark(SVD</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(LocalSparkContext.scala:29)&amp;#010;org.apache.spark.graphx.lib.ShortestPathsSuite.withSpark(S</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>SparkContext has been shutdown</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.LocalSparkContext.withSpark$(Loca</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.tuning.JavaCrossValidatorSuite.setUp(JavaCrossValidatorSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Error while instantiating 'org.apache.spark.sql.internal.SessionStateBuilder':</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.tuning.JavaCrossValidatorSuite.setUp(JavaCrossValidatorSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.tuning.JavaCrossValidatorSuite.setUp(JavaCrossValidatorSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
	at org.apache.spark.ml.source.libsvm.JavaLibSVMRelationSuite.tearDown(JavaLibSVMRelationSuite.java:57)
</li></div><div><li>Error while instantiating 'org.apache.spark.sql.internal.SessionStateBuilder':</li></div><div><li>Error while encoding: java.lang.NullPointerException
input[0, java.lang.Double, true].doubleValue AS value#3</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.stat.JavaSummarizerSuite.setUp(JavaSummarizerSuite.java:42)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.refl</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.tuning.JavaCrossValidatorSuite.setUp(JavaCrossValidatorSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
	at org.apache.spark.ml.util.JavaDefaultReadWriteSuite.tearDown(JavaDefaultReadWriteSuite.java:41)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:4</li></div><div><li>java.lang.NullPointerException
	at org.apache.spark.mllib.classification.JavaStreamingLogisticRegressionSuite.tearDown(JavaStreamingLogisticRegressionSuite.java:55)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.mllib.clustering.JavaLDASuite.setUp(JavaLDASuite.java:41)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.M</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.mllib.clustering.JavaLDASuite.setUp(JavaLDASuite.java:41)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.M</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.mllib.clustering.JavaLDASuite.setUp(JavaLDASuite.java:41)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.M</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.mllib.clustering.JavaLDASuite.setUp(JavaLDASuite.java:41)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.M</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.mllib.clustering.JavaLDASuite.setUp(JavaLDASuite.java:41)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.M</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.mllib.clustering.JavaLDASuite.setUp(JavaLDASuite.java:41)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.M</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.mllib.clustering.JavaLDASuite.setUp(JavaLDASuite.java:41)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.M</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.mllib.clustering.JavaLDASuite.setUp(JavaLDASuite.java:41)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.M</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.mllib.clustering.JavaLDASuite.setUp(JavaLDASuite.java:41)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.M</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.mllib.clustering.JavaLDASuite.setUp(JavaLDASuite.java:41)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at org.apache.spark.mllib.clustering.JavaStreamingKMeansSuite.tearDown(JavaStreamingKMeansSuite.java:54)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.stat.JavaKolmogorovSmirnovTestSuite.setUp(JavaKolmogorovSmirnovTestSuite.java:40)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.j</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.mllib.clustering.JavaLDASuite.setUp(JavaLDASuite.java:41)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.M</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.mllib.clustering.JavaLDASuite.setUp(JavaLDASuite.java:41)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.M</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>SparkContext has been shutdown</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.stat.JavaKolmogorovSmirnovTestSuite.setUp(JavaKolmogorovSmirnovTestSuite.java:40)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.j</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.stat.JavaKolmogorovSmirnovTestSuite.setUp(JavaKolmogorovSmirnovTestSuite.java:40)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.j</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.stat.JavaKolmogorovSmirnovTestSuite.setUp(JavaKolmogorovSmirnovTestSuite.java:40)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.j</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.stat.JavaKolmogorovSmirnovTestSuite.setUp(JavaKolmogorovSmirnovTestSuite.java:40)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.j</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.stat.JavaKolmogorovSmirnovTestSuite.setUp(JavaKolmogorovSmirnovTestSuite.java:40)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.j</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.stat.JavaKolmogorovSmirnovTestSuite.setUp(JavaKolmogorovSmirnovTestSuite.java:40)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.j</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>SparkContext has been shutdown</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:4</li></div><div><li>java.lang.NullPointerException
	at org.apache.spark.mllib.regression.JavaStreamingLinearRegressionSuite.tearDown(JavaStreamingLinearRegressionSuite.java:54)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
	at org.apache.spark.mllib.stat.JavaStatisticsSuite.tearDown(JavaStatisticsSuite.java:65)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
	at org.apache.spark.mllib.stat.JavaStatisticsSuite.tearDown(JavaStatisticsSuite.java:65)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
	at org.apache.spark.mllib.stat.JavaStatisticsSuite.tearDown(JavaStatisticsSuite.java:65)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
	at org.apache.spark.mllib.stat.JavaStatisticsSuite.tearDown(JavaStatisticsSuite.java:65)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)&amp;#010;org.apache.spark.repl.Main$.createSparkSession(Main.scala:112)&amp;#010;&lt;init&gt;(&lt;console&gt;:15)&amp;#010;&lt;init&gt;(&lt;console&gt;:42)&amp;#</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)&amp;#010;org.apache.spark.repl.Main$.createSparkSession(Main.scala:118)&amp;#010;&lt;init&gt;(&lt;console&gt;:15)&amp;#010;&lt;init&gt;(&lt;console&gt;:42)&amp;#</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;Welcome to&amp;#010;      ____              __&amp;#010;     / __/__  ___ _____/ /__&amp;#010;    _\ \/ _ \/ _ `/ __/  '_/&amp;#010;   /___/ .__/\_,_/_/ /_/\_\   version 3.0.0-SNAPSHOT&amp;#010;      /_/&amp;#010;         &amp;#010;Using Scala version 2.12.7 (OpenJDK 64-Bit Server VM, Java 1.8.0_171)&amp;#010;Type in expressions to have them evaluated.&amp;#010;Type :hel</li></div><div><li>isContain was true Interpreter output contained 'Exception':&amp;#010;Welcome to&amp;#010;      ____              __&amp;#010;     / __/__  ___ _____/ /__&amp;#010;    _\ \/ _ \/ _ `/ __/  '_/&amp;#010;   /___/ .__/\_,_/_/ /_/\_\   version 3.0.0-SNAPSHOT&amp;#010;      /_/&amp;#010;         &amp;#010;Using Scala version 2.12.7 (OpenJDK 64-Bit Server VM, Java 1.8.0_171)&amp;#010;Type in expressions to have them evaluated.&amp;#010;Type :</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;java.lang.NoClassDefFoundError: org/spark_project/guava/cache/Weigher&amp;#010;  at org.apache.spark.network.shuffle.ExternalShuffleBlockHandler.&lt;init&gt;(ExternalShuffleBlockHandler.java:64)&amp;#010;  at org.apache.spark.deploy.ExternalShuffleService.newShuffleBlockHandler(ExternalShuffleService.scala:63)&amp;#010;  at org.apache.spark.deploy.Exter</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:17: error: not found: value sc&amp;#010;       val accum = sc.longAccumulator&amp;#010;                   ^&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:18: error: not found: value sc&amp;#010;       sc.parallelize(1 to 10).foreach(x =&gt; accum.add(x))&amp;#010;       ^&amp;#010;&lt;console&gt;:18: error: not found: value accum&amp;#010;       sc.parallelize(1 </li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; v: Int = 7&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:19: error: not found: value sc&amp;#010;       val res1 = sc.parallelize(1 to 10).map(x =&gt; v).collect().reduceLeft(_+_)&amp;#010;                  ^&amp;#010;&amp;#010;scala&gt; v: Int = 10&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:19: error: not found: value sc&amp;#010;       val res2 = sc.parallelize(1 to 10).map(x =&gt;</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt;      |      | defined class C&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:18: error: not found: value sc&amp;#010;       val res = sc.parallelize(1 to 10).map(x =&gt; (new C).foo).collect().reduceLeft(_+_)&amp;#010;                 ^&amp;#010;&amp;#010;scala&gt;      | _result_1542832462735: Int = 1&amp;#010;&amp;#010;scala&gt; </li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; double: (x: Int)Int&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:19: error: not found: value sc&amp;#010;       val res = sc.parallelize(1 to 10).map(x =&gt; double(x)).collect().reduceLeft(_+_)&amp;#010;                 ^&amp;#010;&amp;#010;scala&gt;      | _result_1542832463757: Int = 1&amp;#010;&amp;#010;scala&gt; </li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; v: Int = 7&amp;#010;&amp;#010;scala&gt; getV: ()Int&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:19: error: not found: value sc&amp;#010;       val res1 = sc.parallelize(1 to 10).map(x =&gt; getV()).collect().reduceLeft(_+_)&amp;#010;                  ^&amp;#010;&amp;#010;scala&gt; v: Int = 10&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:19: error: not found: value sc&amp;#010;       val res2</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; array: Array[Int] = Array(0, 0, 0, 0, 0)&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:19: error: not found: value sc&amp;#010;       val broadcastArray = sc.broadcast(array)&amp;#010;                            ^&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:17: error: not found: value sc&amp;#010;       val res1 = sc.parallelize(0 to 4).map(x =&gt; broadcastArray.value(x</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:17: error: not found: value sc&amp;#010;       var file = sc.textFile("/var/lib/jenkins/workspace/spark/repl/target/tmp/spark-9bf9bdea-5001-4e38-8b9e-2237e9040603/input").cache()&amp;#010;                  ^&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:17: error: not found: value file&amp;#010;       val res1 = file.count()&amp;#010;            </li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; v: Int = 7&amp;#010;&amp;#010;scala&gt; getV: ()Int&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:19: error: not found: value sc&amp;#010;       val res1 = sc.parallelize(1 to 10).map(x =&gt; getV()).collect().reduceLeft(_+_)&amp;#010;                  ^&amp;#010;&amp;#010;scala&gt; v: Int = 10&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:19: error: not found: value sc&amp;#010;       val res2</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:17: error: not found: value spark&amp;#010;       import spark.implicits._&amp;#010;              ^&amp;#010;&amp;#010;scala&gt; defined class TestCaseClass&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:22: error: not found: value sc&amp;#010;       sc.parallelize(1 to 10).map(x =&gt; TestCaseClass(x)).toDF().collect()&amp;#010;       ^&amp;#010;&amp;#010;scala&gt; &amp;#010</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; import org.apache.spark.sql.functions._&amp;#010;&amp;#010;scala&gt; import org.apache.spark.sql.{Encoder, Encoders}&amp;#010;&amp;#010;scala&gt; import org.apache.spark.sql.expressions.Aggregator&amp;#010;&amp;#010;scala&gt; import org.apache.spark.sql.TypedColumn&amp;#010;&amp;#010;scala&gt;      |      |      |      |      |      |      | simpleSum: org.apache.sp</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; defined class TestClass&amp;#010;&amp;#010;scala&gt; t: TestClass = TestClass@bad82a3&amp;#010;&amp;#010;scala&gt; import t.testMethod&amp;#010;&amp;#010;scala&gt; defined class TestCaseClass&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:31: error: not found: value sc&amp;#010;       sc.parallelize(1 to 10).map(x =&gt; TestCaseClass(x)).collect()&amp;#010;       ^&amp;#010;&amp;#010;scala&gt;  </li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; defined class Foo&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:28: error: not found: value sc&amp;#010;       val res = sc.parallelize((1 to 100).map(Foo), 10).collect()&amp;#010;                 ^&amp;#010;&amp;#010;scala&gt;      | _result_1542832476246: Int = 1&amp;#010;&amp;#010;scala&gt; </li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; defined class Foo&amp;#010;&amp;#010;scala&gt; list: List[(Int, Foo)] = List((1,Foo(1)), (1,Foo(2)))&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:28: error: not found: value sc&amp;#010;       val res = sc.parallelize(list).groupByKey().collect()&amp;#010;                 ^&amp;#010;&amp;#010;scala&gt;      | _result_1542832476710: Int = 1&amp;#010;&amp;#010;scala&gt; </li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; timeout: Int = 60000&amp;#010;&amp;#010;scala&gt; start: Long = 1542832478079&amp;#010;&amp;#010;scala&gt;      |      |      | &lt;console&gt;:31: error: not found: value sc&amp;#010;       while(sc.statusTracker.getExecutorInfos.size != 3 &amp;&amp;&amp;#010;             ^&amp;#010;&amp;#010;scala&gt;      |      | &amp;#010;scala&gt; import org.apache.spark.storage.StorageLevel._&amp;</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; defined class Click&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:33: error: not found: value spark&amp;#010;       spark.implicits.newProductSeqEncoder[Click]&amp;#010;       ^&amp;#010;&amp;#010;scala&gt;      | _result_1542832480253: Int = 1&amp;#010;&amp;#010;scala&gt; </li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaApplySchemaSuite.setUp(JavaApplySchemaSuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native </li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaApplySchemaSuite.setUp(JavaApplySchemaSuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native </li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaApplySchemaSuite.setUp(JavaApplySchemaSuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native </li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaApplySchemaSuite.setUp(JavaApplySchemaSuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native </li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaApplySchemaSuite.setUp(JavaApplySchemaSuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaApplySchemaSuite.tearDown(JavaApplySchemaSuite.java:60)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaApplySchemaSuite.setUp(JavaApplySchemaSuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaApplySchemaSuite.tearDown(JavaApplySchemaSuite.java:60)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaApplySchemaSuite.setUp(JavaApplySchemaSuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaApplySchemaSuite.tearDown(JavaApplySchemaSuite.java:60)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaBeanDeserializationSuite.tearDown(JavaBeanDeserializationSuite.java:41)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaBeanDeserializationSuite.tearDown(JavaBeanDeserializationSuite.java:41)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaApplySchemaSuite.setUp(JavaApplySchemaSuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native </li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaColumnExpressionSuite.tearDown(JavaColumnExpressionSuite.java:46)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaApplySchemaSuite.setUp(JavaApplySchemaSuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native </li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaColumnExpressionSuite.tearDown(JavaColumnExpressionSuite.java:46)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaApplySchemaSuite.setUp(JavaApplySchemaSuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native </li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaApplySchemaSuite.setUp(JavaApplySchemaSuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native </li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaApplySchemaSuite.setUp(JavaApplySchemaSuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native </li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaApplySchemaSuite.setUp(JavaApplySchemaSuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native </li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaApplySchemaSuite.setUp(JavaApplySchemaSuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native </li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaApplySchemaSuite.setUp(JavaApplySchemaSuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native </li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaApplySchemaSuite.setUp(JavaApplySchemaSuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native </li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaApplySchemaSuite.setUp(JavaApplySchemaSuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native </li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
test.org.apache.spark.sql.JavaApplySchemaSuite.setUp(JavaApplySchemaSuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native </li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaSaveLoadSuite.tearDown(JavaSaveLoadSuite.java:76)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaSaveLoadSuite.tearDown(JavaSaveLoadSuite.java:76)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaUDAFSuite.tearDown(JavaUDAFSuite.java:42)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaUDFSuite.tearDown(JavaUDFSuite.java:50)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaUDFSuite.tearDown(JavaUDFSuite.java:50)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaUDFSuite.tearDown(JavaUDFSuite.java:50)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaUDFSuite.tearDown(JavaUDFSuite.java:50)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaUDFSuite.tearDown(JavaUDFSuite.java:50)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaUDFSuite.tearDown(JavaUDFSuite.java:50)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.tearDown(JavaDataStreamReaderWriterSuite.java:49)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.tearDown(JavaDataStreamReaderWriterSuite.java:49)
</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.sql.hive.HiveMetastoreLazyInitializationSuite.$anonfun$new$1(HiveMetastoreLazyInitializationSuite.scala:31)&amp;#010;org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)&amp;#010;org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)&amp;#010;org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.s</li></div><div><li>java.lang.ExceptionInInitializerError
	at org.apache.spark.sql.hive.JavaDataFrameSuite.setUp(JavaDataFrameSuite.java:50)
Caused by: java.lang.IllegalStateException: 
Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.hive.JavaDataFrameSuite.setUp(JavaDataFrameSuite.java:50)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>Could not initialize class org.apache.spark.sql.hive.test.TestHive$</li></div><div><li>Could not initialize class org.apache.spark.sql.hive.test.TestHive$</li></div><div><li>Could not initialize class org.apache.spark.sql.hive.test.TestHive$</li></div><div><li>Could not initialize class org.apache.spark.sql.hive.test.TestHive$</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(StreamingContext.scala:85)
org.apache.spark.streaming.api.java.JavaStreamingContext.&lt;init&gt;(JavaStreaming</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(StreamingContext.scala:85)
org.apache.spark.streaming.api.java.JavaStreamingContext.&lt;init&gt;(JavaStreaming</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(StreamingContext.scala:85)
org.apache.spark.streaming.api.java.JavaStreamingContext.&lt;init&gt;(JavaStreaming</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(StreamingContext.scala:85)
org.apache.spark.streaming.api.java.JavaStreamingContext.&lt;init&gt;(JavaStreaming</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(StreamingContext.scala:85)
org.apache.spark.streaming.api.java.JavaStreamingContext.&lt;init&gt;(JavaStreaming</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.spark.sql.execution.ui.SQLAppStatusListenerSuite.driver side SQL metrics</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.spark.network.ChunkFetchIntegrationSuite.fetchBothChunks</div></li><li><div>org.apache.spark.network.sasl.SparkSaslSuite.testFileRegionEncryption</div></li><li><div>org.apache.spark.ExecutorPluginSuite.testPluginShutdownWithException</div></li><li><div>org.apache.spark.JavaJdbcRDDSuite.testJavaJdbcRDD</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.leftOuterJoin</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.foldReduce</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.mapsFromPairsToPairs</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.mapsFromPairsToPairs</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.flatMap</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.flatMap</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.foreach</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.foreach</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.map</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.map</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.zip</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.zip</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.keyBy</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.keyBy</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.groupBy</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.groupBy</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.mapPartitions</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.mapPartitions</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.foldByKey</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.foldByKey</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.mapOnPairRDD</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.mapOnPairRDD</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.sequenceFile</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.sequenceFile</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.collectPartitions</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.collectPartitions</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.reduceByKey</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.reduceByKey</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.foreachWithAnonymousClass</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.foreachWithAnonymousClass</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.collectAsMapWithIntArrayValues</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.collectAsMapWithIntArrayValues</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.zipPartitions</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.zipPartitions</div></li><li><div>test.org.apache.spark.JavaAPISuite.groupByOnPairRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.groupByOnPairRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.binaryFilesCaching</div></li><li><div>test.org.apache.spark.JavaAPISuite.binaryFilesCaching</div></li><li><div>test.org.apache.spark.JavaAPISuite.sparkContextUnion</div></li><li><div>test.org.apache.spark.JavaAPISuite.sparkContextUnion</div></li><li><div>test.org.apache.spark.JavaAPISuite.checkpointAndComputation</div></li><li><div>test.org.apache.spark.JavaAPISuite.checkpointAndComputation</div></li><li><div>test.org.apache.spark.JavaAPISuite.leftOuterJoin</div></li><li><div>test.org.apache.spark.JavaAPISuite.leftOuterJoin</div></li><li><div>test.org.apache.spark.JavaAPISuite.keyByOnPairRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.keyByOnPairRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.getNumPartitions</div></li><li><div>test.org.apache.spark.JavaAPISuite.getNumPartitions</div></li><li><div>test.org.apache.spark.JavaAPISuite.wholeTextFiles</div></li><li><div>test.org.apache.spark.JavaAPISuite.wholeTextFiles</div></li><li><div>test.org.apache.spark.JavaAPISuite.binaryFiles</div></li><li><div>test.org.apache.spark.JavaAPISuite.binaryFiles</div></li><li><div>test.org.apache.spark.JavaAPISuite.foldReduce</div></li><li><div>test.org.apache.spark.JavaAPISuite.foldReduce</div></li><li><div>test.org.apache.spark.JavaAPISuite.writeWithNewAPIHadoopFile</div></li><li><div>test.org.apache.spark.JavaAPISuite.writeWithNewAPIHadoopFile</div></li><li><div>test.org.apache.spark.JavaAPISuite.hadoopFile</div></li><li><div>test.org.apache.spark.JavaAPISuite.hadoopFile</div></li><li><div>test.org.apache.spark.JavaAPISuite.lookup</div></li><li><div>test.org.apache.spark.JavaAPISuite.lookup</div></li><li><div>test.org.apache.spark.JavaAPISuite.countAsync</div></li><li><div>test.org.apache.spark.JavaAPISuite.countAsync</div></li><li><div>test.org.apache.spark.JavaAPISuite.textFiles</div></li><li><div>test.org.apache.spark.JavaAPISuite.textFiles</div></li><li><div>test.org.apache.spark.JavaAPISuite.binaryRecords</div></li><li><div>test.org.apache.spark.JavaAPISuite.toLocalIterator</div></li><li><div>test.org.apache.spark.JavaAPISuite.repartitionAndSortWithinPartitions</div></li><li><div>test.org.apache.spark.JavaAPISuite.repartitionAndSortWithinPartitions</div></li><li><div>test.org.apache.spark.JavaAPISuite.reduce</div></li><li><div>test.org.apache.spark.JavaAPISuite.reduce</div></li><li><div>test.org.apache.spark.JavaAPISuite.sample</div></li><li><div>test.org.apache.spark.JavaAPISuite.sample</div></li><li><div>test.org.apache.spark.JavaAPISuite.sortBy</div></li><li><div>test.org.apache.spark.JavaAPISuite.sortBy</div></li><li><div>test.org.apache.spark.JavaAPISuite.mapsFromPairsToPairs</div></li><li><div>test.org.apache.spark.JavaAPISuite.mapsFromPairsToPairs</div></li><li><div>test.org.apache.spark.JavaAPISuite.flatMap</div></li><li><div>test.org.apache.spark.JavaAPISuite.flatMap</div></li><li><div>test.org.apache.spark.JavaAPISuite.cogroup3</div></li><li><div>test.org.apache.spark.JavaAPISuite.cogroup3</div></li><li><div>test.org.apache.spark.JavaAPISuite.cogroup4</div></li><li><div>test.org.apache.spark.JavaAPISuite.cogroup4</div></li><li><div>test.org.apache.spark.JavaAPISuite.randomSplit</div></li><li><div>test.org.apache.spark.JavaAPISuite.randomSplit</div></li><li><div>test.org.apache.spark.JavaAPISuite.persist</div></li><li><div>test.org.apache.spark.JavaAPISuite.persist</div></li><li><div>test.org.apache.spark.JavaAPISuite.foreach</div></li><li><div>test.org.apache.spark.JavaAPISuite.foreach</div></li><li><div>test.org.apache.spark.JavaAPISuite.hadoopFileCompressed</div></li><li><div>test.org.apache.spark.JavaAPISuite.hadoopFileCompressed</div></li><li><div>test.org.apache.spark.JavaAPISuite.textFilesCompressed</div></li><li><div>test.org.apache.spark.JavaAPISuite.textFilesCompressed</div></li><li><div>test.org.apache.spark.JavaAPISuite.testAsyncActionCancellation</div></li><li><div>test.org.apache.spark.JavaAPISuite.testAsyncActionCancellation</div></li><li><div>test.org.apache.spark.JavaAPISuite.checkpointAndRestore</div></li><li><div>test.org.apache.spark.JavaAPISuite.checkpointAndRestore</div></li><li><div>test.org.apache.spark.JavaAPISuite.sortByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.sortByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.aggregateByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.aggregateByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.map</div></li><li><div>test.org.apache.spark.JavaAPISuite.map</div></li><li><div>test.org.apache.spark.JavaAPISuite.max</div></li><li><div>test.org.apache.spark.JavaAPISuite.max</div></li><li><div>test.org.apache.spark.JavaAPISuite.min</div></li><li><div>test.org.apache.spark.JavaAPISuite.min</div></li><li><div>test.org.apache.spark.JavaAPISuite.top</div></li><li><div>test.org.apache.spark.JavaAPISuite.top</div></li><li><div>test.org.apache.spark.JavaAPISuite.zip</div></li><li><div>test.org.apache.spark.JavaAPISuite.zip</div></li><li><div>test.org.apache.spark.JavaAPISuite.fold</div></li><li><div>test.org.apache.spark.JavaAPISuite.fold</div></li><li><div>test.org.apache.spark.JavaAPISuite.glom</div></li><li><div>test.org.apache.spark.JavaAPISuite.glom</div></li><li><div>test.org.apache.spark.JavaAPISuite.take</div></li><li><div>test.org.apache.spark.JavaAPISuite.take</div></li><li><div>test.org.apache.spark.JavaAPISuite.javaDoubleRDDHistoGram</div></li><li><div>test.org.apache.spark.JavaAPISuite.javaDoubleRDDHistoGram</div></li><li><div>test.org.apache.spark.JavaAPISuite.collectUnderlyingScalaRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.collectUnderlyingScalaRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.keyBy</div></li><li><div>test.org.apache.spark.JavaAPISuite.mapPartitionsWithIndex</div></li><li><div>test.org.apache.spark.JavaAPISuite.mapPartitionsWithIndex</div></li><li><div>test.org.apache.spark.JavaAPISuite.sampleByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.sampleByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.intersection</div></li><li><div>test.org.apache.spark.JavaAPISuite.intersection</div></li><li><div>test.org.apache.spark.JavaAPISuite.aggregate</div></li><li><div>test.org.apache.spark.JavaAPISuite.aggregate</div></li><li><div>test.org.apache.spark.JavaAPISuite.cartesian</div></li><li><div>test.org.apache.spark.JavaAPISuite.cartesian</div></li><li><div>test.org.apache.spark.JavaAPISuite.countApproxDistinctByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.countApproxDistinctByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.readWithNewAPIHadoopFile</div></li><li><div>test.org.apache.spark.JavaAPISuite.readWithNewAPIHadoopFile</div></li><li><div>test.org.apache.spark.JavaAPISuite.testRegisterKryoClasses</div></li><li><div>test.org.apache.spark.JavaAPISuite.testRegisterKryoClasses</div></li><li><div>test.org.apache.spark.JavaAPISuite.groupBy</div></li><li><div>test.org.apache.spark.JavaAPISuite.groupBy</div></li><li><div>test.org.apache.spark.JavaAPISuite.sampleByKeyExact</div></li><li><div>test.org.apache.spark.JavaAPISuite.sampleByKeyExact</div></li><li><div>test.org.apache.spark.JavaAPISuite.mapPartitions</div></li><li><div>test.org.apache.spark.JavaAPISuite.mapPartitions</div></li><li><div>test.org.apache.spark.JavaAPISuite.takeOrdered</div></li><li><div>test.org.apache.spark.JavaAPISuite.takeOrdered</div></li><li><div>test.org.apache.spark.JavaAPISuite.foldByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.foldByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.objectFilesOfInts</div></li><li><div>test.org.apache.spark.JavaAPISuite.objectFilesOfInts</div></li><li><div>test.org.apache.spark.JavaAPISuite.treeAggregate</div></li><li><div>test.org.apache.spark.JavaAPISuite.treeAggregate</div></li><li><div>test.org.apache.spark.JavaAPISuite.testGetPersistentRDDs</div></li><li><div>test.org.apache.spark.JavaAPISuite.testGetPersistentRDDs</div></li><li><div>test.org.apache.spark.JavaAPISuite.approximateResults</div></li><li><div>test.org.apache.spark.JavaAPISuite.approximateResults</div></li><li><div>test.org.apache.spark.JavaAPISuite.treeReduce</div></li><li><div>test.org.apache.spark.JavaAPISuite.treeReduce</div></li><li><div>test.org.apache.spark.JavaAPISuite.collectAsMapAndSerialize</div></li><li><div>test.org.apache.spark.JavaAPISuite.collectAsMapAndSerialize</div></li><li><div>test.org.apache.spark.JavaAPISuite.countApproxDistinct</div></li><li><div>test.org.apache.spark.JavaAPISuite.countApproxDistinct</div></li><li><div>test.org.apache.spark.JavaAPISuite.javaDoubleRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.javaDoubleRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.mapOnPairRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.mapOnPairRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.testAsyncActionErrorWrapping</div></li><li><div>test.org.apache.spark.JavaAPISuite.testAsyncActionErrorWrapping</div></li><li><div>test.org.apache.spark.JavaAPISuite.naturalMax</div></li><li><div>test.org.apache.spark.JavaAPISuite.naturalMax</div></li><li><div>test.org.apache.spark.JavaAPISuite.naturalMin</div></li><li><div>test.org.apache.spark.JavaAPISuite.naturalMin</div></li><li><div>test.org.apache.spark.JavaAPISuite.sequenceFile</div></li><li><div>test.org.apache.spark.JavaAPISuite.collectPartitions</div></li><li><div>test.org.apache.spark.JavaAPISuite.collectPartitions</div></li><li><div>test.org.apache.spark.JavaAPISuite.cogroup</div></li><li><div>test.org.apache.spark.JavaAPISuite.cogroup</div></li><li><div>test.org.apache.spark.JavaAPISuite.reduceByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.reduceByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.repartition</div></li><li><div>test.org.apache.spark.JavaAPISuite.repartition</div></li><li><div>test.org.apache.spark.JavaAPISuite.iterator</div></li><li><div>test.org.apache.spark.JavaAPISuite.iterator</div></li><li><div>test.org.apache.spark.JavaAPISuite.emptyRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.emptyRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.zipWithIndex</div></li><li><div>test.org.apache.spark.JavaAPISuite.zipWithIndex</div></li><li><div>test.org.apache.spark.JavaAPISuite.foreachPartition</div></li><li><div>test.org.apache.spark.JavaAPISuite.foreachPartition</div></li><li><div>test.org.apache.spark.JavaAPISuite.combineByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.combineByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.takeAsync</div></li><li><div>test.org.apache.spark.JavaAPISuite.takeAsync</div></li><li><div>test.org.apache.spark.JavaAPISuite.collectAsMapWithIntArrayValues</div></li><li><div>test.org.apache.spark.JavaAPISuite.collectAsMapWithIntArrayValues</div></li><li><div>test.org.apache.spark.JavaAPISuite.objectFilesOfComplexTypes</div></li><li><div>test.org.apache.spark.JavaAPISuite.objectFilesOfComplexTypes</div></li><li><div>test.org.apache.spark.JavaAPISuite.zipWithUniqueId</div></li><li><div>test.org.apache.spark.JavaAPISuite.zipWithUniqueId</div></li><li><div>test.org.apache.spark.JavaAPISuite.collectAsync</div></li><li><div>test.org.apache.spark.JavaAPISuite.collectAsync</div></li><li><div>test.org.apache.spark.JavaAPISuite.foreachAsync</div></li><li><div>test.org.apache.spark.JavaAPISuite.foreachAsync</div></li><li><div>test.org.apache.spark.JavaAPISuite.zipPartitions</div></li><li><div>test.org.apache.spark.JavaAPISuite.zipPartitions</div></li><li><div>test.org.apache.spark.JavaAPISuite.reduceOnJavaDoubleRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.reduceOnJavaDoubleRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.isEmpty</div></li><li><div>test.org.apache.spark.JavaAPISuite.isEmpty</div></li><li><div>test.org.apache.spark.JavaSparkContextSuite.javaSparkContext</div></li><li><div>test.org.apache.spark.JavaSparkContextSuite.scalaSparkContext</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single BooleanType with seed -1709147623185220531</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single ByteType with seed -3919555143023375607</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single ShortType with seed 8561384662913348374</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single IntegerType with seed 304026297574910763</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single LongType with seed -580739969019513000</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single FloatType with seed 2279282329064842619</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single DoubleType with seed 4674024592659163352</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single DecimalType(8,0) with seed 3767101550863559382</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single DecimalType(8,4) with seed 9909692906609911</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single DecimalType(16,0) with seed -8576487407135221635</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single DecimalType(16,11) with seed -4104648621375032742</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single DecimalType(38,0) with seed 2909629238540037729</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single DecimalType(38,38) with seed -3501252253047424307</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single StringType with seed 2041645561532327328</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single BinaryType with seed 4653446803325427556</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.flat schema struct&lt;col_0:decimal(8,4),col_1:bigint,col_2:double,col_3:binary,col_4:decimal(38,38)&gt; with seed -7205048209896592561</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.flat schema struct&lt;col_0:decimal(16,11),col_1:decimal(16,0),col_2:int,col_3:binary,col_4:decimal(8,0)&gt; with seed -6213368686859328797</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.flat schema struct&lt;col_0:decimal(16,11),col_1:decimal(8,0),col_2:string,col_3:int,col_4:decimal(16,0)&gt; with seed 4087558098434540409</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.flat schema struct&lt;col_0:binary,col_1:decimal(8,0),col_2:boolean,col_3:decimal(38,38),col_4:bigint&gt; with seed 8296462836656940639</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.flat schema struct&lt;col_0:decimal(38,0),col_1:decimal(16,0),col_2:boolean,col_3:decimal(8,4),col_4:decimal(38,0)&gt; with seed 3192922614101055592</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.nested schema struct&lt;col_0:tinyint,col_1:decimal(38,0),col_2:binary,col_3:struct&lt;col_0:struct&lt;col_0:struct&lt;col_0:struct&lt;col_0:smallint&gt;&gt;,col_1:bigint,col_2:struct&lt;col_0:struct&lt;col_0:array&lt;bigint&gt;&gt;&gt;,col_3:decimal(8,4)&gt;,col_1:tinyint,col_2:array&lt;smallint&gt;&gt;,col_4:struct&lt;col_0:float&gt;&gt; with seed 6838662997250839589</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.nested schema struct&lt;col_0:struct&lt;col_0:struct&lt;col_0:array&lt;decimal(8,0)&gt;&gt;,col_1:boolean,col_2:array&lt;decimal(38,0)&gt;,col_3:struct&lt;col_0:array&lt;decimal(8,4)&gt;&gt;,col_4:struct&lt;col_0:binary&gt;&gt;,col_1:double,col_2:array&lt;decimal(16,11)&gt;,col_3:array&lt;decimal(16,0)&gt;,col_4:struct&lt;col_0:array&lt;bigint&gt;&gt;,col_5:struct&lt;col_0:array&lt;bigint&gt;&gt;&gt; with seed -8949425111688185310</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.nested schema struct&lt;col_0:decimal(16,0),col_1:struct&lt;col_0:array&lt;decimal(38,38)&gt;&gt;,col_2:array&lt;decimal(8,0)&gt;,col_3:array&lt;decimal(38,38)&gt;,col_4:array&lt;decimal(38,0)&gt;,col_5:struct&lt;col_0:double&gt;,col_6:struct&lt;col_0:array&lt;binary&gt;&gt;,col_7:decimal(38,38),col_8:struct&lt;col_0:struct&lt;col_0:string&gt;&gt;,col_9:decimal(16,0)&gt; with seed 5045342886160651481</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.nested schema struct&lt;col_0:struct&lt;col_0:struct&lt;col_0:decimal(38,38),col_1:string,col_2:decimal(8,0)&gt;,col_1:struct&lt;col_0:double&gt;&gt;,col_1:struct&lt;col_0:smallint&gt;,col_2:struct&lt;col_0:struct&lt;col_0:array&lt;decimal(16,11)&gt;,col_1:array&lt;smallint&gt;&gt;,col_1:tinyint&gt;,col_3:struct&lt;col_0:decimal(8,0)&gt;,col_4:struct&lt;col_0:struct&lt;col_0:struct&lt;col_0:struct&lt;col_0:decimal(16,0)&gt;&gt;&gt;&gt;&gt; with seed 8617731073184928310</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.nested schema struct&lt;col_0:decimal(8,4),col_1:array&lt;float&gt;,col_2:decimal(8,0),col_3:array&lt;smallint&gt;,col_4:array&lt;smallint&gt;,col_5:struct&lt;col_0:struct&lt;col_0:array&lt;decimal(16,0)&gt;&gt;&gt;,col_6:struct&lt;col_0:struct&lt;col_0:array&lt;tinyint&gt;&gt;&gt;,col_7:double,col_8:boolean,col_9:array&lt;decimal(8,4)&gt;&gt; with seed -1895918017264487710</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.read int as string</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.read string as int</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.read float as double</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.read double as float</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.Handle unsupported input of record type</div></li><li><div>org.apache.spark.sql.avro.AvroFunctionsSuite.roundtrip in to_avro and from_avro - int and string</div></li><li><div>org.apache.spark.sql.avro.AvroFunctionsSuite.roundtrip in to_avro and from_avro - struct</div></li><li><div>org.apache.spark.sql.avro.AvroFunctionsSuite.handle invalid input in from_avro</div></li><li><div>org.apache.spark.sql.avro.AvroFunctionsSuite.roundtrip in to_avro and from_avro - array with null</div></li><li><div>org.apache.spark.graphx.EdgeRDDSuite.cache, getStorageLevel</div></li><li><div>org.apache.spark.graphx.EdgeRDDSuite.checkpointing</div></li><li><div>org.apache.spark.graphx.GraphLoaderSuite.GraphLoader.edgeListFile</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.joinVertices</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.collectNeighborIds</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.removeSelfEdges</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.filter</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.convertToCanonicalEdges</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.collectEdgesCycleDirectionOut</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.collectEdgesCycleDirectionIn</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.collectEdgesCycleDirectionEither</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.collectEdgesChainDirectionOut</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.collectEdgesChainDirectionIn</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.collectEdgesChainDirectionEither</div></li><li><div>org.apache.spark.graphx.GraphSuite.Graph.fromEdgeTuples</div></li><li><div>org.apache.spark.graphx.GraphSuite.Graph.fromEdges</div></li><li><div>org.apache.spark.graphx.GraphSuite.Graph.apply</div></li><li><div>org.apache.spark.graphx.GraphSuite.triplets</div></li><li><div>org.apache.spark.graphx.GraphSuite.partitionBy</div></li><li><div>org.apache.spark.graphx.GraphSuite.mapVertices</div></li><li><div>org.apache.spark.graphx.GraphSuite.mapVertices changing type with same erased type</div></li><li><div>org.apache.spark.graphx.GraphSuite.mapEdges</div></li><li><div>org.apache.spark.graphx.GraphSuite.mapTriplets</div></li><li><div>org.apache.spark.graphx.GraphSuite.reverse</div></li><li><div>org.apache.spark.graphx.GraphSuite.reverse with join elimination</div></li><li><div>org.apache.spark.graphx.GraphSuite.subgraph</div></li><li><div>org.apache.spark.graphx.GraphSuite.mask</div></li><li><div>org.apache.spark.graphx.GraphSuite.groupEdges</div></li><li><div>org.apache.spark.graphx.GraphSuite.aggregateMessages</div></li><li><div>org.apache.spark.graphx.GraphSuite.outerJoinVertices</div></li><li><div>org.apache.spark.graphx.GraphSuite.more edge partitions than vertex partitions</div></li><li><div>org.apache.spark.graphx.GraphSuite.checkpoint</div></li><li><div>org.apache.spark.graphx.GraphSuite.cache, getStorageLevel</div></li><li><div>org.apache.spark.graphx.GraphSuite.non-default number of edge partitions</div></li><li><div>org.apache.spark.graphx.GraphSuite.unpersist graph RDD</div></li><li><div>org.apache.spark.graphx.GraphSuite.SPARK-14219: pickRandomVertex</div></li><li><div>org.apache.spark.graphx.PregelSuite.1 iteration</div></li><li><div>org.apache.spark.graphx.PregelSuite.chain propagation</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.filter</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.mapValues</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.minus</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.minus with RDD[(VertexId, VD)]</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.minus with non-equal number of partitions</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.diff</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.diff with RDD[(VertexId, VD)]</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.diff vertices with non-equal number of partitions</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.leftJoin</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.leftJoin vertices with non-equal number of partitions</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.innerJoin</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.innerJoin vertices with the non-equal number of partitions</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.aggregateUsingIndex</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.mergeFunc</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.cache, getStorageLevel</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.checkpoint</div></li><li><div>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Grid Connected Components</div></li><li><div>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Reverse Grid Connected Components</div></li><li><div>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Chain Connected Components</div></li><li><div>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Reverse Chain Connected Components</div></li><li><div>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Connected Components on a Toy Connected Graph</div></li><li><div>org.apache.spark.graphx.lib.LabelPropagationSuite.Label Propagation</div></li><li><div>org.apache.spark.graphx.lib.PageRankSuite.Star PageRank</div></li><li><div>org.apache.spark.graphx.lib.PageRankSuite.Star PersonalPageRank</div></li><li><div>org.apache.spark.graphx.lib.PageRankSuite.Grid PageRank</div></li><li><div>org.apache.spark.graphx.lib.PageRankSuite.Chain PageRank</div></li><li><div>org.apache.spark.graphx.lib.PageRankSuite.Chain PersonalizedPageRank</div></li><li><div>org.apache.spark.graphx.lib.PageRankSuite.Loop with source PageRank</div></li><li><div>org.apache.spark.graphx.lib.PageRankSuite.Loop with sink PageRank</div></li><li><div>org.apache.spark.graphx.lib.SVDPlusPlusSuite.Test SVD++ with mean square error on training set</div></li><li><div>org.apache.spark.graphx.lib.ShortestPathsSuite.Shortest Path Computations</div></li><li><div>org.apache.spark.graphx.lib.StronglyConnectedComponentsSuite.Island Strongly Connected Components</div></li><li><div>org.apache.spark.graphx.lib.StronglyConnectedComponentsSuite.Cycle Strongly Connected Components</div></li><li><div>org.apache.spark.graphx.lib.StronglyConnectedComponentsSuite.2 Cycle Strongly Connected Components</div></li><li><div>org.apache.spark.graphx.lib.TriangleCountSuite.Count a single triangle</div></li><li><div>org.apache.spark.graphx.lib.TriangleCountSuite.Count two triangles</div></li><li><div>org.apache.spark.graphx.lib.TriangleCountSuite.Count two triangles with bi-directed edges</div></li><li><div>org.apache.spark.graphx.lib.TriangleCountSuite.Count a single triangle with duplicate edges</div></li><li><div>org.apache.spark.graphx.util.GraphGeneratorsSuite.GraphGenerators.logNormalGraph</div></li><li><div>org.apache.spark.graphx.util.GraphGeneratorsSuite.SPARK-5064 GraphGenerators.rmatGraph numEdges upper bound</div></li><li><div>org.apache.spark.graphx.util.PeriodicGraphCheckpointerSuite.Persisting</div></li><li><div>org.apache.spark.graphx.util.PeriodicGraphCheckpointerSuite.Checkpointing</div></li><li><div>org.apache.spark.ml.JavaPipelineSuite.pipeline</div></li><li><div>org.apache.spark.ml.JavaPipelineSuite.pipeline</div></li><li><div>org.apache.spark.ml.classification.JavaDecisionTreeClassifierSuite.runDT</div></li><li><div>org.apache.spark.ml.classification.JavaDecisionTreeClassifierSuite.runDT</div></li><li><div>org.apache.spark.ml.classification.JavaGBTClassifierSuite.runDT</div></li><li><div>org.apache.spark.ml.classification.JavaGBTClassifierSuite.runDT</div></li><li><div>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionWithSetters</div></li><li><div>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionWithSetters</div></li><li><div>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionTrainingSummary</div></li><li><div>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionTrainingSummary</div></li><li><div>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionPredictorClassifierMethods</div></li><li><div>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionPredictorClassifierMethods</div></li><li><div>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionDefaultParams</div></li><li><div>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionDefaultParams</div></li><li><div>org.apache.spark.ml.classification.JavaMultilayerPerceptronClassifierSuite.testMLPC</div></li><li><div>org.apache.spark.ml.classification.JavaMultilayerPerceptronClassifierSuite.testMLPC</div></li><li><div>org.apache.spark.ml.classification.JavaNaiveBayesSuite.testNaiveBayes</div></li><li><div>org.apache.spark.ml.classification.JavaNaiveBayesSuite.testNaiveBayes</div></li><li><div>org.apache.spark.ml.classification.JavaNaiveBayesSuite.naiveBayesDefaultParams</div></li><li><div>org.apache.spark.ml.classification.JavaNaiveBayesSuite.naiveBayesDefaultParams</div></li><li><div>org.apache.spark.ml.classification.JavaOneVsRestSuite.oneVsRestDefaultParams</div></li><li><div>org.apache.spark.ml.classification.JavaOneVsRestSuite.oneVsRestDefaultParams</div></li><li><div>org.apache.spark.ml.classification.JavaRandomForestClassifierSuite.runDT</div></li><li><div>org.apache.spark.ml.classification.JavaRandomForestClassifierSuite.runDT</div></li><li><div>org.apache.spark.ml.clustering.JavaKMeansSuite.fitAndTransform</div></li><li><div>org.apache.spark.ml.clustering.JavaKMeansSuite.fitAndTransform</div></li><li><div>org.apache.spark.ml.feature.JavaBucketizerSuite.bucketizerTest</div></li><li><div>org.apache.spark.ml.feature.JavaBucketizerSuite.bucketizerTest</div></li><li><div>org.apache.spark.ml.feature.JavaBucketizerSuite.bucketizerMultipleColumnsTest</div></li><li><div>org.apache.spark.ml.feature.JavaBucketizerSuite.bucketizerMultipleColumnsTest</div></li><li><div>org.apache.spark.ml.feature.JavaDCTSuite.javaCompatibilityTest</div></li><li><div>org.apache.spark.ml.feature.JavaDCTSuite.javaCompatibilityTest</div></li><li><div>org.apache.spark.ml.feature.JavaHashingTFSuite.hashingTF</div></li><li><div>org.apache.spark.ml.feature.JavaHashingTFSuite.hashingTF</div></li><li><div>org.apache.spark.ml.feature.JavaNormalizerSuite.normalizer</div></li><li><div>org.apache.spark.ml.feature.JavaNormalizerSuite.normalizer</div></li><li><div>org.apache.spark.ml.feature.JavaPCASuite.testPCA</div></li><li><div>org.apache.spark.ml.feature.JavaPCASuite.testPCA</div></li><li><div>org.apache.spark.ml.feature.JavaPolynomialExpansionSuite.polynomialExpansionTest</div></li><li><div>org.apache.spark.ml.feature.JavaPolynomialExpansionSuite.polynomialExpansionTest</div></li><li><div>org.apache.spark.ml.feature.JavaStandardScalerSuite.standardScaler</div></li><li><div>org.apache.spark.ml.feature.JavaStandardScalerSuite.standardScaler</div></li><li><div>org.apache.spark.ml.feature.JavaStopWordsRemoverSuite.javaCompatibilityTest</div></li><li><div>org.apache.spark.ml.feature.JavaStopWordsRemoverSuite.javaCompatibilityTest</div></li><li><div>org.apache.spark.ml.feature.JavaStringIndexerSuite.testStringIndexer</div></li><li><div>org.apache.spark.ml.feature.JavaStringIndexerSuite.testStringIndexer</div></li><li><div>org.apache.spark.ml.feature.JavaTokenizerSuite.regexTokenizer</div></li><li><div>org.apache.spark.ml.feature.JavaTokenizerSuite.regexTokenizer</div></li><li><div>org.apache.spark.ml.feature.JavaVectorAssemblerSuite.testVectorAssembler</div></li><li><div>org.apache.spark.ml.feature.JavaVectorAssemblerSuite.testVectorAssembler</div></li><li><div>org.apache.spark.ml.feature.JavaVectorIndexerSuite.vectorIndexerAPI</div></li><li><div>org.apache.spark.ml.feature.JavaVectorIndexerSuite.vectorIndexerAPI</div></li><li><div>org.apache.spark.ml.feature.JavaVectorSlicerSuite.vectorSlice</div></li><li><div>org.apache.spark.ml.feature.JavaVectorSlicerSuite.vectorSlice</div></li><li><div>org.apache.spark.ml.feature.JavaWord2VecSuite.testJavaWord2Vec</div></li><li><div>org.apache.spark.ml.feature.JavaWord2VecSuite.testJavaWord2Vec</div></li><li><div>org.apache.spark.ml.regression.JavaDecisionTreeRegressorSuite.runDT</div></li><li><div>org.apache.spark.ml.regression.JavaDecisionTreeRegressorSuite.runDT</div></li><li><div>org.apache.spark.ml.regression.JavaGBTRegressorSuite.runDT</div></li><li><div>org.apache.spark.ml.regression.JavaGBTRegressorSuite.runDT</div></li><li><div>org.apache.spark.ml.regression.JavaLinearRegressionSuite.linearRegressionDefaultParams</div></li><li><div>org.apache.spark.ml.regression.JavaLinearRegressionSuite.linearRegressionDefaultParams</div></li><li><div>org.apache.spark.ml.regression.JavaLinearRegressionSuite.linearRegressionWithSetters</div></li><li><div>org.apache.spark.ml.regression.JavaLinearRegressionSuite.linearRegressionWithSetters</div></li><li><div>org.apache.spark.ml.regression.JavaRandomForestRegressorSuite.runDT</div></li><li><div>org.apache.spark.ml.regression.JavaRandomForestRegressorSuite.runDT</div></li><li><div>org.apache.spark.ml.source.libsvm.JavaLibSVMRelationSuite.verifyLibSVMDF</div></li><li><div>org.apache.spark.ml.source.libsvm.JavaLibSVMRelationSuite.verifyLibSVMDF</div></li><li><div>org.apache.spark.ml.stat.JavaKolmogorovSmirnovTestSuite.testKSTestNamedDistribution</div></li><li><div>org.apache.spark.ml.stat.JavaKolmogorovSmirnovTestSuite.testKSTestNamedDistribution</div></li><li><div>org.apache.spark.ml.stat.JavaKolmogorovSmirnovTestSuite.testKSTestCDF</div></li><li><div>org.apache.spark.ml.stat.JavaKolmogorovSmirnovTestSuite.testKSTestCDF</div></li><li><div>org.apache.spark.ml.stat.JavaSummarizerSuite.testSummarizer</div></li><li><div>org.apache.spark.ml.stat.JavaSummarizerSuite.testSummarizer</div></li><li><div>org.apache.spark.ml.tuning.JavaCrossValidatorSuite.crossValidationWithLogisticRegression</div></li><li><div>org.apache.spark.ml.tuning.JavaCrossValidatorSuite.crossValidationWithLogisticRegression</div></li><li><div>org.apache.spark.ml.util.JavaDefaultReadWriteSuite.testDefaultReadWrite</div></li><li><div>org.apache.spark.ml.util.JavaDefaultReadWriteSuite.testDefaultReadWrite</div></li><li><div>org.apache.spark.mllib.classification.JavaLogisticRegressionSuite.runLRUsingConstructor</div></li><li><div>org.apache.spark.mllib.classification.JavaLogisticRegressionSuite.runLRUsingConstructor</div></li><li><div>org.apache.spark.mllib.classification.JavaLogisticRegressionSuite.runLRUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.classification.JavaLogisticRegressionSuite.runLRUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.testPredictJavaRDD</div></li><li><div>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.testPredictJavaRDD</div></li><li><div>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.runUsingConstructor</div></li><li><div>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.runUsingConstructor</div></li><li><div>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.runUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.runUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.testModelTypeSetters</div></li><li><div>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.testModelTypeSetters</div></li><li><div>org.apache.spark.mllib.classification.JavaSVMSuite.runSVMUsingConstructor</div></li><li><div>org.apache.spark.mllib.classification.JavaSVMSuite.runSVMUsingConstructor</div></li><li><div>org.apache.spark.mllib.classification.JavaSVMSuite.runSVMUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.classification.JavaSVMSuite.runSVMUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.classification.JavaStreamingLogisticRegressionSuite.javaAPI</div></li><li><div>org.apache.spark.mllib.classification.JavaStreamingLogisticRegressionSuite.javaAPI</div></li><li><div>org.apache.spark.mllib.clustering.JavaBisectingKMeansSuite.twoDimensionalData</div></li><li><div>org.apache.spark.mllib.clustering.JavaBisectingKMeansSuite.twoDimensionalData</div></li><li><div>org.apache.spark.mllib.clustering.JavaGaussianMixtureSuite.runGaussianMixture</div></li><li><div>org.apache.spark.mllib.clustering.JavaGaussianMixtureSuite.runGaussianMixture</div></li><li><div>org.apache.spark.mllib.clustering.JavaKMeansSuite.testPredictJavaRDD</div></li><li><div>org.apache.spark.mllib.clustering.JavaKMeansSuite.testPredictJavaRDD</div></li><li><div>org.apache.spark.mllib.clustering.JavaKMeansSuite.runKMeansUsingConstructor</div></li><li><div>org.apache.spark.mllib.clustering.JavaKMeansSuite.runKMeansUsingConstructor</div></li><li><div>org.apache.spark.mllib.clustering.JavaKMeansSuite.runKMeansUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.clustering.JavaKMeansSuite.runKMeansUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.clustering.JavaLDASuite.onlineOptimizerCompatibility</div></li><li><div>org.apache.spark.mllib.clustering.JavaLDASuite.onlineOptimizerCompatibility</div></li><li><div>org.apache.spark.mllib.clustering.JavaLDASuite.distributedLDAModel</div></li><li><div>org.apache.spark.mllib.clustering.JavaLDASuite.distributedLDAModel</div></li><li><div>org.apache.spark.mllib.clustering.JavaLDASuite.localLDAModel</div></li><li><div>org.apache.spark.mllib.clustering.JavaLDASuite.localLDAModel</div></li><li><div>org.apache.spark.mllib.clustering.JavaLDASuite.localLdaMethods</div></li><li><div>org.apache.spark.mllib.clustering.JavaLDASuite.localLdaMethods</div></li><li><div>org.apache.spark.mllib.clustering.JavaStreamingKMeansSuite.javaAPI</div></li><li><div>org.apache.spark.mllib.clustering.JavaStreamingKMeansSuite.javaAPI</div></li><li><div>org.apache.spark.mllib.evaluation.JavaRankingMetricsSuite.rankingMetrics</div></li><li><div>org.apache.spark.mllib.evaluation.JavaRankingMetricsSuite.rankingMetrics</div></li><li><div>org.apache.spark.mllib.feature.JavaTfIdfSuite.tfIdfMinimumDocumentFrequency</div></li><li><div>org.apache.spark.mllib.feature.JavaTfIdfSuite.tfIdfMinimumDocumentFrequency</div></li><li><div>org.apache.spark.mllib.feature.JavaTfIdfSuite.tfIdf</div></li><li><div>org.apache.spark.mllib.feature.JavaTfIdfSuite.tfIdf</div></li><li><div>org.apache.spark.mllib.feature.JavaWord2VecSuite.word2Vec</div></li><li><div>org.apache.spark.mllib.feature.JavaWord2VecSuite.word2Vec</div></li><li><div>org.apache.spark.mllib.fpm.JavaAssociationRulesSuite.runAssociationRules</div></li><li><div>org.apache.spark.mllib.fpm.JavaFPGrowthSuite.runFPGrowthSaveLoad</div></li><li><div>org.apache.spark.mllib.fpm.JavaFPGrowthSuite.runFPGrowthSaveLoad</div></li><li><div>org.apache.spark.mllib.fpm.JavaFPGrowthSuite.runFPGrowth</div></li><li><div>org.apache.spark.mllib.fpm.JavaFPGrowthSuite.runFPGrowth</div></li><li><div>org.apache.spark.mllib.fpm.JavaPrefixSpanSuite.runPrefixSpan</div></li><li><div>org.apache.spark.mllib.fpm.JavaPrefixSpanSuite.runPrefixSpan</div></li><li><div>org.apache.spark.mllib.fpm.JavaPrefixSpanSuite.runPrefixSpanSaveLoad</div></li><li><div>org.apache.spark.mllib.fpm.JavaPrefixSpanSuite.runPrefixSpanSaveLoad</div></li><li><div>org.apache.spark.mllib.linalg.distributed.JavaRowMatrixSuite.rowMatrixQRDecomposition</div></li><li><div>org.apache.spark.mllib.linalg.distributed.JavaRowMatrixSuite.rowMatrixQRDecomposition</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testNormalVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testNormalVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testArbitrary</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testArbitrary</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testLogNormalVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testLogNormalVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testExponentialVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testExponentialVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testUniformRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testUniformRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testRandomVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testRandomVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testGammaRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testGammaRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testUniformVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testUniformVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testPoissonRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testPoissonRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testNormalRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testNormalRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testPoissonVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testPoissonVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testGammaVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testGammaVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testExponentialRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testExponentialRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testLNormalRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testLNormalRDD</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runALSUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runALSUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSUsingConstructor</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSUsingConstructor</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runRecommend</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runRecommend</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSWithNegativeWeight</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSWithNegativeWeight</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runALSUsingConstructor</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runALSUsingConstructor</div></li><li><div>org.apache.spark.mllib.regression.JavaIsotonicRegressionSuite.testIsotonicRegressionJavaRDD</div></li><li><div>org.apache.spark.mllib.regression.JavaIsotonicRegressionSuite.testIsotonicRegressionJavaRDD</div></li><li><div>org.apache.spark.mllib.regression.JavaIsotonicRegressionSuite.testIsotonicRegressionPredictionsJavaRDD</div></li><li><div>org.apache.spark.mllib.regression.JavaIsotonicRegressionSuite.testIsotonicRegressionPredictionsJavaRDD</div></li><li><div>org.apache.spark.mllib.regression.JavaLassoSuite.runLassoUsingConstructor</div></li><li><div>org.apache.spark.mllib.regression.JavaLassoSuite.runLassoUsingConstructor</div></li><li><div>org.apache.spark.mllib.regression.JavaLassoSuite.runLassoUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.regression.JavaLassoSuite.runLassoUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.testPredictJavaRDD</div></li><li><div>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.testPredictJavaRDD</div></li><li><div>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.runLinearRegressionUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.runLinearRegressionUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.runLinearRegressionUsingConstructor</div></li><li><div>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.runLinearRegressionUsingConstructor</div></li><li><div>org.apache.spark.mllib.regression.JavaRidgeRegressionSuite.runRidgeRegressionUsingConstructor</div></li><li><div>org.apache.spark.mllib.regression.JavaRidgeRegressionSuite.runRidgeRegressionUsingConstructor</div></li><li><div>org.apache.spark.mllib.regression.JavaRidgeRegressionSuite.runRidgeRegressionUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.regression.JavaRidgeRegressionSuite.runRidgeRegressionUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.regression.JavaStreamingLinearRegressionSuite.javaAPI</div></li><li><div>org.apache.spark.mllib.regression.JavaStreamingLinearRegressionSuite.javaAPI</div></li><li><div>org.apache.spark.mllib.stat.JavaStatisticsSuite.testCorr</div></li><li><div>org.apache.spark.mllib.stat.JavaStatisticsSuite.testCorr</div></li><li><div>org.apache.spark.mllib.stat.JavaStatisticsSuite.chiSqTest</div></li><li><div>org.apache.spark.mllib.stat.JavaStatisticsSuite.chiSqTest</div></li><li><div>org.apache.spark.mllib.stat.JavaStatisticsSuite.streamingTest</div></li><li><div>org.apache.spark.mllib.stat.JavaStatisticsSuite.kolmogorovSmirnovTest</div></li><li><div>org.apache.spark.mllib.stat.JavaStatisticsSuite.kolmogorovSmirnovTest</div></li><li><div>org.apache.spark.mllib.tree.JavaDecisionTreeSuite.runDTUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.tree.JavaDecisionTreeSuite.runDTUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.tree.JavaDecisionTreeSuite.runDTUsingConstructor</div></li><li><div>org.apache.spark.mllib.tree.JavaDecisionTreeSuite.runDTUsingConstructor</div></li><li><div>org.apache.spark.mllib.util.JavaMLUtilsSuite.testConvertMatrixColumnsToAndFromML</div></li><li><div>org.apache.spark.mllib.util.JavaMLUtilsSuite.testConvertMatrixColumnsToAndFromML</div></li><li><div>org.apache.spark.mllib.util.JavaMLUtilsSuite.testConvertVectorColumnsToAndFromML</div></li><li><div>org.apache.spark.mllib.util.JavaMLUtilsSuite.testConvertVectorColumnsToAndFromML</div></li><li><div>org.apache.spark.repl.ReplSuite.SPARK-15236: use Hive catalog</div></li><li><div>org.apache.spark.repl.ReplSuite.SPARK-15236: use in-memory catalog</div></li><li><div>org.apache.spark.repl.ReplSuite.broadcast vars</div></li><li><div>org.apache.spark.repl.ReplSuite.line wrapper only initialized once when used as encoder outer scope</div></li><li><div>org.apache.spark.repl.ReplSuite.define case class and create Dataset together with paste mode</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.simple foreach with accumulator</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.external vars</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.external classes</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.external functions</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.external functions that access vars</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.broadcast vars</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.interacting with files</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.local-cluster mode</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.SPARK-2576 importing implicits</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.Datasets and encoders</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.SPARK-2632 importing a method from non serializable class and not using it.</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.collecting objects of class defined in repl</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.collecting objects of class defined in repl - shuffling</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.replicating blocks of object with class defined in repl</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.newProductSeqEncoder with REPL defined class</div></li><li><div>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationCount</div></li><li><div>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationCount</div></li><li><div>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationSumDouble</div></li><li><div>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationSumDouble</div></li><li><div>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationSumLong</div></li><li><div>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationSumLong</div></li><li><div>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationAverage</div></li><li><div>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationAverage</div></li><li><div>test.org.apache.spark.sql.JavaApplySchemaSuite.applySchema</div></li><li><div>test.org.apache.spark.sql.JavaApplySchemaSuite.applySchema</div></li><li><div>test.org.apache.spark.sql.JavaApplySchemaSuite.dataFrameRDDOperations</div></li><li><div>test.org.apache.spark.sql.JavaApplySchemaSuite.dataFrameRDDOperations</div></li><li><div>test.org.apache.spark.sql.JavaApplySchemaSuite.applySchemaToJSON</div></li><li><div>test.org.apache.spark.sql.JavaApplySchemaSuite.applySchemaToJSON</div></li><li><div>test.org.apache.spark.sql.JavaBeanDeserializationSuite.testBeanWithArrayFieldDeserialization</div></li><li><div>test.org.apache.spark.sql.JavaBeanDeserializationSuite.testBeanWithArrayFieldDeserialization</div></li><li><div>test.org.apache.spark.sql.JavaBeanDeserializationSuite.testBeanWithMapFieldsDeserialization</div></li><li><div>test.org.apache.spark.sql.JavaBeanDeserializationSuite.testBeanWithMapFieldsDeserialization</div></li><li><div>test.org.apache.spark.sql.JavaColumnExpressionSuite.isInCollectionCheckExceptionMessage</div></li><li><div>test.org.apache.spark.sql.JavaColumnExpressionSuite.isInCollectionCheckExceptionMessage</div></li><li><div>test.org.apache.spark.sql.JavaColumnExpressionSuite.isInCollectionWorksCorrectlyOnJava</div></li><li><div>test.org.apache.spark.sql.JavaColumnExpressionSuite.isInCollectionWorksCorrectlyOnJava</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testFormatAPI</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testTextAPI</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testJsonAPI</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testLoadAPI</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testOptionsAPI</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testSaveModeAPI</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testCsvAPI</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testParquetAPI</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testTextFileAPI</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCollectAndTake</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCollectAndTake</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testJsonRDDToDataFrame</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testJsonRDDToDataFrame</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testVarargMethods</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testVarargMethods</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testBeanWithoutGetter</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testBeanWithoutGetter</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateStructTypeFromList</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateStructTypeFromList</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testSampleBy</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testSampleBy</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCrosstab</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCrosstab</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testUDF</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testUDF</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFromFromList</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFromFromList</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCircularReferenceBean</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCircularReferenceBean</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testFrequentItems</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testFrequentItems</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testSampleByColumn</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testSampleByColumn</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testExecution</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testExecution</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testTextLoad</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testTextLoad</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.pivot</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.pivot</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testGenericLoad</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testGenericLoad</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCountMinSketch</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCountMinSketch</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.pivotColumnValues</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.pivotColumnValues</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFrameFromJavaBeans</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFrameFromJavaBeans</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCorrelation</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCorrelation</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testBloomFilter</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testBloomFilter</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCovariance</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCovariance</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFrameFromLocalJavaBeans</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFrameFromLocalJavaBeans</div></li><li><div>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationCount</div></li><li><div>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationCount</div></li><li><div>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationSumDouble</div></li><li><div>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationSumDouble</div></li><li><div>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationSumLong</div></li><li><div>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationSumLong</div></li><li><div>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationAnonClass</div></li><li><div>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationAnonClass</div></li><li><div>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationAverage</div></li><li><div>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationAverage</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testRuntimeNullabilityCheck</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testRuntimeNullabilityCheck</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean1</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean1</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean2</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean2</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean3</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean3</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testSerializeNull</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testSerializeNull</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testRandomSplit</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testRandomSplit</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testTypedFilterPreservingSchema</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testTypedFilterPreservingSchema</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJoin</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJoin</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testTake</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testTake</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testToLocalIterator</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testToLocalIterator</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testSpecificLists</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testSpecificLists</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testForeach</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testForeach</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJavaEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJavaEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testPrimitiveEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testPrimitiveEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testEmptyBean</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testEmptyBean</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testCommonOperation</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testCommonOperation</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testNullInTopLevelBean</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testNullInTopLevelBean</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testGroupBy</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testGroupBy</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testSetOperation</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testSetOperation</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testBeanWithEnum</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testBeanWithEnum</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testKryoEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testKryoEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.test</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.test</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJavaBeanEncoder2</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJavaBeanEncoder2</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testCollect</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testCollect</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testKryoEncoderErrorMessageForPrivateClass</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testKryoEncoderErrorMessageForPrivateClass</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJavaBeanEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJavaBeanEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testTupleEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testTupleEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testNestedTupleEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testNestedTupleEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testTupleEncoderSchema</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testTupleEncoderSchema</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testReduce</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testReduce</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testSelect</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testSelect</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJavaEncoderErrorMessageForPrivateClass</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJavaEncoderErrorMessageForPrivateClass</div></li><li><div>test.org.apache.spark.sql.JavaSaveLoadSuite.saveAndLoadWithSchema</div></li><li><div>test.org.apache.spark.sql.JavaSaveLoadSuite.saveAndLoadWithSchema</div></li><li><div>test.org.apache.spark.sql.JavaSaveLoadSuite.saveAndLoad</div></li><li><div>test.org.apache.spark.sql.JavaSaveLoadSuite.saveAndLoad</div></li><li><div>test.org.apache.spark.sql.JavaUDAFSuite.udf1Test</div></li><li><div>test.org.apache.spark.sql.JavaUDAFSuite.udf1Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf1Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf1Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf2Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf2Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf3Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf3Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf4Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf4Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf5Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf5Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf6Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf6Test</div></li><li><div>test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.testForeachBatchAPI</div></li><li><div>test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.testForeachBatchAPI</div></li><li><div>test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.testForeachAPI</div></li><li><div>test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.testForeachAPI</div></li><li><div>org.apache.spark.sql.hive.HiveMetastoreLazyInitializationSuite.lazily initialize Hive client</div></li><li><div>org.apache.spark.sql.hive.JavaDataFrameSuite.testUDAF</div></li><li><div>org.apache.spark.sql.hive.JavaDataFrameSuite.saveTableAndQueryIt</div></li><li><div>org.apache.spark.sql.hive.JavaMetastoreDataSourcesSuite.saveExternalTableAndQueryIt</div></li><li><div>org.apache.spark.sql.hive.JavaMetastoreDataSourcesSuite.saveTableAndQueryIt</div></li><li><div>org.apache.spark.sql.hive.JavaMetastoreDataSourcesSuite.saveExternalTableWithSchemaAndQueryIt</div></li><li><div>org.apache.spark.streaming.JavaMapWithStateSuite.testBasicFunction</div></li><li><div>org.apache.spark.streaming.JavaReceiverAPISuite.testReceiver</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testStreamingContextTransform</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testStreamingContextTransform</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testFlatMapValues</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testFlatMapValues</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testMapPartitions</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testMapPartitions</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairFilter</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairFilter</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testCombineByKey</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testCombineByKey</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testMap</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testMap</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairToNormalRDDTransform</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairToNormalRDDTransform</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairReduceByKey</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairReduceByKey</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairMap</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairMap</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testFlatMap</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testFlatMap</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testReduceByKeyAndWindowWithInverse</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testReduceByKeyAndWindowWithInverse</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testReduceByWindow</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testReduceByWindow</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairFlatMap</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairFlatMap</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairToPairFlatMapWithChangingTypes</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairToPairFlatMapWithChangingTypes</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairMapPartitions</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairMapPartitions</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testVariousTransform</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testVariousTransform</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testTransformWith</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testTransformWith</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testVariousTransformWith</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testVariousTransformWith</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testReduceByKeyAndWindow</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testReduceByKeyAndWindow</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairTransform</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairTransform</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testFilter</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testFilter</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairMap2</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairMap2</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testMapValues</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testMapValues</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testReduce</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testReduce</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testUpdateStateByKey</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testUpdateStateByKey</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testTransform</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testTransform</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testStreamingContextTransform</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testFlatMapValues</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testReduceByWindowWithInverse</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testMapPartitions</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairFilter</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairFilter</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testRepartitionFewerPartitions</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testRepartitionFewerPartitions</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCombineByKey</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCombineByKey</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testContextGetOrCreate</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testContextGetOrCreate</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testWindowWithSlideDuration</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testWindowWithSlideDuration</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testQueueStream</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testQueueStream</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCountByValue</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCountByValue</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testMap</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testMap</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairToNormalRDDTransform</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairToNormalRDDTransform</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairReduceByKey</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCount</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCount</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCheckpointMasterRecovery</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCheckpointMasterRecovery</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairMap</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairMap</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testUnion</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testUnion</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testFlatMap</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testFlatMap</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testReduceByKeyAndWindowWithInverse</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testReduceByKeyAndWindowWithInverse</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testGlom</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testGlom</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testJoin</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testJoin</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairFlatMap</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairFlatMap</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairToPairFlatMapWithChangingTypes</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairToPairFlatMapWithChangingTypes</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairMapPartitions</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairMapPartitions</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testRepartitionMorePartitions</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testRepartitionMorePartitions</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testReduceByWindowWithoutInverse</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testReduceByWindowWithoutInverse</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testLeftOuterJoin</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testLeftOuterJoin</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testVariousTransform</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testVariousTransform</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testTransformWith</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testTransformWith</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testVariousTransformWith</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testVariousTransformWith</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testTextFileStream</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testTextFileStream</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairGroupByKey</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairGroupByKey</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCoGroup</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCoGroup</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testInitialization</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testInitialization</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testGroupByKeyAndWindow</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testGroupByKeyAndWindow</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testReduceByKeyAndWindow</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testReduceByKeyAndWindow</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testForeachRDD</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testForeachRDD</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testFileStream</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testFileStream</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairTransform</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairTransform</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testFilter</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testFilter</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairMap2</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairMap2</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testMapValues</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testMapValues</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testReduce</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testReduce</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testUpdateStateByKey</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testUpdateStateByKey</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testTransform</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testTransform</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testWindow</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testWindow</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCountByValueAndWindow</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCountByValueAndWindow</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testRawSocketStream</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testRawSocketStream</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testUpdateStateByKeyWithInitial</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testUpdateStateByKeyWithInitial</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testContextState</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testContextState</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexDescendingWithStart</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexWithStart</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndexDescendingWithStart</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexDescending</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexWithStart</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexWithLast</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexWithSkip</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexWithMax</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexDescending</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndexDescendingWithLast</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexDescending</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexDescendingWithLast</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndex</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndexWithLast</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexWithStart</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexDescendingWithStart</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexWithLast</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexWithSkip</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndexDescending</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.testRefWithIntNaturalKey</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexDescending</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexDescendingWithStart</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexWithMax</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndex</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexWithLast</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexWithSkip</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexWithMax</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexDescendingWithLast</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexDescendingWithLast</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexDescendingWithStart</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndex</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexWithLast</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexWithSkip</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexWithStart</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndex</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexDescendingWithLast</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndexWithStart</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndex</div></li><li><div>org.apache.spark.util.kvstore.LevelDBSuite.testMultipleTypesWriteReadDelete</div></li><li><div>org.apache.spark.util.kvstore.LevelDBSuite.testObjectWriteReadDelete</div></li><li><div>org.apache.spark.util.kvstore.LevelDBSuite.testSkip</div></li><li><div>org.apache.spark.util.kvstore.LevelDBSuite.testMultipleObjectWriteReadDelete</div></li><li><div>org.apache.spark.util.kvstore.LevelDBSuite.testReopenAndVersionCheckDb</div></li><li><div>org.apache.spark.util.kvstore.LevelDBSuite.testMetadata</div></li><li><div>org.apache.spark.util.kvstore.LevelDBSuite.testUpdate</div></li><li><div>org.apache.spark.util.kvstore.LevelDBSuite.testNegativeIndexValues</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.spark.sql.hive.thriftserver.HiveThriftHttpServerSuite.Checks Hive version</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexDescendingWithStart</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexWithStart</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndexDescendingWithStart</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexDescending</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexWithStart</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexWithLast</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexWithSkip</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexWithMax</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexDescending</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndexDescendingWithLast</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexDescending</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexDescendingWithLast</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndex</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndexWithLast</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexWithStart</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexDescendingWithStart</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexWithLast</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexWithSkip</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndexDescending</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.testRefWithIntNaturalKey</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexDescending</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexDescendingWithStart</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexWithMax</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndex</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexWithLast</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexWithSkip</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexWithMax</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexDescendingWithLast</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexDescendingWithLast</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexDescendingWithStart</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndex</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexWithLast</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexWithSkip</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexWithStart</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndex</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexDescendingWithLast</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndexWithStart</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndex</div></li><li><div>org.apache.spark.util.kvstore.LevelDBSuite.testMultipleTypesWriteReadDelete</div></li><li><div>org.apache.spark.util.kvstore.LevelDBSuite.testObjectWriteReadDelete</div></li><li><div>org.apache.spark.util.kvstore.LevelDBSuite.testSkip</div></li><li><div>org.apache.spark.util.kvstore.LevelDBSuite.testMultipleObjectWriteReadDelete</div></li><li><div>org.apache.spark.util.kvstore.LevelDBSuite.testReopenAndVersionCheckDb</div></li><li><div>org.apache.spark.util.kvstore.LevelDBSuite.testMetadata</div></li><li><div>org.apache.spark.util.kvstore.LevelDBSuite.testUpdate</div></li><li><div>org.apache.spark.util.kvstore.LevelDBSuite.testNegativeIndexValues</div></li><li><div>org.apache.spark.sql.hive.HiveSparkSubmitSuite.SPARK-8020: set sql conf in spark conf</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.spark.network.ChunkFetchIntegrationSuite.fetchFileChunk</div></li><li><div>org.apache.spark.JavaJdbcRDDSuite.testJavaJdbcRDD</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.leftOuterJoin</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.foldReduce</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.foldReduce</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.mapsFromPairsToPairs</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.mapsFromPairsToPairs</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.flatMap</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.flatMap</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.foreach</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.map</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.zip</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.zip</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.keyBy</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.keyBy</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.groupBy</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.groupBy</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.mapPartitions</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.mapPartitions</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.foldByKey</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.foldByKey</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.mapOnPairRDD</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.mapOnPairRDD</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.sequenceFile</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.sequenceFile</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.collectPartitions</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.collectPartitions</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.reduceByKey</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.reduceByKey</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.foreachWithAnonymousClass</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.foreachWithAnonymousClass</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.collectAsMapWithIntArrayValues</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.collectAsMapWithIntArrayValues</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.zipPartitions</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.zipPartitions</div></li><li><div>test.org.apache.spark.JavaAPISuite.groupByOnPairRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.groupByOnPairRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.binaryFilesCaching</div></li><li><div>test.org.apache.spark.JavaAPISuite.binaryFilesCaching</div></li><li><div>test.org.apache.spark.JavaAPISuite.sparkContextUnion</div></li><li><div>test.org.apache.spark.JavaAPISuite.sparkContextUnion</div></li><li><div>test.org.apache.spark.JavaAPISuite.checkpointAndComputation</div></li><li><div>test.org.apache.spark.JavaAPISuite.checkpointAndComputation</div></li><li><div>test.org.apache.spark.JavaAPISuite.leftOuterJoin</div></li><li><div>test.org.apache.spark.JavaAPISuite.leftOuterJoin</div></li><li><div>test.org.apache.spark.JavaAPISuite.keyByOnPairRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.keyByOnPairRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.getNumPartitions</div></li><li><div>test.org.apache.spark.JavaAPISuite.getNumPartitions</div></li><li><div>test.org.apache.spark.JavaAPISuite.wholeTextFiles</div></li><li><div>test.org.apache.spark.JavaAPISuite.wholeTextFiles</div></li><li><div>test.org.apache.spark.JavaAPISuite.binaryFiles</div></li><li><div>test.org.apache.spark.JavaAPISuite.binaryFiles</div></li><li><div>test.org.apache.spark.JavaAPISuite.foldReduce</div></li><li><div>test.org.apache.spark.JavaAPISuite.foldReduce</div></li><li><div>test.org.apache.spark.JavaAPISuite.writeWithNewAPIHadoopFile</div></li><li><div>test.org.apache.spark.JavaAPISuite.writeWithNewAPIHadoopFile</div></li><li><div>test.org.apache.spark.JavaAPISuite.hadoopFile</div></li><li><div>test.org.apache.spark.JavaAPISuite.hadoopFile</div></li><li><div>test.org.apache.spark.JavaAPISuite.lookup</div></li><li><div>test.org.apache.spark.JavaAPISuite.lookup</div></li><li><div>test.org.apache.spark.JavaAPISuite.countAsync</div></li><li><div>test.org.apache.spark.JavaAPISuite.countAsync</div></li><li><div>test.org.apache.spark.JavaAPISuite.textFiles</div></li><li><div>test.org.apache.spark.JavaAPISuite.textFiles</div></li><li><div>test.org.apache.spark.JavaAPISuite.binaryRecords</div></li><li><div>test.org.apache.spark.JavaAPISuite.binaryRecords</div></li><li><div>test.org.apache.spark.JavaAPISuite.toLocalIterator</div></li><li><div>test.org.apache.spark.JavaAPISuite.toLocalIterator</div></li><li><div>test.org.apache.spark.JavaAPISuite.repartitionAndSortWithinPartitions</div></li><li><div>test.org.apache.spark.JavaAPISuite.repartitionAndSortWithinPartitions</div></li><li><div>test.org.apache.spark.JavaAPISuite.reduce</div></li><li><div>test.org.apache.spark.JavaAPISuite.reduce</div></li><li><div>test.org.apache.spark.JavaAPISuite.sample</div></li><li><div>test.org.apache.spark.JavaAPISuite.sample</div></li><li><div>test.org.apache.spark.JavaAPISuite.sortBy</div></li><li><div>test.org.apache.spark.JavaAPISuite.sortBy</div></li><li><div>test.org.apache.spark.JavaAPISuite.mapsFromPairsToPairs</div></li><li><div>test.org.apache.spark.JavaAPISuite.mapsFromPairsToPairs</div></li><li><div>test.org.apache.spark.JavaAPISuite.flatMap</div></li><li><div>test.org.apache.spark.JavaAPISuite.flatMap</div></li><li><div>test.org.apache.spark.JavaAPISuite.cogroup3</div></li><li><div>test.org.apache.spark.JavaAPISuite.cogroup3</div></li><li><div>test.org.apache.spark.JavaAPISuite.cogroup4</div></li><li><div>test.org.apache.spark.JavaAPISuite.cogroup4</div></li><li><div>test.org.apache.spark.JavaAPISuite.randomSplit</div></li><li><div>test.org.apache.spark.JavaAPISuite.randomSplit</div></li><li><div>test.org.apache.spark.JavaAPISuite.persist</div></li><li><div>test.org.apache.spark.JavaAPISuite.persist</div></li><li><div>test.org.apache.spark.JavaAPISuite.foreach</div></li><li><div>test.org.apache.spark.JavaAPISuite.hadoopFileCompressed</div></li><li><div>test.org.apache.spark.JavaAPISuite.hadoopFileCompressed</div></li><li><div>test.org.apache.spark.JavaAPISuite.textFilesCompressed</div></li><li><div>test.org.apache.spark.JavaAPISuite.textFilesCompressed</div></li><li><div>test.org.apache.spark.JavaAPISuite.testAsyncActionCancellation</div></li><li><div>test.org.apache.spark.JavaAPISuite.testAsyncActionCancellation</div></li><li><div>test.org.apache.spark.JavaAPISuite.checkpointAndRestore</div></li><li><div>test.org.apache.spark.JavaAPISuite.checkpointAndRestore</div></li><li><div>test.org.apache.spark.JavaAPISuite.sortByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.sortByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.aggregateByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.aggregateByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.map</div></li><li><div>test.org.apache.spark.JavaAPISuite.map</div></li><li><div>test.org.apache.spark.JavaAPISuite.max</div></li><li><div>test.org.apache.spark.JavaAPISuite.max</div></li><li><div>test.org.apache.spark.JavaAPISuite.min</div></li><li><div>test.org.apache.spark.JavaAPISuite.min</div></li><li><div>test.org.apache.spark.JavaAPISuite.top</div></li><li><div>test.org.apache.spark.JavaAPISuite.top</div></li><li><div>test.org.apache.spark.JavaAPISuite.zip</div></li><li><div>test.org.apache.spark.JavaAPISuite.zip</div></li><li><div>test.org.apache.spark.JavaAPISuite.fold</div></li><li><div>test.org.apache.spark.JavaAPISuite.fold</div></li><li><div>test.org.apache.spark.JavaAPISuite.glom</div></li><li><div>test.org.apache.spark.JavaAPISuite.glom</div></li><li><div>test.org.apache.spark.JavaAPISuite.take</div></li><li><div>test.org.apache.spark.JavaAPISuite.take</div></li><li><div>test.org.apache.spark.JavaAPISuite.javaDoubleRDDHistoGram</div></li><li><div>test.org.apache.spark.JavaAPISuite.javaDoubleRDDHistoGram</div></li><li><div>test.org.apache.spark.JavaAPISuite.collectUnderlyingScalaRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.keyBy</div></li><li><div>test.org.apache.spark.JavaAPISuite.mapPartitionsWithIndex</div></li><li><div>test.org.apache.spark.JavaAPISuite.sampleByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.intersection</div></li><li><div>test.org.apache.spark.JavaAPISuite.aggregate</div></li><li><div>test.org.apache.spark.JavaAPISuite.aggregate</div></li><li><div>test.org.apache.spark.JavaAPISuite.cartesian</div></li><li><div>test.org.apache.spark.JavaAPISuite.cartesian</div></li><li><div>test.org.apache.spark.JavaAPISuite.countApproxDistinctByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.countApproxDistinctByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.readWithNewAPIHadoopFile</div></li><li><div>test.org.apache.spark.JavaAPISuite.readWithNewAPIHadoopFile</div></li><li><div>test.org.apache.spark.JavaAPISuite.testRegisterKryoClasses</div></li><li><div>test.org.apache.spark.JavaAPISuite.testRegisterKryoClasses</div></li><li><div>test.org.apache.spark.JavaAPISuite.groupBy</div></li><li><div>test.org.apache.spark.JavaAPISuite.groupBy</div></li><li><div>test.org.apache.spark.JavaAPISuite.sampleByKeyExact</div></li><li><div>test.org.apache.spark.JavaAPISuite.sampleByKeyExact</div></li><li><div>test.org.apache.spark.JavaAPISuite.mapPartitions</div></li><li><div>test.org.apache.spark.JavaAPISuite.mapPartitions</div></li><li><div>test.org.apache.spark.JavaAPISuite.takeOrdered</div></li><li><div>test.org.apache.spark.JavaAPISuite.takeOrdered</div></li><li><div>test.org.apache.spark.JavaAPISuite.foldByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.foldByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.objectFilesOfInts</div></li><li><div>test.org.apache.spark.JavaAPISuite.objectFilesOfInts</div></li><li><div>test.org.apache.spark.JavaAPISuite.treeAggregate</div></li><li><div>test.org.apache.spark.JavaAPISuite.treeAggregate</div></li><li><div>test.org.apache.spark.JavaAPISuite.testGetPersistentRDDs</div></li><li><div>test.org.apache.spark.JavaAPISuite.testGetPersistentRDDs</div></li><li><div>test.org.apache.spark.JavaAPISuite.approximateResults</div></li><li><div>test.org.apache.spark.JavaAPISuite.approximateResults</div></li><li><div>test.org.apache.spark.JavaAPISuite.treeReduce</div></li><li><div>test.org.apache.spark.JavaAPISuite.treeReduce</div></li><li><div>test.org.apache.spark.JavaAPISuite.collectAsMapAndSerialize</div></li><li><div>test.org.apache.spark.JavaAPISuite.collectAsMapAndSerialize</div></li><li><div>test.org.apache.spark.JavaAPISuite.countApproxDistinct</div></li><li><div>test.org.apache.spark.JavaAPISuite.countApproxDistinct</div></li><li><div>test.org.apache.spark.JavaAPISuite.javaDoubleRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.javaDoubleRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.mapOnPairRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.mapOnPairRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.testAsyncActionErrorWrapping</div></li><li><div>test.org.apache.spark.JavaAPISuite.testAsyncActionErrorWrapping</div></li><li><div>test.org.apache.spark.JavaAPISuite.naturalMax</div></li><li><div>test.org.apache.spark.JavaAPISuite.naturalMax</div></li><li><div>test.org.apache.spark.JavaAPISuite.naturalMin</div></li><li><div>test.org.apache.spark.JavaAPISuite.naturalMin</div></li><li><div>test.org.apache.spark.JavaAPISuite.sequenceFile</div></li><li><div>test.org.apache.spark.JavaAPISuite.sequenceFile</div></li><li><div>test.org.apache.spark.JavaAPISuite.collectPartitions</div></li><li><div>test.org.apache.spark.JavaAPISuite.collectPartitions</div></li><li><div>test.org.apache.spark.JavaAPISuite.cogroup</div></li><li><div>test.org.apache.spark.JavaAPISuite.cogroup</div></li><li><div>test.org.apache.spark.JavaAPISuite.reduceByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.reduceByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.repartition</div></li><li><div>test.org.apache.spark.JavaAPISuite.repartition</div></li><li><div>test.org.apache.spark.JavaAPISuite.iterator</div></li><li><div>test.org.apache.spark.JavaAPISuite.iterator</div></li><li><div>test.org.apache.spark.JavaAPISuite.emptyRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.emptyRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.zipWithIndex</div></li><li><div>test.org.apache.spark.JavaAPISuite.foreachPartition</div></li><li><div>test.org.apache.spark.JavaAPISuite.foreachPartition</div></li><li><div>test.org.apache.spark.JavaAPISuite.combineByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.combineByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.takeAsync</div></li><li><div>test.org.apache.spark.JavaAPISuite.takeAsync</div></li><li><div>test.org.apache.spark.JavaAPISuite.collectAsMapWithIntArrayValues</div></li><li><div>test.org.apache.spark.JavaAPISuite.collectAsMapWithIntArrayValues</div></li><li><div>test.org.apache.spark.JavaAPISuite.objectFilesOfComplexTypes</div></li><li><div>test.org.apache.spark.JavaAPISuite.objectFilesOfComplexTypes</div></li><li><div>test.org.apache.spark.JavaAPISuite.zipWithUniqueId</div></li><li><div>test.org.apache.spark.JavaAPISuite.zipWithUniqueId</div></li><li><div>test.org.apache.spark.JavaAPISuite.collectAsync</div></li><li><div>test.org.apache.spark.JavaAPISuite.collectAsync</div></li><li><div>test.org.apache.spark.JavaAPISuite.foreachAsync</div></li><li><div>test.org.apache.spark.JavaAPISuite.foreachAsync</div></li><li><div>test.org.apache.spark.JavaAPISuite.zipPartitions</div></li><li><div>test.org.apache.spark.JavaAPISuite.zipPartitions</div></li><li><div>test.org.apache.spark.JavaAPISuite.reduceOnJavaDoubleRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.reduceOnJavaDoubleRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.isEmpty</div></li><li><div>test.org.apache.spark.JavaAPISuite.isEmpty</div></li><li><div>test.org.apache.spark.JavaSparkContextSuite.javaSparkContext</div></li><li><div>test.org.apache.spark.JavaSparkContextSuite.scalaSparkContext</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single BooleanType with seed 7020069396768296380</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single ByteType with seed -2399284702189095073</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single ShortType with seed -7719897024036539527</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single IntegerType with seed 3194364168583009346</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single LongType with seed -8233301204981990319</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single FloatType with seed 5142463758832068401</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single DoubleType with seed 8662749565340803335</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single DecimalType(8,0) with seed 5649588326852021390</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single DecimalType(8,4) with seed 3581673481165790187</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single DecimalType(16,0) with seed -9189186181081872341</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single DecimalType(16,11) with seed -316910867452416915</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single DecimalType(38,0) with seed -6869032807411776259</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single DecimalType(38,38) with seed -5492932143993552910</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single StringType with seed -2968675814663939058</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single BinaryType with seed 934282045452751850</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.flat schema struct&lt;col_0:double,col_1:decimal(38,38),col_2:boolean,col_3:bigint,col_4:decimal(16,0)&gt; with seed -5226596492428831048</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.flat schema struct&lt;col_0:bigint,col_1:boolean,col_2:tinyint,col_3:decimal(16,0),col_4:decimal(38,38)&gt; with seed -6991691242331003614</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.flat schema struct&lt;col_0:string,col_1:decimal(8,0),col_2:binary,col_3:tinyint,col_4:double&gt; with seed -7698775092196679665</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.flat schema struct&lt;col_0:float,col_1:decimal(8,0),col_2:string,col_3:boolean,col_4:smallint&gt; with seed 5184276442399998038</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.flat schema struct&lt;col_0:double,col_1:string,col_2:decimal(38,0),col_3:float,col_4:binary&gt; with seed -6346626587057858145</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.nested schema struct&lt;col_0:array&lt;decimal(38,38)&gt;,col_1:double,col_2:array&lt;bigint&gt;,col_3:struct&lt;col_0:struct&lt;col_0:array&lt;tinyint&gt;&gt;&gt;,col_4:bigint,col_5:struct&lt;col_0:struct&lt;col_0:binary&gt;&gt;,col_6:array&lt;int&gt;,col_7:decimal(8,4),col_8:array&lt;float&gt;,col_9:struct&lt;col_0:float&gt;&gt; with seed -6847694401803089910</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.nested schema struct&lt;col_0:struct&lt;col_0:array&lt;decimal(38,0)&gt;,col_1:struct&lt;col_0:array&lt;decimal(8,0)&gt;&gt;&gt;,col_1:struct&lt;col_0:struct&lt;col_0:decimal(8,4),col_1:bigint&gt;,col_1:struct&lt;col_0:struct&lt;col_0:array&lt;int&gt;&gt;&gt;&gt;,col_2:array&lt;decimal(38,0)&gt;,col_3:array&lt;tinyint&gt;,col_4:struct&lt;col_0:array&lt;decimal(8,0)&gt;,col_1:array&lt;decimal(38,0)&gt;&gt;,col_5:boolean&gt; with seed -4135230179903928490</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.nested schema struct&lt;col_0:decimal(38,0),col_1:struct&lt;col_0:decimal(16,11),col_1:struct&lt;col_0:struct&lt;col_0:struct&lt;col_0:array&lt;tinyint&gt;&gt;,col_1:struct&lt;col_0:struct&lt;col_0:array&lt;int&gt;&gt;&gt;&gt;,col_1:struct&lt;col_0:array&lt;string&gt;&gt;,col_2:array&lt;decimal(38,38)&gt;,col_3:array&lt;decimal(16,11)&gt;&gt;,col_2:array&lt;decimal(16,0)&gt;&gt;,col_2:boolean,col_3:array&lt;decimal(8,4)&gt;&gt; with seed 9011471362555659161</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.nested schema struct&lt;col_0:struct&lt;col_0:array&lt;string&gt;,col_1:struct&lt;col_0:struct&lt;col_0:string&gt;,col_1:bigint&gt;,col_2:struct&lt;col_0:tinyint&gt;,col_3:decimal(16,0),col_4:decimal(8,4),col_5:decimal(8,0)&gt;,col_1:struct&lt;col_0:string&gt;,col_2:struct&lt;col_0:array&lt;float&gt;&gt;,col_3:decimal(8,0)&gt; with seed -6084749057723529533</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.nested schema struct&lt;col_0:decimal(8,0),col_1:struct&lt;col_0:struct&lt;col_0:struct&lt;col_0:decimal(38,0)&gt;&gt;,col_1:double,col_2:decimal(8,4),col_3:struct&lt;col_0:float&gt;&gt;,col_2:struct&lt;col_0:binary,col_1:string,col_2:struct&lt;col_0:array&lt;double&gt;&gt;&gt;,col_3:int,col_4:bigint&gt; with seed -686107255735563379</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.read int as string</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.read string as int</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.read float as double</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.read double as float</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.Handle unsupported input of record type</div></li><li><div>org.apache.spark.sql.avro.AvroFunctionsSuite.roundtrip in to_avro and from_avro - int and string</div></li><li><div>org.apache.spark.sql.avro.AvroFunctionsSuite.roundtrip in to_avro and from_avro - struct</div></li><li><div>org.apache.spark.sql.avro.AvroFunctionsSuite.handle invalid input in from_avro</div></li><li><div>org.apache.spark.sql.avro.AvroFunctionsSuite.roundtrip in to_avro and from_avro - array with null</div></li><li><div>org.apache.spark.graphx.EdgeRDDSuite.checkpointing</div></li><li><div>org.apache.spark.graphx.GraphLoaderSuite.GraphLoader.edgeListFile</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.joinVertices</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.collectNeighborIds</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.removeSelfEdges</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.filter</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.convertToCanonicalEdges</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.collectEdgesCycleDirectionOut</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.collectEdgesCycleDirectionIn</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.collectEdgesCycleDirectionEither</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.collectEdgesChainDirectionOut</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.collectEdgesChainDirectionIn</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.collectEdgesChainDirectionEither</div></li><li><div>org.apache.spark.graphx.GraphSuite.Graph.fromEdgeTuples</div></li><li><div>org.apache.spark.graphx.GraphSuite.Graph.fromEdges</div></li><li><div>org.apache.spark.graphx.GraphSuite.Graph.apply</div></li><li><div>org.apache.spark.graphx.GraphSuite.triplets</div></li><li><div>org.apache.spark.graphx.GraphSuite.partitionBy</div></li><li><div>org.apache.spark.graphx.GraphSuite.mapVertices</div></li><li><div>org.apache.spark.graphx.GraphSuite.mapVertices changing type with same erased type</div></li><li><div>org.apache.spark.graphx.GraphSuite.mapEdges</div></li><li><div>org.apache.spark.graphx.GraphSuite.mapTriplets</div></li><li><div>org.apache.spark.graphx.GraphSuite.reverse</div></li><li><div>org.apache.spark.graphx.GraphSuite.reverse with join elimination</div></li><li><div>org.apache.spark.graphx.GraphSuite.subgraph</div></li><li><div>org.apache.spark.graphx.GraphSuite.mask</div></li><li><div>org.apache.spark.graphx.GraphSuite.groupEdges</div></li><li><div>org.apache.spark.graphx.GraphSuite.aggregateMessages</div></li><li><div>org.apache.spark.graphx.GraphSuite.outerJoinVertices</div></li><li><div>org.apache.spark.graphx.GraphSuite.more edge partitions than vertex partitions</div></li><li><div>org.apache.spark.graphx.GraphSuite.checkpoint</div></li><li><div>org.apache.spark.graphx.GraphSuite.cache, getStorageLevel</div></li><li><div>org.apache.spark.graphx.GraphSuite.non-default number of edge partitions</div></li><li><div>org.apache.spark.graphx.GraphSuite.unpersist graph RDD</div></li><li><div>org.apache.spark.graphx.GraphSuite.SPARK-14219: pickRandomVertex</div></li><li><div>org.apache.spark.graphx.PregelSuite.1 iteration</div></li><li><div>org.apache.spark.graphx.PregelSuite.chain propagation</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.filter</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.mapValues</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.minus</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.minus with RDD[(VertexId, VD)]</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.minus with non-equal number of partitions</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.diff</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.diff with RDD[(VertexId, VD)]</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.diff vertices with non-equal number of partitions</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.leftJoin</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.leftJoin vertices with non-equal number of partitions</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.innerJoin</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.innerJoin vertices with the non-equal number of partitions</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.aggregateUsingIndex</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.mergeFunc</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.cache, getStorageLevel</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.checkpoint</div></li><li><div>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Grid Connected Components</div></li><li><div>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Reverse Grid Connected Components</div></li><li><div>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Chain Connected Components</div></li><li><div>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Reverse Chain Connected Components</div></li><li><div>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Connected Components on a Toy Connected Graph</div></li><li><div>org.apache.spark.graphx.lib.LabelPropagationSuite.Label Propagation</div></li><li><div>org.apache.spark.graphx.lib.PageRankSuite.Star PageRank</div></li><li><div>org.apache.spark.graphx.lib.PageRankSuite.Star PersonalPageRank</div></li><li><div>org.apache.spark.graphx.lib.PageRankSuite.Grid PageRank</div></li><li><div>org.apache.spark.graphx.lib.PageRankSuite.Chain PageRank</div></li><li><div>org.apache.spark.graphx.lib.PageRankSuite.Chain PersonalizedPageRank</div></li><li><div>org.apache.spark.graphx.lib.PageRankSuite.Loop with source PageRank</div></li><li><div>org.apache.spark.graphx.lib.PageRankSuite.Loop with sink PageRank</div></li><li><div>org.apache.spark.graphx.lib.SVDPlusPlusSuite.Test SVD++ with mean square error on training set</div></li><li><div>org.apache.spark.graphx.lib.ShortestPathsSuite.Shortest Path Computations</div></li><li><div>org.apache.spark.graphx.lib.StronglyConnectedComponentsSuite.Island Strongly Connected Components</div></li><li><div>org.apache.spark.graphx.lib.StronglyConnectedComponentsSuite.Cycle Strongly Connected Components</div></li><li><div>org.apache.spark.graphx.lib.StronglyConnectedComponentsSuite.2 Cycle Strongly Connected Components</div></li><li><div>org.apache.spark.graphx.lib.TriangleCountSuite.Count a single triangle</div></li><li><div>org.apache.spark.graphx.lib.TriangleCountSuite.Count two triangles</div></li><li><div>org.apache.spark.graphx.lib.TriangleCountSuite.Count two triangles with bi-directed edges</div></li><li><div>org.apache.spark.graphx.lib.TriangleCountSuite.Count a single triangle with duplicate edges</div></li><li><div>org.apache.spark.graphx.util.GraphGeneratorsSuite.GraphGenerators.logNormalGraph</div></li><li><div>org.apache.spark.graphx.util.GraphGeneratorsSuite.SPARK-5064 GraphGenerators.rmatGraph numEdges upper bound</div></li><li><div>org.apache.spark.graphx.util.PeriodicGraphCheckpointerSuite.Persisting</div></li><li><div>org.apache.spark.graphx.util.PeriodicGraphCheckpointerSuite.Checkpointing</div></li><li><div>org.apache.spark.ml.JavaPipelineSuite.pipeline</div></li><li><div>org.apache.spark.ml.JavaPipelineSuite.pipeline</div></li><li><div>org.apache.spark.ml.classification.JavaDecisionTreeClassifierSuite.runDT</div></li><li><div>org.apache.spark.ml.classification.JavaDecisionTreeClassifierSuite.runDT</div></li><li><div>org.apache.spark.ml.classification.JavaGBTClassifierSuite.runDT</div></li><li><div>org.apache.spark.ml.classification.JavaGBTClassifierSuite.runDT</div></li><li><div>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionWithSetters</div></li><li><div>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionWithSetters</div></li><li><div>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionTrainingSummary</div></li><li><div>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionTrainingSummary</div></li><li><div>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionPredictorClassifierMethods</div></li><li><div>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionPredictorClassifierMethods</div></li><li><div>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionDefaultParams</div></li><li><div>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionDefaultParams</div></li><li><div>org.apache.spark.ml.classification.JavaMultilayerPerceptronClassifierSuite.testMLPC</div></li><li><div>org.apache.spark.ml.classification.JavaMultilayerPerceptronClassifierSuite.testMLPC</div></li><li><div>org.apache.spark.ml.classification.JavaNaiveBayesSuite.testNaiveBayes</div></li><li><div>org.apache.spark.ml.classification.JavaNaiveBayesSuite.testNaiveBayes</div></li><li><div>org.apache.spark.ml.classification.JavaNaiveBayesSuite.naiveBayesDefaultParams</div></li><li><div>org.apache.spark.ml.classification.JavaNaiveBayesSuite.naiveBayesDefaultParams</div></li><li><div>org.apache.spark.ml.classification.JavaOneVsRestSuite.oneVsRestDefaultParams</div></li><li><div>org.apache.spark.ml.classification.JavaOneVsRestSuite.oneVsRestDefaultParams</div></li><li><div>org.apache.spark.ml.classification.JavaRandomForestClassifierSuite.runDT</div></li><li><div>org.apache.spark.ml.classification.JavaRandomForestClassifierSuite.runDT</div></li><li><div>org.apache.spark.ml.clustering.JavaKMeansSuite.fitAndTransform</div></li><li><div>org.apache.spark.ml.clustering.JavaKMeansSuite.fitAndTransform</div></li><li><div>org.apache.spark.ml.feature.JavaBucketizerSuite.bucketizerTest</div></li><li><div>org.apache.spark.ml.feature.JavaBucketizerSuite.bucketizerTest</div></li><li><div>org.apache.spark.ml.feature.JavaBucketizerSuite.bucketizerMultipleColumnsTest</div></li><li><div>org.apache.spark.ml.feature.JavaBucketizerSuite.bucketizerMultipleColumnsTest</div></li><li><div>org.apache.spark.ml.feature.JavaDCTSuite.javaCompatibilityTest</div></li><li><div>org.apache.spark.ml.feature.JavaDCTSuite.javaCompatibilityTest</div></li><li><div>org.apache.spark.ml.feature.JavaHashingTFSuite.hashingTF</div></li><li><div>org.apache.spark.ml.feature.JavaNormalizerSuite.normalizer</div></li><li><div>org.apache.spark.ml.feature.JavaNormalizerSuite.normalizer</div></li><li><div>org.apache.spark.ml.feature.JavaPCASuite.testPCA</div></li><li><div>org.apache.spark.ml.feature.JavaPCASuite.testPCA</div></li><li><div>org.apache.spark.ml.feature.JavaPolynomialExpansionSuite.polynomialExpansionTest</div></li><li><div>org.apache.spark.ml.feature.JavaPolynomialExpansionSuite.polynomialExpansionTest</div></li><li><div>org.apache.spark.ml.feature.JavaStandardScalerSuite.standardScaler</div></li><li><div>org.apache.spark.ml.feature.JavaStandardScalerSuite.standardScaler</div></li><li><div>org.apache.spark.ml.feature.JavaStopWordsRemoverSuite.javaCompatibilityTest</div></li><li><div>org.apache.spark.ml.feature.JavaStopWordsRemoverSuite.javaCompatibilityTest</div></li><li><div>org.apache.spark.ml.feature.JavaStringIndexerSuite.testStringIndexer</div></li><li><div>org.apache.spark.ml.feature.JavaStringIndexerSuite.testStringIndexer</div></li><li><div>org.apache.spark.ml.feature.JavaTokenizerSuite.regexTokenizer</div></li><li><div>org.apache.spark.ml.feature.JavaTokenizerSuite.regexTokenizer</div></li><li><div>org.apache.spark.ml.feature.JavaVectorAssemblerSuite.testVectorAssembler</div></li><li><div>org.apache.spark.ml.feature.JavaVectorAssemblerSuite.testVectorAssembler</div></li><li><div>org.apache.spark.ml.feature.JavaVectorIndexerSuite.vectorIndexerAPI</div></li><li><div>org.apache.spark.ml.feature.JavaVectorIndexerSuite.vectorIndexerAPI</div></li><li><div>org.apache.spark.ml.feature.JavaVectorSlicerSuite.vectorSlice</div></li><li><div>org.apache.spark.ml.feature.JavaVectorSlicerSuite.vectorSlice</div></li><li><div>org.apache.spark.ml.feature.JavaWord2VecSuite.testJavaWord2Vec</div></li><li><div>org.apache.spark.ml.feature.JavaWord2VecSuite.testJavaWord2Vec</div></li><li><div>org.apache.spark.ml.regression.JavaDecisionTreeRegressorSuite.runDT</div></li><li><div>org.apache.spark.ml.regression.JavaDecisionTreeRegressorSuite.runDT</div></li><li><div>org.apache.spark.ml.regression.JavaGBTRegressorSuite.runDT</div></li><li><div>org.apache.spark.ml.regression.JavaGBTRegressorSuite.runDT</div></li><li><div>org.apache.spark.ml.regression.JavaLinearRegressionSuite.linearRegressionDefaultParams</div></li><li><div>org.apache.spark.ml.regression.JavaLinearRegressionSuite.linearRegressionWithSetters</div></li><li><div>org.apache.spark.ml.regression.JavaRandomForestRegressorSuite.runDT</div></li><li><div>org.apache.spark.ml.regression.JavaRandomForestRegressorSuite.runDT</div></li><li><div>org.apache.spark.ml.source.libsvm.JavaLibSVMRelationSuite.verifyLibSVMDF</div></li><li><div>org.apache.spark.ml.source.libsvm.JavaLibSVMRelationSuite.verifyLibSVMDF</div></li><li><div>org.apache.spark.ml.stat.JavaKolmogorovSmirnovTestSuite.testKSTestNamedDistribution</div></li><li><div>org.apache.spark.ml.stat.JavaKolmogorovSmirnovTestSuite.testKSTestNamedDistribution</div></li><li><div>org.apache.spark.ml.stat.JavaKolmogorovSmirnovTestSuite.testKSTestCDF</div></li><li><div>org.apache.spark.ml.stat.JavaKolmogorovSmirnovTestSuite.testKSTestCDF</div></li><li><div>org.apache.spark.ml.stat.JavaSummarizerSuite.testSummarizer</div></li><li><div>org.apache.spark.ml.stat.JavaSummarizerSuite.testSummarizer</div></li><li><div>org.apache.spark.ml.tuning.JavaCrossValidatorSuite.crossValidationWithLogisticRegression</div></li><li><div>org.apache.spark.ml.tuning.JavaCrossValidatorSuite.crossValidationWithLogisticRegression</div></li><li><div>org.apache.spark.ml.util.JavaDefaultReadWriteSuite.testDefaultReadWrite</div></li><li><div>org.apache.spark.ml.util.JavaDefaultReadWriteSuite.testDefaultReadWrite</div></li><li><div>org.apache.spark.mllib.classification.JavaLogisticRegressionSuite.runLRUsingConstructor</div></li><li><div>org.apache.spark.mllib.classification.JavaLogisticRegressionSuite.runLRUsingConstructor</div></li><li><div>org.apache.spark.mllib.classification.JavaLogisticRegressionSuite.runLRUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.classification.JavaLogisticRegressionSuite.runLRUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.testPredictJavaRDD</div></li><li><div>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.testPredictJavaRDD</div></li><li><div>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.runUsingConstructor</div></li><li><div>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.runUsingConstructor</div></li><li><div>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.runUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.runUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.testModelTypeSetters</div></li><li><div>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.testModelTypeSetters</div></li><li><div>org.apache.spark.mllib.classification.JavaSVMSuite.runSVMUsingConstructor</div></li><li><div>org.apache.spark.mllib.classification.JavaSVMSuite.runSVMUsingConstructor</div></li><li><div>org.apache.spark.mllib.classification.JavaSVMSuite.runSVMUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.classification.JavaSVMSuite.runSVMUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.classification.JavaStreamingLogisticRegressionSuite.javaAPI</div></li><li><div>org.apache.spark.mllib.classification.JavaStreamingLogisticRegressionSuite.javaAPI</div></li><li><div>org.apache.spark.mllib.clustering.JavaBisectingKMeansSuite.twoDimensionalData</div></li><li><div>org.apache.spark.mllib.clustering.JavaBisectingKMeansSuite.twoDimensionalData</div></li><li><div>org.apache.spark.mllib.clustering.JavaGaussianMixtureSuite.runGaussianMixture</div></li><li><div>org.apache.spark.mllib.clustering.JavaGaussianMixtureSuite.runGaussianMixture</div></li><li><div>org.apache.spark.mllib.clustering.JavaKMeansSuite.testPredictJavaRDD</div></li><li><div>org.apache.spark.mllib.clustering.JavaKMeansSuite.runKMeansUsingConstructor</div></li><li><div>org.apache.spark.mllib.clustering.JavaKMeansSuite.runKMeansUsingConstructor</div></li><li><div>org.apache.spark.mllib.clustering.JavaKMeansSuite.runKMeansUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.clustering.JavaKMeansSuite.runKMeansUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.clustering.JavaLDASuite.onlineOptimizerCompatibility</div></li><li><div>org.apache.spark.mllib.clustering.JavaLDASuite.onlineOptimizerCompatibility</div></li><li><div>org.apache.spark.mllib.clustering.JavaLDASuite.distributedLDAModel</div></li><li><div>org.apache.spark.mllib.clustering.JavaLDASuite.distributedLDAModel</div></li><li><div>org.apache.spark.mllib.clustering.JavaLDASuite.localLDAModel</div></li><li><div>org.apache.spark.mllib.clustering.JavaLDASuite.localLDAModel</div></li><li><div>org.apache.spark.mllib.clustering.JavaLDASuite.localLdaMethods</div></li><li><div>org.apache.spark.mllib.clustering.JavaStreamingKMeansSuite.javaAPI</div></li><li><div>org.apache.spark.mllib.evaluation.JavaRankingMetricsSuite.rankingMetrics</div></li><li><div>org.apache.spark.mllib.evaluation.JavaRankingMetricsSuite.rankingMetrics</div></li><li><div>org.apache.spark.mllib.feature.JavaTfIdfSuite.tfIdfMinimumDocumentFrequency</div></li><li><div>org.apache.spark.mllib.feature.JavaTfIdfSuite.tfIdfMinimumDocumentFrequency</div></li><li><div>org.apache.spark.mllib.feature.JavaTfIdfSuite.tfIdf</div></li><li><div>org.apache.spark.mllib.feature.JavaTfIdfSuite.tfIdf</div></li><li><div>org.apache.spark.mllib.feature.JavaWord2VecSuite.word2Vec</div></li><li><div>org.apache.spark.mllib.feature.JavaWord2VecSuite.word2Vec</div></li><li><div>org.apache.spark.mllib.fpm.JavaAssociationRulesSuite.runAssociationRules</div></li><li><div>org.apache.spark.mllib.fpm.JavaAssociationRulesSuite.runAssociationRules</div></li><li><div>org.apache.spark.mllib.fpm.JavaFPGrowthSuite.runFPGrowthSaveLoad</div></li><li><div>org.apache.spark.mllib.fpm.JavaFPGrowthSuite.runFPGrowthSaveLoad</div></li><li><div>org.apache.spark.mllib.fpm.JavaFPGrowthSuite.runFPGrowth</div></li><li><div>org.apache.spark.mllib.fpm.JavaFPGrowthSuite.runFPGrowth</div></li><li><div>org.apache.spark.mllib.fpm.JavaPrefixSpanSuite.runPrefixSpan</div></li><li><div>org.apache.spark.mllib.fpm.JavaPrefixSpanSuite.runPrefixSpan</div></li><li><div>org.apache.spark.mllib.fpm.JavaPrefixSpanSuite.runPrefixSpanSaveLoad</div></li><li><div>org.apache.spark.mllib.fpm.JavaPrefixSpanSuite.runPrefixSpanSaveLoad</div></li><li><div>org.apache.spark.mllib.linalg.distributed.JavaRowMatrixSuite.rowMatrixQRDecomposition</div></li><li><div>org.apache.spark.mllib.linalg.distributed.JavaRowMatrixSuite.rowMatrixQRDecomposition</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testNormalVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testNormalVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testArbitrary</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testArbitrary</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testLogNormalVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testLogNormalVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testExponentialVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testExponentialVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testUniformRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testUniformRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testRandomVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testRandomVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testGammaRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testGammaRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testUniformVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testUniformVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testPoissonRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testPoissonRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testNormalRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testNormalRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testPoissonVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testPoissonVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testGammaVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testGammaVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testExponentialRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testExponentialRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testLNormalRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testLNormalRDD</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runALSUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runALSUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSUsingConstructor</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSUsingConstructor</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runRecommend</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runRecommend</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSWithNegativeWeight</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSWithNegativeWeight</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runALSUsingConstructor</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runALSUsingConstructor</div></li><li><div>org.apache.spark.mllib.regression.JavaIsotonicRegressionSuite.testIsotonicRegressionJavaRDD</div></li><li><div>org.apache.spark.mllib.regression.JavaIsotonicRegressionSuite.testIsotonicRegressionJavaRDD</div></li><li><div>org.apache.spark.mllib.regression.JavaIsotonicRegressionSuite.testIsotonicRegressionPredictionsJavaRDD</div></li><li><div>org.apache.spark.mllib.regression.JavaIsotonicRegressionSuite.testIsotonicRegressionPredictionsJavaRDD</div></li><li><div>org.apache.spark.mllib.regression.JavaLassoSuite.runLassoUsingConstructor</div></li><li><div>org.apache.spark.mllib.regression.JavaLassoSuite.runLassoUsingConstructor</div></li><li><div>org.apache.spark.mllib.regression.JavaLassoSuite.runLassoUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.regression.JavaLassoSuite.runLassoUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.testPredictJavaRDD</div></li><li><div>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.testPredictJavaRDD</div></li><li><div>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.runLinearRegressionUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.runLinearRegressionUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.runLinearRegressionUsingConstructor</div></li><li><div>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.runLinearRegressionUsingConstructor</div></li><li><div>org.apache.spark.mllib.regression.JavaRidgeRegressionSuite.runRidgeRegressionUsingConstructor</div></li><li><div>org.apache.spark.mllib.regression.JavaRidgeRegressionSuite.runRidgeRegressionUsingConstructor</div></li><li><div>org.apache.spark.mllib.regression.JavaRidgeRegressionSuite.runRidgeRegressionUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.regression.JavaRidgeRegressionSuite.runRidgeRegressionUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.regression.JavaStreamingLinearRegressionSuite.javaAPI</div></li><li><div>org.apache.spark.mllib.regression.JavaStreamingLinearRegressionSuite.javaAPI</div></li><li><div>org.apache.spark.mllib.stat.JavaStatisticsSuite.testCorr</div></li><li><div>org.apache.spark.mllib.stat.JavaStatisticsSuite.testCorr</div></li><li><div>org.apache.spark.mllib.stat.JavaStatisticsSuite.chiSqTest</div></li><li><div>org.apache.spark.mllib.stat.JavaStatisticsSuite.chiSqTest</div></li><li><div>org.apache.spark.mllib.stat.JavaStatisticsSuite.streamingTest</div></li><li><div>org.apache.spark.mllib.stat.JavaStatisticsSuite.streamingTest</div></li><li><div>org.apache.spark.mllib.stat.JavaStatisticsSuite.kolmogorovSmirnovTest</div></li><li><div>org.apache.spark.mllib.stat.JavaStatisticsSuite.kolmogorovSmirnovTest</div></li><li><div>org.apache.spark.mllib.tree.JavaDecisionTreeSuite.runDTUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.tree.JavaDecisionTreeSuite.runDTUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.tree.JavaDecisionTreeSuite.runDTUsingConstructor</div></li><li><div>org.apache.spark.mllib.tree.JavaDecisionTreeSuite.runDTUsingConstructor</div></li><li><div>org.apache.spark.mllib.util.JavaMLUtilsSuite.testConvertMatrixColumnsToAndFromML</div></li><li><div>org.apache.spark.mllib.util.JavaMLUtilsSuite.testConvertMatrixColumnsToAndFromML</div></li><li><div>org.apache.spark.mllib.util.JavaMLUtilsSuite.testConvertVectorColumnsToAndFromML</div></li><li><div>org.apache.spark.mllib.util.JavaMLUtilsSuite.testConvertVectorColumnsToAndFromML</div></li><li><div>org.apache.spark.repl.ReplSuite.SPARK-15236: use Hive catalog</div></li><li><div>org.apache.spark.repl.ReplSuite.SPARK-15236: use in-memory catalog</div></li><li><div>org.apache.spark.repl.ReplSuite.broadcast vars</div></li><li><div>org.apache.spark.repl.ReplSuite.line wrapper only initialized once when used as encoder outer scope</div></li><li><div>org.apache.spark.repl.ReplSuite.define case class and create Dataset together with paste mode</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.simple foreach with accumulator</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.external vars</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.external classes</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.external functions</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.external functions that access vars</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.broadcast vars</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.interacting with files</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.local-cluster mode</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.SPARK-2576 importing implicits</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.Datasets and encoders</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.SPARK-2632 importing a method from non serializable class and not using it.</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.collecting objects of class defined in repl</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.collecting objects of class defined in repl - shuffling</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.replicating blocks of object with class defined in repl</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.newProductSeqEncoder with REPL defined class</div></li><li><div>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationCount</div></li><li><div>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationCount</div></li><li><div>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationSumDouble</div></li><li><div>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationSumDouble</div></li><li><div>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationSumLong</div></li><li><div>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationSumLong</div></li><li><div>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationAverage</div></li><li><div>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationAverage</div></li><li><div>test.org.apache.spark.sql.JavaApplySchemaSuite.applySchema</div></li><li><div>test.org.apache.spark.sql.JavaApplySchemaSuite.applySchema</div></li><li><div>test.org.apache.spark.sql.JavaApplySchemaSuite.dataFrameRDDOperations</div></li><li><div>test.org.apache.spark.sql.JavaApplySchemaSuite.dataFrameRDDOperations</div></li><li><div>test.org.apache.spark.sql.JavaApplySchemaSuite.applySchemaToJSON</div></li><li><div>test.org.apache.spark.sql.JavaApplySchemaSuite.applySchemaToJSON</div></li><li><div>test.org.apache.spark.sql.JavaBeanDeserializationSuite.testBeanWithArrayFieldDeserialization</div></li><li><div>test.org.apache.spark.sql.JavaBeanDeserializationSuite.testBeanWithArrayFieldDeserialization</div></li><li><div>test.org.apache.spark.sql.JavaBeanDeserializationSuite.testBeanWithMapFieldsDeserialization</div></li><li><div>test.org.apache.spark.sql.JavaBeanDeserializationSuite.testBeanWithMapFieldsDeserialization</div></li><li><div>test.org.apache.spark.sql.JavaColumnExpressionSuite.isInCollectionCheckExceptionMessage</div></li><li><div>test.org.apache.spark.sql.JavaColumnExpressionSuite.isInCollectionCheckExceptionMessage</div></li><li><div>test.org.apache.spark.sql.JavaColumnExpressionSuite.isInCollectionWorksCorrectlyOnJava</div></li><li><div>test.org.apache.spark.sql.JavaColumnExpressionSuite.isInCollectionWorksCorrectlyOnJava</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testFormatAPI</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testTextAPI</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testJsonAPI</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testLoadAPI</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testOptionsAPI</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testSaveModeAPI</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testCsvAPI</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testParquetAPI</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testTextFileAPI</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCollectAndTake</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCollectAndTake</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testJsonRDDToDataFrame</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testJsonRDDToDataFrame</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testVarargMethods</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testVarargMethods</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testBeanWithoutGetter</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testBeanWithoutGetter</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateStructTypeFromList</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateStructTypeFromList</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testSampleBy</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testSampleBy</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCrosstab</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCrosstab</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testUDF</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testUDF</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFromFromList</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFromFromList</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCircularReferenceBean</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCircularReferenceBean</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testFrequentItems</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testFrequentItems</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testSampleByColumn</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testSampleByColumn</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testExecution</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testExecution</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testTextLoad</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testTextLoad</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.pivot</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.pivot</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testGenericLoad</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testGenericLoad</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCountMinSketch</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCountMinSketch</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.pivotColumnValues</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.pivotColumnValues</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFrameFromJavaBeans</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFrameFromJavaBeans</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCorrelation</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCorrelation</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testBloomFilter</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testBloomFilter</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCovariance</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCovariance</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFrameFromLocalJavaBeans</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFrameFromLocalJavaBeans</div></li><li><div>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationCount</div></li><li><div>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationCount</div></li><li><div>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationSumDouble</div></li><li><div>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationSumDouble</div></li><li><div>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationSumLong</div></li><li><div>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationSumLong</div></li><li><div>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationAnonClass</div></li><li><div>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationAnonClass</div></li><li><div>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationAverage</div></li><li><div>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationAverage</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testRuntimeNullabilityCheck</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testRuntimeNullabilityCheck</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean1</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean1</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean2</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean2</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean3</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean3</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testSerializeNull</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testSerializeNull</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testRandomSplit</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testRandomSplit</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testTypedFilterPreservingSchema</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testTypedFilterPreservingSchema</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJoin</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJoin</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testTake</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testTake</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testToLocalIterator</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testToLocalIterator</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testSpecificLists</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testSpecificLists</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testForeach</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testForeach</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJavaEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJavaEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testPrimitiveEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testPrimitiveEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testEmptyBean</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testEmptyBean</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testCommonOperation</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testCommonOperation</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testNullInTopLevelBean</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testNullInTopLevelBean</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testGroupBy</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testGroupBy</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testSetOperation</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testSetOperation</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testBeanWithEnum</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testBeanWithEnum</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testKryoEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testKryoEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.test</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.test</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJavaBeanEncoder2</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJavaBeanEncoder2</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testCollect</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testCollect</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testKryoEncoderErrorMessageForPrivateClass</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testKryoEncoderErrorMessageForPrivateClass</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJavaBeanEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJavaBeanEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testTupleEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testTupleEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testNestedTupleEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testNestedTupleEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testTupleEncoderSchema</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testTupleEncoderSchema</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testReduce</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testReduce</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testSelect</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testSelect</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJavaEncoderErrorMessageForPrivateClass</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJavaEncoderErrorMessageForPrivateClass</div></li><li><div>test.org.apache.spark.sql.JavaSaveLoadSuite.saveAndLoadWithSchema</div></li><li><div>test.org.apache.spark.sql.JavaSaveLoadSuite.saveAndLoadWithSchema</div></li><li><div>test.org.apache.spark.sql.JavaSaveLoadSuite.saveAndLoad</div></li><li><div>test.org.apache.spark.sql.JavaSaveLoadSuite.saveAndLoad</div></li><li><div>test.org.apache.spark.sql.JavaUDAFSuite.udf1Test</div></li><li><div>test.org.apache.spark.sql.JavaUDAFSuite.udf1Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf1Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf1Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf2Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf2Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf3Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf3Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf4Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf4Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf5Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf5Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf6Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf6Test</div></li><li><div>test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.testForeachBatchAPI</div></li><li><div>test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.testForeachBatchAPI</div></li><li><div>test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.testForeachAPI</div></li><li><div>test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.testForeachAPI</div></li><li><div>org.apache.spark.sql.hive.HiveMetastoreLazyInitializationSuite.lazily initialize Hive client</div></li><li><div>org.apache.spark.sql.hive.JavaDataFrameSuite.testUDAF</div></li><li><div>org.apache.spark.sql.hive.JavaDataFrameSuite.saveTableAndQueryIt</div></li><li><div>org.apache.spark.sql.hive.JavaMetastoreDataSourcesSuite.saveExternalTableAndQueryIt</div></li><li><div>org.apache.spark.sql.hive.JavaMetastoreDataSourcesSuite.saveTableAndQueryIt</div></li><li><div>org.apache.spark.sql.hive.JavaMetastoreDataSourcesSuite.saveExternalTableWithSchemaAndQueryIt</div></li><li><div>org.apache.spark.streaming.JavaMapWithStateSuite.testBasicFunction</div></li><li><div>org.apache.spark.streaming.JavaReceiverAPISuite.testReceiver</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testStreamingContextTransform</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testFlatMapValues</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testFlatMapValues</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testMapPartitions</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testMapPartitions</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairFilter</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairFilter</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testCombineByKey</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testCombineByKey</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testMap</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testMap</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairToNormalRDDTransform</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairToNormalRDDTransform</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairReduceByKey</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairReduceByKey</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairMap</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairMap</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testFlatMap</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testReduceByKeyAndWindowWithInverse</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testReduceByWindow</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testReduceByWindow</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairFlatMap</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairFlatMap</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairToPairFlatMapWithChangingTypes</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairToPairFlatMapWithChangingTypes</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairMapPartitions</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairMapPartitions</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testVariousTransform</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testVariousTransform</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testTransformWith</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testTransformWith</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testVariousTransformWith</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testVariousTransformWith</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testReduceByKeyAndWindow</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testReduceByKeyAndWindow</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairTransform</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairTransform</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testFilter</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testFilter</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairMap2</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairMap2</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testMapValues</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testMapValues</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testReduce</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testReduce</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testUpdateStateByKey</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testUpdateStateByKey</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testTransform</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testTransform</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testStreamingContextTransform</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testFlatMapValues</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testReduceByWindowWithInverse</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testMapPartitions</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairFilter</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testRepartitionFewerPartitions</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testRepartitionFewerPartitions</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCombineByKey</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCombineByKey</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testContextGetOrCreate</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testWindowWithSlideDuration</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testQueueStream</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testQueueStream</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCountByValue</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCountByValue</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testMap</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testMap</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairToNormalRDDTransform</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairToNormalRDDTransform</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairReduceByKey</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairReduceByKey</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCount</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCount</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCheckpointMasterRecovery</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCheckpointMasterRecovery</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairMap</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testUnion</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testUnion</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testFlatMap</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testFlatMap</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testReduceByKeyAndWindowWithInverse</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testReduceByKeyAndWindowWithInverse</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testGlom</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testGlom</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testJoin</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairFlatMap</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairFlatMap</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairToPairFlatMapWithChangingTypes</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairToPairFlatMapWithChangingTypes</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairMapPartitions</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairMapPartitions</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testRepartitionMorePartitions</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testRepartitionMorePartitions</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testReduceByWindowWithoutInverse</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testReduceByWindowWithoutInverse</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testLeftOuterJoin</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testLeftOuterJoin</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testVariousTransform</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testVariousTransform</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testTransformWith</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testTransformWith</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testVariousTransformWith</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testVariousTransformWith</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testTextFileStream</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testTextFileStream</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairGroupByKey</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairGroupByKey</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCoGroup</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCoGroup</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testInitialization</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testInitialization</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testGroupByKeyAndWindow</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testGroupByKeyAndWindow</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testReduceByKeyAndWindow</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testReduceByKeyAndWindow</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testForeachRDD</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testForeachRDD</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testFileStream</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testFileStream</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairTransform</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairTransform</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testFilter</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testFilter</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairMap2</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairMap2</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testMapValues</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testReduce</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testUpdateStateByKey</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testTransform</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testTransform</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testWindow</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testWindow</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCountByValueAndWindow</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCountByValueAndWindow</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testRawSocketStream</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testRawSocketStream</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testSocketTextStream</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testSocketTextStream</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testUpdateStateByKeyWithInitial</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testUpdateStateByKeyWithInitial</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testContextState</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testContextState</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.spark.network.ChunkFetchIntegrationSuite.fetchBothChunks</div></li><li><div>org.apache.spark.network.sasl.SparkSaslSuite.testFileRegionEncryption</div></li><li><div>org.apache.spark.ExecutorPluginSuite.testAddMultiplePlugins</div></li><li><div>org.apache.spark.ExecutorPluginSuite.testPluginClassDoesNotExist</div></li><li><div>org.apache.spark.ExecutorPluginSuite.testAddPlugin</div></li><li><div>org.apache.spark.ExecutorPluginSuite.testPluginShutdownWithException</div></li><li><div>org.apache.spark.JavaJdbcRDDSuite.testJavaJdbcRDD</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.leftOuterJoin</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.leftOuterJoin</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.foldReduce</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.foldReduce</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.mapsFromPairsToPairs</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.mapsFromPairsToPairs</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.flatMap</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.flatMap</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.foreach</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.foreach</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.map</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.map</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.zip</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.zip</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.keyBy</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.keyBy</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.groupBy</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.groupBy</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.mapPartitions</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.mapPartitions</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.foldByKey</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.foldByKey</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.mapOnPairRDD</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.mapOnPairRDD</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.sequenceFile</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.sequenceFile</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.collectPartitions</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.collectPartitions</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.reduceByKey</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.foreachWithAnonymousClass</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.foreachWithAnonymousClass</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.collectAsMapWithIntArrayValues</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.collectAsMapWithIntArrayValues</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.zipPartitions</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.zipPartitions</div></li><li><div>test.org.apache.spark.JavaAPISuite.groupByOnPairRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.groupByOnPairRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.binaryFilesCaching</div></li><li><div>test.org.apache.spark.JavaAPISuite.binaryFilesCaching</div></li><li><div>test.org.apache.spark.JavaAPISuite.sparkContextUnion</div></li><li><div>test.org.apache.spark.JavaAPISuite.sparkContextUnion</div></li><li><div>test.org.apache.spark.JavaAPISuite.checkpointAndComputation</div></li><li><div>test.org.apache.spark.JavaAPISuite.checkpointAndComputation</div></li><li><div>test.org.apache.spark.JavaAPISuite.leftOuterJoin</div></li><li><div>test.org.apache.spark.JavaAPISuite.leftOuterJoin</div></li><li><div>test.org.apache.spark.JavaAPISuite.keyByOnPairRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.keyByOnPairRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.getNumPartitions</div></li><li><div>test.org.apache.spark.JavaAPISuite.getNumPartitions</div></li><li><div>test.org.apache.spark.JavaAPISuite.wholeTextFiles</div></li><li><div>test.org.apache.spark.JavaAPISuite.wholeTextFiles</div></li><li><div>test.org.apache.spark.JavaAPISuite.binaryFiles</div></li><li><div>test.org.apache.spark.JavaAPISuite.binaryFiles</div></li><li><div>test.org.apache.spark.JavaAPISuite.foldReduce</div></li><li><div>test.org.apache.spark.JavaAPISuite.foldReduce</div></li><li><div>test.org.apache.spark.JavaAPISuite.writeWithNewAPIHadoopFile</div></li><li><div>test.org.apache.spark.JavaAPISuite.writeWithNewAPIHadoopFile</div></li><li><div>test.org.apache.spark.JavaAPISuite.hadoopFile</div></li><li><div>test.org.apache.spark.JavaAPISuite.hadoopFile</div></li><li><div>test.org.apache.spark.JavaAPISuite.lookup</div></li><li><div>test.org.apache.spark.JavaAPISuite.lookup</div></li><li><div>test.org.apache.spark.JavaAPISuite.countAsync</div></li><li><div>test.org.apache.spark.JavaAPISuite.countAsync</div></li><li><div>test.org.apache.spark.JavaAPISuite.textFiles</div></li><li><div>test.org.apache.spark.JavaAPISuite.textFiles</div></li><li><div>test.org.apache.spark.JavaAPISuite.binaryRecords</div></li><li><div>test.org.apache.spark.JavaAPISuite.binaryRecords</div></li><li><div>test.org.apache.spark.JavaAPISuite.toLocalIterator</div></li><li><div>test.org.apache.spark.JavaAPISuite.toLocalIterator</div></li><li><div>test.org.apache.spark.JavaAPISuite.repartitionAndSortWithinPartitions</div></li><li><div>test.org.apache.spark.JavaAPISuite.repartitionAndSortWithinPartitions</div></li><li><div>test.org.apache.spark.JavaAPISuite.reduce</div></li><li><div>test.org.apache.spark.JavaAPISuite.reduce</div></li><li><div>test.org.apache.spark.JavaAPISuite.sample</div></li><li><div>test.org.apache.spark.JavaAPISuite.sample</div></li><li><div>test.org.apache.spark.JavaAPISuite.sortBy</div></li><li><div>test.org.apache.spark.JavaAPISuite.sortBy</div></li><li><div>test.org.apache.spark.JavaAPISuite.mapsFromPairsToPairs</div></li><li><div>test.org.apache.spark.JavaAPISuite.mapsFromPairsToPairs</div></li><li><div>test.org.apache.spark.JavaAPISuite.flatMap</div></li><li><div>test.org.apache.spark.JavaAPISuite.flatMap</div></li><li><div>test.org.apache.spark.JavaAPISuite.cogroup3</div></li><li><div>test.org.apache.spark.JavaAPISuite.cogroup3</div></li><li><div>test.org.apache.spark.JavaAPISuite.cogroup4</div></li><li><div>test.org.apache.spark.JavaAPISuite.cogroup4</div></li><li><div>test.org.apache.spark.JavaAPISuite.randomSplit</div></li><li><div>test.org.apache.spark.JavaAPISuite.randomSplit</div></li><li><div>test.org.apache.spark.JavaAPISuite.persist</div></li><li><div>test.org.apache.spark.JavaAPISuite.persist</div></li><li><div>test.org.apache.spark.JavaAPISuite.foreach</div></li><li><div>test.org.apache.spark.JavaAPISuite.foreach</div></li><li><div>test.org.apache.spark.JavaAPISuite.hadoopFileCompressed</div></li><li><div>test.org.apache.spark.JavaAPISuite.hadoopFileCompressed</div></li><li><div>test.org.apache.spark.JavaAPISuite.textFilesCompressed</div></li><li><div>test.org.apache.spark.JavaAPISuite.textFilesCompressed</div></li><li><div>test.org.apache.spark.JavaAPISuite.testAsyncActionCancellation</div></li><li><div>test.org.apache.spark.JavaAPISuite.testAsyncActionCancellation</div></li><li><div>test.org.apache.spark.JavaAPISuite.checkpointAndRestore</div></li><li><div>test.org.apache.spark.JavaAPISuite.checkpointAndRestore</div></li><li><div>test.org.apache.spark.JavaAPISuite.sortByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.sortByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.aggregateByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.aggregateByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.map</div></li><li><div>test.org.apache.spark.JavaAPISuite.map</div></li><li><div>test.org.apache.spark.JavaAPISuite.max</div></li><li><div>test.org.apache.spark.JavaAPISuite.max</div></li><li><div>test.org.apache.spark.JavaAPISuite.min</div></li><li><div>test.org.apache.spark.JavaAPISuite.min</div></li><li><div>test.org.apache.spark.JavaAPISuite.top</div></li><li><div>test.org.apache.spark.JavaAPISuite.top</div></li><li><div>test.org.apache.spark.JavaAPISuite.zip</div></li><li><div>test.org.apache.spark.JavaAPISuite.fold</div></li><li><div>test.org.apache.spark.JavaAPISuite.fold</div></li><li><div>test.org.apache.spark.JavaAPISuite.glom</div></li><li><div>test.org.apache.spark.JavaAPISuite.glom</div></li><li><div>test.org.apache.spark.JavaAPISuite.take</div></li><li><div>test.org.apache.spark.JavaAPISuite.take</div></li><li><div>test.org.apache.spark.JavaAPISuite.javaDoubleRDDHistoGram</div></li><li><div>test.org.apache.spark.JavaAPISuite.javaDoubleRDDHistoGram</div></li><li><div>test.org.apache.spark.JavaAPISuite.collectUnderlyingScalaRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.collectUnderlyingScalaRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.keyBy</div></li><li><div>test.org.apache.spark.JavaAPISuite.keyBy</div></li><li><div>test.org.apache.spark.JavaAPISuite.mapPartitionsWithIndex</div></li><li><div>test.org.apache.spark.JavaAPISuite.mapPartitionsWithIndex</div></li><li><div>test.org.apache.spark.JavaAPISuite.sampleByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.sampleByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.intersection</div></li><li><div>test.org.apache.spark.JavaAPISuite.intersection</div></li><li><div>test.org.apache.spark.JavaAPISuite.aggregate</div></li><li><div>test.org.apache.spark.JavaAPISuite.aggregate</div></li><li><div>test.org.apache.spark.JavaAPISuite.cartesian</div></li><li><div>test.org.apache.spark.JavaAPISuite.cartesian</div></li><li><div>test.org.apache.spark.JavaAPISuite.countApproxDistinctByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.countApproxDistinctByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.readWithNewAPIHadoopFile</div></li><li><div>test.org.apache.spark.JavaAPISuite.readWithNewAPIHadoopFile</div></li><li><div>test.org.apache.spark.JavaAPISuite.testRegisterKryoClasses</div></li><li><div>test.org.apache.spark.JavaAPISuite.testRegisterKryoClasses</div></li><li><div>test.org.apache.spark.JavaAPISuite.groupBy</div></li><li><div>test.org.apache.spark.JavaAPISuite.groupBy</div></li><li><div>test.org.apache.spark.JavaAPISuite.sampleByKeyExact</div></li><li><div>test.org.apache.spark.JavaAPISuite.sampleByKeyExact</div></li><li><div>test.org.apache.spark.JavaAPISuite.mapPartitions</div></li><li><div>test.org.apache.spark.JavaAPISuite.mapPartitions</div></li><li><div>test.org.apache.spark.JavaAPISuite.takeOrdered</div></li><li><div>test.org.apache.spark.JavaAPISuite.takeOrdered</div></li><li><div>test.org.apache.spark.JavaAPISuite.foldByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.foldByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.objectFilesOfInts</div></li><li><div>test.org.apache.spark.JavaAPISuite.objectFilesOfInts</div></li><li><div>test.org.apache.spark.JavaAPISuite.treeAggregate</div></li><li><div>test.org.apache.spark.JavaAPISuite.treeAggregate</div></li><li><div>test.org.apache.spark.JavaAPISuite.testGetPersistentRDDs</div></li><li><div>test.org.apache.spark.JavaAPISuite.testGetPersistentRDDs</div></li><li><div>test.org.apache.spark.JavaAPISuite.approximateResults</div></li><li><div>test.org.apache.spark.JavaAPISuite.approximateResults</div></li><li><div>test.org.apache.spark.JavaAPISuite.treeReduce</div></li><li><div>test.org.apache.spark.JavaAPISuite.treeReduce</div></li><li><div>test.org.apache.spark.JavaAPISuite.collectAsMapAndSerialize</div></li><li><div>test.org.apache.spark.JavaAPISuite.collectAsMapAndSerialize</div></li><li><div>test.org.apache.spark.JavaAPISuite.countApproxDistinct</div></li><li><div>test.org.apache.spark.JavaAPISuite.countApproxDistinct</div></li><li><div>test.org.apache.spark.JavaAPISuite.javaDoubleRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.javaDoubleRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.mapOnPairRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.mapOnPairRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.testAsyncActionErrorWrapping</div></li><li><div>test.org.apache.spark.JavaAPISuite.naturalMax</div></li><li><div>test.org.apache.spark.JavaAPISuite.naturalMin</div></li><li><div>test.org.apache.spark.JavaAPISuite.sequenceFile</div></li><li><div>test.org.apache.spark.JavaAPISuite.sequenceFile</div></li><li><div>test.org.apache.spark.JavaAPISuite.collectPartitions</div></li><li><div>test.org.apache.spark.JavaAPISuite.collectPartitions</div></li><li><div>test.org.apache.spark.JavaAPISuite.cogroup</div></li><li><div>test.org.apache.spark.JavaAPISuite.cogroup</div></li><li><div>test.org.apache.spark.JavaAPISuite.reduceByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.reduceByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.repartition</div></li><li><div>test.org.apache.spark.JavaAPISuite.repartition</div></li><li><div>test.org.apache.spark.JavaAPISuite.iterator</div></li><li><div>test.org.apache.spark.JavaAPISuite.iterator</div></li><li><div>test.org.apache.spark.JavaAPISuite.emptyRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.emptyRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.zipWithIndex</div></li><li><div>test.org.apache.spark.JavaAPISuite.zipWithIndex</div></li><li><div>test.org.apache.spark.JavaAPISuite.foreachPartition</div></li><li><div>test.org.apache.spark.JavaAPISuite.foreachPartition</div></li><li><div>test.org.apache.spark.JavaAPISuite.combineByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.combineByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.takeAsync</div></li><li><div>test.org.apache.spark.JavaAPISuite.takeAsync</div></li><li><div>test.org.apache.spark.JavaAPISuite.collectAsMapWithIntArrayValues</div></li><li><div>test.org.apache.spark.JavaAPISuite.collectAsMapWithIntArrayValues</div></li><li><div>test.org.apache.spark.JavaAPISuite.objectFilesOfComplexTypes</div></li><li><div>test.org.apache.spark.JavaAPISuite.objectFilesOfComplexTypes</div></li><li><div>test.org.apache.spark.JavaAPISuite.zipWithUniqueId</div></li><li><div>test.org.apache.spark.JavaAPISuite.zipWithUniqueId</div></li><li><div>test.org.apache.spark.JavaAPISuite.collectAsync</div></li><li><div>test.org.apache.spark.JavaAPISuite.collectAsync</div></li><li><div>test.org.apache.spark.JavaAPISuite.foreachAsync</div></li><li><div>test.org.apache.spark.JavaAPISuite.foreachAsync</div></li><li><div>test.org.apache.spark.JavaAPISuite.zipPartitions</div></li><li><div>test.org.apache.spark.JavaAPISuite.zipPartitions</div></li><li><div>test.org.apache.spark.JavaAPISuite.reduceOnJavaDoubleRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.reduceOnJavaDoubleRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.isEmpty</div></li><li><div>test.org.apache.spark.JavaAPISuite.isEmpty</div></li><li><div>test.org.apache.spark.JavaSparkContextSuite.scalaSparkContext</div></li><li><div>org.apache.spark.graphx.EdgeRDDSuite.cache, getStorageLevel</div></li><li><div>org.apache.spark.graphx.EdgeRDDSuite.checkpointing</div></li><li><div>org.apache.spark.graphx.GraphLoaderSuite.GraphLoader.edgeListFile</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.joinVertices</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.collectNeighborIds</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.removeSelfEdges</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.filter</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.convertToCanonicalEdges</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.collectEdgesCycleDirectionOut</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.collectEdgesCycleDirectionIn</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.collectEdgesCycleDirectionEither</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.collectEdgesChainDirectionOut</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.collectEdgesChainDirectionIn</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.collectEdgesChainDirectionEither</div></li><li><div>org.apache.spark.graphx.GraphSuite.Graph.fromEdgeTuples</div></li><li><div>org.apache.spark.graphx.GraphSuite.Graph.fromEdges</div></li><li><div>org.apache.spark.graphx.GraphSuite.Graph.apply</div></li><li><div>org.apache.spark.graphx.GraphSuite.triplets</div></li><li><div>org.apache.spark.graphx.GraphSuite.partitionBy</div></li><li><div>org.apache.spark.graphx.GraphSuite.mapVertices</div></li><li><div>org.apache.spark.graphx.GraphSuite.mapVertices changing type with same erased type</div></li><li><div>org.apache.spark.graphx.GraphSuite.mapEdges</div></li><li><div>org.apache.spark.graphx.GraphSuite.mapTriplets</div></li><li><div>org.apache.spark.graphx.GraphSuite.reverse</div></li><li><div>org.apache.spark.graphx.GraphSuite.reverse with join elimination</div></li><li><div>org.apache.spark.graphx.GraphSuite.subgraph</div></li><li><div>org.apache.spark.graphx.GraphSuite.mask</div></li><li><div>org.apache.spark.graphx.GraphSuite.groupEdges</div></li><li><div>org.apache.spark.graphx.GraphSuite.aggregateMessages</div></li><li><div>org.apache.spark.graphx.GraphSuite.outerJoinVertices</div></li><li><div>org.apache.spark.graphx.GraphSuite.more edge partitions than vertex partitions</div></li><li><div>org.apache.spark.graphx.GraphSuite.checkpoint</div></li><li><div>org.apache.spark.graphx.GraphSuite.cache, getStorageLevel</div></li><li><div>org.apache.spark.graphx.GraphSuite.non-default number of edge partitions</div></li><li><div>org.apache.spark.graphx.GraphSuite.unpersist graph RDD</div></li><li><div>org.apache.spark.graphx.GraphSuite.SPARK-14219: pickRandomVertex</div></li><li><div>org.apache.spark.graphx.PregelSuite.1 iteration</div></li><li><div>org.apache.spark.graphx.PregelSuite.chain propagation</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.filter</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.mapValues</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.minus</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.minus with RDD[(VertexId, VD)]</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.minus with non-equal number of partitions</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.diff</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.diff with RDD[(VertexId, VD)]</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.diff vertices with non-equal number of partitions</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.leftJoin</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.leftJoin vertices with non-equal number of partitions</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.innerJoin</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.innerJoin vertices with the non-equal number of partitions</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.aggregateUsingIndex</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.mergeFunc</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.cache, getStorageLevel</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.checkpoint</div></li><li><div>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Grid Connected Components</div></li><li><div>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Reverse Grid Connected Components</div></li><li><div>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Chain Connected Components</div></li><li><div>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Reverse Chain Connected Components</div></li><li><div>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Connected Components on a Toy Connected Graph</div></li><li><div>org.apache.spark.graphx.lib.LabelPropagationSuite.Label Propagation</div></li><li><div>org.apache.spark.graphx.lib.PageRankSuite.Star PageRank</div></li><li><div>org.apache.spark.graphx.lib.PageRankSuite.Star PersonalPageRank</div></li><li><div>org.apache.spark.graphx.lib.PageRankSuite.Grid PageRank</div></li><li><div>org.apache.spark.graphx.lib.PageRankSuite.Chain PageRank</div></li><li><div>org.apache.spark.graphx.lib.PageRankSuite.Chain PersonalizedPageRank</div></li><li><div>org.apache.spark.graphx.lib.PageRankSuite.Loop with source PageRank</div></li><li><div>org.apache.spark.graphx.lib.PageRankSuite.Loop with sink PageRank</div></li><li><div>org.apache.spark.graphx.lib.SVDPlusPlusSuite.Test SVD++ with mean square error on training set</div></li><li><div>org.apache.spark.graphx.lib.ShortestPathsSuite.Shortest Path Computations</div></li><li><div>org.apache.spark.graphx.lib.StronglyConnectedComponentsSuite.Island Strongly Connected Components</div></li><li><div>org.apache.spark.graphx.lib.StronglyConnectedComponentsSuite.Cycle Strongly Connected Components</div></li><li><div>org.apache.spark.graphx.lib.StronglyConnectedComponentsSuite.2 Cycle Strongly Connected Components</div></li><li><div>org.apache.spark.graphx.lib.TriangleCountSuite.Count a single triangle</div></li><li><div>org.apache.spark.graphx.lib.TriangleCountSuite.Count two triangles</div></li><li><div>org.apache.spark.graphx.lib.TriangleCountSuite.Count two triangles with bi-directed edges</div></li><li><div>org.apache.spark.graphx.lib.TriangleCountSuite.Count a single triangle with duplicate edges</div></li><li><div>org.apache.spark.graphx.util.GraphGeneratorsSuite.GraphGenerators.logNormalGraph</div></li><li><div>org.apache.spark.graphx.util.GraphGeneratorsSuite.SPARK-5064 GraphGenerators.rmatGraph numEdges upper bound</div></li><li><div>org.apache.spark.graphx.util.PeriodicGraphCheckpointerSuite.Persisting</div></li><li><div>org.apache.spark.graphx.util.PeriodicGraphCheckpointerSuite.Checkpointing</div></li><li><div>org.apache.spark.ml.JavaPipelineSuite.pipeline</div></li><li><div>org.apache.spark.ml.JavaPipelineSuite.pipeline</div></li><li><div>org.apache.spark.ml.classification.JavaDecisionTreeClassifierSuite.runDT</div></li><li><div>org.apache.spark.ml.classification.JavaDecisionTreeClassifierSuite.runDT</div></li><li><div>org.apache.spark.ml.classification.JavaGBTClassifierSuite.runDT</div></li><li><div>org.apache.spark.ml.classification.JavaGBTClassifierSuite.runDT</div></li><li><div>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionWithSetters</div></li><li><div>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionWithSetters</div></li><li><div>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionTrainingSummary</div></li><li><div>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionTrainingSummary</div></li><li><div>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionPredictorClassifierMethods</div></li><li><div>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionPredictorClassifierMethods</div></li><li><div>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionDefaultParams</div></li><li><div>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionDefaultParams</div></li><li><div>org.apache.spark.ml.classification.JavaMultilayerPerceptronClassifierSuite.testMLPC</div></li><li><div>org.apache.spark.ml.classification.JavaMultilayerPerceptronClassifierSuite.testMLPC</div></li><li><div>org.apache.spark.ml.classification.JavaNaiveBayesSuite.testNaiveBayes</div></li><li><div>org.apache.spark.ml.classification.JavaNaiveBayesSuite.testNaiveBayes</div></li><li><div>org.apache.spark.ml.classification.JavaNaiveBayesSuite.naiveBayesDefaultParams</div></li><li><div>org.apache.spark.ml.classification.JavaNaiveBayesSuite.naiveBayesDefaultParams</div></li><li><div>org.apache.spark.ml.classification.JavaOneVsRestSuite.oneVsRestDefaultParams</div></li><li><div>org.apache.spark.ml.classification.JavaOneVsRestSuite.oneVsRestDefaultParams</div></li><li><div>org.apache.spark.ml.classification.JavaRandomForestClassifierSuite.runDT</div></li><li><div>org.apache.spark.ml.classification.JavaRandomForestClassifierSuite.runDT</div></li><li><div>org.apache.spark.ml.clustering.JavaKMeansSuite.fitAndTransform</div></li><li><div>org.apache.spark.ml.clustering.JavaKMeansSuite.fitAndTransform</div></li><li><div>org.apache.spark.ml.feature.JavaBucketizerSuite.bucketizerTest</div></li><li><div>org.apache.spark.ml.feature.JavaBucketizerSuite.bucketizerMultipleColumnsTest</div></li><li><div>org.apache.spark.ml.feature.JavaBucketizerSuite.bucketizerMultipleColumnsTest</div></li><li><div>org.apache.spark.ml.feature.JavaDCTSuite.javaCompatibilityTest</div></li><li><div>org.apache.spark.ml.feature.JavaDCTSuite.javaCompatibilityTest</div></li><li><div>org.apache.spark.ml.feature.JavaHashingTFSuite.hashingTF</div></li><li><div>org.apache.spark.ml.feature.JavaHashingTFSuite.hashingTF</div></li><li><div>org.apache.spark.ml.feature.JavaNormalizerSuite.normalizer</div></li><li><div>org.apache.spark.ml.feature.JavaNormalizerSuite.normalizer</div></li><li><div>org.apache.spark.ml.feature.JavaPCASuite.testPCA</div></li><li><div>org.apache.spark.ml.feature.JavaPCASuite.testPCA</div></li><li><div>org.apache.spark.ml.feature.JavaPolynomialExpansionSuite.polynomialExpansionTest</div></li><li><div>org.apache.spark.ml.feature.JavaPolynomialExpansionSuite.polynomialExpansionTest</div></li><li><div>org.apache.spark.ml.feature.JavaStandardScalerSuite.standardScaler</div></li><li><div>org.apache.spark.ml.feature.JavaStandardScalerSuite.standardScaler</div></li><li><div>org.apache.spark.ml.feature.JavaStopWordsRemoverSuite.javaCompatibilityTest</div></li><li><div>org.apache.spark.ml.feature.JavaStopWordsRemoverSuite.javaCompatibilityTest</div></li><li><div>org.apache.spark.ml.feature.JavaStringIndexerSuite.testStringIndexer</div></li><li><div>org.apache.spark.ml.feature.JavaStringIndexerSuite.testStringIndexer</div></li><li><div>org.apache.spark.ml.feature.JavaTokenizerSuite.regexTokenizer</div></li><li><div>org.apache.spark.ml.feature.JavaVectorAssemblerSuite.testVectorAssembler</div></li><li><div>org.apache.spark.ml.feature.JavaVectorAssemblerSuite.testVectorAssembler</div></li><li><div>org.apache.spark.ml.feature.JavaVectorIndexerSuite.vectorIndexerAPI</div></li><li><div>org.apache.spark.ml.feature.JavaVectorIndexerSuite.vectorIndexerAPI</div></li><li><div>org.apache.spark.ml.feature.JavaVectorSlicerSuite.vectorSlice</div></li><li><div>org.apache.spark.ml.feature.JavaVectorSlicerSuite.vectorSlice</div></li><li><div>org.apache.spark.ml.feature.JavaWord2VecSuite.testJavaWord2Vec</div></li><li><div>org.apache.spark.ml.feature.JavaWord2VecSuite.testJavaWord2Vec</div></li><li><div>org.apache.spark.ml.regression.JavaDecisionTreeRegressorSuite.runDT</div></li><li><div>org.apache.spark.ml.regression.JavaDecisionTreeRegressorSuite.runDT</div></li><li><div>org.apache.spark.ml.regression.JavaGBTRegressorSuite.runDT</div></li><li><div>org.apache.spark.ml.regression.JavaGBTRegressorSuite.runDT</div></li><li><div>org.apache.spark.ml.regression.JavaLinearRegressionSuite.linearRegressionDefaultParams</div></li><li><div>org.apache.spark.ml.regression.JavaLinearRegressionSuite.linearRegressionDefaultParams</div></li><li><div>org.apache.spark.ml.regression.JavaLinearRegressionSuite.linearRegressionWithSetters</div></li><li><div>org.apache.spark.ml.regression.JavaLinearRegressionSuite.linearRegressionWithSetters</div></li><li><div>org.apache.spark.ml.regression.JavaRandomForestRegressorSuite.runDT</div></li><li><div>org.apache.spark.ml.regression.JavaRandomForestRegressorSuite.runDT</div></li><li><div>org.apache.spark.ml.source.libsvm.JavaLibSVMRelationSuite.verifyLibSVMDF</div></li><li><div>org.apache.spark.ml.source.libsvm.JavaLibSVMRelationSuite.verifyLibSVMDF</div></li><li><div>org.apache.spark.ml.stat.JavaKolmogorovSmirnovTestSuite.testKSTestNamedDistribution</div></li><li><div>org.apache.spark.ml.stat.JavaKolmogorovSmirnovTestSuite.testKSTestCDF</div></li><li><div>org.apache.spark.ml.stat.JavaSummarizerSuite.testSummarizer</div></li><li><div>org.apache.spark.ml.tuning.JavaCrossValidatorSuite.crossValidationWithLogisticRegression</div></li><li><div>org.apache.spark.ml.tuning.JavaCrossValidatorSuite.crossValidationWithLogisticRegression</div></li><li><div>org.apache.spark.ml.util.JavaDefaultReadWriteSuite.testDefaultReadWrite</div></li><li><div>org.apache.spark.ml.util.JavaDefaultReadWriteSuite.testDefaultReadWrite</div></li><li><div>org.apache.spark.mllib.classification.JavaLogisticRegressionSuite.runLRUsingConstructor</div></li><li><div>org.apache.spark.mllib.classification.JavaLogisticRegressionSuite.runLRUsingConstructor</div></li><li><div>org.apache.spark.mllib.classification.JavaLogisticRegressionSuite.runLRUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.classification.JavaLogisticRegressionSuite.runLRUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.testPredictJavaRDD</div></li><li><div>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.testPredictJavaRDD</div></li><li><div>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.runUsingConstructor</div></li><li><div>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.runUsingConstructor</div></li><li><div>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.runUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.runUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.testModelTypeSetters</div></li><li><div>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.testModelTypeSetters</div></li><li><div>org.apache.spark.mllib.classification.JavaSVMSuite.runSVMUsingConstructor</div></li><li><div>org.apache.spark.mllib.classification.JavaSVMSuite.runSVMUsingConstructor</div></li><li><div>org.apache.spark.mllib.classification.JavaSVMSuite.runSVMUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.classification.JavaSVMSuite.runSVMUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.classification.JavaStreamingLogisticRegressionSuite.javaAPI</div></li><li><div>org.apache.spark.mllib.classification.JavaStreamingLogisticRegressionSuite.javaAPI</div></li><li><div>org.apache.spark.mllib.clustering.JavaBisectingKMeansSuite.twoDimensionalData</div></li><li><div>org.apache.spark.mllib.clustering.JavaBisectingKMeansSuite.twoDimensionalData</div></li><li><div>org.apache.spark.mllib.clustering.JavaGaussianMixtureSuite.runGaussianMixture</div></li><li><div>org.apache.spark.mllib.clustering.JavaGaussianMixtureSuite.runGaussianMixture</div></li><li><div>org.apache.spark.mllib.clustering.JavaKMeansSuite.testPredictJavaRDD</div></li><li><div>org.apache.spark.mllib.clustering.JavaKMeansSuite.testPredictJavaRDD</div></li><li><div>org.apache.spark.mllib.clustering.JavaKMeansSuite.runKMeansUsingConstructor</div></li><li><div>org.apache.spark.mllib.clustering.JavaKMeansSuite.runKMeansUsingConstructor</div></li><li><div>org.apache.spark.mllib.clustering.JavaKMeansSuite.runKMeansUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.clustering.JavaKMeansSuite.runKMeansUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.clustering.JavaLDASuite.onlineOptimizerCompatibility</div></li><li><div>org.apache.spark.mllib.clustering.JavaLDASuite.distributedLDAModel</div></li><li><div>org.apache.spark.mllib.clustering.JavaLDASuite.distributedLDAModel</div></li><li><div>org.apache.spark.mllib.clustering.JavaLDASuite.localLDAModel</div></li><li><div>org.apache.spark.mllib.clustering.JavaLDASuite.localLDAModel</div></li><li><div>org.apache.spark.mllib.clustering.JavaLDASuite.localLdaMethods</div></li><li><div>org.apache.spark.mllib.clustering.JavaLDASuite.localLdaMethods</div></li><li><div>org.apache.spark.mllib.clustering.JavaStreamingKMeansSuite.javaAPI</div></li><li><div>org.apache.spark.mllib.clustering.JavaStreamingKMeansSuite.javaAPI</div></li><li><div>org.apache.spark.mllib.evaluation.JavaRankingMetricsSuite.rankingMetrics</div></li><li><div>org.apache.spark.mllib.evaluation.JavaRankingMetricsSuite.rankingMetrics</div></li><li><div>org.apache.spark.mllib.feature.JavaTfIdfSuite.tfIdfMinimumDocumentFrequency</div></li><li><div>org.apache.spark.mllib.feature.JavaTfIdfSuite.tfIdfMinimumDocumentFrequency</div></li><li><div>org.apache.spark.mllib.feature.JavaTfIdfSuite.tfIdf</div></li><li><div>org.apache.spark.mllib.feature.JavaTfIdfSuite.tfIdf</div></li><li><div>org.apache.spark.mllib.feature.JavaWord2VecSuite.word2Vec</div></li><li><div>org.apache.spark.mllib.feature.JavaWord2VecSuite.word2Vec</div></li><li><div>org.apache.spark.mllib.fpm.JavaAssociationRulesSuite.runAssociationRules</div></li><li><div>org.apache.spark.mllib.fpm.JavaAssociationRulesSuite.runAssociationRules</div></li><li><div>org.apache.spark.mllib.fpm.JavaFPGrowthSuite.runFPGrowthSaveLoad</div></li><li><div>org.apache.spark.mllib.fpm.JavaFPGrowthSuite.runFPGrowthSaveLoad</div></li><li><div>org.apache.spark.mllib.fpm.JavaFPGrowthSuite.runFPGrowth</div></li><li><div>org.apache.spark.mllib.fpm.JavaFPGrowthSuite.runFPGrowth</div></li><li><div>org.apache.spark.mllib.fpm.JavaPrefixSpanSuite.runPrefixSpan</div></li><li><div>org.apache.spark.mllib.fpm.JavaPrefixSpanSuite.runPrefixSpanSaveLoad</div></li><li><div>org.apache.spark.mllib.fpm.JavaPrefixSpanSuite.runPrefixSpanSaveLoad</div></li><li><div>org.apache.spark.mllib.linalg.distributed.JavaRowMatrixSuite.rowMatrixQRDecomposition</div></li><li><div>org.apache.spark.mllib.linalg.distributed.JavaRowMatrixSuite.rowMatrixQRDecomposition</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testNormalVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testNormalVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testArbitrary</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testArbitrary</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testLogNormalVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testLogNormalVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testExponentialVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testExponentialVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testUniformRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testUniformRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testRandomVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testRandomVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testGammaRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testGammaRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testUniformVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testUniformVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testPoissonRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testPoissonRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testNormalRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testNormalRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testPoissonVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testPoissonVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testGammaVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testGammaVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testExponentialRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testExponentialRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testLNormalRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testLNormalRDD</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runALSUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runALSUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSUsingConstructor</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSUsingConstructor</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runRecommend</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runRecommend</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSWithNegativeWeight</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSWithNegativeWeight</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runALSUsingConstructor</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runALSUsingConstructor</div></li><li><div>org.apache.spark.mllib.regression.JavaIsotonicRegressionSuite.testIsotonicRegressionJavaRDD</div></li><li><div>org.apache.spark.mllib.regression.JavaIsotonicRegressionSuite.testIsotonicRegressionJavaRDD</div></li><li><div>org.apache.spark.mllib.regression.JavaIsotonicRegressionSuite.testIsotonicRegressionPredictionsJavaRDD</div></li><li><div>org.apache.spark.mllib.regression.JavaLassoSuite.runLassoUsingConstructor</div></li><li><div>org.apache.spark.mllib.regression.JavaLassoSuite.runLassoUsingConstructor</div></li><li><div>org.apache.spark.mllib.regression.JavaLassoSuite.runLassoUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.regression.JavaLassoSuite.runLassoUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.testPredictJavaRDD</div></li><li><div>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.runLinearRegressionUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.runLinearRegressionUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.runLinearRegressionUsingConstructor</div></li><li><div>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.runLinearRegressionUsingConstructor</div></li><li><div>org.apache.spark.mllib.regression.JavaRidgeRegressionSuite.runRidgeRegressionUsingConstructor</div></li><li><div>org.apache.spark.mllib.regression.JavaRidgeRegressionSuite.runRidgeRegressionUsingConstructor</div></li><li><div>org.apache.spark.mllib.regression.JavaRidgeRegressionSuite.runRidgeRegressionUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.regression.JavaRidgeRegressionSuite.runRidgeRegressionUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.regression.JavaStreamingLinearRegressionSuite.javaAPI</div></li><li><div>org.apache.spark.mllib.regression.JavaStreamingLinearRegressionSuite.javaAPI</div></li><li><div>org.apache.spark.mllib.stat.JavaStatisticsSuite.testCorr</div></li><li><div>org.apache.spark.mllib.stat.JavaStatisticsSuite.testCorr</div></li><li><div>org.apache.spark.mllib.stat.JavaStatisticsSuite.chiSqTest</div></li><li><div>org.apache.spark.mllib.stat.JavaStatisticsSuite.chiSqTest</div></li><li><div>org.apache.spark.mllib.stat.JavaStatisticsSuite.streamingTest</div></li><li><div>org.apache.spark.mllib.stat.JavaStatisticsSuite.streamingTest</div></li><li><div>org.apache.spark.mllib.stat.JavaStatisticsSuite.kolmogorovSmirnovTest</div></li><li><div>org.apache.spark.mllib.stat.JavaStatisticsSuite.kolmogorovSmirnovTest</div></li><li><div>org.apache.spark.mllib.tree.JavaDecisionTreeSuite.runDTUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.tree.JavaDecisionTreeSuite.runDTUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.tree.JavaDecisionTreeSuite.runDTUsingConstructor</div></li><li><div>org.apache.spark.mllib.tree.JavaDecisionTreeSuite.runDTUsingConstructor</div></li><li><div>org.apache.spark.mllib.util.JavaMLUtilsSuite.testConvertMatrixColumnsToAndFromML</div></li><li><div>org.apache.spark.mllib.util.JavaMLUtilsSuite.testConvertMatrixColumnsToAndFromML</div></li><li><div>org.apache.spark.mllib.util.JavaMLUtilsSuite.testConvertVectorColumnsToAndFromML</div></li><li><div>org.apache.spark.mllib.util.JavaMLUtilsSuite.testConvertVectorColumnsToAndFromML</div></li><li><div>org.apache.spark.repl.ReplSuite.SPARK-15236: use Hive catalog</div></li><li><div>org.apache.spark.repl.ReplSuite.SPARK-15236: use in-memory catalog</div></li><li><div>org.apache.spark.repl.ReplSuite.broadcast vars</div></li><li><div>org.apache.spark.repl.ReplSuite.line wrapper only initialized once when used as encoder outer scope</div></li><li><div>org.apache.spark.repl.ReplSuite.define case class and create Dataset together with paste mode</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.simple foreach with accumulator</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.external vars</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.external classes</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.external functions</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.external functions that access vars</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.broadcast vars</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.interacting with files</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.local-cluster mode</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.SPARK-2576 importing implicits</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.Datasets and encoders</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.SPARK-2632 importing a method from non serializable class and not using it.</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.collecting objects of class defined in repl</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.collecting objects of class defined in repl - shuffling</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.replicating blocks of object with class defined in repl</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.newProductSeqEncoder with REPL defined class</div></li><li><div>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationCount</div></li><li><div>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationCount</div></li><li><div>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationSumDouble</div></li><li><div>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationSumDouble</div></li><li><div>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationSumLong</div></li><li><div>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationSumLong</div></li><li><div>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationAverage</div></li><li><div>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationAverage</div></li><li><div>test.org.apache.spark.sql.JavaApplySchemaSuite.applySchema</div></li><li><div>test.org.apache.spark.sql.JavaApplySchemaSuite.applySchema</div></li><li><div>test.org.apache.spark.sql.JavaApplySchemaSuite.dataFrameRDDOperations</div></li><li><div>test.org.apache.spark.sql.JavaApplySchemaSuite.dataFrameRDDOperations</div></li><li><div>test.org.apache.spark.sql.JavaApplySchemaSuite.applySchemaToJSON</div></li><li><div>test.org.apache.spark.sql.JavaApplySchemaSuite.applySchemaToJSON</div></li><li><div>test.org.apache.spark.sql.JavaBeanDeserializationSuite.testBeanWithArrayFieldDeserialization</div></li><li><div>test.org.apache.spark.sql.JavaBeanDeserializationSuite.testBeanWithArrayFieldDeserialization</div></li><li><div>test.org.apache.spark.sql.JavaBeanDeserializationSuite.testBeanWithMapFieldsDeserialization</div></li><li><div>test.org.apache.spark.sql.JavaBeanDeserializationSuite.testBeanWithMapFieldsDeserialization</div></li><li><div>test.org.apache.spark.sql.JavaColumnExpressionSuite.isInCollectionCheckExceptionMessage</div></li><li><div>test.org.apache.spark.sql.JavaColumnExpressionSuite.isInCollectionCheckExceptionMessage</div></li><li><div>test.org.apache.spark.sql.JavaColumnExpressionSuite.isInCollectionWorksCorrectlyOnJava</div></li><li><div>test.org.apache.spark.sql.JavaColumnExpressionSuite.isInCollectionWorksCorrectlyOnJava</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testFormatAPI</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testTextAPI</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testJsonAPI</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testLoadAPI</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testOptionsAPI</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testSaveModeAPI</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testCsvAPI</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testParquetAPI</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testTextFileAPI</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCollectAndTake</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCollectAndTake</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testJsonRDDToDataFrame</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testJsonRDDToDataFrame</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testVarargMethods</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testVarargMethods</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testBeanWithoutGetter</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testBeanWithoutGetter</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateStructTypeFromList</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateStructTypeFromList</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testSampleBy</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testSampleBy</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCrosstab</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCrosstab</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testUDF</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testUDF</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFromFromList</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFromFromList</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCircularReferenceBean</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCircularReferenceBean</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testFrequentItems</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testFrequentItems</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testSampleByColumn</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testSampleByColumn</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testExecution</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testExecution</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testTextLoad</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testTextLoad</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.pivot</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.pivot</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testGenericLoad</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testGenericLoad</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCountMinSketch</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCountMinSketch</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.pivotColumnValues</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.pivotColumnValues</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFrameFromJavaBeans</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFrameFromJavaBeans</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCorrelation</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCorrelation</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testBloomFilter</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testBloomFilter</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCovariance</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCovariance</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFrameFromLocalJavaBeans</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFrameFromLocalJavaBeans</div></li><li><div>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationCount</div></li><li><div>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationCount</div></li><li><div>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationSumDouble</div></li><li><div>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationSumDouble</div></li><li><div>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationSumLong</div></li><li><div>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationSumLong</div></li><li><div>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationAnonClass</div></li><li><div>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationAnonClass</div></li><li><div>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationAverage</div></li><li><div>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationAverage</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testRuntimeNullabilityCheck</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testRuntimeNullabilityCheck</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean1</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean1</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean2</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean2</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean3</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean3</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testSerializeNull</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testSerializeNull</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testRandomSplit</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testRandomSplit</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testTypedFilterPreservingSchema</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testTypedFilterPreservingSchema</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJoin</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJoin</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testTake</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testTake</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testToLocalIterator</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testToLocalIterator</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testSpecificLists</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testSpecificLists</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testForeach</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testForeach</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJavaEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJavaEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testPrimitiveEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testPrimitiveEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testEmptyBean</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testEmptyBean</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testCommonOperation</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testCommonOperation</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testNullInTopLevelBean</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testNullInTopLevelBean</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testGroupBy</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testGroupBy</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testSetOperation</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testSetOperation</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testBeanWithEnum</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testBeanWithEnum</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testKryoEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testKryoEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.test</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.test</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJavaBeanEncoder2</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJavaBeanEncoder2</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testCollect</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testCollect</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testKryoEncoderErrorMessageForPrivateClass</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testKryoEncoderErrorMessageForPrivateClass</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJavaBeanEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJavaBeanEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testTupleEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testTupleEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testNestedTupleEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testNestedTupleEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testTupleEncoderSchema</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testTupleEncoderSchema</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testReduce</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testReduce</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testSelect</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testSelect</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJavaEncoderErrorMessageForPrivateClass</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJavaEncoderErrorMessageForPrivateClass</div></li><li><div>test.org.apache.spark.sql.JavaSaveLoadSuite.saveAndLoadWithSchema</div></li><li><div>test.org.apache.spark.sql.JavaSaveLoadSuite.saveAndLoadWithSchema</div></li><li><div>test.org.apache.spark.sql.JavaSaveLoadSuite.saveAndLoad</div></li><li><div>test.org.apache.spark.sql.JavaSaveLoadSuite.saveAndLoad</div></li><li><div>test.org.apache.spark.sql.JavaUDAFSuite.udf1Test</div></li><li><div>test.org.apache.spark.sql.JavaUDAFSuite.udf1Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf1Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf1Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf2Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf2Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf3Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf3Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf4Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf4Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf5Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf5Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf6Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf6Test</div></li><li><div>test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.testForeachBatchAPI</div></li><li><div>test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.testForeachBatchAPI</div></li><li><div>test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.testForeachAPI</div></li><li><div>test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.testForeachAPI</div></li><li><div>org.apache.spark.sql.hive.HiveMetastoreLazyInitializationSuite.lazily initialize Hive client</div></li><li><div>org.apache.spark.sql.hive.JavaDataFrameSuite.testUDAF</div></li><li><div>org.apache.spark.sql.hive.JavaDataFrameSuite.saveTableAndQueryIt</div></li><li><div>org.apache.spark.sql.hive.JavaMetastoreDataSourcesSuite.saveExternalTableAndQueryIt</div></li><li><div>org.apache.spark.sql.hive.JavaMetastoreDataSourcesSuite.saveTableAndQueryIt</div></li><li><div>org.apache.spark.sql.hive.JavaMetastoreDataSourcesSuite.saveExternalTableWithSchemaAndQueryIt</div></li><li><div>org.apache.spark.streaming.JavaMapWithStateSuite.testBasicFunction</div></li><li><div>org.apache.spark.streaming.JavaReceiverAPISuite.testReceiver</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testStreamingContextTransform</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testFlatMapValues</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testMapPartitions</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testMapPartitions</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairFilter</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairFilter</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testCombineByKey</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testCombineByKey</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testMap</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testMap</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairToNormalRDDTransform</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairToNormalRDDTransform</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairReduceByKey</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairReduceByKey</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairMap</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairMap</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testFlatMap</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testFlatMap</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testReduceByKeyAndWindowWithInverse</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testReduceByKeyAndWindowWithInverse</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testReduceByWindow</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testReduceByWindow</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairFlatMap</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairFlatMap</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairToPairFlatMapWithChangingTypes</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairToPairFlatMapWithChangingTypes</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairMapPartitions</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairMapPartitions</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testVariousTransform</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testVariousTransform</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testTransformWith</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testTransformWith</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testVariousTransformWith</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testVariousTransformWith</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testReduceByKeyAndWindow</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testReduceByKeyAndWindow</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairTransform</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairTransform</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testFilter</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testFilter</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairMap2</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairMap2</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testMapValues</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testMapValues</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testReduce</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testReduce</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testUpdateStateByKey</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testUpdateStateByKey</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testTransform</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testTransform</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testStreamingContextTransform</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testFlatMapValues</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testReduceByWindowWithInverse</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testMapPartitions</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairFilter</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairFilter</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testRepartitionFewerPartitions</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testRepartitionFewerPartitions</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCombineByKey</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCombineByKey</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testContextGetOrCreate</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testContextGetOrCreate</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testWindowWithSlideDuration</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testWindowWithSlideDuration</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testQueueStream</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testQueueStream</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCountByValue</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCountByValue</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testMap</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testMap</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairToNormalRDDTransform</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairToNormalRDDTransform</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairReduceByKey</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairReduceByKey</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCount</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCount</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCheckpointMasterRecovery</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairMap</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairMap</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testUnion</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testUnion</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testFlatMap</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testFlatMap</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testReduceByKeyAndWindowWithInverse</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testReduceByKeyAndWindowWithInverse</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testGlom</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testGlom</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testJoin</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testJoin</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairFlatMap</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairFlatMap</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairToPairFlatMapWithChangingTypes</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairToPairFlatMapWithChangingTypes</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairMapPartitions</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairMapPartitions</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testRepartitionMorePartitions</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testRepartitionMorePartitions</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testReduceByWindowWithoutInverse</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testReduceByWindowWithoutInverse</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testLeftOuterJoin</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testLeftOuterJoin</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testTransformWith</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testTextFileStream</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairGroupByKey</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCoGroup</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testInitialization</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testInitialization</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testGroupByKeyAndWindow</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testGroupByKeyAndWindow</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testReduceByKeyAndWindow</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testReduceByKeyAndWindow</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testForeachRDD</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testForeachRDD</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testFileStream</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testFileStream</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairTransform</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairTransform</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testFilter</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testFilter</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairMap2</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testMapValues</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testReduce</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testReduce</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testUpdateStateByKey</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testUpdateStateByKey</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testTransform</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testTransform</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testWindow</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testWindow</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCountByValueAndWindow</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCountByValueAndWindow</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testRawSocketStream</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testRawSocketStream</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testSocketTextStream</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testSocketTextStream</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testUpdateStateByKeyWithInitial</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testUpdateStateByKeyWithInitial</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testContextState</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testContextState</div></li></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="sqoop"><div style="font-weight:bold;" class="panel-heading">SQOOP<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Yussuf)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/trunk</div><div><b>Last Revision: </b>4a22691f45d7d66157ff6dfaa8fca5581e0a8955</div><div><b>Last Run: </b>14-12-2018 01:55 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC UBUNTU18</th><th>X86 UBUNTU18</th><th>PPC RHEL72</th><th>X86 RHEL72</th><th>PPC RHEL75</th><th>X86 RHEL75</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 906</div><div>Failed Count : 0</div><div>Skipped Count : 51</div></td><td><div>Total Count : 906</div><div>Failed Count : 0</div><div>Skipped Count : 51</div></td><td><div>Total Count : 906</div><div>Failed Count : 0</div><div>Skipped Count : 51</div></td><td><div>Total Count : 906</div><div>Failed Count : 0</div><div>Skipped Count : 51</div></td><td><div>Total Count : 906</div><div>Failed Count : 0</div><div>Skipped Count : 51</div></td><td><div>Total Count : 906</div><div>Failed Count : 0</div><div>Skipped Count : 51</div></td><td><div>Total Count : 906</div><div>Failed Count : 0</div><div>Skipped Count : 51</div></td><td><div>Total Count : 906</div><div>Failed Count : 0</div><div>Skipped Count : 51</div></td></tr><tr><td>Result</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="storm"><div style="font-weight:bold;" class="panel-heading">STORM<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Alisha)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>43859e9e82ad419d09756702395b2747c7326eaa</div><div><b>Last Run: </b>13-12-2018 01:18 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC UBUNTU18</th><th>X86 UBUNTU18</th><th>PPC RHEL72</th><th>X86 RHEL72</th><th>PPC RHEL75</th><th>X86 RHEL75</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 1148</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 1164</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 1148</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 1161</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 1164</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 1164</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 1164</div><div>Failed Count : 3</div><div>Skipped Count : 5</div></td><td><div>Total Count : 1164</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td></tr><tr><td>Result</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.storm.cassandra.trident.MapStateTest.opaqueStateTest</li></div><div><li>org.apache.storm.sql.TestStormSql.testExternalDataSourceNested</li></div><div><li>org.apache.storm.sql.TestStormSql.testExternalDataSource</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>java.lang.IllegalStateException: It took over 60000ms to shut down slot Thread[SLOT_1027,5,main]</li></div><div><li>java.lang.RuntimeException: No nimbus leader participant host found, have you started your nimbus hosts?</li></div><div><li>java.lang.RuntimeException: No nimbus leader participant host found, have you started your nimbus hosts?</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.storm.cassandra.trident.MapStateTest.opaqueStateTest</div></li><li><div>org.apache.storm.sql.TestStormSql.testExternalDataSourceNested</div></li><li><div>org.apache.storm.sql.TestStormSql.testExternalDataSource</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="tez"><div style="font-weight:bold;" class="panel-heading">TEZ<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Prajyot)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>f49d665393a884295f529ffc7b9493cfa7bb3853</div><div><b>Last Run: </b>13-12-2018 03:53 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC UBUNTU18</th><th>X86 UBUNTU18</th><th>PPC RHEL72</th><th>X86 RHEL72</th><th>PPC RHEL75</th><th>X86 RHEL75</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 1834</div><div>Failed Count : 3</div><div>Skipped Count : 14</div></td><td><div>Total Count : 1837</div><div>Failed Count : 1</div><div>Skipped Count : 14</div></td><td><div>Total Count : 1837</div><div>Failed Count : 2</div><div>Skipped Count : 14</div></td><td><div>Total Count : 1837</div><div>Failed Count : 3</div><div>Skipped Count : 14</div></td><td><div>Total Count : 1834</div><div>Failed Count : 3</div><div>Skipped Count : 14</div></td><td><div>Total Count : 1837</div><div>Failed Count : 1</div><div>Skipped Count : 14</div></td><td><div>Total Count : 1837</div><div>Failed Count : 4</div><div>Skipped Count : 14</div></td><td><div>Total Count : 1837</div><div>Failed Count : 2</div><div>Skipped Count : 14</div></td></tr><tr><td>Result</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"><div><li>org.apache.tez.tests.TestExtServicesWithLocalMode.test1</li></div><div><li>org.apache.tez.auxservices.TestShuffleHandler.testRecoveryFromOtherVersions</li></div><div><li>org.apache.tez.auxservices.TestShuffleHandler.testRecovery</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.tez.tests.TestExtServicesWithLocalMode.test1</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.tez.dag.app.dag.impl.TestVertexImpl.testVertexTaskAttemptProcessorFailure</li></div><div><li>org.apache.tez.tests.TestExtServicesWithLocalMode.test1</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.tez.tests.TestExtServicesWithLocalMode.test1</li></div><div><li>org.apache.tez.test.TestDAGRecovery.testBasicRecovery</li></div><div><li>org.apache.tez.test.TestRecovery.testRecovery_OrderedWordCount</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.tez.tests.TestExtServicesWithLocalMode.test1</li></div><div><li>org.apache.tez.auxservices.TestShuffleHandler.testRecoveryFromOtherVersions</li></div><div><li>org.apache.tez.auxservices.TestShuffleHandler.testRecovery</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.tez.tests.TestExtServicesWithLocalMode.test1</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.tez.tests.TestExtServicesWithLocalMode.test1</li></div><div><li>org.apache.tez.mapreduce.processor.reduce.TestReduceProcessor.testReduceProcessor</li></div><div><li>org.apache.tez.runtime.library.output.TestOnFileSortedOutput.testInvalidSorter[test[false, LEGACY, 1, -1, ENABLED]]</li></div><div><li>org.apache.tez.analyzer.TestAnalyzer.testWithSimpleHistory</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.tez.tests.TestExtServicesWithLocalMode.test1</li></div><div><li>org.apache.tez.test.TestRecovery.testRecovery_HashJoin</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"><div><li>org.apache.tez.dag.api.TezUncheckedException: java.lang.reflect.InvocationTargetException</li></div><div><li>Could not load library. Reasons: [no leveldbjni64-1.8 in java.library.path, no leveldbjni-1.8 in java.library.path, no leveldbjni in java.library.path, /tmp/libleveldbjni-64-1-2494814415456289509.8: /tmp/libleveldbjni-64-1-2494814415456289509.8: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64-bit platform)]</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.tez.dag.api.TezUncheckedException: java.lang.reflect.InvocationTargetException</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>test timed out after 5000 milliseconds</li></div><div><li>org.apache.tez.dag.api.TezUncheckedException: java.lang.reflect.InvocationTargetException</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.tez.dag.api.TezUncheckedException: java.lang.reflect.InvocationTargetException</li></div><div><li>port out of range:-1</li></div><div><li>expected:&lt;TASK_FINISHED&gt; but was:&lt;DAG_SUBMITTED&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.tez.dag.api.TezUncheckedException: java.lang.reflect.InvocationTargetException</li></div><div><li>Could not load library. Reasons: [no leveldbjni64-1.8 in java.library.path, no leveldbjni-1.8 in java.library.path, no leveldbjni in java.library.path, /tmp/libleveldbjni-64-1-4950324003136849951.8: /tmp/libleveldbjni-64-1-4950324003136849951.8: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64-bit platform)]</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.tez.dag.api.TezUncheckedException: java.lang.reflect.InvocationTargetException</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.tez.dag.api.TezUncheckedException: java.lang.reflect.InvocationTargetException</li></div><div><li>test timed out after 5000 milliseconds</li></div><div><li>test timed out after 5000 milliseconds</li></div><div><li>org.codehaus.jettison.json.JSONException: Unterminated string at character 1330 of {"entity":"vertex_1544695421183_0001_13_00","entitytype":"TEZ_VERTEX_ID","events":[{"ts":1544695475736,"eventtype":"VERTEX_FINISHED"}],"otherinfo":{"endTime":1544695475736,"timeTaken":122,"status":"SUCCEEDED","diagnostics":"","counters":{"counterGroups":[{"counterGroupName":"org.apache.tez.common.counters.TaskCounte</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.tez.dag.api.TezUncheckedException: java.lang.reflect.InvocationTargetException</li></div><div><li>While parsing a protocol message, the input ended unexpectedly in the middle of a field.  This could mean either than the input has been truncated or that an embedded message misreported its own length.</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.tez.auxservices.TestShuffleHandler.testRecoveryFromOtherVersions</div></li><li><div>org.apache.tez.auxservices.TestShuffleHandler.testRecovery</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.tez.dag.app.dag.impl.TestVertexImpl.testVertexTaskAttemptProcessorFailure</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.tez.test.TestDAGRecovery.testBasicRecovery</div></li><li><div>org.apache.tez.test.TestRecovery.testRecovery_OrderedWordCount</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.tez.auxservices.TestShuffleHandler.testRecoveryFromOtherVersions</div></li><li><div>org.apache.tez.auxservices.TestShuffleHandler.testRecovery</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.tez.mapreduce.processor.reduce.TestReduceProcessor.testReduceProcessor</div></li><li><div>org.apache.tez.runtime.library.output.TestOnFileSortedOutput.testInvalidSorter[test[false, LEGACY, 1, -1, ENABLED]]</div></li><li><div>org.apache.tez.analyzer.TestAnalyzer.testWithSimpleHistory</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.tez.test.TestRecovery.testRecovery_HashJoin</div></li></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="zeppelin"><div style="font-weight:bold;" class="panel-heading">ZEPPELIN<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Alisha)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>d37cc1b611dc761b1ec69c29d6b8ceaa84f301f4</div><div><b>Last Run: </b>14-09-2018 03:06 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC UBUNTU18</th><th>X86 UBUNTU18</th><th>PPC RHEL72</th><th>X86 RHEL72</th><th>PPC RHEL75</th><th>X86 RHEL75</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 868</div><div>Failed Count : 1</div><div>Skipped Count : 5</div></td><td><div>Total Count : 869</div><div>Failed Count : 3</div><div>Skipped Count : 5</div></td><td><div>Total Count : 869</div><div>Failed Count : 3</div><div>Skipped Count : 5</div></td><td><div>Total Count : 869</div><div>Failed Count : 2</div><div>Skipped Count : 5</div></td><td><div>Total Count : 872</div><div>Failed Count : 8</div><div>Skipped Count : 5</div></td><td><div>Total Count : 858</div><div>Failed Count : 3</div><div>Skipped Count : 5</div></td><td><div>Total Count : 859</div><div>Failed Count : 17</div><div>Skipped Count : 5</div></td><td><div>Total Count : 869</div><div>Failed Count : 3</div><div>Skipped Count : 5</div></td></tr><tr><td>Result</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"><div><li>org.apache.zeppelin.rest.NotebookRestApiTest.testRunWithServerRestart</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.zeppelin.rest.InterpreterRestApiTest.testRestartInterpreterPerNote</li></div><div><li>org.apache.zeppelin.rest.NotebookRestApiTest.testRunWithServerRestart</li></div><div><li>org.apache.zeppelin.rest.ZeppelinRestApiTest.org.apache.zeppelin.rest.ZeppelinRestApiTest</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.zeppelin.ignite.IgniteInterpreterTest.testInterpretInvalidInput</li></div><div><li>org.apache.zeppelin.ignite.IgniteInterpreterTest.testInterpretInvalidInput</li></div><div><li>org.apache.zeppelin.rest.NotebookRestApiTest.testRunWithServerRestart</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.zeppelin.rest.NotebookRestApiTest.testRunWithServerRestart</li></div><div><li>org.apache.zeppelin.rest.ZeppelinRestApiTest.org.apache.zeppelin.rest.ZeppelinRestApiTest</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.zeppelin.ignite.IgniteInterpreterTest.testInterpret</li></div><div><li>org.apache.zeppelin.ignite.IgniteInterpreterTest.testInterpret</li></div><div><li>org.apache.zeppelin.ignite.IgniteInterpreterTest.testInterpretInvalidInput</li></div><div><li>org.apache.zeppelin.ignite.IgniteInterpreterTest.testInterpretInvalidInput</li></div><div><li>org.apache.zeppelin.ignite.IgniteSqlInterpreterTest.testSql</li></div><div><li>org.apache.zeppelin.ignite.IgniteSqlInterpreterTest.testSql</li></div><div><li>org.apache.zeppelin.rest.NotebookRestApiTest.testRunWithServerRestart</li></div><div><li>org.apache.zeppelin.rest.ZeppelinRestApiTest.org.apache.zeppelin.rest.ZeppelinRestApiTest</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.zeppelin.spark.IPySparkInterpreterTest.testRedefinitionZeppelinContext</li></div><div><li>org.apache.zeppelin.rest.NotebookRestApiTest.testRunWithServerRestart</li></div><div><li>org.apache.zeppelin.helium.HeliumBundleFactoryTest.bundlePackage</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.zeppelin.python.IPythonInterpreterTest.testIPythonPlotting</li></div><div><li>org.apache.zeppelin.python.IPythonInterpreterTest.testZeppelinContext</li></div><div><li>org.apache.zeppelin.python.PythonInterpreterMatplotlibTest.testClose</li></div><div><li>org.apache.zeppelin.python.PythonInterpreterMatplotlibTest.showPlot</li></div><div><li>org.apache.zeppelin.python.PythonInterpreterMatplotlibTest.dependenciesAreInstalled</li></div><div><li>org.apache.zeppelin.python.PythonInterpreterMatplotlibTest.testNoClose</li></div><div><li>org.apache.zeppelin.python.PythonInterpreterTest.testZeppelinContext</li></div><div><li>org.apache.zeppelin.spark.IPySparkInterpreterTest.testIPySpark</li></div><div><li>org.apache.zeppelin.spark.IPySparkInterpreterTest.testIPythonPlotting</li></div><div><li>org.apache.zeppelin.spark.IPySparkInterpreterTest.testZeppelinContext</li></div><div><li>org.apache.zeppelin.spark.PySparkInterpreterTest.testPySpark</li></div><div><li>org.apache.zeppelin.spark.PySparkInterpreterTest.testZeppelinContext</li></div><div><li>org.apache.zeppelin.rest.InterpreterRestApiTest.testRestartInterpreterPerNote</li></div><div><li>org.apache.zeppelin.rest.NotebookRestApiTest.testRunWithServerRestart</li></div><div><li>org.apache.zeppelin.rest.ZeppelinRestApiTest.org.apache.zeppelin.rest.ZeppelinRestApiTest</li></div><div><li>org.apache.zeppelin.rest.ZeppelinSparkClusterTest.pySparkDepLoaderTest[3]</li></div><div><li>org.apache.zeppelin.rest.ZeppelinSparkClusterTest.testZeppelinContextResource[3]</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.zeppelin.rest.InterpreterRestApiTest.testRestartInterpreterPerNote</li></div><div><li>org.apache.zeppelin.rest.NotebookRestApiTest.testRunWithServerRestart</li></div><div><li>org.apache.zeppelin.rest.ZeppelinRestApiTest.org.apache.zeppelin.rest.ZeppelinRestApiTest</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"><div><li>java.lang.NullPointerException: null
	at org.apache.zeppelin.service.JobManagerService$NoteJobInfo.&lt;init&gt;(JobManagerService.java:124)
	at org.apache.zeppelin.service.JobManagerService.getNoteJobInfo(JobManagerService.java:52)
	at org.apache.zeppelin.socket.NotebookServer$NotebookInformationListener.onParagraphStatusChange(NotebookServer.java:1896)
	at org.apache.zeppelin.socket.NotebookServer.onSt</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>shared interpreter restart:
Expected: HTTP response &lt;200&gt; from /api/interpreter/setting/restart/md
     but: got &lt;500&gt; Request failed.</li></div><div><li>java.lang.NullPointerException: null
	at org.apache.zeppelin.service.JobManagerService$NoteJobInfo.&lt;init&gt;(JobManagerService.java:124)
	at org.apache.zeppelin.service.JobManagerService.getNoteJobInfo(JobManagerService.java:52)
	at org.apache.zeppelin.socket.NotebookServer$NotebookInformationListener.onParagraphStatusChange(NotebookServer.java:1896)
	at org.apache.zeppelin.socket.NotebookServer.onSt</li></div><div><li>java.lang.NullPointerException: null
	at org.apache.zeppelin.service.JobManagerService$NoteJobInfo.&lt;init&gt;(JobManagerService.java:124)
	at org.apache.zeppelin.service.JobManagerService.getNoteJobInfo(JobManagerService.java:52)
	at org.apache.zeppelin.socket.NotebookServer$NotebookInformationListener.onParagraphStatusChange(NotebookServer.java:1896)
	at org.apache.zeppelin.socket.NotebookServer.onSt</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Unable to establish plain connection. Was remote cluster configured with SSL? [rmtAddr=/127.0.0.1:47507, errMsg="Failed to deserialize object with given class loader: sun.misc.Launcher$AppClassLoader@135fbaa4"]</li></div><div><li>java.lang.NullPointerException: null
	at org.apache.zeppelin.ignite.IgniteInterpreterTest.tearDown(IgniteInterpreterTest.java:75)
</li></div><div><li>java.lang.NullPointerException: null
	at org.apache.zeppelin.service.JobManagerService$NoteJobInfo.&lt;init&gt;(JobManagerService.java:124)
	at org.apache.zeppelin.service.JobManagerService.getNoteJobInfo(JobManagerService.java:52)
	at org.apache.zeppelin.socket.NotebookServer$NotebookInformationListener.onParagraphStatusChange(NotebookServer.java:1896)
	at org.apache.zeppelin.socket.NotebookServer.onSt</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>java.lang.NullPointerException: null
	at org.apache.zeppelin.service.JobManagerService$NoteJobInfo.&lt;init&gt;(JobManagerService.java:124)
	at org.apache.zeppelin.service.JobManagerService.getNoteJobInfo(JobManagerService.java:52)
	at org.apache.zeppelin.socket.NotebookServer$NotebookInformationListener.onParagraphStatusChange(NotebookServer.java:1896)
	at org.apache.zeppelin.socket.NotebookServer.onSt</li></div><div><li>java.lang.NullPointerException: null
	at org.apache.zeppelin.service.JobManagerService$NoteJobInfo.&lt;init&gt;(JobManagerService.java:124)
	at org.apache.zeppelin.service.JobManagerService.getNoteJobInfo(JobManagerService.java:52)
	at org.apache.zeppelin.socket.NotebookServer$NotebookInformationListener.onParagraphStatusChange(NotebookServer.java:1896)
	at org.apache.zeppelin.socket.NotebookServer.onSt</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Unable to establish plain connection. Was remote cluster configured with SSL? [rmtAddr=/127.0.0.1:47507, errMsg="Failed to deserialize object with given class loader: sun.misc.Launcher$AppClassLoader@75b84c92"]</li></div><div><li>java.lang.NullPointerException: null
	at org.apache.zeppelin.ignite.IgniteInterpreterTest.tearDown(IgniteInterpreterTest.java:75)
</li></div><div><li>Unable to establish plain connection. Was remote cluster configured with SSL? [rmtAddr=/127.0.0.1:47507, errMsg="Failed to deserialize object with given class loader: sun.misc.Launcher$AppClassLoader@75b84c92"]</li></div><div><li>java.lang.NullPointerException: null
	at org.apache.zeppelin.ignite.IgniteInterpreterTest.tearDown(IgniteInterpreterTest.java:75)
</li></div><div><li>Unable to establish plain connection. Was remote cluster configured with SSL? [rmtAddr=/127.0.0.1:47507, errMsg="Failed to deserialize object with given class loader: sun.misc.Launcher$AppClassLoader@75b84c92"]</li></div><div><li>java.lang.NullPointerException: null
	at org.apache.zeppelin.ignite.IgniteSqlInterpreterTest.tearDown(IgniteSqlInterpreterTest.java:87)
</li></div><div><li>java.lang.NullPointerException: null
	at org.apache.zeppelin.service.JobManagerService$NoteJobInfo.&lt;init&gt;(JobManagerService.java:124)
	at org.apache.zeppelin.service.JobManagerService.getNoteJobInfo(JobManagerService.java:52)
	at org.apache.zeppelin.socket.NotebookServer$NotebookInformationListener.onParagraphStatusChange(NotebookServer.java:1896)
	at org.apache.zeppelin.socket.NotebookServer.onSt</li></div><div><li>java.lang.NullPointerException: null
	at org.apache.zeppelin.service.JobManagerService$NoteJobInfo.&lt;init&gt;(JobManagerService.java:124)
	at org.apache.zeppelin.service.JobManagerService.getNoteJobInfo(JobManagerService.java:52)
	at org.apache.zeppelin.socket.NotebookServer$NotebookInformationListener.onParagraphStatusChange(NotebookServer.java:1896)
	at org.apache.zeppelin.socket.NotebookServer.onSt</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Fail to open IPythonInterpreter</li></div><div><li>java.lang.NullPointerException: null
	at org.apache.zeppelin.service.JobManagerService$NoteJobInfo.&lt;init&gt;(JobManagerService.java:124)
	at org.apache.zeppelin.service.JobManagerService.getNoteJobInfo(JobManagerService.java:52)
	at org.apache.zeppelin.socket.NotebookServer$NotebookInformationListener.onParagraphStatusChange(NotebookServer.java:1896)
	at org.apache.zeppelin.socket.NotebookServer.onSt</li></div><div><li>com.github.eirslett.maven.plugins.frontend.lib.TaskRunnerException: 'yarn install --fetch-retries=2 --fetch-retry-factor=1 --fetch-retry-mintimeout=5000 --registry=http://registry.npmjs.org/' failed. (error code 1)</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>expected:&lt;1&gt; but was:&lt;2&gt;</li></div><div><li>Index: 0, Size: 0</li></div><div><li>Index: 0, Size: 0</li></div><div><li>[%text Fail to execute line 1: import matplotlib
Traceback (most recent call last):
  File "/tmp/1536900576226-0/zeppelin_python.py", line 158, in &lt;module&gt;
    exec(code, _zcUserQueryNameSpace)
  File "&lt;stdin&gt;", line 1, in &lt;module&gt;
  File "/usr/lib64/python2.7/site-packages/matplotlib/__init__.py", line 124, in &lt;module&gt;
    from . import cbook
ImportError: cannot import name cbook
] expected:&lt;SUCC</li></div><div><li>Index: 0, Size: 0</li></div><div><li>expected:&lt;1&gt; but was:&lt;2&gt;</li></div><div><li>expected:&lt;[]+---+---+
| _1| _2|
...&gt; but was:&lt;[/usr/lib64/python2.7/site-packages/pandas/core/nanops.py:39: UserWarning: The installed version of bottleneck 0.7.0 is not supported in pandas and will be not be used
The minimum supported version is 1.0.0

  ver=ver, min_ver=_MIN_BOTTLENECK_VERSION), UserWarning)
]+---+---+
| _1| _2|
...&gt;</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>expected:&lt;id name
1 a
2 b
3 c
[]&gt; but was:&lt;id name
1 a
2 b
3 c
[/usr/lib64/python2.7/site-packages/pandas/core/nanops.py:39: UserWarning: The installed version of bottleneck 0.7.0 is not supported in pandas and will be not be used
The minimum supported version is 1.0.0
  ver=ver, min_ver=_MIN_BOTTLENECK_VERSION), UserWarning)
]&gt;</li></div><div><li>expected:&lt;[]+---+---+
| _1| _2|
...&gt; but was:&lt;[/usr/lib64/python2.7/site-packages/pandas/core/nanops.py:39: UserWarning: The installed version of bottleneck 0.7.0 is not supported in pandas and will be not be used
The minimum supported version is 1.0.0

  ver=ver, min_ver=_MIN_BOTTLENECK_VERSION), UserWarning)
]+---+---+
| _1| _2|
...&gt;</li></div><div><li>expected:&lt;1&gt; but was:&lt;2&gt;</li></div><div><li>shared interpreter restart:
Expected: HTTP response &lt;200&gt; from /api/interpreter/setting/restart/md
     but: got &lt;500&gt; Request failed.</li></div><div><li>java.lang.NullPointerException: null
	at org.apache.zeppelin.service.JobManagerService$NoteJobInfo.&lt;init&gt;(JobManagerService.java:124)
	at org.apache.zeppelin.service.JobManagerService.getNoteJobInfo(JobManagerService.java:52)
	at org.apache.zeppelin.socket.NotebookServer$NotebookInformationListener.onParagraphStatusChange(NotebookServer.java:1896)
	at org.apache.zeppelin.socket.NotebookServer.onSt</li></div><div><li>java.lang.NullPointerException: null
	at org.apache.zeppelin.service.JobManagerService$NoteJobInfo.&lt;init&gt;(JobManagerService.java:124)
	at org.apache.zeppelin.service.JobManagerService.getNoteJobInfo(JobManagerService.java:52)
	at org.apache.zeppelin.socket.NotebookServer$NotebookInformationListener.onParagraphStatusChange(NotebookServer.java:1896)
	at org.apache.zeppelin.socket.NotebookServer.onSt</li></div><div><li>expected:&lt;[]2
&gt; but was:&lt;[/usr/lib64/python2.7/site-packages/pandas/core/nanops.py:39: UserWarning: The installed version of bottleneck 0.7.0 is not supported in pandas and will be not be used
The minimum supported version is 1.0.0

  ver=ver, min_ver=_MIN_BOTTLENECK_VERSION), UserWarning)
]2
&gt;</li></div><div><li>expected:&lt;[]hello world
&gt; but was:&lt;[/usr/lib64/python2.7/site-packages/pandas/core/nanops.py:39: UserWarning: The installed version of bottleneck 0.7.0 is not supported in pandas and will be not be used
The minimum supported version is 1.0.0

  ver=ver, min_ver=_MIN_BOTTLENECK_VERSION), UserWarning)
]hello world
&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>shared interpreter restart:
Expected: HTTP response &lt;200&gt; from /api/interpreter/setting/restart/md
     but: got &lt;500&gt; Request failed.</li></div><div><li>java.lang.NullPointerException: null
	at org.apache.zeppelin.service.JobManagerService$NoteJobInfo.&lt;init&gt;(JobManagerService.java:124)
	at org.apache.zeppelin.service.JobManagerService.getNoteJobInfo(JobManagerService.java:52)
	at org.apache.zeppelin.socket.NotebookServer$NotebookInformationListener.onParagraphStatusChange(NotebookServer.java:1896)
	at org.apache.zeppelin.socket.NotebookServer.onSt</li></div><div><li>java.lang.NullPointerException: null
	at org.apache.zeppelin.service.JobManagerService$NoteJobInfo.&lt;init&gt;(JobManagerService.java:124)
	at org.apache.zeppelin.service.JobManagerService.getNoteJobInfo(JobManagerService.java:52)
	at org.apache.zeppelin.socket.NotebookServer$NotebookInformationListener.onParagraphStatusChange(NotebookServer.java:1896)
	at org.apache.zeppelin.socket.NotebookServer.onSt</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.zeppelin.rest.InterpreterRestApiTest.testRestartInterpreterPerNote</div></li><li><div>org.apache.zeppelin.rest.ZeppelinRestApiTest.org.apache.zeppelin.rest.ZeppelinRestApiTest</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.zeppelin.ignite.IgniteInterpreterTest.testInterpretInvalidInput</div></li><li><div>org.apache.zeppelin.ignite.IgniteInterpreterTest.testInterpretInvalidInput</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.zeppelin.rest.ZeppelinRestApiTest.org.apache.zeppelin.rest.ZeppelinRestApiTest</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.zeppelin.ignite.IgniteInterpreterTest.testInterpret</div></li><li><div>org.apache.zeppelin.ignite.IgniteInterpreterTest.testInterpret</div></li><li><div>org.apache.zeppelin.ignite.IgniteInterpreterTest.testInterpretInvalidInput</div></li><li><div>org.apache.zeppelin.ignite.IgniteInterpreterTest.testInterpretInvalidInput</div></li><li><div>org.apache.zeppelin.ignite.IgniteSqlInterpreterTest.testSql</div></li><li><div>org.apache.zeppelin.ignite.IgniteSqlInterpreterTest.testSql</div></li><li><div>org.apache.zeppelin.rest.ZeppelinRestApiTest.org.apache.zeppelin.rest.ZeppelinRestApiTest</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.zeppelin.spark.IPySparkInterpreterTest.testRedefinitionZeppelinContext</div></li><li><div>org.apache.zeppelin.helium.HeliumBundleFactoryTest.bundlePackage</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.zeppelin.python.IPythonInterpreterTest.testIPythonPlotting</div></li><li><div>org.apache.zeppelin.python.IPythonInterpreterTest.testZeppelinContext</div></li><li><div>org.apache.zeppelin.python.PythonInterpreterMatplotlibTest.testClose</div></li><li><div>org.apache.zeppelin.python.PythonInterpreterMatplotlibTest.showPlot</div></li><li><div>org.apache.zeppelin.python.PythonInterpreterMatplotlibTest.dependenciesAreInstalled</div></li><li><div>org.apache.zeppelin.python.PythonInterpreterMatplotlibTest.testNoClose</div></li><li><div>org.apache.zeppelin.python.PythonInterpreterTest.testZeppelinContext</div></li><li><div>org.apache.zeppelin.spark.IPySparkInterpreterTest.testIPySpark</div></li><li><div>org.apache.zeppelin.spark.IPySparkInterpreterTest.testIPythonPlotting</div></li><li><div>org.apache.zeppelin.spark.IPySparkInterpreterTest.testZeppelinContext</div></li><li><div>org.apache.zeppelin.spark.PySparkInterpreterTest.testPySpark</div></li><li><div>org.apache.zeppelin.spark.PySparkInterpreterTest.testZeppelinContext</div></li><li><div>org.apache.zeppelin.rest.ZeppelinSparkClusterTest.pySparkDepLoaderTest[3]</div></li><li><div>org.apache.zeppelin.rest.ZeppelinSparkClusterTest.testZeppelinContextResource[3]</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="zookeeper"><div style="font-weight:bold;" class="panel-heading">ZOOKEEPER<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Pravin)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>46fc819622bf08cbd0781dea279aff734b492902</div><div><b>Last Run: </b>14-12-2018 03:51 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC UBUNTU18</th><th>X86 UBUNTU18</th><th>PPC RHEL72</th><th>X86 RHEL72</th><th>PPC RHEL75</th><th>X86 RHEL75</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 2388</div><div>Failed Count : 2</div><div>Skipped Count : 1</div></td><td><div>Total Count : 2383</div><div>Failed Count : 0</div><div>Skipped Count : 1</div></td><td><div>Total Count : 2377</div><div>Failed Count : 2</div><div>Skipped Count : 1</div></td><td><div>Total Count : 2383</div><div>Failed Count : 0</div><div>Skipped Count : 1</div></td><td><div>Total Count : 2377</div><div>Failed Count : 2</div><div>Skipped Count : 1</div></td><td><div>Total Count : 2383</div><div>Failed Count : 1</div><div>Skipped Count : 1</div></td><td><div>Total Count : 2388</div><div>Failed Count : 2</div><div>Skipped Count : 1</div></td><td><div>Total Count : 2383</div><div>Failed Count : 1</div><div>Skipped Count : 1</div></td></tr><tr><td>Result</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"><div><li>org.apache.zookeeper.server.quorum.QuorumPeerMainTest.testQuorumV6</li></div><div><li>org.apache.zookeeper.test.CnxManagerTest.testWorkerThreads</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.zookeeper.server.quorum.QuorumPeerMainTest.testQuorumV6</li></div><div><li>org.apache.zookeeper.server.quorum.Zab1_0Test.testNormalObserverRun</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.zookeeper.server.quorum.QuorumPeerMainTest.testQuorumV6</li></div><div><li>org.apache.zookeeper.server.quorum.Zab1_0Test.testNormalObserverRun</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.zookeeper.server.quorum.QuorumPeerMainTest.testLeaderOutOfView</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.zookeeper.server.quorum.QuorumPeerMainTest.testQuorumV6</li></div><div><li>org.apache.zookeeper.test.DisconnectedWatcherTest.testManyChildWatchersAutoReset</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.zookeeper.server.watch.WatchManagerTest.testDeadWatchers[1]</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"><div><li>waiting for server 1 being up</li></div><div><li>Unable to run quorum server </li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>waiting for server 1 being up</li></div><div><li>Timeout occurred. Please note the time in the report does not reflect the time until the timeout.</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>waiting for server 1 being up</li></div><div><li>Timeout occurred. Please note the time in the report does not reflect the time until the timeout.</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>junit.framework.AssertionFailedError
	at org.apache.zookeeper.server.quorum.QuorumPeerMainTest.testLeaderOutOfView(QuorumPeerMainTest.java:973)
	at org.apache.zookeeper.JUnit4ZKTestRunner$LoggedInvokeMethod.evaluate(JUnit4ZKTestRunner.java:79)
</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>waiting for server 1 being up</li></div><div><li>test timed out after 840000 milliseconds</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>junit.framework.AssertionFailedError
	at org.apache.zookeeper.server.watch.WatchManagerTest.testDeadWatchers(WatchManagerTest.java:400)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.lang.Thread.run(Thread.java:748)
</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.zookeeper.server.quorum.QuorumPeerMainTest.testQuorumV6</div></li><li><div>org.apache.zookeeper.test.CnxManagerTest.testWorkerThreads</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.zookeeper.server.quorum.QuorumPeerMainTest.testQuorumV6</div></li><li><div>org.apache.zookeeper.server.quorum.Zab1_0Test.testNormalObserverRun</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.zookeeper.server.quorum.QuorumPeerMainTest.testQuorumV6</div></li><li><div>org.apache.zookeeper.server.quorum.Zab1_0Test.testNormalObserverRun</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.zookeeper.server.quorum.QuorumPeerMainTest.testLeaderOutOfView</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.zookeeper.server.quorum.QuorumPeerMainTest.testQuorumV6</div></li><li><div>org.apache.zookeeper.test.DisconnectedWatcherTest.testManyChildWatchersAutoReset</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.zookeeper.server.watch.WatchManagerTest.testDeadWatchers[1]</div></li></ol></td></tr></tbody></table></div></div><div id="ubuntu16" style="font-weight:bold;font-size:12;display:none" class="panel panel-info" name="summary"><div class="panel-heading"><div class="panel-title">UBUNTU16 SUMMARY</div></div><div class="bs-callout bs-callout-info"><div><b>OS: </b>UBUNTU 16.04</div></div><table style="font-size:14" class="table table-striped"><tbody><tr><th>Package Name</th><th>PPC</th><th>X86</th><th></th></tr><tr><td><a href="#" id="anchor_accumulo" onclick="showme(this.id);">ACCUMULO</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_ambari" onclick="showme(this.id);">AMBARI</a></td><td><img src="resources/red.png" align="top" style="width: 16px; height: 16px;" title="FAILURE"></img>0</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_atlas" onclick="showme(this.id);">ATLAS</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_calcite" onclick="showme(this.id);">CALCITE</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_datafu" onclick="showme(this.id);">DATAFU</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_druid" onclick="showme(this.id);">DRUID</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>6 (4)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2</td></tr><tr><td><a href="#" id="anchor_falcon" onclick="showme(this.id);">FALCON</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_flume" onclick="showme(this.id);">FLUME</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>15 (14)</td></tr><tr><td><a href="#" id="anchor_hadoop" onclick="showme(this.id);">HADOOP</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>39 (31)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>20 (12)</td></tr><tr><td><a href="#" id="anchor_hbase" onclick="showme(this.id);">HBASE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>51 (45)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>6 (1)</td></tr><tr><td><a href="#" id="anchor_hive" onclick="showme(this.id);">HIVE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3 (2)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td></tr><tr><td><a href="#" id="anchor_kafka" onclick="showme(this.id);">KAFKA</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_knox" onclick="showme(this.id);">KNOX</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_metron" onclick="showme(this.id);">METRON</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>14 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>13</td></tr><tr><td><a href="#" id="anchor_oozie" onclick="showme(this.id);">OOZIE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td></tr><tr><td><a href="#" id="anchor_phoenix" onclick="showme(this.id);">PHOENIX</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_pig" onclick="showme(this.id);">PIG</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>9</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>9</td></tr><tr><td><a href="#" id="anchor_ranger" onclick="showme(this.id);">RANGER</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_slider" onclick="showme(this.id);">SLIDER</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_spark" onclick="showme(this.id);">SPARK</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>890 (888)</td></tr><tr><td><a href="#" id="anchor_sqoop" onclick="showme(this.id);">SQOOP</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_storm" onclick="showme(this.id);">STORM</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_tez" onclick="showme(this.id);">TEZ</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3 (2)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td></tr><tr><td><a href="#" id="anchor_zeppelin" onclick="showme(this.id);">ZEPPELIN</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3 (2)</td></tr><tr><td><a href="#" id="anchor_zookeeper" onclick="showme(this.id);">ZOOKEEPER</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (2)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr></tbody></table></div><div id="ubuntu18" style="font-weight:bold;font-size:12;display:none" class="panel panel-info" name="summary"><div class="panel-heading"><div class="panel-title">UBUNTU18 SUMMARY</div></div><div class="bs-callout bs-callout-info"><div><b>OS: </b>UBUNTU 18.04</div></div><table style="font-size:14" class="table table-striped"><tbody><tr><th>Package Name</th><th>PPC</th><th>X86</th><th></th></tr><tr><td><a href="#" id="anchor_accumulo" onclick="showme(this.id);">ACCUMULO</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (2)</td></tr><tr><td><a href="#" id="anchor_ambari" onclick="showme(this.id);">AMBARI</a></td><td><img src="resources/red.png" align="top" style="width: 16px; height: 16px;" title="FAILURE"></img>0</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_atlas" onclick="showme(this.id);">ATLAS</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_calcite" onclick="showme(this.id);">CALCITE</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_datafu" onclick="showme(this.id);">DATAFU</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td></tr><tr><td><a href="#" id="anchor_druid" onclick="showme(this.id);">DRUID</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>6 (4)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2</td></tr><tr><td><a href="#" id="anchor_falcon" onclick="showme(this.id);">FALCON</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_flume" onclick="showme(this.id);">FLUME</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (2)</td></tr><tr><td><a href="#" id="anchor_hadoop" onclick="showme(this.id);">HADOOP</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>23 (14)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>20 (11)</td></tr><tr><td><a href="#" id="anchor_hbase" onclick="showme(this.id);">HBASE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>21 (15)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>9 (3)</td></tr><tr><td><a href="#" id="anchor_hive" onclick="showme(this.id);">HIVE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3</td></tr><tr><td><a href="#" id="anchor_kafka" onclick="showme(this.id);">KAFKA</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td></tr><tr><td><a href="#" id="anchor_knox" onclick="showme(this.id);">KNOX</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_metron" onclick="showme(this.id);">METRON</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>13</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>13</td></tr><tr><td><a href="#" id="anchor_oozie" onclick="showme(this.id);">OOZIE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3 (2)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3 (2)</td></tr><tr><td><a href="#" id="anchor_phoenix" onclick="showme(this.id);">PHOENIX</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_pig" onclick="showme(this.id);">PIG</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>9</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>9</td></tr><tr><td><a href="#" id="anchor_ranger" onclick="showme(this.id);">RANGER</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_slider" onclick="showme(this.id);">SLIDER</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_spark" onclick="showme(this.id);">SPARK</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>48 (46)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3 (1)</td></tr><tr><td><a href="#" id="anchor_sqoop" onclick="showme(this.id);">SQOOP</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_storm" onclick="showme(this.id);">STORM</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_tez" onclick="showme(this.id);">TEZ</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3 (2)</td></tr><tr><td><a href="#" id="anchor_zeppelin" onclick="showme(this.id);">ZEPPELIN</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3 (2)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (1)</td></tr><tr><td><a href="#" id="anchor_zookeeper" onclick="showme(this.id);">ZOOKEEPER</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (2)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr></tbody></table></div><div id="rhel72" style="font-weight:bold;font-size:12;display:none" class="panel panel-info" name="summary"><div class="panel-heading"><div class="panel-title">RHEL72 SUMMARY</div></div><div class="bs-callout bs-callout-info"><div><b>OS: </b>RHEL 7.2</div></div><table style="font-size:14" class="table table-striped"><tbody><tr><th>Package Name</th><th>PPC</th><th>X86</th><th></th></tr><tr><td><a href="#" id="anchor_accumulo" onclick="showme(this.id);">ACCUMULO</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_ambari" onclick="showme(this.id);">AMBARI</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_atlas" onclick="showme(this.id);">ATLAS</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_calcite" onclick="showme(this.id);">CALCITE</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_datafu" onclick="showme(this.id);">DATAFU</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_druid" onclick="showme(this.id);">DRUID</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>6 (4)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2</td></tr><tr><td><a href="#" id="anchor_falcon" onclick="showme(this.id);">FALCON</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_flume" onclick="showme(this.id);">FLUME</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_hadoop" onclick="showme(this.id);">HADOOP</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>10 (6)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>11 (7)</td></tr><tr><td><a href="#" id="anchor_hbase" onclick="showme(this.id);">HBASE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>23 (23)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_hive" onclick="showme(this.id);">HIVE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3 (2)</td></tr><tr><td><a href="#" id="anchor_kafka" onclick="showme(this.id);">KAFKA</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_knox" onclick="showme(this.id);">KNOX</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_metron" onclick="showme(this.id);">METRON</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>13</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>13</td></tr><tr><td><a href="#" id="anchor_oozie" onclick="showme(this.id);">OOZIE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>638 (638)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_phoenix" onclick="showme(this.id);">PHOENIX</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_pig" onclick="showme(this.id);">PIG</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>9</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>10 (1)</td></tr><tr><td><a href="#" id="anchor_ranger" onclick="showme(this.id);">RANGER</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_slider" onclick="showme(this.id);">SLIDER</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_spark" onclick="showme(this.id);">SPARK</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>49 (47)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>871 (869)</td></tr><tr><td><a href="#" id="anchor_sqoop" onclick="showme(this.id);">SQOOP</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_storm" onclick="showme(this.id);">STORM</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_tez" onclick="showme(this.id);">TEZ</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3 (2)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td></tr><tr><td><a href="#" id="anchor_zeppelin" onclick="showme(this.id);">ZEPPELIN</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>8 (7)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3 (2)</td></tr><tr><td><a href="#" id="anchor_zookeeper" onclick="showme(this.id);">ZOOKEEPER</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (2)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td></tr></tbody></table></div><div id="rhel75" style="font-weight:bold;font-size:12;display:none" class="panel panel-info" name="summary"><div class="panel-heading"><div class="panel-title">RHEL75 SUMMARY</div></div><div class="bs-callout bs-callout-info"><div><b>OS: </b>RHEL 7.5</div></div><table style="font-size:14" class="table table-striped"><tbody><tr><th>Package Name</th><th>PPC</th><th>X86</th><th></th></tr><tr><td><a href="#" id="anchor_accumulo" onclick="showme(this.id);">ACCUMULO</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_ambari" onclick="showme(this.id);">AMBARI</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_atlas" onclick="showme(this.id);">ATLAS</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_calcite" onclick="showme(this.id);">CALCITE</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_datafu" onclick="showme(this.id);">DATAFU</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_druid" onclick="showme(this.id);">DRUID</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>6 (4)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2</td></tr><tr><td><a href="#" id="anchor_falcon" onclick="showme(this.id);">FALCON</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_flume" onclick="showme(this.id);">FLUME</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_hadoop" onclick="showme(this.id);">HADOOP</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>25 (12)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>25 (12)</td></tr><tr><td><a href="#" id="anchor_hbase" onclick="showme(this.id);">HBASE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>40 (40)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td></tr><tr><td><a href="#" id="anchor_hive" onclick="showme(this.id);">HIVE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3 (2)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td></tr><tr><td><a href="#" id="anchor_kafka" onclick="showme(this.id);">KAFKA</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_knox" onclick="showme(this.id);">KNOX</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_metron" onclick="showme(this.id);">METRON</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>13</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>13</td></tr><tr><td><a href="#" id="anchor_oozie" onclick="showme(this.id);">OOZIE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (2)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td></tr><tr><td><a href="#" id="anchor_phoenix" onclick="showme(this.id);">PHOENIX</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_pig" onclick="showme(this.id);">PIG</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>9</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>9</td></tr><tr><td><a href="#" id="anchor_ranger" onclick="showme(this.id);">RANGER</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_slider" onclick="showme(this.id);">SLIDER</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_spark" onclick="showme(this.id);">SPARK</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>842 (840)</td></tr><tr><td><a href="#" id="anchor_sqoop" onclick="showme(this.id);">SQOOP</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_storm" onclick="showme(this.id);">STORM</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3 (3)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_tez" onclick="showme(this.id);">TEZ</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>4 (3)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (1)</td></tr><tr><td><a href="#" id="anchor_zeppelin" onclick="showme(this.id);">ZEPPELIN</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>17 (14)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3</td></tr><tr><td><a href="#" id="anchor_zookeeper" onclick="showme(this.id);">ZOOKEEPER</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (2)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td></tr></tbody></table></div><div id="ppcx86" style="display:block;font-weight:bold" class="panel panel-info" name="summary"><div class="panel-heading"><div class="panel-title">FULL SUMMARY</div></div><table style="font-size:14" id="summarytable" class="table table-striped"><tbody><tr><th></th></tr><tr><th>Package Name</th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC UBUNTU18</th><th>X86 UBUNTU18</th><th>PPC RHEL72</th><th>X86 RHEL72</th><th>PPC RHEL75</th><th>X86 RHEL75</th></tr><tr><td><a href="#" id="anchor_accumulo" onclick="showme(this.id);">ACCUMULO</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (2)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_ambari" onclick="showme(this.id);">AMBARI</a></td><td>N/A</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td>N/A</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_atlas" onclick="showme(this.id);">ATLAS</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_calcite" onclick="showme(this.id);">CALCITE</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_datafu" onclick="showme(this.id);">DATAFU</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_druid" onclick="showme(this.id);">DRUID</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>6 (4)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>6 (4)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>6 (4)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>6 (4)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2</td></tr><tr><td><a href="#" id="anchor_falcon" onclick="showme(this.id);">FALCON</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_flume" onclick="showme(this.id);">FLUME</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>15 (14)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (2)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_hadoop" onclick="showme(this.id);">HADOOP</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>39 (31)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>20 (12)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>23 (14)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>20 (11)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>10 (6)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>11 (7)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>25 (12)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>25 (12)</td></tr><tr><td><a href="#" id="anchor_hbase" onclick="showme(this.id);">HBASE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>51 (45)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>6 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>21 (15)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>9 (3)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>23 (23)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>40 (40)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td></tr><tr><td><a href="#" id="anchor_hive" onclick="showme(this.id);">HIVE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3 (2)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3 (2)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3 (2)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td></tr><tr><td><a href="#" id="anchor_kafka" onclick="showme(this.id);">KAFKA</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_knox" onclick="showme(this.id);">KNOX</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_metron" onclick="showme(this.id);">METRON</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>14 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>13</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>13</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>13</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>13</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>13</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>13</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>13</td></tr><tr><td><a href="#" id="anchor_oozie" onclick="showme(this.id);">OOZIE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3 (2)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3 (2)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>638 (638)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (2)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td></tr><tr><td><a href="#" id="anchor_phoenix" onclick="showme(this.id);">PHOENIX</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_pig" onclick="showme(this.id);">PIG</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>9</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>9</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>9</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>9</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>9</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>10 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>9</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>9</td></tr><tr><td><a href="#" id="anchor_ranger" onclick="showme(this.id);">RANGER</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_slider" onclick="showme(this.id);">SLIDER</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_spark" onclick="showme(this.id);">SPARK</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>890 (888)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>48 (46)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>49 (47)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>871 (869)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>842 (840)</td></tr><tr><td><a href="#" id="anchor_sqoop" onclick="showme(this.id);">SQOOP</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_storm" onclick="showme(this.id);">STORM</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3 (3)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_tez" onclick="showme(this.id);">TEZ</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3 (2)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3 (2)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3 (2)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>4 (3)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (1)</td></tr><tr><td><a href="#" id="anchor_zeppelin" onclick="showme(this.id);">ZEPPELIN</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3 (2)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3 (2)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>8 (7)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3 (2)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>17 (14)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3</td></tr><tr><td><a href="#" id="anchor_zookeeper" onclick="showme(this.id);">ZOOKEEPER</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (2)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (2)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (2)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (2)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td></tr></tbody></table></div></div></body></html>