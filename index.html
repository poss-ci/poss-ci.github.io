<html><head><script src="resources/jquery.min.js"></script><link href="resources/bootstrap.min.css" rel="stylesheet"></link><link href="resources/bootstrap-theme.min.css" rel="stylesheet"></link><script src="resources/bootstrap.min.js"></script><script src="helper.js">function hideAll(){console.log("hideAll")}function showme(e){console.log("showme");var l,n=e.substring(7),o=document.getElementsByName("data");for(l=0;l&lt;o.length;l++)o[l].style.display="none";var t=document.getElementsByName("summary");for(l=0;l&lt;t.length;l++)t[l].style.display="none";document.getElementById(n).style.display="block"}</script><style>table, th, td { vertical-align:top; padding: 3px} table {table-layout:fixed} td {word-wrap:break-word} .bs-callout { padding: 5px; margin: 5px 0; border: 1px solid #eee; border-left-width: 5px; border-radius: 3px; font-weight:normal; }.bs-callout-info {border-left-color: #5bc0de;}</style></head><body><nav style="background-color: #F0F8FF;" class="navbar navbar-light"><div class="container-fluid"><ul class="nav nav-pills"><li role="presentation"><a style="font-weight:bold" href="#" id="anchor_ppcx86" onclick="showme(this.id);">FULL SUMMARY</a></li><li role="presentation"><a style="font-weight:bold" href="#" id="anchor_ppcubuntu16" onclick="showme(this.id);">PPC UBUNTU16</a></li><li role="presentation"><a style="font-weight:bold" href="#" id="anchor_x86ubuntu16" onclick="showme(this.id);">X86 UBUNTU16</a></li><li role="presentation"><a style="font-weight:bold" href="#" id="anchor_ppcrhel7" onclick="showme(this.id);">PPC RHEL7</a></li><li role="presentation"><a style="font-weight:bold" href="#" id="anchor_x86rhel7" onclick="showme(this.id);">X86 RHEL7</a></li><li role="presentation"><a style="font-weight:bold" href="#" id="anchor_ppcrhel75" onclick="showme(this.id);">PPC RHEL75</a></li><p align="right" role="presentation" style="color:grey;padding-top:10px">Report Date: 16-04-2018 11:41 UTC</p></ul><div align="right" style="color:grey;font-size:12">Notations:<img src="resources/red.png" style="width: 16px; height: 16px;">Build failed </img><img src="resources/blue.png" style="width: 16px; height: 16px;">Build success with no failure </img><img src="resources/yellow.png" style="width: 16px; height: 16px;">N (M) Build success with N test failures &amp; M unique failures </img></div></div></nav><div style="table-cell" class="col-sm-2 col-md-2 sidebar"><div class="list-group"><a href="#" class="list-group-item list-group-item-action active" onclick="showme(this.id);" id="anchor_ppcx86">Packages</a><a class="list-group-item list-group-item-action" href="#" id="anchor_accumulo" onclick="showme(this.id);" title="Owned by Sonia">ACCUMULO</a><a class="list-group-item list-group-item-action" href="#" id="anchor_ambari" onclick="showme(this.id);" title="Owned by Valencia">AMBARI</a><a class="list-group-item list-group-item-action" href="#" id="anchor_atlas" onclick="showme(this.id);" title="Owned by Yussuf">ATLAS</a><a class="list-group-item list-group-item-action" href="#" id="anchor_falcon" onclick="showme(this.id);" title="Owned by Sonia">FALCON</a><a class="list-group-item list-group-item-action" href="#" id="anchor_flume" onclick="showme(this.id);" title="Owned by Pravin">FLUME</a><a class="list-group-item list-group-item-action" href="#" id="anchor_hadooptmp" onclick="showme(this.id);" title="Owned by Parita">HADOOP</a><a class="list-group-item list-group-item-action" href="#" id="anchor_hbase" onclick="showme(this.id);" title="Owned by Valencia">HBASE</a><a class="list-group-item list-group-item-action" href="#" id="anchor_hive" onclick="showme(this.id);" title="Owned by Alisha/Pravin">HIVE</a><a class="list-group-item list-group-item-action" href="#" id="anchor_kafka" onclick="showme(this.id);" title="Owned by Sonia">KAFKA</a><a class="list-group-item list-group-item-action" href="#" id="anchor_knox" onclick="showme(this.id);" title="Owned by Yussuf">KNOX</a><a class="list-group-item list-group-item-action" href="#" id="anchor_metron" onclick="showme(this.id);" title="Owned by Pravin">METRON</a><a class="list-group-item list-group-item-action" href="#" id="anchor_oozie" onclick="showme(this.id);" title="Owned by Alisha">OOZIE</a><a class="list-group-item list-group-item-action" href="#" id="anchor_phoenix" onclick="showme(this.id);" title="Owned by Valencia">PHOENIX</a><a class="list-group-item list-group-item-action" href="#" id="anchor_pig" onclick="showme(this.id);" title="Owned by Yussuf">PIG</a><a class="list-group-item list-group-item-action" href="#" id="anchor_ranger" onclick="showme(this.id);" title="Owned by Sneha">RANGER</a><a class="list-group-item list-group-item-action" href="#" id="anchor_slider" onclick="showme(this.id);" title="Owned by Yussuf">SLIDER</a><a class="list-group-item list-group-item-action" href="#" id="anchor_spark" onclick="showme(this.id);" title="Owned by Parita">SPARK</a><a class="list-group-item list-group-item-action" href="#" id="anchor_sqoop" onclick="showme(this.id);" title="Owned by Talat">SQOOP</a><a class="list-group-item list-group-item-action" href="#" id="anchor_storm" onclick="showme(this.id);" title="Owned by Parita">STORM</a><a class="list-group-item list-group-item-action" href="#" id="anchor_tez" onclick="showme(this.id);" title="Owned by Valencia">TEZ</a><a class="list-group-item list-group-item-action" href="#" id="anchor_zeppelin" onclick="showme(this.id);" title="Owned by Sneha">ZEPPELIN</a><a class="list-group-item list-group-item-action" href="#" id="anchor_zookeeper" onclick="showme(this.id);" title="Owned by Pravin">ZOOKEEPER</a></div></div><div style="display: table-cell"><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="accumulo"><div style="font-weight:bold;" class="panel-heading">ACCUMULO<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Sonia)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>3a8bb1611928d2f59a70393d93c9059b78c8a97e</div><div><b>Last Run: </b>10-04-2018 20:26 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th><th>PPC RHEL7.5</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 1707</div><div>Failed Count : 0</div><div>Skipped Count : 6</div></td><td><div>Total Count : 1707</div><div>Failed Count : 2</div><div>Skipped Count : 6</div></td><td><div>Total Count : 1707</div><div>Failed Count : 0</div><div>Skipped Count : 6</div></td><td><div>Total Count : 1707</div><div>Failed Count : 0</div><div>Skipped Count : 6</div></td><td><div>Total Count : 1707</div><div>Failed Count : 0</div><div>Skipped Count : 6</div></td></tr><tr><td>Result</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.accumulo.minicluster.MiniAccumuloClusterTest.testPerTableClasspath</li></div><div><li>org.apache.accumulo.minicluster.MiniAccumuloClusterTest.test</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>test timed out after 60000 milliseconds</li></div><div><li>test timed out after 30000 milliseconds</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.accumulo.minicluster.MiniAccumuloClusterTest.testPerTableClasspath</div></li><li><div>org.apache.accumulo.minicluster.MiniAccumuloClusterTest.test</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="ambari"><div style="font-weight:bold;" class="panel-heading">AMBARI<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Valencia)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> origin/trunk</div><div><b>Last Revision: </b>85e71227e5fa6043a0477c7a96998a21e7355860</div><div><b>Last Run: </b>11-04-2018 00:31 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th><th>PPC RHEL7.5</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 0</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 5477</div><div>Failed Count : 1</div><div>Skipped Count : 101</div></td><td><div>Total Count : 5477</div><div>Failed Count : 2</div><div>Skipped Count : 101</div></td><td><div>Total Count : 5477</div><div>Failed Count : 0</div><div>Skipped Count : 101</div></td><td><div>Total Count : 5477</div><div>Failed Count : 5</div><div>Skipped Count : 101</div></td></tr><tr><td>Result</td><td><img src="resources/red.png" align="top" style="width: 16px; height: 16px;"></img>FAILURE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.ambari.server.state.cluster.ClusterDeadlockTest.testDeadlockWhileRestartingComponents</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.ambari.server.controller.internal.JMXHostProviderTest.testJMXHistoryServerHttpsPort</li></div><div><li>org.apache.ambari.server.controller.metrics.RestMetricsPropertyProviderTest.testRestMetricsPropertyProviderAsAdministrator</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.ambari.server.controller.metrics.RestMetricsPropertyProviderTest.testRestMetricsPropertyProviderAsAdministrator</li></div><div><li>org.apache.ambari.server.controller.test.BufferedThreadPoolExecutorCompletionServiceTest.testMaxPoolSizeThreadsLaunched</li></div><div><li>org.apache.ambari.server.controller.test.BufferedThreadPoolExecutorCompletionServiceTest.testScalingThreadPoolExecutor</li></div><div><li>org.apache.ambari.server.security.encryption.CredentialStoreTest.testInMemoryCredentialStoreService_CredentialExpired</li></div><div><li>org.apache.ambari.server.state.cluster.ClusterDeadlockTest.testDeadlockWhileRestartingComponents</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>[Deadlocked Thread:
------------------
"Thread-22" Id=54 WAITING on org.eclipse.persistence.internal.identitymaps.HardCacheWeakIdentityMap$ReferenceCacheKey@4465f2c3
 at java.lang.Object.wait(Native Method)
 -  waiting on org.eclipse.persistence.internal.identitymaps.HardCacheWeakIdentityMap$ReferenceCacheKey@4465f2c3
 at java.lang.Object.wait(Object.java:502)
 at org.eclipse.persistence.internal.</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>ServiceComponent not found, clusterName=c1, serviceName=HDFS, serviceComponentName=JOURNALNODE</li></div><div><li>expected:&lt;400&gt; but was:&lt;397&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;400&gt; but was:&lt;345&gt;</li></div><div><li>expected:&lt;10&gt; but was:&lt;8&gt;</li></div><div><li>expected:&lt;10&gt; but was:&lt;8&gt;</li></div><div><li>java.lang.NullPointerException
	at org.apache.ambari.server.security.encryption.CredentialStoreTest.getExpiredCredentialTest(CredentialStoreTest.java:169)
	at org.apache.ambari.server.security.encryption.CredentialStoreTest.testInMemoryCredentialStoreService_CredentialExpired(CredentialStoreTest.java:90)
</li></div><div><li>[Deadlocked Thread:
------------------
"Thread-22" Id=53 WAITING on java.util.concurrent.locks.ReentrantReadWriteLock$NonfairSync@5973a38d owned by "Thread-29" Id=60
 at sun.misc.Unsafe.park(Native Method)
 -  waiting on java.util.concurrent.locks.ReentrantReadWriteLock$NonfairSync@5973a38d
 at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
 at java.util.concurrent.locks.Abstrac</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.ambari.server.state.cluster.ClusterDeadlockTest.testDeadlockWhileRestartingComponents</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.ambari.server.controller.internal.JMXHostProviderTest.testJMXHistoryServerHttpsPort</div></li><li><div>org.apache.ambari.server.controller.metrics.RestMetricsPropertyProviderTest.testRestMetricsPropertyProviderAsAdministrator</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="atlas"><div style="font-weight:bold;" class="panel-heading">ATLAS<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Yussuf)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>54c31d5c8e601757e19e26d3c30f2414532e2f8f</div><div><b>Last Run: </b>11-04-2018 15:22 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th><th>PPC RHEL7.5</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 839</div><div>Failed Count : 1</div><div>Skipped Count : 6</div></td><td><div>Total Count : 838</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 838</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 838</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 838</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td></tr><tr><td>Result</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"><div><li>org.apache.atlas.repository.audit.CassandraAuditRepositoryTest.setup</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"><div><li>com.datastax.driver.core.exceptions.NoHostAvailableException: All host(s) tried for query failed (tried: localhost/127.0.0.1:9042 (com.datastax.driver.core.exceptions.TransportException: [localhost/127.0.0.1:9042] Cannot connect))</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.atlas.repository.audit.CassandraAuditRepositoryTest.setup</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="falcon"><div style="font-weight:bold;" class="panel-heading">FALCON<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Sonia)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>1a5b4f6a509187498a267b0e375c6a065f947af5</div><div><b>Last Run: </b>12-04-2018 12:50 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th><th>PPC RHEL7.5</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 1007</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 1004</div><div>Failed Count : 3</div><div>Skipped Count : 7</div></td><td><div>Total Count : 1007</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 1003</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 1001</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td></tr><tr><td>Result</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.falcon.entity.v0.EntityGraphTest.initConfigStore</li></div><div><li>org.apache.falcon.messaging.JMSMessageConsumerTest.testJMSMessagesForOozieCoord</li></div><div><li>org.apache.falcon.notification.service.SchedulerServiceTest.testDeRegistration</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>Unable to restore configurations for entity type PROCESS</li></div><div><li>
Wanted but not invoked:
workflowJobEndNotificationService.notifyFailure(
    &lt;any&gt;
);
-&gt; at org.apache.falcon.messaging.JMSMessageConsumerTest.testJMSMessagesForOozieCoord(JMSMessageConsumerTest.java:290)

However, there were other interactions with this mock:
-&gt; at org.apache.falcon.messaging.JMSMessageConsumer.invokeListener(JMSMessageConsumer.java:213)
</li></div><div><li>expected [1] but found [null]</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.falcon.entity.v0.EntityGraphTest.initConfigStore</div></li><li><div>org.apache.falcon.messaging.JMSMessageConsumerTest.testJMSMessagesForOozieCoord</div></li><li><div>org.apache.falcon.notification.service.SchedulerServiceTest.testDeRegistration</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="flume"><div style="font-weight:bold;" class="panel-heading">FLUME<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Pravin)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/trunk</div><div><b>Last Revision: </b>0d437810dc850192b48fa3b31608ffcd23b1f1e9</div><div><b>Last Run: </b>10-04-2018 01:48 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th><th>PPC RHEL7.5</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 1182</div><div>Failed Count : 0</div><div>Skipped Count : 6</div></td><td><div>Total Count : 1208</div><div>Failed Count : 0</div><div>Skipped Count : 6</div></td><td><div>Total Count : 1182</div><div>Failed Count : 1</div><div>Skipped Count : 6</div></td><td><div>Total Count : 1208</div><div>Failed Count : 1</div><div>Skipped Count : 6</div></td><td><div>Total Count : 1179</div><div>Failed Count : 1</div><div>Skipped Count : 6</div></td></tr><tr><td>Result</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.flume.channel.file.TestLog.testMinimumRequiredSpaceTooSmallForPut</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.flume.channel.file.TestLog.testMinimumRequiredSpaceTooSmallForPut</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.flume.channel.file.TestLog.testMinimumRequiredSpaceTooSmallForPut</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>java.lang.AssertionError
	at org.apache.flume.channel.file.TestLog.doTestMinimumRequiredSpaceTooSmallForPut(TestLog.java:241)
	at org.apache.flume.channel.file.TestLog.testMinimumRequiredSpaceTooSmallForPut(TestLog.java:210)
</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>java.lang.AssertionError
	at org.apache.flume.channel.file.TestLog.doTestMinimumRequiredSpaceTooSmallForPut(TestLog.java:241)
	at org.apache.flume.channel.file.TestLog.testMinimumRequiredSpaceTooSmallForPut(TestLog.java:210)
</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>java.lang.AssertionError
	at org.apache.flume.channel.file.TestLog.doTestMinimumRequiredSpaceTooSmallForPut(TestLog.java:241)
	at org.apache.flume.channel.file.TestLog.testMinimumRequiredSpaceTooSmallForPut(TestLog.java:210)
</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="hadooptmp"><div style="font-weight:bold;" class="panel-heading">HADOOP<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Parita)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/trunk</div><div><b>Last Revision: </b>896b473f1b477976e449184ceea075bedd71d6e8</div><div><b>Last Run: </b>16-04-2018 09:52 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th><th>PPC RHEL7.5</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 19329</div><div>Failed Count : 34</div><div>Skipped Count : 1159</div></td><td><div>Total Count : 19363</div><div>Failed Count : 35</div><div>Skipped Count : 1160</div></td><td><div>Total Count : 19405</div><div>Failed Count : 15</div><div>Skipped Count : 1161</div></td><td><div>Total Count : 19448</div><div>Failed Count : 33</div><div>Skipped Count : 1160</div></td><td><div>Total Count : 19216</div><div>Failed Count : 99</div><div>Skipped Count : 1158</div></td></tr><tr><td>Result</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.crypto.key.kms.server.TestKMS.testDelegationTokensOpsHttpKerberized</li></div><div><li>org.apache.hadoop.crypto.key.kms.server.TestKMS.testDelegationTokensOpsHttpPseudo</li></div><div><li>org.apache.hadoop.crypto.key.kms.server.TestKMS.doKMSHAZKWithDelegationTokenAccess</li></div><div><li>org.apache.hadoop.crypto.key.kms.server.TestKMS.testDelegationTokensOpsHttpsKerberized</li></div><div><li>org.apache.hadoop.crypto.key.kms.server.TestKMS.testKMSHAZKDelegationTokenRenewCancelLegacy</li></div><div><li>org.apache.hadoop.crypto.key.kms.server.TestKMS.testStartStopHttpKerberos</li></div><div><li>org.apache.hadoop.crypto.key.kms.server.TestKMS.testStartStopHttpPseudo</li></div><div><li>org.apache.hadoop.crypto.key.kms.server.TestKMS.testDelegationTokensUpdatedInUGI</li></div><div><li>org.apache.hadoop.crypto.key.kms.server.TestKMS.testStartStopHttpsPseudo</li></div><div><li>org.apache.hadoop.crypto.key.kms.server.TestKMS.testStartStopHttpsKerberos</li></div><div><li>org.apache.hadoop.crypto.key.kms.server.TestKMS.testKMSHAZKDelegationTokenRenewCancel</li></div><div><li>org.apache.hadoop.crypto.key.kms.server.TestKMS.testDelegationTokensOpsHttpsPseudo</li></div><div><li>org.apache.hadoop.hdfs.TestDFSInotifyEventInputStreamKerberized.testWithKerberizedCluster</li></div><div><li>org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureWithRandomECPolicy.testCloseWithExceptionsInStreamer</li></div><div><li>org.apache.hadoop.hdfs.TestDistributedFileSystem.testDFSClientPeerWriteTimeout</li></div><div><li>org.apache.hadoop.hdfs.TestEncryptionZonesWithKMS.testDelegationToken</li></div><div><li>org.apache.hadoop.hdfs.TestLeaseRecovery2.testHardLeaseRecoveryAfterNameNodeRestart2</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailure.testVolumeFailure</li></div><div><li>org.apache.hadoop.hdfs.server.namenode.TestNamenodeCapacityReport.testXceiverCount</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testAuthUrlConnectTimeout[timeoutSource=ConnectionFactory]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testConnectTimeout[timeoutSource=ConnectionFactory]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testRedirectConnectTimeout[timeoutSource=ConnectionFactory]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testTwoStepWriteConnectTimeout[timeoutSource=ConnectionFactory]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testAuthUrlConnectTimeout[timeoutSource=Configuration]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testConnectTimeout[timeoutSource=Configuration]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testRedirectConnectTimeout[timeoutSource=Configuration]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testTwoStepWriteConnectTimeout[timeoutSource=Configuration]</li></div><div><li>org.apache.hadoop.mapred.TestMRTimelineEventHandling.testMapreduceJobTimelineServiceEnabled</li></div><div><li>org.apache.hadoop.fs.s3a.s3guard.TestDynamoDBMetadataStore.org.apache.hadoop.fs.s3a.s3guard.TestDynamoDBMetadataStore</li></div><div><li>org.apache.hadoop.yarn.applications.distributedshell.TestDistributedShell.testDSShellWithoutDomainV2</li></div><div><li>org.apache.hadoop.registry.server.dns.TestRegistryDNS.testExternalCNAMERecord</li></div><div><li>org.apache.hadoop.registry.server.dns.TestSecureRegistryDNS.testExternalCNAMERecord</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.TestAuxServices.testRemoteAuxServiceClassPath</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestQueueManagementDynamicEditPolicy.testEditSchedule</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.crypto.key.kms.server.TestKMS.testDelegationTokensOpsHttpKerberized</li></div><div><li>org.apache.hadoop.crypto.key.kms.server.TestKMS.testDelegationTokensOpsHttpPseudo</li></div><div><li>org.apache.hadoop.crypto.key.kms.server.TestKMS.doKMSHAZKWithDelegationTokenAccess</li></div><div><li>org.apache.hadoop.crypto.key.kms.server.TestKMS.testDelegationTokensOpsHttpsKerberized</li></div><div><li>org.apache.hadoop.crypto.key.kms.server.TestKMS.testKMSHAZKDelegationTokenRenewCancelLegacy</li></div><div><li>org.apache.hadoop.crypto.key.kms.server.TestKMS.testStartStopHttpKerberos</li></div><div><li>org.apache.hadoop.crypto.key.kms.server.TestKMS.testStartStopHttpPseudo</li></div><div><li>org.apache.hadoop.crypto.key.kms.server.TestKMS.testDelegationTokensUpdatedInUGI</li></div><div><li>org.apache.hadoop.crypto.key.kms.server.TestKMS.testStartStopHttpsPseudo</li></div><div><li>org.apache.hadoop.crypto.key.kms.server.TestKMS.testStartStopHttpsKerberos</li></div><div><li>org.apache.hadoop.crypto.key.kms.server.TestKMS.testKMSHAZKDelegationTokenRenewCancel</li></div><div><li>org.apache.hadoop.crypto.key.kms.server.TestKMS.testDelegationTokensOpsHttpsPseudo</li></div><div><li>org.apache.hadoop.fs.contract.router.web.TestRouterWebHDFSContractAppend.testRenameFileBeingAppended</li></div><div><li>org.apache.hadoop.hdfs.TestDFSClientRetries.testLeaseRenewAndDFSOutputStreamDeadLock</li></div><div><li>org.apache.hadoop.hdfs.TestDFSClientRetries.testLeaseRenewSocketTimeout</li></div><div><li>org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureWithRandomECPolicy.testCloseWithExceptionsInStreamer</li></div><div><li>org.apache.hadoop.hdfs.TestEncryptionZonesWithKMS.testDelegationToken</li></div><div><li>org.apache.hadoop.hdfs.TestReconstructStripedFile.testNNSendsErasureCodingTasks</li></div><div><li>org.apache.hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA.testUpdatePipeline</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testAuthUrlConnectTimeout[timeoutSource=ConnectionFactory]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testConnectTimeout[timeoutSource=ConnectionFactory]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testRedirectConnectTimeout[timeoutSource=ConnectionFactory]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testTwoStepWriteConnectTimeout[timeoutSource=ConnectionFactory]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testAuthUrlConnectTimeout[timeoutSource=Configuration]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testConnectTimeout[timeoutSource=Configuration]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testRedirectConnectTimeout[timeoutSource=Configuration]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testTwoStepWriteConnectTimeout[timeoutSource=Configuration]</li></div><div><li>org.apache.hadoop.mapred.TestMRTimelineEventHandling.testMapreduceJobTimelineServiceEnabled</li></div><div><li>org.apache.hadoop.yarn.sls.TestReservationSystemInvariants.testSimulatorRunning[Testing with: SYNTH, org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler, (nodeFile null)]</li></div><div><li>org.apache.hadoop.yarn.sls.TestSLSRunner.testSimulatorRunning[Testing with: SYNTH, org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler, (nodeFile null)]</li></div><div><li>org.apache.hadoop.yarn.applications.distributedshell.TestDistributedShell.testDSAttemptFailuresValidityIntervalFailed</li></div><div><li>org.apache.hadoop.yarn.applications.distributedshell.TestDistributedShell.testDSShellWithoutDomainV2DefaultFlow</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.TestAuxServices.testRemoteAuxServiceClassPath</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacitySchedulerSchedulingRequestUpdate.testNodePartitionPendingResourceUpdate</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestQueueManagementDynamicEditPolicy.testEditSchedule</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.io.nativeio.TestNativeIO.testStat</li></div><div><li>org.apache.hadoop.io.nativeio.TestNativeIO.testMultiThreadedStat</li></div><div><li>org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureWithRandomECPolicy.testCloseWithExceptionsInStreamer</li></div><div><li>org.apache.hadoop.hdfs.TestReadStripedFileWithMissingBlocks.testReadFileWithMissingBlocks</li></div><div><li>org.apache.hadoop.hdfs.server.balancer.TestBalancerRPCDelay.testBalancerRPCDelay</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.TestNNHandlesBlockReportPerStorage.blockReport_08</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.TestNNHandlesCombinedBlockReport.blockReport_09</li></div><div><li>org.apache.hadoop.mapred.TestClusterMRNotification.testMR</li></div><div><li>org.apache.hadoop.mapreduce.v2.TestSpeculativeExecutionWithMRApp.testSepculateSuccessfulWithUpdateEvents</li></div><div><li>org.apache.hadoop.fs.s3a.s3guard.TestDynamoDBMetadataStore.org.apache.hadoop.fs.s3a.s3guard.TestDynamoDBMetadataStore</li></div><div><li>org.apache.hadoop.yarn.applications.distributedshell.TestDistributedShell.testDSShellWithoutDomainV2CustomizedFlow</li></div><div><li>org.apache.hadoop.yarn.applications.distributedshell.TestDistributedShell.testDSShellWithoutDomainV2</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithContainerResourceChange[1]</li></div><div><li>TestDockerUtil.test_add_rw_mounts</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.TestRMRestart.testRMRestartRecoveringNodeLabelManager[CAPACITY]</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.metrics2.sink.TestRollingFileSystemSinkWithLocal.testSilentExistingWrite</li></div><div><li>org.apache.hadoop.metrics2.sink.TestRollingFileSystemSinkWithLocal.testExistingWrite2</li></div><div><li>org.apache.hadoop.metrics2.sink.TestRollingFileSystemSinkWithLocal.testWrite</li></div><div><li>org.apache.hadoop.metrics2.sink.TestRollingFileSystemSinkWithLocal.testSilentWrite</li></div><div><li>org.apache.hadoop.metrics2.sink.TestRollingFileSystemSinkWithLocal.testExistingWrite</li></div><div><li>org.apache.hadoop.crypto.key.kms.server.TestKMS.testDelegationTokensOpsHttpKerberized</li></div><div><li>org.apache.hadoop.crypto.key.kms.server.TestKMS.testDelegationTokensOpsHttpPseudo</li></div><div><li>org.apache.hadoop.crypto.key.kms.server.TestKMS.doKMSHAZKWithDelegationTokenAccess</li></div><div><li>org.apache.hadoop.crypto.key.kms.server.TestKMS.testDelegationTokensOpsHttpsKerberized</li></div><div><li>org.apache.hadoop.crypto.key.kms.server.TestKMS.testKMSHAZKDelegationTokenRenewCancelLegacy</li></div><div><li>org.apache.hadoop.crypto.key.kms.server.TestKMS.testStartStopHttpKerberos</li></div><div><li>org.apache.hadoop.crypto.key.kms.server.TestKMS.testStartStopHttpPseudo</li></div><div><li>org.apache.hadoop.crypto.key.kms.server.TestKMS.testDelegationTokensUpdatedInUGI</li></div><div><li>org.apache.hadoop.crypto.key.kms.server.TestKMS.testStartStopHttpsPseudo</li></div><div><li>org.apache.hadoop.crypto.key.kms.server.TestKMS.testStartStopHttpsKerberos</li></div><div><li>org.apache.hadoop.crypto.key.kms.server.TestKMS.testKMSHAZKDelegationTokenRenewCancel</li></div><div><li>org.apache.hadoop.crypto.key.kms.server.TestKMS.testDelegationTokensOpsHttpsPseudo</li></div><div><li>org.apache.hadoop.fs.contract.router.web.TestRouterWebHDFSContractAppend.testRenameFileBeingAppended</li></div><div><li>org.apache.hadoop.hdfs.TestEncryptionZonesWithKMS.testDelegationToken</li></div><div><li>org.apache.hadoop.hdfs.TestLeaseRecovery2.testHardLeaseRecoveryAfterNameNodeRestart2</li></div><div><li>org.apache.hadoop.hdfs.security.token.block.TestBlockToken.testCraftedProtobufBlockTokenBytesIsProtobuf</li></div><div><li>org.apache.hadoop.metrics2.sink.TestRollingFileSystemSinkWithHdfs.testSilentOverwrite</li></div><div><li>org.apache.hadoop.metrics2.sink.TestRollingFileSystemSinkWithHdfs.testWrite</li></div><div><li>org.apache.hadoop.metrics2.sink.TestRollingFileSystemSinkWithHdfs.testFlushThread</li></div><div><li>org.apache.hadoop.metrics2.sink.TestRollingFileSystemSinkWithHdfs.testNoAppend</li></div><div><li>org.apache.hadoop.metrics2.sink.TestRollingFileSystemSinkWithHdfs.testAppend</li></div><div><li>org.apache.hadoop.metrics2.sink.TestRollingFileSystemSinkWithHdfs.testSilentAppend</li></div><div><li>org.apache.hadoop.metrics2.sink.TestRollingFileSystemSinkWithSecureHdfs.testWithSecureHDFS</li></div><div><li>org.apache.hadoop.yarn.applications.distributedshell.TestDistributedShell.testDSShellWithoutDomainV2CustomizedFlow</li></div><div><li>TestDockerUtil.test_add_rw_mounts</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerResync.testNMSentContainerStatusOnResync</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.TestRMEmbeddedElector.testCallbackSynchronization</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestContainerResizing.testIncreaseContainerUnreservedWhenContainerCompleted</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.io.nativeio.TestNativeIO.testStat</li></div><div><li>org.apache.hadoop.io.nativeio.TestNativeIO.testMultiThreadedStat</li></div><div><li>org.apache.hadoop.hdfs.nfs.nfs3.TestWrites.testOverlappingWrites</li></div><div><li>org.apache.hadoop.fs.contract.router.web.TestRouterWebHDFSContractAppend.testRenameFileBeingAppended</li></div><div><li>org.apache.hadoop.hdfs.TestBlockStoragePolicy.testChangeWarmRep</li></div><div><li>org.apache.hadoop.hdfs.TestDFSShell.testMoveWithTargetPortEmpty</li></div><div><li>org.apache.hadoop.hdfs.TestDFSUpgradeFromImage.testUpgradeFromRel1BBWImage</li></div><div><li>org.apache.hadoop.hdfs.TestReadStripedFileWithMissingBlocks.testReadFileWithMissingBlocks</li></div><div><li>org.apache.hadoop.hdfs.server.blockmanagement.TestReconstructStripedBlocksWithRackAwareness.testReconstructForNotEnoughRacks</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.TestNNHandlesCombinedBlockReport.blockReport_08</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.TestNNHandlesCombinedBlockReport.blockReport_09</li></div><div><li>org.apache.hadoop.hdfs.server.namenode.TestAddOverReplicatedStripedBlocks.testProcessOverReplicatedSBSmallerThanFullBlocks</li></div><div><li>org.apache.hadoop.mapreduce.v2.hs.TestJobHistoryServer.testStartStopServer</li></div><div><li>org.apache.hadoop.mapreduce.v2.hs.TestJobHistoryServer.testReports</li></div><div><li>org.apache.hadoop.mapreduce.v2.hs.TestJobHistoryServer.testLaunch</li></div><div><li>org.apache.hadoop.mapred.TestClientRedirect.testRedirect</li></div><div><li>org.apache.hadoop.mapred.TestMRTimelineEventHandling.testMRTimelineEventHandling</li></div><div><li>org.apache.hadoop.mapred.TestMRTimelineEventHandling.testMapreduceJobTimelineServiceEnabled</li></div><div><li>org.apache.hadoop.mapreduce.security.TestJHSSecurity.testDelegationToken</li></div><div><li>org.apache.hadoop.mapred.TestShuffleHandler.testSendMapCount</li></div><div><li>org.apache.hadoop.tools.TestHadoopArchiveLogsRunner.testHadoopArchiveLogs</li></div><div><li>org.apache.hadoop.tools.TestHadoopArchiveLogsRunner.testHadoopArchiveLogsWithArchiveError</li></div><div><li>org.apache.hadoop.fs.s3a.s3guard.TestDynamoDBMetadataStore.org.apache.hadoop.fs.s3a.s3guard.TestDynamoDBMetadataStore</li></div><div><li>org.apache.hadoop.yarn.sls.scheduler.TestTaskRunner.testPreStartQueueing</li></div><div><li>org.apache.hadoop.yarn.service.client.TestApiServiceClient.org.apache.hadoop.yarn.service.client.TestApiServiceClient</li></div><div><li>org.apache.hadoop.yarn.service.TestYarnNativeServices.testCreateFlexStopDestroyService</li></div><div><li>org.apache.hadoop.yarn.client.TestApplicationMasterServiceProtocolForTimelineV2.testAllocateForTimelineV2OnHA</li></div><div><li>org.apache.hadoop.yarn.client.TestGetGroups.org.apache.hadoop.yarn.client.TestGetGroups</li></div><div><li>org.apache.hadoop.yarn.client.TestResourceManagerAdministrationProtocolPBClientImpl.org.apache.hadoop.yarn.client.TestResourceManagerAdministrationProtocolPBClientImpl</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithContainerResourceChange[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithContainerResourceChange[1]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestYarnClient.testShouldNotRetryForeverForNonNetworkExceptions[CAPACITY]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestYarnClient.testClientStop[CAPACITY]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestYarnClient.testShouldNotRetryForeverForNonNetworkExceptions[FAIR]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestYarnClient.testClientStop[FAIR]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestTimelineClient.testDelegationTokenOperationsRetry</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestTimelineClient.testCheckRetryCount</li></div><div><li>org.apache.hadoop.yarn.server.applicationhistoryservice.TestApplicationHistoryServer.testHostedUIs</li></div><div><li>org.apache.hadoop.yarn.server.applicationhistoryservice.TestApplicationHistoryServer.testLaunchWithArguments</li></div><div><li>org.apache.hadoop.yarn.server.applicationhistoryservice.TestApplicationHistoryServer.testStartStopServer</li></div><div><li>org.apache.hadoop.yarn.server.applicationhistoryservice.TestApplicationHistoryServer.testFilterOverrides</li></div><div><li>org.apache.hadoop.yarn.server.applicationhistoryservice.TestApplicationHistoryServer.testLaunch</li></div><div><li>org.apache.hadoop.yarn.server.timeline.security.TestTimelineAuthenticationFilterForV1.org.apache.hadoop.yarn.server.timeline.security.TestTimelineAuthenticationFilterForV1</li></div><div><li>org.apache.hadoop.yarn.server.timeline.webapp.TestTimelineWebServicesWithSSL.org.apache.hadoop.yarn.server.timeline.webapp.TestTimelineWebServicesWithSSL</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerReboot.testClearLocalDirWhenNodeReboot</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.TestNodeStatusUpdater.testStopReentrant</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.TestNodeStatusUpdater.testSignalContainerToContainerManager</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.TestNodeStatusUpdater.testRMVersionLessThanMinimum</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.TestNodeStatusUpdater.testNMConnectionToRM</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.TestNodeStatusUpdater.testNMShutdownForRegistrationFailure</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.TestNodeStatusUpdater.testApplicationKeepAlive</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.TestNodeStatusUpdater.testNodeStatusUpdaterRetryAndNMShutdown</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.TestNodeStatusUpdater.testNMRegistration</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.TestNodeStatusUpdater.testCompletedContainerStatusBackup</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.TestNodeStatusUpdater.testNodeDecommision</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.TestNodeStatusUpdaterForLabels.testNodeStatusUpdaterForNodeLabels</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.TestNodeStatusUpdaterForLabels.testInvalidNodeLabelsFromProvider</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.api.protocolrecords.impl.pb.TestPBLocalizerRPC.testLocalizerRPC</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManagerRecovery.testContainerSchedulerRecovery</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManagerRecovery.testResourceMappingRecoveryForContainer</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManagerRecovery.testContainerResizeRecovery</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.TestResourcePluginManager.testLinuxContainerExecutorWithResourcePluginsEnabled</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.TestResourcePluginManager.testNodeStatusUpdaterWithResourcePluginsEnabled</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.TestContainerSchedulerQueuing.testKillOnlyRequiredOpportunisticContainers</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServer.testNMWebApp</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.TestAMAuthorization.testAuthorizedAccess[0]</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.TestAMAuthorization.testUnauthorizedAccess[0]</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.TestAMAuthorization.testAuthorizedAccess[1]</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.TestAMAuthorization.testUnauthorizedAccess[1]</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.TestClientRMService.testForceKillApplication</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.TestOpportunisticContainerAllocatorAMService.testContainerAutoUpdateContainer</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService.testRefreshNodesResourceWithResourceReturnInRegistration</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService.testServiceAclsRefreshWithLocalConfigurationProvider</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService.testServiceAclsRefreshWithFileSystemBasedConfigurationProvider</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService.testRMInitialsWithFileSystemBasedConfigurationProvider</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.TestRMRestart.testRMRestartRecoveringNodeLabelManager[FAIR]</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.metrics.TestCombinedSystemMetricsPublisher.testTimelineServiceEventPublishingV1Enabled</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.metrics.TestCombinedSystemMetricsPublisher.testTimelineServiceEventPublishingNoService</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.metrics.TestCombinedSystemMetricsPublisher.testTimelineServiceEventPublishingV2Enabled</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.metrics.TestCombinedSystemMetricsPublisher.testTimelineServiceEventPublishingV1V2Enabled</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.metrics.TestSystemMetricsPublisher.org.apache.hadoop.yarn.server.resourcemanager.metrics.TestSystemMetricsPublisher</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.TestSchedulerUtils.testValidateResourceBlacklistRequest</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacityScheduler.testAllocateDoesNotBlockOnSchedulerLock</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestContainerResizing.testIncreaseContainerUnreservedWhenContainerCompleted</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestContainerResizing.testSimpleDecreaseContainer</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestIncreaseAllocationExpirer.testDecreaseAfterIncreaseWithAllocationExpiration</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestQueueManagementDynamicEditPolicy.testEditSchedule</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesDelegationTokenAuthentication.org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesDelegationTokenAuthentication</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesHttpStaticUserPermissions.org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesHttpStaticUserPermissions</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebappAuthentication.testSimpleAuth[0]</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebappAuthentication.testSimpleAuth[1]</li></div><div><li>org.apache.hadoop.yarn.server.router.clientrm.TestRouterClientRMService.testRequestInterceptorChainCreation</li></div><div><li>org.apache.hadoop.yarn.server.router.clientrm.TestRouterClientRMService.testUsersChainMapWithLRUCache</li></div><div><li>org.apache.hadoop.yarn.server.router.clientrm.TestRouterClientRMService.testRouterClientRMServiceE2E</li></div><div><li>org.apache.hadoop.yarn.server.router.webapp.TestRouterWebServices.testRequestInterceptorChainCreation</li></div><div><li>org.apache.hadoop.yarn.server.router.webapp.TestRouterWebServices.testUsersChainMapWithLRUCache</li></div><div><li>org.apache.hadoop.yarn.server.router.webapp.TestRouterWebServices.testRouterWebServicesE2E</li></div><div><li>org.apache.hadoop.yarn.server.router.webapp.TestRouterWebServicesREST.org.apache.hadoop.yarn.server.router.webapp.TestRouterWebServicesREST</li></div><div><li>org.apache.hadoop.yarn.server.TestRMNMSecretKeys.testNMUpdation</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"><div><li>java.lang.NullPointerException
	at org.apache.hadoop.crypto.key.kms.server.TestKMS$16$1.run(TestKMS.java:2244)
	at org.apache.hadoop.crypto.key.kms.server.TestKMS$16$1.run(TestKMS.java:2226)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1682)
	a</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.crypto.key.kms.server.TestKMS$16$1.run(TestKMS.java:2244)
	at org.apache.hadoop.crypto.key.kms.server.TestKMS$16$1.run(TestKMS.java:2226)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1682)
	a</li></div><div><li>expected:&lt;2&gt; but was:&lt;1&gt;</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.crypto.key.kms.server.TestKMS$16$1.run(TestKMS.java:2244)
	at org.apache.hadoop.crypto.key.kms.server.TestKMS$16$1.run(TestKMS.java:2226)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1682)
	a</li></div><div><li>expected:&lt;2&gt; but was:&lt;1&gt;</li></div><div><li>expected:&lt;2&gt; but was:&lt;1&gt;</li></div><div><li>expected:&lt;2&gt; but was:&lt;1&gt;</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertNotNull(Assert.java:621)
	at org.junit.Assert.assertNotNull(Assert.java:631)
	at org.apache.hadoop.crypto.key.kms.server.TestKMS$18$1.run(TestKMS.java:2394)
	at org.apache.hadoop.crypto.key.kms.server.TestKMS$18$1.run(TestKMS.java:2379)
	at java.security.Acc</li></div><div><li>expected:&lt;2&gt; but was:&lt;1&gt;</li></div><div><li>expected:&lt;2&gt; but was:&lt;1&gt;</li></div><div><li>expected:&lt;2&gt; but was:&lt;1&gt;</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.crypto.key.kms.server.TestKMS$16$1.run(TestKMS.java:2244)
	at org.apache.hadoop.crypto.key.kms.server.TestKMS$16$1.run(TestKMS.java:2226)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1682)
	a</li></div><div><li>DestHost:destPort localhost:13652 , LocalHost:localPort pts00607-vm16.persistent.co.in/10.88.67.131:0. Failed on local exception: java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]</li></div><div><li>Failed: the number of failed blocks = 2 &gt; the number of parity blocks = 1</li></div><div><li>write timedout too late in 1288 ms.</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div><div><li>lease holder should now be the NN</li></div><div><li>test timed out after 120000 milliseconds</li></div><div><li>Test resulted in an unexpected exit</li></div><div><li> Expected to find 'localhost:34265: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:34265: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li> Expected to find 'localhost:43350: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:43350: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li> Expected to find 'localhost:43733: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:43733: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li>expected timeout</li></div><div><li> Expected to find 'localhost:41385: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:41385: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li> Expected to find 'localhost:39560: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:39560: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li> Expected to find 'localhost:35878: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:35878: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li>expected timeout</li></div><div><li>Job didn't finish in 30 seconds</li></div><div><li>initTable on TestDynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.model.AmazonDynamoDBException: The request processing has failed because of an unknown error, exception or failure. (Service: AmazonDynamoDBv2; Status Code: 500; Error Code: InternalFailure; Request ID: 5a6cd59d-a51b-45e9-80b4-c059e9d5dbc9): The request processing has failed because of an unknown error, exception or failure.</li></div><div><li>Unexpected number of YARN_CONTAINER_FINISHED event published. expected:&lt;1&gt; but was:&lt;0&gt;</li></div><div><li>No A records in answer</li></div><div><li>No A records in answer</li></div><div><li>The remote jarfile should not be writable by group or others. The current Permission is 436</li></div><div><li>expected:&lt;0.5&gt; but was:&lt;0.0&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>java.lang.NullPointerException
	at org.apache.hadoop.crypto.key.kms.server.TestKMS$16$1.run(TestKMS.java:2244)
	at org.apache.hadoop.crypto.key.kms.server.TestKMS$16$1.run(TestKMS.java:2226)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1682)
	a</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.crypto.key.kms.server.TestKMS$16$1.run(TestKMS.java:2244)
	at org.apache.hadoop.crypto.key.kms.server.TestKMS$16$1.run(TestKMS.java:2226)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1682)
	a</li></div><div><li>expected:&lt;2&gt; but was:&lt;1&gt;</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.crypto.key.kms.server.TestKMS$16$1.run(TestKMS.java:2244)
	at org.apache.hadoop.crypto.key.kms.server.TestKMS$16$1.run(TestKMS.java:2226)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1682)
	a</li></div><div><li>expected:&lt;2&gt; but was:&lt;1&gt;</li></div><div><li>expected:&lt;2&gt; but was:&lt;1&gt;</li></div><div><li>expected:&lt;2&gt; but was:&lt;1&gt;</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertNotNull(Assert.java:621)
	at org.junit.Assert.assertNotNull(Assert.java:631)
	at org.apache.hadoop.crypto.key.kms.server.TestKMS$18$1.run(TestKMS.java:2394)
	at org.apache.hadoop.crypto.key.kms.server.TestKMS$18$1.run(TestKMS.java:2379)
	at java.security.Acc</li></div><div><li>expected:&lt;2&gt; but was:&lt;1&gt;</li></div><div><li>expected:&lt;2&gt; but was:&lt;1&gt;</li></div><div><li>expected:&lt;2&gt; but was:&lt;1&gt;</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.crypto.key.kms.server.TestKMS$16$1.run(TestKMS.java:2244)
	at org.apache.hadoop.crypto.key.kms.server.TestKMS$16$1.run(TestKMS.java:2226)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1682)
	a</li></div><div><li>Failed to append to non-existent file /test/test/target for client 127.0.0.1
 at org.apache.hadoop.hdfs.server.namenode.FSDirAppendOp.appendFile(FSDirAppendOp.java:104)
 at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.appendFile(FSNamesystem.java:2621)
 at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.append(NameNodeRpcServer.java:805)
 at org.apache.hadoop.hdfs.protocolPB.Client</li></div><div><li>test timed out after 120000 milliseconds</li></div><div><li>Lease should be empty.</li></div><div><li>Failed: the number of failed blocks = 2 &gt; the number of parity blocks = 1</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div><div><li>Timed out waiting for condition. Thread diagnostics:
Timestamp: 2018-04-15 10:16:02,261

"refreshUsed-/var/lib/jenkins/workspace/hadooptmp/label/x86ub16/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-1202053148-10.53.17.125-1523830512217" daemon prio=5 tid=3977 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        a</li></div><div><li>After waiting the operation updatePipeline still has not taken effect on NN yet</li></div><div><li> Expected to find 'localhost:32926: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:32926: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li> Expected to find 'localhost:42876: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:42876: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li> Expected to find 'localhost:37179: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:37179: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li>expected timeout</li></div><div><li> Expected to find 'localhost:39973: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:39973: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li> Expected to find 'localhost:33978: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:33978: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li> Expected to find 'localhost:36091: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:36091: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li>expected timeout</li></div><div><li>Job didn't finish in 30 seconds</li></div><div><li>TestSLSRunner catched exception from child thread (TaskRunner.TaskDefinition): [java.lang.reflect.UndeclaredThrowableException]</li></div><div><li>TestSLSRunner catched exception from child thread (TaskRunner.TaskDefinition): [java.lang.reflect.UndeclaredThrowableException]</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertFalse(Assert.java:64)
	at org.junit.Assert.assertFalse(Assert.java:74)
	at org.apache.hadoop.yarn.applications.distributedshell.TestDistributedShell.testDSAttemptFailuresValidityIntervalFailed(TestDistributedShell.java:870)
	at sun.reflect.NativeMethodAccess</li></div><div><li>Unexpected number of YARN_CONTAINER_FINISHED event published. expected:&lt;1&gt; but was:&lt;0&gt;</li></div><div><li>The remote jarfile should not be writable by group or others. The current Permission is 436</li></div><div><li>expected:&lt;0&gt; but was:&lt;8192&gt;</li></div><div><li>expected:&lt;0.5&gt; but was:&lt;0.0&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>length != 10(unixSymbolicPermission=-rw-r--r--.)</li></div><div><li>java.lang.IllegalArgumentException: length != 10(unixSymbolicPermission=-rw-r--r--.)</li></div><div><li>Failed: the number of failed blocks = 2 &gt; the number of parity blocks = 1</li></div><div><li>test timed out after 300000 milliseconds</li></div><div><li>test timed out after 100000 milliseconds</li></div><div><li>Wrong number of PendingReplication blocks expected:&lt;2&gt; but was:&lt;0&gt;</li></div><div><li>Wrong number of PendingReplication blocks expected:&lt;2&gt; but was:&lt;1&gt;</li></div><div><li>Job didn't finish in 30 seconds</li></div><div><li>Timed out waiting for condition. Thread diagnostics:
Timestamp: 2018-04-16 07:02:15,714

"org.eclipse.jetty.server.session.HashSessionManager@1ddae9b5Timer" daemon prio=5 tid=32 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.Abs</li></div><div><li>initTable on TestDynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.model.AmazonDynamoDBException: The request processing has failed because of an unknown error, exception or failure. (Service: AmazonDynamoDBv2; Status Code: 500; Error Code: InternalFailure; Request ID: 8d0ea32f-b237-4808-984d-d4df8b94b503): The request processing has failed because of an unknown error, exception or failure.</li></div><div><li>Unexpected number of YARN_CONTAINER_FINISHED event published. expected:&lt;1&gt; but was:&lt;0&gt;</li></div><div><li>Unexpected number of YARN_CONTAINER_FINISHED event published. expected:&lt;1&gt; but was:&lt;0&gt;</li></div><div><li>expected:&lt;1&gt; but was:&lt;0&gt;</li></div><div><li>/var/lib/jenkins/workspace/hadooptmp/label/master/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/test/utils/test_docker_util.cc:932
      Expected: 0
To be equal to: ret
      Which is: 14</li></div><div><li>expected:&lt;2&gt; but was:&lt;3&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>The sink created too few log files. 1 files were created</li></div><div><li>The sink created too few log files. 2 files were created</li></div><div><li>No valid log directories found</li></div><div><li>No valid log directories found</li></div><div><li>The sink created too few log files. 1 files were created</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.crypto.key.kms.server.TestKMS$16$1.run(TestKMS.java:2244)
	at org.apache.hadoop.crypto.key.kms.server.TestKMS$16$1.run(TestKMS.java:2226)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1682)
	a</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.crypto.key.kms.server.TestKMS$16$1.run(TestKMS.java:2244)
	at org.apache.hadoop.crypto.key.kms.server.TestKMS$16$1.run(TestKMS.java:2226)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1682)
	a</li></div><div><li>expected:&lt;2&gt; but was:&lt;1&gt;</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.crypto.key.kms.server.TestKMS$16$1.run(TestKMS.java:2244)
	at org.apache.hadoop.crypto.key.kms.server.TestKMS$16$1.run(TestKMS.java:2226)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1682)
	a</li></div><div><li>expected:&lt;2&gt; but was:&lt;1&gt;</li></div><div><li>expected:&lt;2&gt; but was:&lt;1&gt;</li></div><div><li>expected:&lt;2&gt; but was:&lt;1&gt;</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertNotNull(Assert.java:621)
	at org.junit.Assert.assertNotNull(Assert.java:631)
	at org.apache.hadoop.crypto.key.kms.server.TestKMS$18$1.run(TestKMS.java:2394)
	at org.apache.hadoop.crypto.key.kms.server.TestKMS$18$1.run(TestKMS.java:2379)
	at java.security.Acc</li></div><div><li>expected:&lt;2&gt; but was:&lt;1&gt;</li></div><div><li>expected:&lt;2&gt; but was:&lt;1&gt;</li></div><div><li>expected:&lt;2&gt; but was:&lt;1&gt;</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.crypto.key.kms.server.TestKMS$16$1.run(TestKMS.java:2244)
	at org.apache.hadoop.crypto.key.kms.server.TestKMS$16$1.run(TestKMS.java:2226)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1682)
	a</li></div><div><li>Failed to append to non-existent file /test/test/target for client 127.0.0.1
 at org.apache.hadoop.hdfs.server.namenode.FSDirAppendOp.appendFile(FSDirAppendOp.java:104)
 at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.appendFile(FSNamesystem.java:2621)
 at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.append(NameNodeRpcServer.java:805)
 at org.apache.hadoop.hdfs.protocolPB.Client</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div><div><li>lease holder should now be the NN</li></div><div><li>Received RuntimeException but it was not expected.</li></div><div><li>The sink created too few log files. 1 files were created</li></div><div><li>No valid log directories found</li></div><div><li>File does not exist: hdfs://localhost:40486/tmp/201804160100/testsrc-in-ibmibm1899.persistent.co.in.log</li></div><div><li>The sink created too few log files. 1 files were created</li></div><div><li>Sink did not produce the expected output. Actual output was: Extra stuff
</li></div><div><li>Sink did not produce the expected output. Actual output was: Extra stuff
</li></div><div><li>No valid log directories found</li></div><div><li>Unexpected number of YARN_CONTAINER_FINISHED event published. expected:&lt;1&gt; but was:&lt;0&gt;</li></div><div><li>/var/lib/jenkins/workspace/hadooptmp/label/x86rh7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/test/utils/test_docker_util.cc:932
      Expected: 0
To be equal to: ret
      Which is: 14</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertFalse(Assert.java:64)
	at org.junit.Assert.assertFalse(Assert.java:74)
	at org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerResync.testNMSentContainerStatusOnResync(TestNodeManagerResync.java:333)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Nati</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.ha.ActiveStandbyElector$3.run(ActiveStandbyElector.java:1033)
	at org.apache.hadoop.ha.ActiveStandbyElector$3.run(ActiveStandbyElector.java:1030)
	at org.apache.hadoop.ha.ActiveStandbyElector.zkDoWithRetries(ActiveStandbyElector.java:1095)
	at org.apache.hadoop.ha.ActiveStandbyElector.zkDoWithRetries(ActiveStandbyElector.java:1087)
	at org.apach</li></div><div><li>expected:&lt;1024&gt; but was:&lt;7168&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>length != 10(unixSymbolicPermission=-rw-r--r--.)</li></div><div><li>java.lang.IllegalArgumentException: length != 10(unixSymbolicPermission=-rw-r--r--.)</li></div><div><li>Write can't finish.</li></div><div><li>Failed to append to non-existent file /test/test/target for client 127.0.0.1
 at org.apache.hadoop.hdfs.server.namenode.FSDirAppendOp.appendFile(FSDirAppendOp.java:104)
 at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.appendFile(FSNamesystem.java:2621)
 at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.append(NameNodeRpcServer.java:805)
 at org.apache.hadoop.hdfs.protocolPB.Client</li></div><div><li>expected:&lt;3&gt; but was:&lt;2&gt;</li></div><div><li>test timed out after 30000 milliseconds</li></div><div><li>Failed to recover lease of /1kb-multiple-checksum-blocks-64-16</li></div><div><li>test timed out after 300000 milliseconds</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div><div><li>Wrong number of PendingReplication blocks expected:&lt;2&gt; but was:&lt;1&gt;</li></div><div><li>Wrong number of PendingReplication blocks expected:&lt;2&gt; but was:&lt;1&gt;</li></div><div><li>expected:&lt;8&gt; but was:&lt;7&gt;</li></div><div><li>java.net.BindException: Problem binding to [0.0.0.0:10033] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException</li></div><div><li>java.net.BindException: Problem binding to [0.0.0.0:10033] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException</li></div><div><li>expected:&lt;0&gt; but was:&lt;-1&gt;</li></div><div><li>java.net.BindException: Problem binding to [0.0.0.0:10020] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException</li></div><div><li>Job didn't finish in 30 seconds</li></div><div><li>Job didn't finish in 30 seconds</li></div><div><li>java.net.BindException: Problem binding to [0.0.0.0:10033] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException</li></div><div><li>Failed to bind to: 0.0.0.0/0.0.0.0:13562</li></div><div><li>org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.net.BindException: Problem binding to [0.0.0.0:8030] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException</li></div><div><li>org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.net.BindException: Problem binding to [0.0.0.0:8030] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException</li></div><div><li>initTable on TestDynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.model.AmazonDynamoDBException: The request processing has failed because of an unknown error, exception or failure. (Service: AmazonDynamoDBv2; Status Code: 500; Error Code: InternalFailure; Request ID: 72ae9068-7bd8-41fa-862b-4e8fc4dbed73): The request processing has failed because of an unknown error, exception or failure.</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.yarn.sls.scheduler.TestTaskRunner.testPreStartQueueing(TestTaskRunner.java:244)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImp</li></div><div><li>Address already in use</li></div><div><li>expected:&lt;3&gt; but was:&lt;2&gt;</li></div><div><li>java.net.BindException: Problem binding to [0.0.0.0:10200] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException</li></div><div><li>ResourceManager failed to start up.</li></div><div><li>ResourceManager failed to start up.</li></div><div><li>expected:&lt;1&gt; but was:&lt;0&gt;</li></div><div><li>expected:&lt;1&gt; but was:&lt;0&gt;</li></div><div><li>Unexpected exception, expected&lt;org.apache.hadoop.yarn.exceptions.ApplicationNotFoundException&gt; but was&lt;org.apache.hadoop.yarn.exceptions.YarnRuntimeException&gt;</li></div><div><li>java.net.BindException: Problem binding to [0.0.0.0:8030] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException</li></div><div><li>Unexpected exception, expected&lt;org.apache.hadoop.yarn.exceptions.ApplicationNotFoundException&gt; but was&lt;org.apache.hadoop.yarn.exceptions.YarnRuntimeException&gt;</li></div><div><li>java.net.BindException: Problem binding to [0.0.0.0:8030] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException</li></div><div><li>org.apache.hadoop.security.authentication.client.AuthenticationException: Error while authenticating with endpoint: http://0.0.0.0:8188/ws/v1/timeline/?op=GETDELEGATIONTOKEN&amp;renewer=jenkins</li></div><div><li>Handler exception for reason other than retry: java.io.IOException: org.apache.hadoop.security.authentication.client.AuthenticationException: Error while authenticating with endpoint: http://0.0.0.0:8188/ws/v1/timeline/</li></div><div><li>java.net.BindException: Problem binding to [0.0.0.0:10200] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException</li></div><div><li>expected:&lt;0&gt; but was:&lt;-1&gt;</li></div><div><li>java.net.BindException: Problem binding to [0.0.0.0:10200] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException</li></div><div><li>java.net.BindException: Problem binding to [0.0.0.0:10200] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException</li></div><div><li>expected:&lt;0&gt; but was:&lt;-1&gt;</li></div><div><li>Couldn't setup TimelineServer</li></div><div><li>java.net.BindException: Problem binding to [0.0.0.0:10200] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException</li></div><div><li>NMWebapps failed to start.</li></div><div><li>NMWebapps failed to start.</li></div><div><li>NMWebapps failed to start.</li></div><div><li>NMWebapps failed to start.</li></div><div><li>NM should have tried re-connecting to RM during period of at least 5000 ms, but stopped retrying within 55000 ms: org.apache.hadoop.yarn.exceptions.YarnRuntimeException: NMWebapps failed to start.</li></div><div><li>NMWebapps failed to start.</li></div><div><li>NMWebapps failed to start.</li></div><div><li>NMWebapps failed to start.</li></div><div><li>NodeManager failed to start</li></div><div><li>NMWebapps failed to start.</li></div><div><li>NMWebapps failed to start.</li></div><div><li>NMWebapps failed to start.</li></div><div><li>NMWebapps failed to start.</li></div><div><li>java.net.BindException: Problem binding to [0.0.0.0:8040] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException</li></div><div><li>java.net.BindException: Problem binding to [0.0.0.0:8040] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException</li></div><div><li>java.net.BindException: Problem binding to [0.0.0.0:8040] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException</li></div><div><li>java.net.BindException: Problem binding to [0.0.0.0:8040] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException</li></div><div><li>NMWebapps failed to start.</li></div><div><li>NMWebapps failed to start.</li></div><div><li>expected:&lt;2&gt; but was:&lt;0&gt;</li></div><div><li>NMWebapps failed to start.</li></div><div><li>java.net.BindException: Problem binding to [0.0.0.0:8030] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException</li></div><div><li>java.net.BindException: Problem binding to [0.0.0.0:8030] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException</li></div><div><li>java.net.BindException: Problem binding to [0.0.0.0:8030] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException</li></div><div><li>java.net.BindException: Problem binding to [0.0.0.0:8030] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException</li></div><div><li>Error starting http server</li></div><div><li>expected:&lt;1&gt; but was:&lt;0&gt;</li></div><div><li>expected:&lt;&lt;memory:[4096, vCores:4]&gt;&gt; but was:&lt;&lt;memory:[2048, vCores:2]&gt;&gt;</li></div><div><li>Using localConfigurationProvider. Should not get any exception.</li></div><div><li>Should not get any exceptions</li></div><div><li>Should not get any exceptions</li></div><div><li>expected:&lt;2&gt; but was:&lt;0&gt;</li></div><div><li>java.net.BindException: Problem binding to [0.0.0.0:10200] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException</li></div><div><li>java.net.BindException: Problem binding to [0.0.0.0:10200] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException</li></div><div><li>java.net.BindException: Problem binding to [0.0.0.0:10200] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException</li></div><div><li>java.net.BindException: Problem binding to [0.0.0.0:10200] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException</li></div><div><li>java.net.BindException: Problem binding to [0.0.0.0:10200] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException</li></div><div><li>java.net.BindException: Problem binding to [0.0.0.0:8030] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException</li></div><div><li>java.net.BindException: Problem binding to [0.0.0.0:8030] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException</li></div><div><li>expected:&lt;1024&gt; but was:&lt;7168&gt;</li></div><div><li>expected:&lt;1024&gt; but was:&lt;3072&gt;</li></div><div><li>expected:&lt;3072&gt; but was:&lt;4096&gt;</li></div><div><li>expected:&lt;0.5&gt; but was:&lt;0.0&gt;</li></div><div><li>Couldn't create MiniKDC</li></div><div><li>Couldn't create MiniKDC</li></div><div><li>Error starting http server</li></div><div><li>Error starting http server</li></div><div><li>java.net.BindException: Problem binding to [0.0.0.0:8050] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException</li></div><div><li>java.net.BindException: Problem binding to [0.0.0.0:8050] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException</li></div><div><li>java.net.BindException: Problem binding to [0.0.0.0:8050] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException</li></div><div><li>java.net.BindException: Problem binding to [0.0.0.0:8050] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException</li></div><div><li>java.net.BindException: Problem binding to [0.0.0.0:8050] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException</li></div><div><li>java.net.BindException: Problem binding to [0.0.0.0:8050] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException</li></div><div><li>Web app not running</li></div><div><li>java.net.BindException: Problem binding to [0.0.0.0:8030] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.hdfs.TestDFSInotifyEventInputStreamKerberized.testWithKerberizedCluster</div></li><li><div>org.apache.hadoop.hdfs.TestDistributedFileSystem.testDFSClientPeerWriteTimeout</div></li><li><div>org.apache.hadoop.hdfs.TestLeaseRecovery2.testHardLeaseRecoveryAfterNameNodeRestart2</div></li><li><div>org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailure.testVolumeFailure</div></li><li><div>org.apache.hadoop.hdfs.server.namenode.TestNamenodeCapacityReport.testXceiverCount</div></li><li><div>org.apache.hadoop.fs.s3a.s3guard.TestDynamoDBMetadataStore.org.apache.hadoop.fs.s3a.s3guard.TestDynamoDBMetadataStore</div></li><li><div>org.apache.hadoop.yarn.applications.distributedshell.TestDistributedShell.testDSShellWithoutDomainV2</div></li><li><div>org.apache.hadoop.registry.server.dns.TestRegistryDNS.testExternalCNAMERecord</div></li><li><div>org.apache.hadoop.registry.server.dns.TestSecureRegistryDNS.testExternalCNAMERecord</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.fs.contract.router.web.TestRouterWebHDFSContractAppend.testRenameFileBeingAppended</div></li><li><div>org.apache.hadoop.hdfs.TestDFSClientRetries.testLeaseRenewAndDFSOutputStreamDeadLock</div></li><li><div>org.apache.hadoop.hdfs.TestDFSClientRetries.testLeaseRenewSocketTimeout</div></li><li><div>org.apache.hadoop.hdfs.TestReconstructStripedFile.testNNSendsErasureCodingTasks</div></li><li><div>org.apache.hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA.testUpdatePipeline</div></li><li><div>org.apache.hadoop.yarn.sls.TestReservationSystemInvariants.testSimulatorRunning[Testing with: SYNTH, org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler, (nodeFile null)]</div></li><li><div>org.apache.hadoop.yarn.sls.TestSLSRunner.testSimulatorRunning[Testing with: SYNTH, org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler, (nodeFile null)]</div></li><li><div>org.apache.hadoop.yarn.applications.distributedshell.TestDistributedShell.testDSAttemptFailuresValidityIntervalFailed</div></li><li><div>org.apache.hadoop.yarn.applications.distributedshell.TestDistributedShell.testDSShellWithoutDomainV2DefaultFlow</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacitySchedulerSchedulingRequestUpdate.testNodePartitionPendingResourceUpdate</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.io.nativeio.TestNativeIO.testStat</div></li><li><div>org.apache.hadoop.io.nativeio.TestNativeIO.testMultiThreadedStat</div></li><li><div>org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureWithRandomECPolicy.testCloseWithExceptionsInStreamer</div></li><li><div>org.apache.hadoop.hdfs.TestReadStripedFileWithMissingBlocks.testReadFileWithMissingBlocks</div></li><li><div>org.apache.hadoop.hdfs.server.balancer.TestBalancerRPCDelay.testBalancerRPCDelay</div></li><li><div>org.apache.hadoop.hdfs.server.datanode.TestNNHandlesBlockReportPerStorage.blockReport_08</div></li><li><div>org.apache.hadoop.hdfs.server.datanode.TestNNHandlesCombinedBlockReport.blockReport_09</div></li><li><div>org.apache.hadoop.mapred.TestClusterMRNotification.testMR</div></li><li><div>org.apache.hadoop.mapreduce.v2.TestSpeculativeExecutionWithMRApp.testSepculateSuccessfulWithUpdateEvents</div></li><li><div>org.apache.hadoop.fs.s3a.s3guard.TestDynamoDBMetadataStore.org.apache.hadoop.fs.s3a.s3guard.TestDynamoDBMetadataStore</div></li><li><div>org.apache.hadoop.yarn.applications.distributedshell.TestDistributedShell.testDSShellWithoutDomainV2</div></li><li><div>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithContainerResourceChange[1]</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.TestRMRestart.testRMRestartRecoveringNodeLabelManager[CAPACITY]</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.metrics2.sink.TestRollingFileSystemSinkWithLocal.testSilentExistingWrite</div></li><li><div>org.apache.hadoop.metrics2.sink.TestRollingFileSystemSinkWithLocal.testExistingWrite2</div></li><li><div>org.apache.hadoop.metrics2.sink.TestRollingFileSystemSinkWithLocal.testWrite</div></li><li><div>org.apache.hadoop.metrics2.sink.TestRollingFileSystemSinkWithLocal.testSilentWrite</div></li><li><div>org.apache.hadoop.metrics2.sink.TestRollingFileSystemSinkWithLocal.testExistingWrite</div></li><li><div>org.apache.hadoop.crypto.key.kms.server.TestKMS.testDelegationTokensOpsHttpKerberized</div></li><li><div>org.apache.hadoop.crypto.key.kms.server.TestKMS.testDelegationTokensOpsHttpPseudo</div></li><li><div>org.apache.hadoop.crypto.key.kms.server.TestKMS.doKMSHAZKWithDelegationTokenAccess</div></li><li><div>org.apache.hadoop.crypto.key.kms.server.TestKMS.testDelegationTokensOpsHttpsKerberized</div></li><li><div>org.apache.hadoop.crypto.key.kms.server.TestKMS.testKMSHAZKDelegationTokenRenewCancelLegacy</div></li><li><div>org.apache.hadoop.crypto.key.kms.server.TestKMS.testStartStopHttpKerberos</div></li><li><div>org.apache.hadoop.crypto.key.kms.server.TestKMS.testStartStopHttpPseudo</div></li><li><div>org.apache.hadoop.crypto.key.kms.server.TestKMS.testDelegationTokensUpdatedInUGI</div></li><li><div>org.apache.hadoop.crypto.key.kms.server.TestKMS.testStartStopHttpsPseudo</div></li><li><div>org.apache.hadoop.crypto.key.kms.server.TestKMS.testStartStopHttpsKerberos</div></li><li><div>org.apache.hadoop.crypto.key.kms.server.TestKMS.testKMSHAZKDelegationTokenRenewCancel</div></li><li><div>org.apache.hadoop.crypto.key.kms.server.TestKMS.testDelegationTokensOpsHttpsPseudo</div></li><li><div>org.apache.hadoop.fs.contract.router.web.TestRouterWebHDFSContractAppend.testRenameFileBeingAppended</div></li><li><div>org.apache.hadoop.hdfs.TestEncryptionZonesWithKMS.testDelegationToken</div></li><li><div>org.apache.hadoop.hdfs.TestLeaseRecovery2.testHardLeaseRecoveryAfterNameNodeRestart2</div></li><li><div>org.apache.hadoop.hdfs.security.token.block.TestBlockToken.testCraftedProtobufBlockTokenBytesIsProtobuf</div></li><li><div>org.apache.hadoop.metrics2.sink.TestRollingFileSystemSinkWithHdfs.testSilentOverwrite</div></li><li><div>org.apache.hadoop.metrics2.sink.TestRollingFileSystemSinkWithHdfs.testWrite</div></li><li><div>org.apache.hadoop.metrics2.sink.TestRollingFileSystemSinkWithHdfs.testFlushThread</div></li><li><div>org.apache.hadoop.metrics2.sink.TestRollingFileSystemSinkWithHdfs.testNoAppend</div></li><li><div>org.apache.hadoop.metrics2.sink.TestRollingFileSystemSinkWithHdfs.testAppend</div></li><li><div>org.apache.hadoop.metrics2.sink.TestRollingFileSystemSinkWithHdfs.testSilentAppend</div></li><li><div>org.apache.hadoop.metrics2.sink.TestRollingFileSystemSinkWithSecureHdfs.testWithSecureHDFS</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerResync.testNMSentContainerStatusOnResync</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.TestRMEmbeddedElector.testCallbackSynchronization</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestContainerResizing.testIncreaseContainerUnreservedWhenContainerCompleted</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="hbase"><div style="font-weight:bold;" class="panel-heading">HBASE<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Valencia)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>edf50495022c798666f91e7adccb3b93e4200230</div><div><b>Last Run: </b>14-04-2018 16:03 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th><th>PPC RHEL7.5</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 4562</div><div>Failed Count : 2</div><div>Skipped Count : 39</div></td><td><div>Total Count : 4575</div><div>Failed Count : 6</div><div>Skipped Count : 39</div></td><td><div>Total Count : 4562</div><div>Failed Count : 0</div><div>Skipped Count : 39</div></td><td><div>Total Count : 4574</div><div>Failed Count : 1</div><div>Skipped Count : 39</div></td><td><div>Total Count : 4561</div><div>Failed Count : 5</div><div>Skipped Count : 39</div></td></tr><tr><td>Result</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.hbase.regionserver.TestEncryptionKeyRotation.testCFKeyRotation</li></div><div><li>org.apache.hadoop.hbase.replication.TestReplicationTrackerZKImpl.testGetListOfRegionServers</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.hbase.TestServerSideScanMetricsFromClientSide.testRowsSeenMetricWithAsync</li></div><div><li>org.apache.hadoop.hbase.TestServerSideScanMetricsFromClientSide.org.apache.hadoop.hbase.TestServerSideScanMetricsFromClientSide</li></div><div><li>org.apache.hadoop.hbase.TestServerSideScanMetricsFromClientSide.org.apache.hadoop.hbase.TestServerSideScanMetricsFromClientSide</li></div><div><li>org.apache.hadoop.hbase.master.cleaner.TestHFileLinkCleaner.org.apache.hadoop.hbase.master.cleaner.TestHFileLinkCleaner</li></div><div><li>org.apache.hadoop.hbase.regionserver.TestEncryptionKeyRotation.testCFKeyRotation</li></div><div><li>org.apache.hadoop.hbase.regionserver.TestScannerHeartbeatMessages.testHeartbeatWithSparseRowFilter</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.hbase.master.cleaner.TestLogsCleaner.testZooKeeperNormal</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.hbase.rsgroup.TestEnableRSGroup.testEnableRSGroup</li></div><div><li>org.apache.hadoop.hbase.rsgroup.TestRSGroups.org.apache.hadoop.hbase.rsgroup.TestRSGroups</li></div><div><li>org.apache.hadoop.hbase.rsgroup.TestRSGroups.org.apache.hadoop.hbase.rsgroup.TestRSGroups</li></div><div><li>org.apache.hadoop.hbase.client.TestMultiParallel.testActiveThreadsCount</li></div><div><li>org.apache.hadoop.hbase.master.TestAssignmentManagerMetrics.testRITAssignmentManagerMetrics</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"><div><li>Store file hdfs://localhost:33384/user/jenkins/test-data/0cba702e-d854-4701-83b7-f0644850c33b/data/default/testCFKeyRotation/1c5db80f3e3377f79de43eb3bbf222d5/cf/787b3a64ef6947668d1df612ae0a771b has incorrect key</li></div><div><li>expected:&lt;1&gt; but was:&lt;2&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>java.lang.NullPointerException
</li></div><div><li>test timed out after 780 seconds</li></div><div><li>Appears to be stuck in thread RS-EventLoopGroup-5-4</li></div><div><li>test timed out after 780 seconds</li></div><div><li>Store file hdfs://localhost:35195/user/jenkins/test-data/bce21e20-4c16-40ec-947e-a374fd821555/data/default/testCFKeyRotation/62803a46d13883fc9e9aa71c047284d6/cf/d88933cb0e9d41c1b4240750ed9472b6 has incorrect key</li></div><div><li>Heartbeat messages are enabled, exceptions should NOT be thrown. Exception trace:org.apache.hadoop.hbase.client.RetriesExhaustedException: Failed after attempts=16, exceptions:
Sun Apr 15 07:28:44 UTC 2018, null, java.net.SocketTimeoutException: callTimeout=1000, callDuration=1036: Call to 51fcf4f08b1d/172.17.0.2:41424 failed on local exception: org.apache.hadoop.hbase.ipc.CallTimeoutException: Ca</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>java.lang.AssertionError
	at org.apache.hadoop.hbase.master.cleaner.TestLogsCleaner.testZooKeeperNormal(TestLogsCleaner.java:269)
</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>java.lang.AssertionError
	at org.apache.hadoop.hbase.rsgroup.TestEnableRSGroup.testEnableRSGroup(TestEnableRSGroup.java:80)
</li></div><div><li>test timed out after 780 seconds</li></div><div><li>Appears to be stuck in thread RS-EventLoopGroup-5-4</li></div><div><li>expected:&lt;4&gt; but was:&lt;5&gt;</li></div><div><li>Metrics Should be equal expected:&lt;1&gt; but was:&lt;2&gt;</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.hbase.replication.TestReplicationTrackerZKImpl.testGetListOfRegionServers</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.hbase.TestServerSideScanMetricsFromClientSide.testRowsSeenMetricWithAsync</div></li><li><div>org.apache.hadoop.hbase.TestServerSideScanMetricsFromClientSide.org.apache.hadoop.hbase.TestServerSideScanMetricsFromClientSide</div></li><li><div>org.apache.hadoop.hbase.TestServerSideScanMetricsFromClientSide.org.apache.hadoop.hbase.TestServerSideScanMetricsFromClientSide</div></li><li><div>org.apache.hadoop.hbase.master.cleaner.TestHFileLinkCleaner.org.apache.hadoop.hbase.master.cleaner.TestHFileLinkCleaner</div></li><li><div>org.apache.hadoop.hbase.regionserver.TestScannerHeartbeatMessages.testHeartbeatWithSparseRowFilter</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.hbase.master.cleaner.TestLogsCleaner.testZooKeeperNormal</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="hive"><div style="font-weight:bold;" class="panel-heading">HIVE<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Alisha/Pravin)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>6f9090c1de9bd6d2d98312a74b73802044cfa23e</div><div><b>Last Run: </b>13-04-2018 15:05 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th><th>PPC RHEL7.5</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 6912</div><div>Failed Count : 13</div><div>Skipped Count : 224</div></td><td><div>Total Count : 6912</div><div>Failed Count : 12</div><div>Skipped Count : 224</div></td><td><div>Total Count : 6912</div><div>Failed Count : 3</div><div>Skipped Count : 224</div></td><td><div>Total Count : 6912</div><div>Failed Count : 2</div><div>Skipped Count : 224</div></td><td><div>Total Count : 6905</div><div>Failed Count : 4</div><div>Skipped Count : 224</div></td></tr><tr><td>Result</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hive.hcatalog.mapreduce.TestHCatMultiOutputFormat.testOutputFormat</li></div><div><li>org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskCommunicator.testFinishableStateUpdateFailure</li></div><div><li>org.apache.hadoop.hive.ql.TestTxnCommandsForMmTable.testInsertOverwriteForPartitionedMmTable</li></div><div><li>org.apache.hadoop.hive.ql.TestTxnCommandsForOrcMmTable.testInsertOverwriteForPartitionedMmTable</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapRedPlan1</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapRedPlan2</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapRedPlan3</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapRedPlan4</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapRedPlan5</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapRedPlan6</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapPlan1</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapPlan2</li></div><div><li>org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.testStringLikeMultiByte</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hive.hcatalog.mapreduce.TestHCatMultiOutputFormat.testOutputFormat</li></div><div><li>org.apache.hadoop.hive.llap.daemon.impl.comparator.TestAMReporter.testMultipleAM</li></div><div><li>org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskCommunicator.testFinishableStateUpdateFailure</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapRedPlan1</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapRedPlan2</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapRedPlan3</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapRedPlan4</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapRedPlan5</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapRedPlan6</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapPlan1</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapPlan2</li></div><div><li>org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.testStringLikeMultiByte</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskCommunicator.testFinishableStateUpdateFailure</li></div><div><li>org.apache.hadoop.hive.ql.TestTxnCommandsForMmTable.testInsertOverwriteForPartitionedMmTable</li></div><div><li>org.apache.hadoop.hive.ql.TestTxnCommandsForOrcMmTable.testInsertOverwriteForPartitionedMmTable</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskCommunicator.testFinishableStateUpdateFailure</li></div><div><li>org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.testStringLikeMultiByte</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.hive.llap.daemon.impl.comparator.TestAMReporter.testMultipleAM</li></div><div><li>org.apache.hadoop.hive.ql.TestTxnCommandsForMmTable.testInsertOverwriteForPartitionedMmTable</li></div><div><li>org.apache.hadoop.hive.ql.TestTxnCommandsForOrcMmTable.testInsertOverwriteForPartitionedMmTable</li></div><div><li>org.apache.hadoop.hive.ql.exec.tez.TestDynamicPartitionPruner.testSingleSourceMultipleFiltersOrdering1</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hive.hcatalog.mapreduce.TestHCatMultiOutputFormat.testOutputFormat(TestHCatMultiOutputFormat.java:301)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeM</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos$SignableVertexSpec$Builder.setUser(LlapDaemonProtocolProtos.java:4899)
	at org.apache.hadoop.hive.llap.tez.Converters.constructSignableVertexSpec(Converters.java:135)
	at org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.constructSubmitWorkRequest(LlapTaskCommunicator.java:806)
	at org.apac</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.hive.ql.TestTxnCommandsForMmTable.testInsertOverwriteForPartitionedMmTable(TestTxnCommandsForMmTable.java:359)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.hive.ql.TestTxnCommandsForMmTable.testInsertOverwriteForPartitionedMmTable(TestTxnCommandsForMmTable.java:359)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;1&gt; but was:&lt;0&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hive.hcatalog.mapreduce.TestHCatMultiOutputFormat.testOutputFormat(TestHCatMultiOutputFormat.java:301)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeM</li></div><div><li>test timed out after 5000 milliseconds</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos$SignableVertexSpec$Builder.setUser(LlapDaemonProtocolProtos.java:4899)
	at org.apache.hadoop.hive.llap.tez.Converters.constructSignableVertexSpec(Converters.java:135)
	at org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.constructSubmitWorkRequest(LlapTaskCommunicator.java:806)
	at org.apac</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;1&gt; but was:&lt;0&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>java.lang.NullPointerException
	at org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos$SignableVertexSpec$Builder.setUser(LlapDaemonProtocolProtos.java:4899)
	at org.apache.hadoop.hive.llap.tez.Converters.constructSignableVertexSpec(Converters.java:135)
	at org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.constructSubmitWorkRequest(LlapTaskCommunicator.java:806)
	at org.apac</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.hive.ql.TestTxnCommandsForMmTable.testInsertOverwriteForPartitionedMmTable(TestTxnCommandsForMmTable.java:359)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.hive.ql.TestTxnCommandsForMmTable.testInsertOverwriteForPartitionedMmTable(TestTxnCommandsForMmTable.java:359)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>java.lang.NullPointerException
	at org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos$SignableVertexSpec$Builder.setUser(LlapDaemonProtocolProtos.java:4899)
	at org.apache.hadoop.hive.llap.tez.Converters.constructSignableVertexSpec(Converters.java:135)
	at org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.constructSubmitWorkRequest(LlapTaskCommunicator.java:806)
	at org.apac</li></div><div><li>expected:&lt;1&gt; but was:&lt;0&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>test timed out after 5000 milliseconds</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.hive.ql.TestTxnCommandsForMmTable.testInsertOverwriteForPartitionedMmTable(TestTxnCommandsForMmTable.java:359)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.hive.ql.TestTxnCommandsForMmTable.testInsertOverwriteForPartitionedMmTable(TestTxnCommandsForMmTable.java:359)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl</li></div><div><li>test timed out after 5000 milliseconds</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.hive.ql.TestTxnCommandsForMmTable.testInsertOverwriteForPartitionedMmTable</div></li><li><div>org.apache.hadoop.hive.ql.TestTxnCommandsForOrcMmTable.testInsertOverwriteForPartitionedMmTable</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.hive.llap.daemon.impl.comparator.TestAMReporter.testMultipleAM</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.hive.ql.TestTxnCommandsForMmTable.testInsertOverwriteForPartitionedMmTable</div></li><li><div>org.apache.hadoop.hive.ql.TestTxnCommandsForOrcMmTable.testInsertOverwriteForPartitionedMmTable</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.testStringLikeMultiByte</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="kafka"><div style="font-weight:bold;" class="panel-heading">KAFKA<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Sonia)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/trunk</div><div><b>Last Revision: </b>fb3a9485a8f830336e2cd07d959cf6aa0a5356a5</div><div><b>Last Run: </b>12-04-2018 19:00 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th><th>PPC RHEL7.5</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 8847</div><div>Failed Count : 0</div><div>Skipped Count : 9</div></td><td><div>Total Count : 8847</div><div>Failed Count : 0</div><div>Skipped Count : 9</div></td><td><div>Total Count : 8847</div><div>Failed Count : 0</div><div>Skipped Count : 9</div></td><td><div>Total Count : 8847</div><div>Failed Count : 2</div><div>Skipped Count : 9</div></td><td><div>Total Count : 8854</div><div>Failed Count : 1</div><div>Skipped Count : 5</div></td></tr><tr><td>Result</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>kafka.api.UserQuotaTest.testThrottledProducerConsumer</li></div><div><li>kafka.api.UserQuotaTest.testQuotaOverrideDelete</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>kafka.security.auth.SimpleAclAuthorizerTest.testHighConcurrencyModificationOfResourceAcls</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>java.lang.AssertionError: Should have been throttled</li></div><div><li>java.lang.AssertionError: Should have been throttled</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>java.lang.AssertionError: expected acls Set(User:36 has Allow permission for operations: Read from hosts: *, User:7 has Allow permission for operations: Read from hosts: *, User:21 has Allow permission for operations: Read from hosts: *, User:39 has Allow permission for operations: Read from hosts: *, User:43 has Allow permission for operations: Read from hosts: *, User:3 has Allow permission for </li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>kafka.api.UserQuotaTest.testThrottledProducerConsumer</div></li><li><div>kafka.api.UserQuotaTest.testQuotaOverrideDelete</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="knox"><div style="font-weight:bold;" class="panel-heading">KNOX<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Yussuf)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>689d9bfd69de4afb544f76a97967ce0f4726a0dd</div><div><b>Last Run: </b>10-04-2018 01:04 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th><th>PPC RHEL7.5</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 976</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 975</div><div>Failed Count : 1</div><div>Skipped Count : 2</div></td><td><div>Total Count : 976</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 976</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 977</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td></tr><tr><td>Result</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.knox.gateway.GatewayLdapDynamicGroupFuncTest.org.apache.knox.gateway.GatewayLdapDynamicGroupFuncTest</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>Timed out 10000 waiting for URL http://localhost:38920/gateway/testdg-cluster/test-service-path/test-service-resource</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.knox.gateway.GatewayLdapDynamicGroupFuncTest.org.apache.knox.gateway.GatewayLdapDynamicGroupFuncTest</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="metron"><div style="font-weight:bold;" class="panel-heading">METRON<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Pravin)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>1d3e7fcd7db64e7cb6d986802240fcbae174ae91</div><div><b>Last Run: </b>16-04-2018 01:39 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th><th>PPC RHEL7.5</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 1680</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1680</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1680</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1680</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1679</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td></tr><tr><td>Result</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="oozie"><div style="font-weight:bold;" class="panel-heading">OOZIE<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Alisha)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>d55245667b259d858d4dded849ded001896ba2af</div><div><b>Last Run: </b>14-04-2018 15:45 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th><th>PPC RHEL7.5</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 2112</div><div>Failed Count : 30</div><div>Skipped Count : 2</div></td><td><div>Total Count : 2112</div><div>Failed Count : 6</div><div>Skipped Count : 2</div></td><td><div>Total Count : 2112</div><div>Failed Count : 1</div><div>Skipped Count : 2</div></td><td><div>Total Count : 2112</div><div>Failed Count : 2</div><div>Skipped Count : 2</div></td><td><div>Total Count : 594</div><div>Failed Count : 16</div><div>Skipped Count : 0</div></td></tr><tr><td>Result</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"><div><li>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testCredentialsSkip</li></div><div><li>org.apache.oozie.jms.TestDefaultConnectionContext.testThreadLocalSession</li></div><div><li>org.apache.oozie.jms.TestHCatMessageHandler.testDropEventTypeMessage</li></div><div><li>org.apache.oozie.jms.TestHCatMessageHandler.testCacheUpdateByMessage</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testCoordinatorActionSelectors</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testCoordinatorActionSelectorsNegative</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectors</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorJobSuccessEvent</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobSuspendEvent</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectorsAnd</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorJobFailureEvent</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testConnectionDrop</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectorsNegative</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectorsOr</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobSuccessEvent</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorActionWaitingEvent</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobFailureEvent</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorActionStartEvent</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobStartedEvent</li></div><div><li>org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLADurationMetEvent</li></div><div><li>org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAStartMetEvent</li></div><div><li>org.apache.oozie.jms.TestJMSSLAEventListener.testSLAJobSelectorsNegative</li></div><div><li>org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAEndMetEvent</li></div><div><li>org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAStartMissEvent</li></div><div><li>org.apache.oozie.jms.TestJMSSLAEventListener.testSLAJobSelectors</li></div><div><li>org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAEndMissEvent</li></div><div><li>org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLADurationMissEvent</li></div><div><li>org.apache.oozie.service.TestJMSAccessorService.testConnectionRetry</li></div><div><li>org.apache.oozie.service.TestPartitionDependencyManagerEhcache.testPartitionDependency</li></div><div><li>org.apache.oozie.tools.TestOozieDBCLI.testOozieDBCLI</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testCredentialsSkip</li></div><div><li>org.apache.oozie.command.coord.TestCoordActionInputCheckXCommand.testNone</li></div><div><li>org.apache.oozie.command.coord.TestCoordActionInputCheckXCommandNonUTC.testCoordActionInputCheckXCommandUniqueness</li></div><div><li>org.apache.oozie.command.wf.TestWorkflowActionRetryInfoXCommand.testRetryConsoleUrlForked</li></div><div><li>org.apache.oozie.action.hadoop.TestHiveActionExecutor.testHiveAction</li></div><div><li>org.apache.oozie.tools.TestOozieDBCLI.testOozieDBCLI</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.oozie.tools.TestOozieDBCLI.testOozieDBCLI</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.oozie.command.coord.TestCoordActionsKillXCommand.testActionKillCommandDate</li></div><div><li>org.apache.oozie.tools.TestOozieDBCLI.testOozieDBCLI</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.oozie.test.TestWorkflow.testParallelFsAndShellWorkflowCompletesSuccessfully</li></div><div><li>org.apache.oozie.action.hadoop.TestHiveActionExecutor.testHiveAction</li></div><div><li>org.apache.oozie.action.hadoop.TestPigActionExecutor.testPig</li></div><div><li>org.apache.oozie.action.hadoop.TestPigActionExecutor.testExecutionStatsWithMaxStatsSizeLimit</li></div><div><li>org.apache.oozie.action.hadoop.TestMapReduceActionExecutor.testMapReduceWithUberJarEnabled</li></div><div><li>org.apache.oozie.action.hadoop.TestMapReduceActionExecutor.testSetMapredJobName</li></div><div><li>org.apache.oozie.action.hadoop.TestMapReduceActionExecutor.testEndWithoutConfiguration</li></div><div><li>org.apache.oozie.action.hadoop.TestMapReduceActionExecutor.testMapReduceWithConfigClass</li></div><div><li>org.apache.oozie.action.hadoop.TestMapReduceActionExecutor.testMapReduceWithCredentials</li></div><div><li>org.apache.oozie.action.hadoop.TestMapReduceActionExecutor.testJobNameSetForMapReduceChild</li></div><div><li>org.apache.oozie.action.hadoop.TestMapReduceActionExecutor.testStreamingConfOverride</li></div><div><li>org.apache.oozie.action.hadoop.TestMapReduceActionExecutor.testStreaming</li></div><div><li>org.apache.oozie.action.hadoop.TestMapReduceActionExecutor.testSetExecutionStats_when_user_has_specified_stats_write_TRUE</li></div><div><li>org.apache.oozie.action.hadoop.TestMapReduceActionExecutor.testSetExecutionStats_when_user_has_specified_stats_write_FALSE</li></div><div><li>org.apache.oozie.action.hadoop.TestMapReduceActionExecutor.testMapReduce</li></div><div><li>org.apache.oozie.tools.TestOozieDBCLI.testOozieDBCLI</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"><div><li>JA020: Could not load credentials of type [abc] with name [abcname]]; perhaps it was not defined in oozie-site.xml?</li></div><div><li>java.lang.NullPointerException
	at org.apache.oozie.jms.TestDefaultConnectionContext.testThreadLocalSession(TestDefaultConnectionContext.java:74)
</li></div><div><li>Could not create Transport. Reason: javax.management.InstanceAlreadyExistsException: org.apache.activemq:type=Broker,brokerName=localhost</li></div><div><li>Could not create Transport. Reason: javax.management.InstanceAlreadyExistsException: org.apache.activemq:type=Broker,brokerName=localhost</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testCoordinatorActionSelectors(TestJMSJobEventListener.java:544)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testCoordinatorActionSelectorsNegative(TestJMSJobEventListener.java:568)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectors(TestJMSJobEventListener.java:239)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorJobSuccessEvent(TestJMSJobEventListener.java:477)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobSuspendEvent(TestJMSJobEventListener.java:214)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectorsAnd(TestJMSJobEventListener.java:316)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorJobFailureEvent(TestJMSJobEventListener.java:517)
</li></div><div><li>org.apache.activemq:type=Broker,brokerName=localhost</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectorsNegative(TestJMSJobEventListener.java:262)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectorsOr(TestJMSJobEventListener.java:289)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobSuccessEvent(TestJMSJobEventListener.java:143)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorActionWaitingEvent(TestJMSJobEventListener.java:403)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobFailureEvent(TestJMSJobEventListener.java:180)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorActionStartEvent(TestJMSJobEventListener.java:439)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobStartedEvent(TestJMSJobEventListener.java:108)
</li></div><div><li>java.lang.NullPointerException
	at org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLADurationMetEvent(TestJMSSLAEventListener.java:382)
</li></div><div><li>java.lang.NullPointerException
	at org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAStartMetEvent(TestJMSSLAEventListener.java:292)
</li></div><div><li>java.lang.NullPointerException
	at org.apache.oozie.jms.TestJMSSLAEventListener.testSLAJobSelectorsNegative(TestJMSSLAEventListener.java:261)
</li></div><div><li>java.lang.NullPointerException
	at org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAEndMetEvent(TestJMSSLAEventListener.java:332)
</li></div><div><li>java.lang.NullPointerException
	at org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAStartMissEvent(TestJMSSLAEventListener.java:103)
</li></div><div><li>java.lang.NullPointerException
	at org.apache.oozie.jms.TestJMSSLAEventListener.testSLAJobSelectors(TestJMSSLAEventListener.java:231)
</li></div><div><li>java.lang.NullPointerException
	at org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAEndMissEvent(TestJMSSLAEventListener.java:143)
</li></div><div><li>java.lang.NullPointerException
	at org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLADurationMissEvent(TestJMSSLAEventListener.java:191)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.service.TestJMSAccessorService.testConnectionRetry(TestJMSAccessorService.java:183)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.service.TestPartitionDependencyManagerEhcache.testPartitionDependency(TestPartitionDependencyManagerEhcache.java:47)
</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>JA020: Could not load credentials of type [abc] with name [abcname]]; perhaps it was not defined in oozie-site.xml?</li></div><div><li>expected:&lt;SKIPPED&gt; but was:&lt;WAITING&gt;</li></div><div><li>junit.framework.AssertionFailedError
</li></div><div><li>java.lang.NullPointerException
	at org.apache.oozie.command.wf.TestWorkflowActionRetryInfoXCommand.validateRetryConsoleUrl(TestWorkflowActionRetryInfoXCommand.java:190)
	at org.apache.oozie.command.wf.TestWorkflowActionRetryInfoXCommand.testRetryConsoleUrlForked(TestWorkflowActionRetryInfoXCommand.java:125)
</li></div><div><li>YARN App state for app application_1523763601356_0005 expected:&lt;FINISHED&gt; but was:&lt;RUNNING&gt;</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;KILLED&gt; but was:&lt;RUNNING&gt;</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;SUCCEEDED&gt; but was:&lt;RUNNING&gt;</li></div><div><li>YARN App state for app application_1523643006965_0002 expected:&lt;FINISHED&gt; but was:&lt;RUNNING&gt;</li></div><div><li>YARN App state for app application_1523641928323_0001 expected:&lt;FINISHED&gt; but was:&lt;RUNNING&gt;</li></div><div><li>YARN App state for app application_1523641928323_0003 expected:&lt;FINISHED&gt; but was:&lt;RUNNING&gt;</li></div><div><li>YARN App state for app application_1523640469575_0003 expected:&lt;FINISHED&gt; but was:&lt;RUNNING&gt;</li></div><div><li>YARN App state for app application_1523640469575_0004 expected:&lt;FINISHED&gt; but was:&lt;ACCEPTED&gt;</li></div><div><li>YARN App state for app application_1523640469575_0007 expected:&lt;FINISHED&gt; but was:&lt;RUNNING&gt;</li></div><div><li>  JA006: Call From pts00449-vm36/10.88.67.159 to pts00449-vm36:0 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>  JA006: Call From pts00449-vm36/10.88.67.159 to pts00449-vm36:0 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>  JA006: Call From pts00449-vm36/10.88.67.159 to pts00449-vm36:0 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>  JA006: Call From pts00449-vm36/10.88.67.159 to pts00449-vm36:0 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>  JA006: Call From pts00449-vm36/10.88.67.159 to pts00449-vm36:0 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>  JA006: Call From pts00449-vm36/10.88.67.159 to pts00449-vm36:0 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>  JA006: Call From pts00449-vm36/10.88.67.159 to pts00449-vm36:0 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>  JA006: Call From pts00449-vm36/10.88.67.159 to pts00449-vm36:0 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.oozie.jms.TestDefaultConnectionContext.testThreadLocalSession</div></li><li><div>org.apache.oozie.jms.TestHCatMessageHandler.testDropEventTypeMessage</div></li><li><div>org.apache.oozie.jms.TestHCatMessageHandler.testCacheUpdateByMessage</div></li><li><div>org.apache.oozie.jms.TestJMSJobEventListener.testCoordinatorActionSelectors</div></li><li><div>org.apache.oozie.jms.TestJMSJobEventListener.testCoordinatorActionSelectorsNegative</div></li><li><div>org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectors</div></li><li><div>org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorJobSuccessEvent</div></li><li><div>org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobSuspendEvent</div></li><li><div>org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectorsAnd</div></li><li><div>org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorJobFailureEvent</div></li><li><div>org.apache.oozie.jms.TestJMSJobEventListener.testConnectionDrop</div></li><li><div>org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectorsNegative</div></li><li><div>org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectorsOr</div></li><li><div>org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobSuccessEvent</div></li><li><div>org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorActionWaitingEvent</div></li><li><div>org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobFailureEvent</div></li><li><div>org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorActionStartEvent</div></li><li><div>org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobStartedEvent</div></li><li><div>org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLADurationMetEvent</div></li><li><div>org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAStartMetEvent</div></li><li><div>org.apache.oozie.jms.TestJMSSLAEventListener.testSLAJobSelectorsNegative</div></li><li><div>org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAEndMetEvent</div></li><li><div>org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAStartMissEvent</div></li><li><div>org.apache.oozie.jms.TestJMSSLAEventListener.testSLAJobSelectors</div></li><li><div>org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAEndMissEvent</div></li><li><div>org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLADurationMissEvent</div></li><li><div>org.apache.oozie.service.TestJMSAccessorService.testConnectionRetry</div></li><li><div>org.apache.oozie.service.TestPartitionDependencyManagerEhcache.testPartitionDependency</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.oozie.command.coord.TestCoordActionInputCheckXCommand.testNone</div></li><li><div>org.apache.oozie.command.coord.TestCoordActionInputCheckXCommandNonUTC.testCoordActionInputCheckXCommandUniqueness</div></li><li><div>org.apache.oozie.command.wf.TestWorkflowActionRetryInfoXCommand.testRetryConsoleUrlForked</div></li><li><div>org.apache.oozie.action.hadoop.TestHiveActionExecutor.testHiveAction</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.oozie.command.coord.TestCoordActionsKillXCommand.testActionKillCommandDate</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="phoenix"><div style="font-weight:bold;" class="panel-heading">PHOENIX<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Valencia)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>ca58fae2037b0da9331aa97d9836ca195f289bb1</div><div><b>Last Run: </b>12-04-2018 23:45 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th><th>PPC RHEL7.5</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 1691</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 1691</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 1691</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 1691</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 1691</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td></tr><tr><td>Result</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="pig"><div style="font-weight:bold;" class="panel-heading">PIG<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Yussuf)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/trunk</div><div><b>Last Revision: </b>963fa588a1fcf2c5e040ab6b7e59175bdaee8a0a</div><div><b>Last Run: </b>12-04-2018 02:58 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th><th>PPC RHEL7.5</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 895</div><div>Failed Count : 20</div><div>Skipped Count : 0</div></td><td><div>Total Count : 895</div><div>Failed Count : 20</div><div>Skipped Count : 0</div></td><td><div>Total Count : 895</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 895</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 895</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td></tr><tr><td>Result</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"><div><li>org.apache.pig.test.TestBuiltin.testRANDOMWithJob</li></div><div><li>org.apache.pig.test.TestBuiltin.testSFPig</li></div><div><li>org.apache.pig.test.TestBuiltin.testUniqueID</li></div><div><li>org.apache.pig.test.TestLoad.testCommaSeparatedString3</li></div><div><li>org.apache.pig.test.TestLoad.testCommaSeparatedString5</li></div><div><li>org.apache.pig.test.TestLoad.testLoadRemoteAbsScheme</li></div><div><li>org.apache.pig.test.TestLocalRearrange.testMultiQueryJiraPig1194</li></div><div><li>org.apache.pig.test.TestPigServer.testRegisterRemoteScript</li></div><div><li>org.apache.pig.test.TestSchema.testEnabledDisambiguationPassesForDupeAliases</li></div><div><li>org.apache.pig.test.TestSchema.testSchemaSerialization</li></div><div><li>org.apache.pig.test.TestSchema.testDisabledDisambiguationContainsNoColonsForNestedSchema</li></div><div><li>org.apache.pig.test.TestSchema.testDisabledDisambiguationContainsNoColons</li></div><div><li>org.apache.pig.test.TestStore.testStore</li></div><div><li>org.apache.pig.test.TestStore.testStoreComplexDataWithNull</li></div><div><li>org.apache.pig.test.TestStore.testBinStorageGetSchema</li></div><div><li>org.apache.pig.test.TestStore.testStoreComplexData</li></div><div><li>org.apache.pig.test.TestStore.testSetStoreSchema</li></div><div><li>org.apache.pig.test.TestStore.testSuccessFileCreation1</li></div><div><li>org.apache.pig.test.TestStore.testCleanupOnFailureMultiStore</li></div><div><li>org.apache.pig.test.TestStore.testEmptyPartFileCreation</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.pig.test.TestBuiltin.testRANDOMWithJob</li></div><div><li>org.apache.pig.test.TestBuiltin.testSFPig</li></div><div><li>org.apache.pig.test.TestBuiltin.testUniqueID</li></div><div><li>org.apache.pig.test.TestLoad.testCommaSeparatedString3</li></div><div><li>org.apache.pig.test.TestLoad.testCommaSeparatedString5</li></div><div><li>org.apache.pig.test.TestLoad.testLoadRemoteAbsScheme</li></div><div><li>org.apache.pig.test.TestLocalRearrange.testMultiQueryJiraPig1194</li></div><div><li>org.apache.pig.test.TestPigServer.testRegisterRemoteScript</li></div><div><li>org.apache.pig.test.TestSchema.testEnabledDisambiguationPassesForDupeAliases</li></div><div><li>org.apache.pig.test.TestSchema.testSchemaSerialization</li></div><div><li>org.apache.pig.test.TestSchema.testDisabledDisambiguationContainsNoColonsForNestedSchema</li></div><div><li>org.apache.pig.test.TestSchema.testDisabledDisambiguationContainsNoColons</li></div><div><li>org.apache.pig.test.TestStore.testStore</li></div><div><li>org.apache.pig.test.TestStore.testStoreComplexDataWithNull</li></div><div><li>org.apache.pig.test.TestStore.testBinStorageGetSchema</li></div><div><li>org.apache.pig.test.TestStore.testStoreComplexData</li></div><div><li>org.apache.pig.test.TestStore.testSetStoreSchema</li></div><div><li>org.apache.pig.test.TestStore.testSuccessFileCreation1</li></div><div><li>org.apache.pig.test.TestStore.testCleanupOnFailureMultiStore</li></div><div><li>org.apache.pig.test.TestStore.testEmptyPartFileCreation</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"><div><li>Unable to open iterator for alias B</li></div><div><li>Input path does not exist: hdfs://localhost:36624/user/jenkins/testSFPig-output.txt</li></div><div><li>Unable to open iterator for alias B</li></div><div><li>Unable to open iterator for alias a</li></div><div><li>Unable to open iterator for alias a</li></div><div><li>Unable to open iterator for alias a</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.pig.test.TestLocalRearrange.testMultiQueryJiraPig1194(TestLocalRearrange.java:231)
</li></div><div><li>Unable to open iterator for alias b</li></div><div><li>Unable to open iterator for alias C</li></div><div><li>Unable to open iterator for alias c</li></div><div><li>Unable to open iterator for alias F</li></div><div><li>Unable to open iterator for alias E</li></div><div><li>File /tmp/TestStore/TestStore-output-440038552292681492.txt does not exist.</li></div><div><li>File /tmp/TestStore/TestStore-output--2875575765890540265.txt does not exist.</li></div><div><li>Checking binstorage getSchema output</li></div><div><li>File /tmp/TestStore/TestStore-output-7956101129445419113.txt does not exist.</li></div><div><li>Checking if file /tmp/TestStore/_commitJob_called does  exists in MAPREDUCE mode expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>Checking if _SUCCESS file exists in MAPREDUCE mode expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>Checking if file /tmp/TestStore/_setupTask_called1 does  exists in MAPREDUCE mode expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>File /tmp/TestStore/TestStore-output--6307512084133103158.txt_1 does not exist.</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Unable to open iterator for alias B</li></div><div><li>Input path does not exist: hdfs://localhost:37033/user/jenkins/testSFPig-output.txt</li></div><div><li>Unable to open iterator for alias B</li></div><div><li>Unable to open iterator for alias a</li></div><div><li>Unable to open iterator for alias a</li></div><div><li>Unable to open iterator for alias a</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.pig.test.TestLocalRearrange.testMultiQueryJiraPig1194(TestLocalRearrange.java:231)
</li></div><div><li>Unable to open iterator for alias b</li></div><div><li>Unable to open iterator for alias C</li></div><div><li>Unable to open iterator for alias c</li></div><div><li>Unable to open iterator for alias F</li></div><div><li>Unable to open iterator for alias E</li></div><div><li>File /tmp/TestStore/TestStore-output-7377931323736307920.txt does not exist.</li></div><div><li>File /tmp/TestStore/TestStore-output-8457111807823108611.txt does not exist.</li></div><div><li>Checking binstorage getSchema output</li></div><div><li>File /tmp/TestStore/TestStore-output--5719343724464918895.txt does not exist.</li></div><div><li>Checking if file /tmp/TestStore/_commitJob_called does  exists in MAPREDUCE mode expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>Checking if _SUCCESS file exists in MAPREDUCE mode expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>Checking if file /tmp/TestStore/_setupTask_called1 does  exists in MAPREDUCE mode expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>File /tmp/TestStore/TestStore-output--1116529779161770150.txt_1 does not exist.</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="ranger"><div style="font-weight:bold;" class="panel-heading">RANGER<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Sneha)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>343668b42afe7265c08064c7fb0bf40f7184ea1e</div><div><b>Last Run: </b>12-04-2018 01:58 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th><th>PPC RHEL7.5</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 1077</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1077</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1077</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1077</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1077</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td></tr><tr><td>Result</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="slider"><div style="font-weight:bold;" class="panel-heading">SLIDER<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Yussuf)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/develop</div><div><b>Last Revision: </b>4032999f35db4877b6b8ffc5e97a59837e22365b</div><div><b>Last Run: </b>12-04-2018 20:54 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th><th>PPC RHEL7.5</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 607</div><div>Failed Count : 24</div><div>Skipped Count : 13</div></td><td><div>Total Count : 607</div><div>Failed Count : 24</div><div>Skipped Count : 13</div></td><td><div>Total Count : 607</div><div>Failed Count : 0</div><div>Skipped Count : 13</div></td><td><div>Total Count : 607</div><div>Failed Count : 0</div><div>Skipped Count : 13</div></td><td><div>Total Count : 607</div><div>Failed Count : 0</div><div>Skipped Count : 13</div></td></tr><tr><td>Result</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"><div><li>org.apache.slider.agent.actions.TestActionExists.testExistsLiveCluster</li></div><div><li>org.apache.slider.agent.actions.TestActionList.testActionListSuite</li></div><div><li>org.apache.slider.agent.actions.TestActionStatus.testSuite</li></div><div><li>org.apache.slider.agent.freezethaw.TestFreezeCommands.testFreezeCommands</li></div><div><li>org.apache.slider.agent.freezethaw.TestFreezeThawFlexStandaloneAM.testFreezeThawFlexStandaloneAM</li></div><div><li>org.apache.slider.agent.rest.TestStandaloneREST.testStandaloneREST</li></div><div><li>org.apache.slider.agent.standalone.TestBuildStandaloneAM.testBuildCluster</li></div><div><li>org.apache.slider.agent.standalone.TestBuildStandaloneAM.testUpdateCluster</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneAMDestroy.testStandaloneAMDestroy</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneAMKill.testKillStandaloneAM</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneAMRestart.testStandaloneAMRestartWithDefaultRetryWindow</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneAMRestart.testStandaloneAMRestart</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneAMRestart.testStandaloneAMRestartWithRetryWindow</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneAgentAM.testStandaloneAgentAM</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneYarnRegistryAM.testStandaloneYarnRegistryAM</li></div><div><li>org.apache.slider.client.TestDiagnostics.testContainerDiagsNoAppContainer</li></div><div><li>org.apache.slider.client.TestDiagnostics.testContainerDiagsWithAppPackage</li></div><div><li>org.apache.slider.client.TestUpgradeCommandOptions.testAll</li></div><div><li>org.apache.slider.providers.agent.TestAddonPackage.testEchoApplicationAddPackage</li></div><div><li>org.apache.slider.providers.agent.TestAgentAAEcho.testAgentEcho</li></div><div><li>org.apache.slider.providers.agent.TestAgentAMManagementWS.testAgentAMManagementWS</li></div><div><li>org.apache.slider.providers.agent.TestAgentEcho.testAgentEcho</li></div><div><li>org.apache.slider.server.appmaster.TestDelayInContainerLaunch.testDelayInContainerLaunch</li></div><div><li>org.apache.slider.server.appmaster.web.rest.publisher.TestPublisherRestResources.testRestURIs</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.slider.agent.actions.TestActionExists.testExistsLiveCluster</li></div><div><li>org.apache.slider.agent.actions.TestActionList.testActionListSuite</li></div><div><li>org.apache.slider.agent.actions.TestActionStatus.testSuite</li></div><div><li>org.apache.slider.agent.freezethaw.TestFreezeCommands.testFreezeCommands</li></div><div><li>org.apache.slider.agent.freezethaw.TestFreezeThawFlexStandaloneAM.testFreezeThawFlexStandaloneAM</li></div><div><li>org.apache.slider.agent.rest.TestStandaloneREST.testStandaloneREST</li></div><div><li>org.apache.slider.agent.standalone.TestBuildStandaloneAM.testBuildCluster</li></div><div><li>org.apache.slider.agent.standalone.TestBuildStandaloneAM.testUpdateCluster</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneAMDestroy.testStandaloneAMDestroy</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneAMKill.testKillStandaloneAM</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneAMRestart.testStandaloneAMRestartWithDefaultRetryWindow</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneAMRestart.testStandaloneAMRestart</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneAMRestart.testStandaloneAMRestartWithRetryWindow</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneAgentAM.testStandaloneAgentAM</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneYarnRegistryAM.testStandaloneYarnRegistryAM</li></div><div><li>org.apache.slider.client.TestDiagnostics.testContainerDiagsNoAppContainer</li></div><div><li>org.apache.slider.client.TestDiagnostics.testContainerDiagsWithAppPackage</li></div><div><li>org.apache.slider.client.TestUpgradeCommandOptions.testAll</li></div><div><li>org.apache.slider.providers.agent.TestAddonPackage.testEchoApplicationAddPackage</li></div><div><li>org.apache.slider.providers.agent.TestAgentAAEcho.testAgentEcho</li></div><div><li>org.apache.slider.providers.agent.TestAgentAMManagementWS.testAgentAMManagementWS</li></div><div><li>org.apache.slider.providers.agent.TestAgentEcho.testAgentEcho</li></div><div><li>org.apache.slider.server.appmaster.TestDelayInContainerLaunch.testDelayInContainerLaunch</li></div><div><li>org.apache.slider.server.appmaster.web.rest.publisher.TestPublisherRestResources.testRestURIs</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"><div><li>Launch failed with exit code -1</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Application not running: application_1523567490954_0001 state=FAILED </li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Failed on local exception: java.io.FileNotFoundException: http://fed3c6299332:38650/cluster/app/application_1523567043884_0001; Host Details : local host is: "localhost"; destination host is: "http://fed3c6299332:38650/cluster/app/application_1523567043884_0001":38650; </li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>assert report.yarnApplicationState == YarnApplicationState.RUNNING
       |      |                                            |
       |      FAILED                                       RUNNING
       applicationId { id: 1 cluster_timestamp: 1523567435770 } user: "jenkins" queue: "default" name: "testkillstandaloneam" host: "N/A" rpc_port: -1 yarn_application_state: FAILED trackingUrl: "http://fe</li></div><div><li>Cluster teststandaloneamrestartwithdefaultretrywindow not live after 30000 ms</li></div><div><li>Cluster teststandaloneamrestart not live after 30000 ms</li></div><div><li>Cluster teststandaloneamrestartwithretrywindow not live after 30000 ms</li></div><div><li>assert uri.port in 60000..60010
       |   |            |
       |   39179        [60000, 60001, 60002, 60003, 60004, 60005, 60006, 60007, 60008, 60009, 60010]
       http://fed3c6299332:39179/cluster/app/application_1523567292978_0001</li></div><div><li>Application not running: application_1523567316241_0001 state=FAILED </li></div><div><li>assert 0 == status
         |  |
         |  -1
         false</li></div><div><li>Launch failed with exit code -1</li></div><div><li>Upgrade command should have failed</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Launch failed with exit code -1</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Application not running: application_1523567784813_0001 state=FAILED </li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Failed on local exception: java.io.FileNotFoundException: http://e0433f1ba9e6:39835/cluster/app/application_1523567810564_0001; Host Details : local host is: "localhost"; destination host is: "http://e0433f1ba9e6:39835/cluster/app/application_1523567810564_0001":39835; </li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>assert report.yarnApplicationState == YarnApplicationState.RUNNING
       |      |                                            |
       |      FAILED                                       RUNNING
       applicationId { id: 1 cluster_timestamp: 1523567422016 } user: "jenkins" queue: "default" name: "testkillstandaloneam" host: "N/A" rpc_port: -1 yarn_application_state: FAILED trackingUrl: "http://e0</li></div><div><li>Cluster teststandaloneamrestartwithdefaultretrywindow not live after 30000 ms</li></div><div><li>Cluster teststandaloneamrestart not live after 30000 ms</li></div><div><li>Cluster teststandaloneamrestartwithretrywindow not live after 30000 ms</li></div><div><li>assert uri.port in 60000..60010
       |   |            |
       |   39519        [60000, 60001, 60002, 60003, 60004, 60005, 60006, 60007, 60008, 60009, 60010]
       http://e0433f1ba9e6:39519/cluster/app/application_1523567396609_0001</li></div><div><li>Application not running: application_1523567474579_0001 state=FAILED </li></div><div><li>assert 0 == status
         |  |
         |  -1
         false</li></div><div><li>Launch failed with exit code -1</li></div><div><li>Upgrade command should have failed</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="spark"><div style="font-weight:bold;" class="panel-heading">SPARK<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Parita)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>73f28530d6f6dd8aba758ea818c456cf911a5f41</div><div><b>Last Run: </b>14-04-2018 15:26 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th><th>PPC RHEL7.5</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 13950</div><div>Failed Count : 2</div><div>Skipped Count : 668</div></td><td><div>Total Count : 15631</div><div>Failed Count : 1</div><div>Skipped Count : 675</div></td><td><div>Total Count : 13950</div><div>Failed Count : 3</div><div>Skipped Count : 668</div></td><td><div>Total Count : 15630</div><div>Failed Count : 1</div><div>Skipped Count : 675</div></td><td><div>Total Count : 9547</div><div>Failed Count : 404</div><div>Skipped Count : 60</div></td></tr><tr><td>Result</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"><div><li>org.apache.spark.sql.streaming.continuous.ContinuousStressSuite.only one epoch</li></div><div><li>org.apache.spark.sql.hive.HiveSparkSubmitSuite.SPARK-8020: set sql conf in spark conf</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.spark.sql.hive.HiveSparkSubmitSuite.SPARK-8020: set sql conf in spark conf</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.spark.deploy.yarn.YarnShuffleAuthSuite.external shuffle service</li></div><div><li>org.apache.spark.sql.streaming.continuous.ContinuousStressSuite.only one epoch</li></div><div><li>org.apache.spark.sql.hive.HiveSparkSubmitSuite.SPARK-8020: set sql conf in spark conf</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.spark.sql.hive.HiveSparkSubmitSuite.SPARK-8020: set sql conf in spark conf</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.spark.shuffle.sort.UnsafeShuffleWriterSuite.mergeSpillsWithFileStreamAndSnappy</li></div><div><li>org.apache.spark.shuffle.sort.UnsafeShuffleWriterSuite.mergeSpillsWithTransferToAndSnappy</li></div><div><li>org.apache.spark.sql.kafka010.KafkaContinuousSourceSuite.subscribing topic by pattern from latest offsets (failOnDataLoss: false)</li></div><div><li>org.apache.spark.mllib.fpm.JavaFPGrowthSuite.runFPGrowthSaveLoad</li></div><div><li>org.apache.spark.mllib.fpm.JavaPrefixSpanSuite.runPrefixSpanSaveLoad</li></div><div><li>org.apache.spark.sql.CachedTableSuite.SPARK-19765: UNCACHE TABLE should un-cache all cached plans that refer to this table</li></div><div><li>org.apache.spark.sql.CachedTableSuite.refreshByPath should refresh all cached plans with the specified path</li></div><div><li>org.apache.spark.sql.DataFrameJoinSuite.broadcast join hint using broadcast function</li></div><div><li>org.apache.spark.sql.DataFrameSuite.inputFiles</li></div><div><li>org.apache.spark.sql.DataFrameSuite.SPARK-6941: Better error message for inserting into RDD-based Table</li></div><div><li>org.apache.spark.sql.DataFrameSuite.SPARK-11301: fix case sensitivity for filter on partitioned columns</li></div><div><li>org.apache.spark.sql.DataFrameSuite.fix case sensitivity of partition by</li></div><div><li>org.apache.spark.sql.DatasetSuite.SPARK-22472: add null check for top-level primitive values</li></div><div><li>org.apache.spark.sql.FileBasedDataSourceSuite.SPARK-23072 Write and read back unicode column names - parquet</li></div><div><li>org.apache.spark.sql.FileBasedDataSourceSuite.SPARK-22146 read files containing special characters using parquet</li></div><div><li>org.apache.spark.sql.FileBasedDataSourceSuite.Enabling/disabling ignoreMissingFiles using parquet</li></div><div><li>org.apache.spark.sql.MathFunctionsSuite.round/bround with table columns</li></div><div><li>org.apache.spark.sql.MetadataCacheSuite.SPARK-16336 Suggest doing table refresh when encountering FileNotFoundException</li></div><div><li>org.apache.spark.sql.MetadataCacheSuite.SPARK-16337 temporary view refresh</li></div><div><li>org.apache.spark.sql.MetadataCacheSuite.case sensitivity support in temporary view refresh</li></div><div><li>org.apache.spark.sql.SQLQuerySuite.specifying database name for a temporary view is not allowed</li></div><div><li>org.apache.spark.sql.SQLQuerySuite.SPARK-15327: fail to compile generated code with complex data structure</li></div><div><li>org.apache.spark.sql.SQLQuerySuite.data source table created in InMemoryCatalog should be able to read/write</li></div><div><li>org.apache.spark.sql.SQLQuerySuite.SPARK-15752 optimize metadata only query for datasource table</li></div><div><li>org.apache.spark.sql.SQLQuerySuite.SPARK-16975: Column-partition path starting '_' should be handled correctly</li></div><div><li>org.apache.spark.sql.SQLQuerySuite.SPARK-16674: field names containing dots for both fields and partitioned fields</li></div><div><li>org.apache.spark.sql.SQLQuerySuite.SPARK-18053: ARRAY equality is broken</li></div><div><li>org.apache.spark.sql.SQLQuerySuite.SPARK-19157: should be able to change spark.sql.runSQLOnFiles at runtime</li></div><div><li>org.apache.spark.sql.SQLQuerySuite.should be able to resolve a persistent view</li></div><div><li>org.apache.spark.sql.SQLQuerySuite.SPARK-19059: read file based table whose name starts with underscore</li></div><div><li>org.apache.spark.sql.SQLQuerySuite.SPARK-23079: constraints should be inferred correctly with aliases</li></div><div><li>org.apache.spark.sql.SQLQuerySuite.SPARK-22356: overlapped columns between data and partition schema in data source tables</li></div><div><li>org.apache.spark.sql.SQLQueryTestSuite.columnresolution-negative.sql</li></div><div><li>org.apache.spark.sql.SQLQueryTestSuite.columnresolution.sql</li></div><div><li>org.apache.spark.sql.SQLQueryTestSuite.describe-part-after-analyze.sql</li></div><div><li>org.apache.spark.sql.SQLQueryTestSuite.describe-table-column.sql</li></div><div><li>org.apache.spark.sql.SQLQueryTestSuite.describe.sql</li></div><div><li>org.apache.spark.sql.SQLQueryTestSuite.null-handling.sql</li></div><div><li>org.apache.spark.sql.SQLQueryTestSuite.order-by-nulls-ordering.sql</li></div><div><li>org.apache.spark.sql.SQLQueryTestSuite.tablesample-negative.sql</li></div><div><li>org.apache.spark.sql.SQLQueryTestSuite.typeCoercion/native/decimalArithmeticOperations.sql</li></div><div><li>org.apache.spark.sql.StatisticsCollectionSuite.SPARK-18856: non-empty partitioned table should not report zero size</li></div><div><li>org.apache.spark.sql.StatisticsCollectionSuite.analyzing views is not supported</li></div><div><li>org.apache.spark.sql.StatisticsCollectionSuite.analyze column command - unsupported types and invalid columns</li></div><div><li>org.apache.spark.sql.StatisticsCollectionSuite.test table-level statistics for data source table</li></div><div><li>org.apache.spark.sql.StatisticsCollectionSuite.analyze column command - result verification</li></div><div><li>org.apache.spark.sql.StatisticsCollectionSuite.column stats collection for null columns</li></div><div><li>org.apache.spark.sql.StatisticsCollectionSuite.change stats after truncate command</li></div><div><li>org.apache.spark.sql.StatisticsCollectionSuite.change stats after set location command</li></div><div><li>org.apache.spark.sql.StatisticsCollectionSuite.change stats after insert command for datasource table</li></div><div><li>org.apache.spark.sql.StatisticsCollectionSuite.invalidation of tableRelationCache after inserts</li></div><div><li>org.apache.spark.sql.StatisticsCollectionSuite.invalidation of tableRelationCache after table truncation</li></div><div><li>org.apache.spark.sql.StatisticsCollectionSuite.invalidation of tableRelationCache after alter table add partition</li></div><div><li>org.apache.spark.sql.SubquerySuite.SPARK-21835: Join in correlated subquery should be duplicateResolved: case 1</li></div><div><li>org.apache.spark.sql.SubquerySuite.SPARK-21835: Join in correlated subquery should be duplicateResolved: case 2</li></div><div><li>org.apache.spark.sql.UDFSuite.SPARK-8005 input_file_name</li></div><div><li>org.apache.spark.sql.UserDefinedTypeSuite.Standard mode - UDTs with Parquet</li></div><div><li>org.apache.spark.sql.UserDefinedTypeSuite.Legacy mode - UDTs with Parquet</li></div><div><li>org.apache.spark.sql.UserDefinedTypeSuite.Standard mode - Repartition UDTs with Parquet</li></div><div><li>org.apache.spark.sql.UserDefinedTypeSuite.Legacy mode - Repartition UDTs with Parquet</li></div><div><li>org.apache.spark.sql.execution.DataSourceScanExecRedactionSuite.treeString is redacted</li></div><div><li>org.apache.spark.sql.execution.DataSourceScanExecRedactionSuite.explain is redacted using SQLConf</li></div><div><li>org.apache.spark.sql.execution.GlobalTempViewSuite.CREATE GLOBAL TEMP VIEW USING</li></div><div><li>org.apache.spark.sql.execution.SameResultSuite.FileSourceScanExec: different orders of data filters and partition filters</li></div><div><li>org.apache.spark.sql.execution.SimpleSQLViewSuite.create view for partitioned parquet table</li></div><div><li>org.apache.spark.sql.execution.SimpleSQLViewSuite.Using view after adding more columns</li></div><div><li>org.apache.spark.sql.execution.SimpleSQLViewSuite.correctly handle a view with custom column names</li></div><div><li>org.apache.spark.sql.execution.SimpleSQLViewSuite.resolve a view when the dataTypes of referenced table columns changed</li></div><div><li>org.apache.spark.sql.execution.SimpleSQLViewSuite.sparkSession API view resolution with different default database</li></div><div><li>org.apache.spark.sql.execution.WholeStageCodegenSuite.Skip splitting consume function when parameter number exceeds JVM limit</li></div><div><li>org.apache.spark.sql.execution.columnar.InMemoryColumnarQuerySuite.SPARK-22673: InMemoryRelation should utilize existing stats of the plan to be cached</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.CTAS a managed table with the existing empty directory</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.create a managed table with the existing empty directory</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.Create partitioned data source table without user specified schema</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.Create partitioned data source table with user specified schema</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.Create non-partitioned data source table without user specified schema</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.Create non-partitioned data source table with user specified schema</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.alter table: rename cached table</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.Create Data Source Table As Select</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.truncate table - datasource table</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.truncate partitioned table - datasource table</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.create temporary view with mismatched schema</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.truncate table - external table, temporary table, view (not allowed)</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.truncate table - non-partitioned table (not allowed)</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.SPARK-16034 Partition columns should match when appending to existing data source tables</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.SET LOCATION for managed table</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.insert data to a data source table which has a non-existing location should succeed</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.insert into a data source table with a non-existing partition location should succeed</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.read data from a data source table with non-existing partition location should succeed</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.create datasource table with a non-existing location</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.CTAS for external data source table with a non-existing location</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.CTAS for external data source table with a existed location</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.data source table:partition column name containing a b</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.data source table:partition column name containing a:b</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.data source table:partition column name containing a%b</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.data source table:partition column name containing a,b</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.location uri contains a b for datasource table</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.location uri contains a:b for datasource table</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.location uri contains a%b for datasource table</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.location uri contains a b for database</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.location uri contains a:b for database</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.location uri contains a%b for database</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.alter datasource table add columns - parquet</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.alter datasource table add columns - partitioned - parquet</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.basic DDL using locale tr - caseSensitive true</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.basic DDL using locale tr - caseSensitive false</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.Create Hive Table As Select</li></div><div><li>org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite.SPARK-23348: append data to data source table with saveAsTable</li></div><div><li>org.apache.spark.sql.execution.datasources.FileFormatWriterSuite.empty file should be skipped while write to file</li></div><div><li>org.apache.spark.sql.execution.datasources.FileFormatWriterSuite.SPARK-22252: FileFormatWriter should respect the input query schema</li></div><div><li>org.apache.spark.sql.execution.datasources.FileIndexSuite.SPARK-20367 - properly unescape column names in inferPartitioning</li></div><div><li>org.apache.spark.sql.execution.datasources.FileSourceStrategySuite.[SPARK-16818] partition pruned file scans implement sameResult correctly</li></div><div><li>org.apache.spark.sql.execution.datasources.FileSourceStrategySuite.[SPARK-16818] exchange reuse respects differences in partition pruning</li></div><div><li>org.apache.spark.sql.execution.datasources.FileSourceStrategySuite.[SPARK-18753] keep pushed-down null literal as a filter in Spark-side post-filter</li></div><div><li>org.apache.spark.sql.execution.datasources.HadoopFsRelationSuite.sizeInBytes should be the total size of all files</li></div><div><li>org.apache.spark.sql.execution.datasources.HadoopFsRelationSuite.SPARK-22790: spark.sql.sources.compressionFactor takes effect</li></div><div><li>org.apache.spark.sql.execution.datasources.json.JsonSuite.SPARK-7565 MapType in JsonRDD</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetCompressionCodecPrecedenceSuite.Create parquet table with compression</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetEncodingSuite.All Types Dictionary</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetEncodingSuite.All Types Null</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetEncodingSuite.Read row group containing both dictionary and plain encoded pages</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormatSuite.read parquet footers in parallel</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.basic data types (without binary)</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.raw binary</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.string</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.Standard mode - fixed-length decimals</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.Legacy mode - fixed-length decimals</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.date type</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.Standard mode - map</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.Legacy mode - map</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.Standard mode - array</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.Legacy mode - array</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.Standard mode - array and double</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.Legacy mode - array and double</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.Standard mode - struct</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.Legacy mode - struct</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.Standard mode - nested struct with array of array as field</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.Legacy mode - nested struct with array of array as field</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.Standard mode - nested map with struct as value type</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.Legacy mode - nested map with struct as value type</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.nulls</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.nones</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.compression codec</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.save - overwrite</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.save - ignore</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.save - throw</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.save - append</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.SPARK-8121: spark.sql.parquet.output.committer.class shouldn't be overridden</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.SPARK-7837 Do not close output writer twice when commitTask() fails</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.SPARK-11044 Parquet writer version fixed as version1</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.null and non-null strings</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.read dictionary and plain encoded timestamp_millis written as INT64</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.SPARK-12589 copy() on rows returned from reader works for strings</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.VectorizedParquetRecordReader - direct path read</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.VectorizedParquetRecordReader - partition column types</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.SPARK-23173 Writing a file with data converted from JSON with and incorrect user schema</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetInteroperabilitySuite.parquet timestamp conversion</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaSuite.schema merging failure error message</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaSuite.schema mismatch failure error message for parquet reader</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaSuite.schema mismatch failure error message for parquet vectorized reader</li></div><div><li>org.apache.spark.sql.execution.datasources.parquet.ParquetThriftCompatibilitySuite.Read Parquet file generated by parquet-thrift</li></div><div><li>org.apache.spark.sql.execution.metric.SQLMetricsSuite.range metrics</li></div><div><li>org.apache.spark.sql.execution.metric.SQLMetricsSuite.writing data out metrics: parquet</li></div><div><li>org.apache.spark.sql.execution.metric.SQLMetricsSuite.writing data out metrics with dynamic partition: parquet</li></div><div><li>org.apache.spark.sql.execution.ui.SQLAppStatusListenerSuite.driver side SQL metrics</li></div><div><li>org.apache.spark.sql.internal.CatalogSuite.dropTempView should not un-cache and drop metastore table if a same-name table exists</li></div><div><li>org.apache.spark.sql.internal.CatalogSuite.createTable without 'path' in options</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.read bucketed data</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.avoid shuffle when join 2 bucketed tables</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.only shuffle one side when join bucketed table and non-bucketed table</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.only shuffle one side when 2 bucketed tables have different bucket number</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.only shuffle one side when 2 bucketed tables have different bucket keys</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.shuffle when join keys are not equal to bucket keys</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.shuffle when join 2 bucketed tables with bucketing disabled</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.check sort and shuffle when bucket and sort columns are join keys</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.avoid shuffle and sort when sort columns are a super set of join keys</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.only sort one side when sort columns are different</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.only sort one side when sort columns are same but their ordering is different</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.avoid shuffle when grouping keys are equal to bucket keys</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.avoid shuffle when grouping keys are a super-set of bucket keys</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.SPARK-17698 Join predicates should not contain filter clauses</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.SPARK-19122 Re-order join predicates if they match with the child's output partitioning</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.SPARK-19122 No re-ordering should happen if set of join columns != set of child's partitioning columns</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.SPARK-22042 ReorderJoinPredicates can break when child's partitioning is not decided</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.error if there exists any malformed bucket files</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite.disable bucketing when the output doesn't contain all bucketing columns</li></div><div><li>org.apache.spark.sql.sources.BucketedWriteWithoutHiveSupportSuite.write bucketed data</li></div><div><li>org.apache.spark.sql.sources.BucketedWriteWithoutHiveSupportSuite.write bucketed data with sortBy</li></div><div><li>org.apache.spark.sql.sources.BucketedWriteWithoutHiveSupportSuite.write bucketed data without partitionBy</li></div><div><li>org.apache.spark.sql.sources.BucketedWriteWithoutHiveSupportSuite.write bucketed data without partitionBy with sortBy</li></div><div><li>org.apache.spark.sql.sources.BucketedWriteWithoutHiveSupportSuite.write bucketed data with bucketing disabled</li></div><div><li>org.apache.spark.sql.sources.CreateTableAsSelectSuite.create table using as select - with partitioned by</li></div><div><li>org.apache.spark.sql.sources.CreateTableAsSelectSuite.create table using as select - with valid number of buckets</li></div><div><li>org.apache.spark.sql.sources.CreateTableAsSelectSuite.SPARK-17409: CTAS of decimal calculation</li></div><div><li>org.apache.spark.sql.sources.PartitionedWriteSuite.write many partitions</li></div><div><li>org.apache.spark.sql.sources.PartitionedWriteSuite.write many partitions with repeats</li></div><div><li>org.apache.spark.sql.sources.PartitionedWriteSuite.partitioned columns should appear at the end of schema</li></div><div><li>org.apache.spark.sql.sources.PartitionedWriteSuite.maxRecordsPerFile setting in non-partitioned write path</li></div><div><li>org.apache.spark.sql.sources.PartitionedWriteSuite.maxRecordsPerFile setting in dynamic partition writes</li></div><div><li>org.apache.spark.sql.sources.PartitionedWriteSuite.append data to an existing partitioned table without custom partition path</li></div><div><li>org.apache.spark.sql.sources.PartitionedWriteSuite.timeZone setting in dynamic partition writes</li></div><div><li>org.apache.spark.sql.streaming.DeduplicateSuite.deduplicate with file sink</li></div><div><li>org.apache.spark.sql.streaming.FileStreamSinkSuite.unpartitioned writing and batch reading</li></div><div><li>org.apache.spark.sql.streaming.FileStreamSinkSuite.SPARK-21167: encode and decode path correctly</li></div><div><li>org.apache.spark.sql.streaming.FileStreamSinkSuite.partitioned writing and batch reading</li></div><div><li>org.apache.spark.sql.streaming.FileStreamSinkSuite.partitioned writing and batch reading with 'basePath'</li></div><div><li>org.apache.spark.sql.streaming.FileStreamSinkSuite.writing with aggregation</li></div><div><li>org.apache.spark.sql.streaming.FileStreamSinkSuite.SPARK-23288 writing and checking output metrics</li></div><div><li>org.apache.spark.sql.streaming.FileStreamSourceSuite.FileStreamSource schema: parquet, existing files, no schema</li></div><div><li>org.apache.spark.sql.streaming.FileStreamSourceSuite.FileStreamSource schema: parquet, existing files, schema</li></div><div><li>org.apache.spark.sql.streaming.FileStreamSourceSuite.read from parquet files</li></div><div><li>org.apache.spark.sql.streaming.FileStreamSourceSuite.read from parquet files with changing schema</li></div><div><li>org.apache.spark.sql.streaming.FileStreamStressSuite.fault tolerance stress test - unpartitioned output</li></div><div><li>org.apache.spark.sql.streaming.FileStreamStressSuite.fault tolerance stress test - partitioned output</li></div><div><li>org.apache.spark.sql.streaming.test.DataStreamReaderWriterSuite.SPARK-18510: use user specified types for partition columns in file sources</li></div><div><li>org.apache.spark.sql.test.DataFrameReaderWriterSuite.write path implements onTaskCommit API correctly</li></div><div><li>org.apache.spark.sql.test.DataFrameReaderWriterSuite.parquet - API and behavior regarding schema</li></div><div><li>org.apache.spark.sql.test.DataFrameReaderWriterSuite.column nullability and comment - write and then read</li></div><div><li>org.apache.spark.sql.test.DataFrameReaderWriterSuite.SPARK-17230: write out results of decimal calculation</li></div><div><li>org.apache.spark.sql.test.DataFrameReaderWriterSuite.saveAsTable with mode Append should not fail if the table not exists but a same-name temp view exist</li></div><div><li>org.apache.spark.sql.test.DataFrameReaderWriterSuite.saveAsTable with mode Append should not fail if the table already exists and a same-name temp view exist</li></div><div><li>org.apache.spark.sql.test.DataFrameReaderWriterSuite.saveAsTable with mode ErrorIfExists should not fail if the table not exists but a same-name temp view exist</li></div><div><li>org.apache.spark.sql.test.DataFrameReaderWriterSuite.saveAsTable with mode Overwrite should not drop the temp view if the table not exists but a same-name temp view exist</li></div><div><li>org.apache.spark.sql.test.DataFrameReaderWriterSuite.saveAsTable with mode Overwrite should not fail if the table already exists and a same-name temp view exist</li></div><div><li>org.apache.spark.sql.test.DataFrameReaderWriterSuite.saveAsTable with mode Ignore should create the table if the table not exists but a same-name temp view exist</li></div><div><li>org.apache.spark.sql.test.DataFrameReaderWriterSuite.SPARK-18510: use user specified types for partition columns in file sources</li></div><div><li>org.apache.spark.sql.test.DataFrameReaderWriterSuite.SPARK-18899: append to a bucketed table using DataFrameWriter with mismatched bucketing</li></div><div><li>org.apache.spark.sql.test.DataFrameReaderWriterSuite.SPARK-18912: number of columns mismatch for non-file-based data source table</li></div><div><li>org.apache.spark.sql.test.DataFrameReaderWriterSuite.SPARK-18913: append to a table with special column names</li></div><div><li>org.apache.spark.sql.test.DataFrameReaderWriterSuite.SPARK-20460 Check name duplication in schema</li></div><div><li>org.apache.spark.sql.test.DataFrameReaderWriterSuite.use Spark jobs to list files</li></div><div><li>org.apache.spark.sql.util.DataFrameCallbackSuite.execute callback functions for DataFrameWriter</li></div><div><li>org.apache.spark.sql.hive.HiveMetastoreLazyInitializationSuite.lazily initialize Hive client</li></div><div><li>org.apache.spark.sql.hive.CachedTableSuite.REFRESH TABLE also needs to recache the data (data source tables)</li></div><div><li>org.apache.spark.sql.hive.CachedTableSuite.SPARK-15678: REFRESH PATH</li></div><div><li>org.apache.spark.sql.hive.CachedTableSuite.Cache/Uncache Qualified Tables</li></div><div><li>org.apache.spark.sql.hive.CachedTableSuite.SPARK-11246 cache parquet table</li></div><div><li>org.apache.spark.sql.hive.CompressionCodecSuite.both table-level and session-level compression are set</li></div><div><li>org.apache.spark.sql.hive.CompressionCodecSuite.table-level compression is not set but session-level compressions is set</li></div><div><li>org.apache.spark.sql.hive.CompressionCodecSuite.test table containing mixed compression codec</li></div><div><li>org.apache.spark.sql.hive.HiveParquetSuite.Case insensitive attribute names</li></div><div><li>org.apache.spark.sql.hive.HiveParquetSuite.SELECT on Parquet table</li></div><div><li>org.apache.spark.sql.hive.HiveParquetSuite.Simple column projection + filter on Parquet table</li></div><div><li>org.apache.spark.sql.hive.HiveParquetSuite.Converting Hive to Parquet Table via saveAsParquetFile</li></div><div><li>org.apache.spark.sql.hive.HiveParquetSuite.INSERT OVERWRITE TABLE Parquet table</li></div><div><li>org.apache.spark.sql.hive.InsertSuite.Data source table - INSERT OVERWRITE - partition IF NOT EXISTS</li></div><div><li>org.apache.spark.sql.hive.InsertSuite.Data source table - SPARK-16037: INSERT statement should match columns by position</li></div><div><li>org.apache.spark.sql.hive.InsertSuite.Data source table - INSERT INTO a partitioned table (semantic and error handling)</li></div><div><li>org.apache.spark.sql.hive.InsertSuite.Data source table - insertInto() should match columns by position and ignore column names</li></div><div><li>org.apache.spark.sql.hive.InsertSuite.Data source table - insertInto() should match unnamed columns by position</li></div><div><li>org.apache.spark.sql.hive.InsertSuite.SPARK-21165: FileFormatWriter should only rely on attributes from analyzed plan</li></div><div><li>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.convert partition provider to hive with repair table</li></div><div><li>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.when partition management is enabled, new tables have partition provider hive</li></div><div><li>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.when partition management is disabled, new tables have no partition provider</li></div><div><li>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.when partition management is disabled, we preserve the old behavior even for new tables</li></div><div><li>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.insert overwrite partition of legacy datasource table</li></div><div><li>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.insert overwrite partition of new datasource table overwrites just partition</li></div><div><li>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.SPARK-18544 append with saveAsTable - partition management true</li></div><div><li>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.SPARK-18635 special chars in partition values - partition management true</li></div><div><li>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.SPARK-18659 insert overwrite table files - partition management true</li></div><div><li>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.SPARK-18659 insert overwrite table with lowercase - partition management true</li></div><div><li>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.SPARK-19887 partition value is null - partition management true</li></div><div><li>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.SPARK-18544 append with saveAsTable - partition management false</li></div><div><li>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.SPARK-18635 special chars in partition values - partition management false</li></div><div><li>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.SPARK-18659 insert overwrite table files - partition management false</li></div><div><li>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.SPARK-18659 insert overwrite table with lowercase - partition management false</li></div><div><li>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.SPARK-19887 partition value is null - partition management false</li></div><div><li>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.insert into partial dynamic partitions</li></div><div><li>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.insert into fully dynamic partitions</li></div><div><li>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.insert into static partition</li></div><div><li>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.overwrite partial dynamic partitions</li></div><div><li>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.overwrite fully dynamic partitions</li></div><div><li>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.overwrite static partition</li></div><div><li>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.append data with DataFrameWriter</li></div><div><li>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.SPARK-19359: renaming partition should not leave useless directories</li></div><div><li>org.apache.spark.sql.hive.ShowCreateTableSuite.data source table using Dataset API</li></div><div><li>org.apache.spark.sql.hive.StatisticsSuite.SPARK-18856: non-empty partitioned table should not report zero size</li></div><div><li>org.apache.spark.sql.hive.StatisticsSuite.size estimation for relations is based on row size * number of rows</li></div><div><li>org.apache.spark.sql.hive.StatisticsSuite.analyze non hive compatible datasource tables</li></div><div><li>org.apache.spark.sql.hive.StatisticsSuite.test statistics of LogicalRelation converted from Hive serde tables</li></div><div><li>org.apache.spark.sql.hive.StatisticsSuite.verify serialized column stats after analyzing columns</li></div><div><li>org.apache.spark.sql.hive.StatisticsSuite.verify column stats can be deserialized from tblproperties</li></div><div><li>org.apache.spark.sql.hive.StatisticsSuite.serialization and deserialization of histograms to/from hive metastore</li></div><div><li>org.apache.spark.sql.hive.StatisticsSuite.test table-level statistics for data source table created in HiveExternalCatalog</li></div><div><li>org.apache.spark.sql.hive.StatisticsSuite.test table-level statistics for partitioned data source table</li></div><div><li>org.apache.spark.sql.hive.StatisticsSuite.test refreshing table stats of cached data source table by `ANALYZE TABLE` statement</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.12: CTAS for managed data source tables</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.13: CTAS for managed data source tables</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.0.14: CTAS for managed data source tables</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.0: CTAS for managed data source tables</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.1: CTAS for managed data source tables</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.1.2: CTAS for managed data source tables</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.0: CTAS for managed data source tables</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.1: CTAS for managed data source tables</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.2: CTAS for managed data source tables</li></div><div><li>org.apache.spark.sql.hive.client.VersionsSuite.2.3: CTAS for managed data source tables</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.Create partitioned data source table without user specified schema</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.Create partitioned data source table with user specified schema</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.Create non-partitioned data source table without user specified schema</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.Create non-partitioned data source table with user specified schema</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.alter table: rename cached table</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.Create Data Source Table As Select</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.truncate table - datasource table</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.truncate partitioned table - datasource table</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.create temporary view with mismatched schema</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.truncate table - external table, temporary table, view (not allowed)</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.truncate table - non-partitioned table (not allowed)</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.SPARK-16034 Partition columns should match when appending to existing data source tables</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.SET LOCATION for managed table</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.insert data to a data source table which has a non-existing location should succeed</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.insert into a data source table with a non-existing partition location should succeed</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.read data from a data source table with non-existing partition location should succeed</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.create datasource table with a non-existing location</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.CTAS for external data source table with a non-existing location</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.CTAS for external data source table with a existed location</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.data source table:partition column name containing a b</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.data source table:partition column name containing a:b</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.data source table:partition column name containing a%b</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.data source table:partition column name containing a,b</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.location uri contains a b for datasource table</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.location uri contains a:b for datasource table</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.location uri contains a%b for datasource table</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.location uri contains a b for database</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.location uri contains a:b for database</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.location uri contains a%b for database</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.alter datasource table add columns - parquet</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.alter datasource table add columns - partitioned - parquet</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.basic DDL using locale tr - caseSensitive true</li></div><div><li>org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.basic DDL using locale tr - caseSensitive false</li></div><div><li>org.apache.spark.sql.hive.execution.HiveDDLSuite.drop external tables in default database</li></div><div><li>org.apache.spark.sql.hive.execution.HiveDDLSuite.drop external data source table in default database</li></div><div><li>org.apache.spark.sql.hive.execution.HiveDDLSuite.SPARK-19129: drop partition with a empty string will drop the whole table</li></div><div><li>org.apache.spark.sql.hive.execution.HiveDDLSuite.drop views</li></div><div><li>org.apache.spark.sql.hive.execution.HiveDDLSuite.alter views - rename</li></div><div><li>org.apache.spark.sql.hive.execution.HiveDDLSuite.alter views - set/unset tblproperties</li></div><div><li>org.apache.spark.sql.hive.execution.HiveDDLSuite.alter views and alter table - misuse</li></div><div><li>org.apache.spark.sql.hive.execution.HiveDDLSuite.drop view using drop table</li></div><div><li>org.apache.spark.sql.hive.execution.HiveDDLSuite.create view with mismatched schema</li></div><div><li>org.apache.spark.sql.hive.execution.HiveDDLSuite.CREATE TABLE LIKE a temporary view</li></div><div><li>org.apache.spark.sql.hive.execution.HiveDDLSuite.CREATE TABLE LIKE an external data source table</li></div><div><li>org.apache.spark.sql.hive.execution.HiveDDLSuite.CREATE TABLE LIKE a view</li></div><div><li>org.apache.spark.sql.hive.execution.HiveDDLSuite.desc table for data source table - no user-defined schema</li></div><div><li>org.apache.spark.sql.hive.execution.HiveDDLSuite.truncate table - datasource table</li></div><div><li>org.apache.spark.sql.hive.execution.HiveDDLSuite.truncate partitioned table - datasource table</li></div><div><li>org.apache.spark.sql.hive.execution.HiveDDLSuite.create hive serde table with Catalog</li></div><div><li>org.apache.spark.sql.hive.execution.HiveDDLSuite.create hive serde table with DataFrameWriter.saveAsTable</li></div><div><li>org.apache.spark.sql.hive.execution.HiveDDLSuite.append data to hive serde table</li></div><div><li>org.apache.spark.sql.hive.execution.HiveDDLSuite.partitioned table should always put partition columns at the end of table schema</li></div><div><li>org.apache.spark.sql.hive.execution.HiveDDLSuite.partition column name of parquet table containing a b</li></div><div><li>org.apache.spark.sql.hive.execution.HiveDDLSuite.partition column name of parquet table containing a:b</li></div><div><li>org.apache.spark.sql.hive.execution.HiveDDLSuite.partition column name of parquet table containing a%b</li></div><div><li>org.apache.spark.sql.hive.execution.HiveDDLSuite.partition column name of parquet table containing a,b</li></div><div><li>org.apache.spark.sql.hive.execution.HiveDDLSuite.alter hive serde table add columns -- partitioned - PARQUET</li></div><div><li>org.apache.spark.sql.hive.execution.HiveDDLSuite.alter hive serde table add columns -- with predicate - PARQUET</li></div><div><li>org.apache.spark.sql.hive.execution.HiveDDLSuite.SPARK-22252: FileFormatWriter should respect the input query schema in HIVE</li></div><div><li>org.apache.spark.sql.hive.execution.HiveSQLViewSuite.create view for partitioned parquet table</li></div><div><li>org.apache.spark.sql.hive.execution.HiveSQLViewSuite.Using view after adding more columns</li></div><div><li>org.apache.spark.sql.hive.execution.HiveSQLViewSuite.correctly handle a view with custom column names</li></div><div><li>org.apache.spark.sql.hive.execution.HiveSQLViewSuite.resolve a view when the dataTypes of referenced table columns changed</li></div><div><li>org.apache.spark.sql.hive.execution.HiveSQLViewSuite.sparkSession API view resolution with different default database</li></div><div><li>org.apache.spark.sql.hive.execution.HiveSQLViewSuite.create a permanent/temp view using a hive, built-in, and permanent user function</li></div><div><li>org.apache.spark.sql.hive.execution.HiveSQLViewSuite.create a permanent/temp view using a temporary function</li></div><div><li>org.apache.spark.sql.hive.execution.HiveSQLViewSuite.SPARK-14933 - create view from hive parquet table</li></div><div><li>org.apache.spark.sql.hive.execution.SQLQuerySuite.SPARK-6851: Self-joined converted parquet tables</li></div><div><li>org.apache.spark.sql.hive.execution.SQLQuerySuite.CTAS without serde without location</li></div><div><li>org.apache.spark.sql.hive.execution.SQLQuerySuite.CTAS with default fileformat</li></div><div><li>org.apache.spark.sql.hive.execution.SQLQuerySuite.CTAS without serde with location</li></div><div><li>org.apache.spark.sql.hive.execution.SQLQuerySuite.CTAS with serde</li></div><div><li>org.apache.spark.sql.hive.execution.SQLQuerySuite.specifying database name for a temporary view is not allowed</li></div><div><li>org.apache.spark.sql.hive.execution.SQLQuerySuite.SPARK-10741: Sort on Aggregate using parquet</li></div><div><li>org.apache.spark.sql.hive.execution.SQLQuerySuite.run sql directly on files - parquet</li></div><div><li>org.apache.spark.sql.hive.execution.SQLQuerySuite.run sql directly on files - hive</li></div><div><li>org.apache.spark.sql.hive.execution.SQLQuerySuite.SPARK-11453: append data to partitioned table</li></div><div><li>org.apache.spark.sql.hive.execution.SQLQuerySuite.insert into datasource table</li></div><div><li>org.apache.spark.sql.hive.execution.SQLQuerySuite.SPARK-15752 optimize metadata only query for hive table</li></div><div><li>org.apache.spark.sql.hive.execution.SQLQuerySuite.SPARK-17354: Partitioning by dates/timestamps works with Parquet vectorized reader</li></div><div><li>org.apache.spark.sql.hive.execution.SQLQuerySuite.SPARK-19912 String literals should be escaped for Hive metastore partition pruning</li></div><div><li>org.apache.spark.sql.hive.orc.HiveOrcHadoopFsRelationSuite.saveAsTable()/load() - partitioned table - boolean type</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.read bucketed data</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.avoid shuffle when join 2 bucketed tables</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.only shuffle one side when join bucketed table and non-bucketed table</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.only shuffle one side when 2 bucketed tables have different bucket number</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.only shuffle one side when 2 bucketed tables have different bucket keys</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.shuffle when join keys are not equal to bucket keys</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.shuffle when join 2 bucketed tables with bucketing disabled</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.check sort and shuffle when bucket and sort columns are join keys</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.avoid shuffle and sort when sort columns are a super set of join keys</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.only sort one side when sort columns are different</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.only sort one side when sort columns are same but their ordering is different</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.avoid shuffle when grouping keys are equal to bucket keys</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.avoid shuffle when grouping keys are a super-set of bucket keys</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.SPARK-17698 Join predicates should not contain filter clauses</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.SPARK-19122 Re-order join predicates if they match with the child's output partitioning</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.SPARK-19122 No re-ordering should happen if set of join columns != set of child's partitioning columns</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.SPARK-22042 ReorderJoinPredicates can break when child's partitioning is not decided</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.error if there exists any malformed bucket files</li></div><div><li>org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite.disable bucketing when the output doesn't contain all bucketing columns</li></div><div><li>org.apache.spark.sql.sources.BucketedWriteWithHiveSupportSuite.write bucketed data</li></div><div><li>org.apache.spark.sql.sources.BucketedWriteWithHiveSupportSuite.write bucketed data with sortBy</li></div><div><li>org.apache.spark.sql.sources.BucketedWriteWithHiveSupportSuite.write bucketed data without partitionBy</li></div><div><li>org.apache.spark.sql.sources.BucketedWriteWithHiveSupportSuite.write bucketed data without partitionBy with sortBy</li></div><div><li>org.apache.spark.sql.sources.BucketedWriteWithHiveSupportSuite.write bucketed data with bucketing disabled</li></div><div><li>org.apache.spark.sql.sources.SimpleTextHadoopFsRelationSuite.saveAsTable()/load() - partitioned table - boolean type</li></div><div><li>org.apache.spark.streaming.CheckpointSuite.get correct spark.driver.[host|port] from checkpoint</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"><div><li>&amp;#010;Assert on query failed: : Exception thrown in awaitResult: &amp;#010;org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:205)&amp;#010; org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)&amp;#010; org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:101)&amp;#010; org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:109)&amp;#010; org.apache.spark.util.RpcUtils$.makeDri</li></div><div><li>Timeout of './bin/spark-submit' '--class' 'org.apache.spark.sql.hive.SparkSQLConfTest' '--name' 'SparkSQLConfTest' '--master' 'local-cluster[2,1,1024]' '--conf' 'spark.ui.enabled=false' '--conf' 'spark.master.rest.enabled=false' '--conf' 'spark.sql.hive.metastore.version=0.12' '--conf' 'spark.sql.hive.metastore.jars=maven' '--driver-java-options' '-Dderby.system.durability=test' 'file:/var/lib/jen</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Timeout of './bin/spark-submit' '--class' 'org.apache.spark.sql.hive.SparkSQLConfTest' '--name' 'SparkSQLConfTest' '--master' 'local-cluster[2,1,1024]' '--conf' 'spark.ui.enabled=false' '--conf' 'spark.master.rest.enabled=false' '--conf' 'spark.sql.hive.metastore.version=0.12' '--conf' 'spark.sql.hive.metastore.jars=maven' '--driver-java-options' '-Dderby.system.durability=test' 'file:/var/lib/jen</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>The code passed to eventually never returned normally. Attempted 130 times over 2.0047144909 minutes. Last failure message: handle.getState().isFinal() was false.</li></div><div><li>&amp;#010;Assert on query failed: : Exception thrown in awaitResult: &amp;#010;org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:205)&amp;#010; org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)&amp;#010; org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:101)&amp;#010; org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:109)&amp;#010; org.apache.spark.util.RpcUtils$.makeDri</li></div><div><li>Timeout of './bin/spark-submit' '--class' 'org.apache.spark.sql.hive.SparkSQLConfTest' '--name' 'SparkSQLConfTest' '--master' 'local-cluster[2,1,1024]' '--conf' 'spark.ui.enabled=false' '--conf' 'spark.master.rest.enabled=false' '--conf' 'spark.sql.hive.metastore.version=0.12' '--conf' 'spark.sql.hive.metastore.jars=maven' '--driver-java-options' '-Dderby.system.durability=test' 'file:/var/lib/jen</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Timeout of './bin/spark-submit' '--class' 'org.apache.spark.sql.hive.SparkSQLConfTest' '--name' 'SparkSQLConfTest' '--master' 'local-cluster[2,1,1024]' '--conf' 'spark.ui.enabled=false' '--conf' 'spark.master.rest.enabled=false' '--conf' 'spark.sql.hive.metastore.version=0.12' '--conf' 'spark.sql.hive.metastore.jars=maven' '--driver-java-options' '-Dderby.system.durability=test' 'file:/var/lib/jen</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>java.lang.reflect.InvocationTargetException
	at org.apache.spark.shuffle.sort.UnsafeShuffleWriterSuite.testMergingSpills(UnsafeShuffleWriterSuite.java:311)
	at org.apache.spark.shuffle.sort.UnsafeShuffleWriterSuite.testMergingSpills(UnsafeShuffleWriterSuite.java:296)
	at org.apache.spark.shuffle.sort.UnsafeShuffleWriterSuite.mergeSpillsWithFileStreamAndSnappy(UnsafeShuffleWriterSuite.java:364)
Cau</li></div><div><li>java.lang.reflect.InvocationTargetException
	at org.apache.spark.shuffle.sort.UnsafeShuffleWriterSuite.testMergingSpills(UnsafeShuffleWriterSuite.java:311)
	at org.apache.spark.shuffle.sort.UnsafeShuffleWriterSuite.testMergingSpills(UnsafeShuffleWriterSuite.java:296)
	at org.apache.spark.shuffle.sort.UnsafeShuffleWriterSuite.mergeSpillsWithTransferToAndSnappy(UnsafeShuffleWriterSuite.java:359)
Cau</li></div><div><li>&amp;#010;Timed out waiting for stream: The code passed to failAfter did not complete within 30 seconds.&amp;#010;java.lang.Thread.getStackTrace(Thread.java:1559)&amp;#010; org.scalatest.concurrent.TimeLimits$class.failAfterImpl(TimeLimits.scala:234)&amp;#010; org.apache.spark.sql.kafka010.KafkaSourceTest.failAfterImpl(KafkaMicroBatchSourceSuite.scala:52)&amp;#010; org.scalatest.concurrent.TimeLimits$class.failAfter(</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Can not create the managed table('`t`'). The associated location('file:/var/lib/jenkins/workspace/spark/sql/core/spark-warehouse/t') already exists.;</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Expected "[]", but got "[org.apache.spark.SparkException&amp;#010;Job aborted.]" Result did not match for query #2&amp;#010;CREATE TABLE t1 USING parquet AS SELECT 1 AS i1</li></div><div><li>Expected "[]", but got "[org.apache.spark.SparkException&amp;#010;Job aborted.]" Result did not match for query #2&amp;#010;CREATE TABLE t1 USING parquet AS SELECT 1 AS i1</li></div><div><li>Expected "[]", but got "[org.apache.spark.sql.AnalysisException&amp;#010;Can not create the managed table('`default`.`t`'). The associated location('file:/var/lib/jenkins/workspace/spark/sql/core/spark-warehouse/t') already exists.;]" Result did not match for query #0&amp;#010;CREATE TABLE t (key STRING, value STRING, ds STRING, hr INT) USING parquet&amp;#010;    PARTITIONED BY (ds, hr)</li></div><div><li>Expected "[]", but got "[org.apache.spark.SparkException&amp;#010;Job aborted.]" Result did not match for query #17&amp;#010;INSERT INTO desc_col_table values 1, 2, 3, 4</li></div><div><li>Expected "[]", but got "[org.apache.spark.sql.AnalysisException&amp;#010;Can not create the managed table('`default`.`t`'). The associated location('file:/var/lib/jenkins/workspace/spark/sql/core/spark-warehouse/t') already exists.;]" Result did not match for query #0&amp;#010;CREATE TABLE t (a STRING, b INT, c STRING, d STRING) USING parquet&amp;#010;  OPTIONS (a '1', b '2')&amp;#010;  PARTITIONED BY (c, d) CLUS</li></div><div><li>Expected "[]", but got "[org.apache.spark.SparkException&amp;#010;Job aborted.]" Result did not match for query #1&amp;#010;insert into t1 values(1,0,0)</li></div><div><li>Expected "[]", but got "[org.apache.spark.SparkException&amp;#010;Job aborted.]" Result did not match for query #1&amp;#010;INSERT INTO spark_10747 VALUES (6, 12, 10), (6, 11, 4), (6, 9, 10), (6, 15, 8),&amp;#010;(6, 15, 8), (6, 7, 4), (6, 7, 8), (6, 13, null), (6, 10, null)</li></div><div><li>Expected "[]", but got "[org.apache.spark.SparkException&amp;#010;Job aborted.]" Result did not match for query #2&amp;#010;CREATE TABLE t1 USING parquet AS SELECT 1 AS i1</li></div><div><li>Expected "[]", but got "[org.apache.spark.SparkException&amp;#010;Job aborted.]" Result did not match for query #5&amp;#010;insert into decimals_test values(1, 100.0, 999.0), (2, 12345.123, 12345.123),&amp;#010;  (3, 0.1234567891011, 1234.1), (4, 123456789123456789.0, 1.123456789123456789)</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Can not create the managed table('`t1`'). The associated location('file:/var/lib/jenkins/workspace/spark/sql/core/spark-warehouse/t1') already exists.;</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>"[Job aborted due to stage failure: Task 1 in stage 37.0 failed 1 times, most recent failure: Lost task 1.0 in stage 37.0 (TID 59, localhost, executor driver): org.apache.spark.SparkException: Task failed while writing rows.&amp;#010; at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:294)&amp;#010</li></div><div><li>"Job aborted due to stage failure: Task 0 in stage 38.0 failed 1 times, most recent failure: Lost task 0.0 in stage 38.0 (TID 60, localhost, executor driver): org.apache.spark.SparkException: Task failed while writing rows.&amp;#010; at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:294)&amp;#010;</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted due to stage failure: Task 0 in stage 68.0 failed 1 times, most recent failure: Lost task 0.0 in stage 68.0 (TID 98, localhost, executor driver): java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy&amp;#010; at org.apache.parquet.hadoop.codec.SnappyDecompressor.decompress(SnappyDecompressor.java:62)&amp;#010; at org.apache.parquet.hadoop.codec.NonBlockedDecompre</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost, executor driver): java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy&amp;#010; at org.apache.parquet.hadoop.codec.SnappyDecompressor.decompress(SnappyDecompressor.java:62)&amp;#010; at org.apache.parquet.hadoop.codec.NonBlockedDecompresso</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>java.util.NoSuchElementException was thrown.</li></div><div><li>Job aborted.</li></div><div><li>Can not create the managed table('`default`.`t`'). The associated location('file:/var/lib/jenkins/workspace/spark/sql/core/spark-warehouse/t') already exists.;</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Can not create the managed table('`bucketed_table1`'). The associated location('file:/var/lib/jenkins/workspace/spark/sql/core/spark-warehouse/bucketed_table1') already exists.;</li></div><div><li>Can not create the managed table('`bucketed_table1`'). The associated location('file:/var/lib/jenkins/workspace/spark/sql/core/spark-warehouse/bucketed_table1') already exists.;</li></div><div><li>Can not create the managed table('`bucketed_table1`'). The associated location('file:/var/lib/jenkins/workspace/spark/sql/core/spark-warehouse/bucketed_table1') already exists.;</li></div><div><li>Can not create the managed table('`bucketed_table1`'). The associated location('file:/var/lib/jenkins/workspace/spark/sql/core/spark-warehouse/bucketed_table1') already exists.;</li></div><div><li>Can not create the managed table('`bucketed_table1`'). The associated location('file:/var/lib/jenkins/workspace/spark/sql/core/spark-warehouse/bucketed_table1') already exists.;</li></div><div><li>Can not create the managed table('`bucketed_table1`'). The associated location('file:/var/lib/jenkins/workspace/spark/sql/core/spark-warehouse/bucketed_table1') already exists.;</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Can not create the managed table('`bucketed_table1`'). The associated location('file:/var/lib/jenkins/workspace/spark/sql/core/spark-warehouse/bucketed_table1') already exists.;</li></div><div><li>Can not create the managed table('`bucketed_table1`'). The associated location('file:/var/lib/jenkins/workspace/spark/sql/core/spark-warehouse/bucketed_table1') already exists.;</li></div><div><li>Can not create the managed table('`bucketed_table1`'). The associated location('file:/var/lib/jenkins/workspace/spark/sql/core/spark-warehouse/bucketed_table1') already exists.;</li></div><div><li>Job aborted.</li></div><div><li>Can not create the managed table('`bucketed_table`'). The associated location('file:/var/lib/jenkins/workspace/spark/sql/core/spark-warehouse/bucketed_table') already exists.;</li></div><div><li>Can not create the managed table('`bucketed_table`'). The associated location('file:/var/lib/jenkins/workspace/spark/sql/core/spark-warehouse/bucketed_table') already exists.;</li></div><div><li>Can not create the managed table('`bucketed_table`'). The associated location('file:/var/lib/jenkins/workspace/spark/sql/core/spark-warehouse/bucketed_table') already exists.;</li></div><div><li>Can not create the managed table('`bucketed_table`'). The associated location('file:/var/lib/jenkins/workspace/spark/sql/core/spark-warehouse/bucketed_table') already exists.;</li></div><div><li>Can not create the managed table('`bucketed_table`'). The associated location('file:/var/lib/jenkins/workspace/spark/sql/core/spark-warehouse/bucketed_table') already exists.;</li></div><div><li>Can not create the managed table('`bucketed_table`'). The associated location('file:/var/lib/jenkins/workspace/spark/sql/core/spark-warehouse/bucketed_table') already exists.;</li></div><div><li>Can not create the managed table('`bucketed_table`'). The associated location('file:/var/lib/jenkins/workspace/spark/sql/core/spark-warehouse/bucketed_table') already exists.;</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Query [id = 14c941ba-aa42-427a-821d-a969ea8528b6, runId = 28b3aaf3-62f9-4430-a1b3-bf0bdc70dc19] terminated with exception: Job aborted.</li></div><div><li>Query [id = dbd3af90-1a3b-413a-a9ca-a87a59f2a081, runId = 116cdfe2-da8b-4e1b-a4c0-2dbb644c3047] terminated with exception: Job aborted.</li></div><div><li>Query [id = 87a23eb1-6b70-49bd-a86c-0782fe4b2812, runId = b476569b-776e-4ee2-9ca6-98c562d6bcd1] terminated with exception: Job aborted.</li></div><div><li>Query [id = 7b8d11b8-c12b-4c6e-a653-73f63a91b46c, runId = 21686bce-c764-4e68-984a-31a571e67ba6] terminated with exception: Job aborted.</li></div><div><li>Query [id = 43071127-a359-444a-bcd4-0b9321aa9ae0, runId = f4857a0c-0663-4481-bc38-787c97107221] terminated with exception: Job aborted.</li></div><div><li>Query [id = b994f8c0-fc01-4d59-a4d4-947242d8ff60, runId = c43a3e48-bdf6-420a-aebe-3950e3f55bea] terminated with exception: Job aborted.</li></div><div><li>Query [id = 978b61f8-bc8f-432a-b529-f7b8913c4b1c, runId = 33d16fcf-3235-494a-a83b-e12c30ada3d6] terminated with exception: Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>&amp;#010;Error adding data: Job aborted.&amp;#010;org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:233)&amp;#010; org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:154)&amp;#010; org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)&amp;#010; org.apac</li></div><div><li>Job aborted.</li></div><div><li>Query [id = 6914ebbb-6a2c-49d2-9dac-64062b9209d8, runId = 53150280-534e-4953-b211-6f784d46a540] terminated with exception: Job aborted.</li></div><div><li>Query [id = b6abbbe7-28ae-455c-a440-e936302d77d3, runId = af6faa56-1fbf-44e5-99e1-ace392fdcc48] terminated with exception: Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Can not create the managed table('`t`'). The associated location('file:/var/lib/jenkins/workspace/spark/sql/core/spark-warehouse/t') already exists.;</li></div><div><li>Can not create the managed table('`default`.`t`'). The associated location('file:/var/lib/jenkins/workspace/spark/sql/core/spark-warehouse/t') already exists.;</li></div><div><li>Can not create the managed table('`t`'). The associated location('file:/var/lib/jenkins/workspace/spark/sql/core/spark-warehouse/t') already exists.;</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>Job aborted.</li></div><div><li>"[10002]" did not equal "[9999]"</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.spark.sql.streaming.continuous.ContinuousStressSuite.only one epoch</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.spark.deploy.yarn.YarnShuffleAuthSuite.external shuffle service</div></li><li><div>org.apache.spark.sql.streaming.continuous.ContinuousStressSuite.only one epoch</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="sqoop"><div style="font-weight:bold;" class="panel-heading">SQOOP<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Talat)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/trunk</div><div><b>Last Revision: </b>af7a594d987ece6c1990be950c48d94bbab8271f</div><div><b>Last Run: </b>13-04-2018 01:55 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th><th>PPC RHEL7.5</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 734</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 734</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 734</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 734</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 734</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td></tr><tr><td>Result</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="storm"><div style="font-weight:bold;" class="panel-heading">STORM<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Parita)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>4f5fdd928517df719db3b8d54351d0c5421193b7</div><div><b>Last Run: </b>12-04-2018 01:18 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th><th>PPC RHEL7.5</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 1187</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 1187</div><div>Failed Count : 2</div><div>Skipped Count : 5</div></td><td><div>Total Count : 1187</div><div>Failed Count : 2</div><div>Skipped Count : 5</div></td><td><div>Total Count : 1187</div><div>Failed Count : 2</div><div>Skipped Count : 5</div></td><td><div>Total Count : 1181</div><div>Failed Count : 1</div><div>Skipped Count : 5</div></td></tr><tr><td>Result</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.storm.utils.JCQueueTest.testFirstMessageFirst</li></div><div><li>org.apache.storm.nimbus.LocalNimbusTest.testSubmitTopologyToLocalNimbus</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.storm.kafka.KafkaUtilsTest.generateTuplesWithMessageAndMetadataScheme</li></div><div><li>org.apache.storm.utils.JCQueueTest.testFirstMessageFirst</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.storm.flux.multilang.MultilangEnvironmentTest.testInvokeNode</li></div><div><li>org.apache.storm.utils.JCQueueTest.testFirstMessageFirst</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.storm.utils.JCQueueTest.testFirstMessageFirst</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>Unable to send halt interrupt</li></div><div><li>It took over 60000ms to shut down slot Thread[SLOT_1024,5,main]</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.kafka.common.errors.TimeoutException: Failed to update metadata after 1000 ms.</li></div><div><li>Unable to send halt interrupt</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Cannot run program "node": error=2, No such file or directory</li></div><div><li>Unable to send halt interrupt</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Unable to send halt interrupt</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.storm.utils.JCQueueTest.testFirstMessageFirst</div></li><li><div>org.apache.storm.nimbus.LocalNimbusTest.testSubmitTopologyToLocalNimbus</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.storm.kafka.KafkaUtilsTest.generateTuplesWithMessageAndMetadataScheme</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.storm.flux.multilang.MultilangEnvironmentTest.testInvokeNode</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="tez"><div style="font-weight:bold;" class="panel-heading">TEZ<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Valencia)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>871ea80c04eccd6eb985fc2dd5cf46e0ecfc59cf</div><div><b>Last Run: </b>12-04-2018 03:53 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th><th>PPC RHEL7.5</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 1768</div><div>Failed Count : 2</div><div>Skipped Count : 14</div></td><td><div>Total Count : 1768</div><div>Failed Count : 7</div><div>Skipped Count : 14</div></td><td><div>Total Count : 1768</div><div>Failed Count : 2</div><div>Skipped Count : 14</div></td><td><div>Total Count : 1768</div><div>Failed Count : 1</div><div>Skipped Count : 14</div></td><td><div>Total Count : 1765</div><div>Failed Count : 11</div><div>Skipped Count : 14</div></td></tr><tr><td>Result</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"><div><li>org.apache.tez.dag.app.TestMockDAGAppMaster.testInternalPreemption</li></div><div><li>org.apache.tez.tests.TestExtServicesWithLocalMode.test1</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.tez.tests.TestExtServicesWithLocalMode.test1</li></div><div><li>org.apache.tez.mapreduce.TestMRRJobsDAGApi.testMultipleMRRSleepJobViaSession</li></div><div><li>org.apache.tez.mapreduce.TestMRRJobsDAGApi.testNonDefaultFSStagingDir</li></div><div><li>org.apache.tez.mapreduce.TestMRRJobsDAGApi.testMRRSleepJobDagSubmit</li></div><div><li>org.apache.tez.mapreduce.TestMRRJobsDAGApi.testMRRSleepJobDagSubmitAndKillViaRPC</li></div><div><li>org.apache.tez.mapreduce.TestMRRJobsDAGApi.testMRRSleepJobDagSubmitAndKill</li></div><div><li>org.apache.tez.mapreduce.TestMRRJobsDAGApi.testAMRelocalizationConflict</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.tez.tests.TestExtServicesWithLocalMode.test1</li></div><div><li>org.apache.tez.test.TestExceptionPropagation.testExceptionPropagationSession</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.tez.tests.TestExtServicesWithLocalMode.test1</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.tez.dag.app.dag.impl.TestCommit.testDAGCommitFail3_OnVertexSuccess</li></div><div><li>org.apache.tez.history.TestHistoryParser.org.apache.tez.history.TestHistoryParser</li></div><div><li>org.apache.tez.dag.history.ats.acls.TestATSHistoryWithACLs.testDisableSessionLogging</li></div><div><li>org.apache.tez.dag.history.ats.acls.TestATSHistoryWithACLs.testDagLoggingEnabled</li></div><div><li>org.apache.tez.dag.history.ats.acls.TestATSHistoryWithACLs.testSimpleAMACls</li></div><div><li>org.apache.tez.dag.history.ats.acls.TestATSHistoryWithACLs.testDAGACls</li></div><div><li>org.apache.tez.dag.history.ats.acls.TestATSHistoryWithACLs.testDagLoggingDisabled</li></div><div><li>org.apache.tez.dag.history.logging.ats.TestATSHistoryWithMiniCluster.testDisabledACls</li></div><div><li>org.apache.tez.test.TestExceptionPropagation.testExceptionPropagationSession</li></div><div><li>org.apache.tez.test.TestRecovery.testTwoRoundsRecoverying</li></div><div><li>org.apache.tez.analyzer.TestAnalyzer.org.apache.tez.analyzer.TestAnalyzer</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;KILLED&gt; but was:&lt;FAILED&gt;</li></div><div><li>org.apache.tez.dag.api.TezUncheckedException: java.lang.reflect.InvocationTargetException</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.tez.dag.api.TezUncheckedException: java.lang.reflect.InvocationTargetException</li></div><div><li>expected:&lt;READY&gt; but was:&lt;RUNNING&gt;</li></div><div><li>test timed out after 60000 milliseconds</li></div><div><li>test timed out after 60000 milliseconds</li></div><div><li>test timed out after 60000 milliseconds</li></div><div><li>test timed out after 60000 milliseconds</li></div><div><li>test timed out after 120000 milliseconds</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.tez.dag.api.TezUncheckedException: java.lang.reflect.InvocationTargetException</li></div><div><li>Application not running, applicationId=application_1523506859655_0001, yarnApplicationState=FAILED, finalApplicationStatus=FAILED, trackingUrl=N/A, diagnostics=[DAG completed with an ERROR state. Shutting down AM, Session stats:submittedDAGs=9, successfulDAGs=0, failedDAGs=10, killedDAGs=0]</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.tez.dag.api.TezUncheckedException: java.lang.reflect.InvocationTargetException</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;1&gt; but was:&lt;0&gt;</li></div><div><li>java.io.IOException: ApplicationHistoryServer failed to start. Final state is STOPPED</li></div><div><li>test timed out after 50000 milliseconds</li></div><div><li>expected:&lt;200&gt; but was:&lt;401&gt;</li></div><div><li>test timed out after 50000 milliseconds</li></div><div><li>test timed out after 50000 milliseconds</li></div><div><li>expected:&lt;404&gt; but was:&lt;401&gt;</li></div><div><li>test timed out after 50000 milliseconds</li></div><div><li>Application not running, applicationId=application_1523721414718_0001, yarnApplicationState=FAILED, finalApplicationStatus=FAILED, trackingUrl=N/A, diagnostics=[DAG completed with an ERROR state. Shutting down AM, Session stats:submittedDAGs=7, successfulDAGs=0, failedDAGs=8, killedDAGs=0]</li></div><div><li>OrderedWordCount failed</li></div><div><li>java.io.IOException: ApplicationHistoryServer failed to start. Final state is STOPPED</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.tez.dag.app.TestMockDAGAppMaster.testInternalPreemption</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.tez.mapreduce.TestMRRJobsDAGApi.testMultipleMRRSleepJobViaSession</div></li><li><div>org.apache.tez.mapreduce.TestMRRJobsDAGApi.testNonDefaultFSStagingDir</div></li><li><div>org.apache.tez.mapreduce.TestMRRJobsDAGApi.testMRRSleepJobDagSubmit</div></li><li><div>org.apache.tez.mapreduce.TestMRRJobsDAGApi.testMRRSleepJobDagSubmitAndKillViaRPC</div></li><li><div>org.apache.tez.mapreduce.TestMRRJobsDAGApi.testMRRSleepJobDagSubmitAndKill</div></li><li><div>org.apache.tez.mapreduce.TestMRRJobsDAGApi.testAMRelocalizationConflict</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.tez.test.TestExceptionPropagation.testExceptionPropagationSession</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="zeppelin"><div style="font-weight:bold;" class="panel-heading">ZEPPELIN<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Sneha)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>7aa94ce93fbce382d920f01bd27f6e2750a64cf8</div><div><b>Last Run: </b>15-04-2018 19:58 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th><th>PPC RHEL7.5</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 782</div><div>Failed Count : 9</div><div>Skipped Count : 5</div></td><td><div>Total Count : 782</div><div>Failed Count : 8</div><div>Skipped Count : 5</div></td><td><div>Total Count : 782</div><div>Failed Count : 10</div><div>Skipped Count : 5</div></td><td><div>Total Count : 782</div><div>Failed Count : 8</div><div>Skipped Count : 5</div></td><td><div>Total Count : 775</div><div>Failed Count : 10</div><div>Skipped Count : 5</div></td></tr><tr><td>Result</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"><div><li>org.apache.zeppelin.pig.PigInterpreterTest.testIncludeJobStats</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTest.testBasics</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTezTest.testIncludeJobStats</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTezTest.testBasics</li></div><div><li>org.apache.zeppelin.pig.PigQueryInterpreterTest.testMaxResult</li></div><div><li>org.apache.zeppelin.pig.PigQueryInterpreterTest.testBasics</li></div><div><li>org.apache.zeppelin.python.IPythonInterpreterTest.testIPython</li></div><div><li>org.apache.zeppelin.spark.IPySparkInterpreterTest.testBasics</li></div><div><li>org.apache.zeppelin.notebook.NotebookTest.testClearParagraphOutput</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.zeppelin.pig.PigInterpreterTest.testIncludeJobStats</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTest.testBasics</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTezTest.testIncludeJobStats</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTezTest.testBasics</li></div><div><li>org.apache.zeppelin.pig.PigQueryInterpreterTest.testMaxResult</li></div><div><li>org.apache.zeppelin.pig.PigQueryInterpreterTest.testBasics</li></div><div><li>org.apache.zeppelin.python.IPythonInterpreterTest.testIPython</li></div><div><li>org.apache.zeppelin.spark.IPySparkInterpreterTest.testBasics</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.zeppelin.pig.PigInterpreterTest.testIncludeJobStats</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTest.testBasics</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTezTest.testIncludeJobStats</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTezTest.testBasics</li></div><div><li>org.apache.zeppelin.pig.PigQueryInterpreterTest.testMaxResult</li></div><div><li>org.apache.zeppelin.pig.PigQueryInterpreterTest.testBasics</li></div><div><li>org.apache.zeppelin.python.IPythonInterpreterTest.testIPython</li></div><div><li>org.apache.zeppelin.spark.IPySparkInterpreterTest.testBasics</li></div><div><li>org.apache.zeppelin.interpreter.InterpreterOutputChangeWatcherTest.test</li></div><div><li>org.apache.zeppelin.notebook.NotebookTest.testAbortParagraphStatusOnInterpreterRestart</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.zeppelin.pig.PigInterpreterTest.testIncludeJobStats</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTest.testBasics</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTezTest.testIncludeJobStats</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTezTest.testBasics</li></div><div><li>org.apache.zeppelin.pig.PigQueryInterpreterTest.testMaxResult</li></div><div><li>org.apache.zeppelin.pig.PigQueryInterpreterTest.testBasics</li></div><div><li>org.apache.zeppelin.python.IPythonInterpreterTest.testIPython</li></div><div><li>org.apache.zeppelin.spark.IPySparkInterpreterTest.testBasics</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.zeppelin.elasticsearch.ElasticsearchInterpreterTest.org.apache.zeppelin.elasticsearch.ElasticsearchInterpreterTest</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTest.testIncludeJobStats</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTest.testBasics</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTezTest.testIncludeJobStats</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTezTest.testBasics</li></div><div><li>org.apache.zeppelin.pig.PigQueryInterpreterTest.testMaxResult</li></div><div><li>org.apache.zeppelin.pig.PigQueryInterpreterTest.testBasics</li></div><div><li>org.apache.zeppelin.python.IPythonInterpreterTest.testIPython</li></div><div><li>org.apache.zeppelin.spark.DepInterpreterTest.testDefault</li></div><div><li>org.apache.zeppelin.spark.IPySparkInterpreterTest.testBasics</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>org.apache.hadoop.yarn.api.records.LocalResource.setShouldBeUploadedToSharedCache(Z)V</li></div><div><li>org.apache.hadoop.yarn.api.records.LocalResource.setShouldBeUploadedToSharedCache(Z)V</li></div><div><li>expected:&lt;TABLE&gt; but was:&lt;TEXT&gt;</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>No Figure Text</li></div><div><li>expected:&lt;[你好]
&gt; but was:&lt;[??]
&gt;</li></div><div><li>Index: 0, Size: 0</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>org.apache.hadoop.yarn.api.records.LocalResource.setShouldBeUploadedToSharedCache(Z)V</li></div><div><li>org.apache.hadoop.yarn.api.records.LocalResource.setShouldBeUploadedToSharedCache(Z)V</li></div><div><li>expected:&lt;TABLE&gt; but was:&lt;TEXT&gt;</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>No Figure Text</li></div><div><li>expected:&lt;[你好]
&gt; but was:&lt;[??]
&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>org.apache.hadoop.yarn.api.records.LocalResource.setShouldBeUploadedToSharedCache(Z)V</li></div><div><li>org.apache.hadoop.yarn.api.records.LocalResource.setShouldBeUploadedToSharedCache(Z)V</li></div><div><li>expected:&lt;TABLE&gt; but was:&lt;TEXT&gt;</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>expected:&lt;[你好]
&gt; but was:&lt;[??]
&gt;</li></div><div><li>java.lang.AssertionError: null
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertNotNull(Assert.java:712)
	at org.junit.Assert.assertNotNull(Assert.java:722)
	at org.apache.zeppelin.interpreter.InterpreterOutputChangeWatcherTest.test(InterpreterOutputChangeWatcherTest.java:99)
</li></div><div><li>expected:&lt;ABORT&gt; but was:&lt;PENDING&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>org.apache.hadoop.yarn.api.records.LocalResource.setShouldBeUploadedToSharedCache(Z)V</li></div><div><li>org.apache.hadoop.yarn.api.records.LocalResource.setShouldBeUploadedToSharedCache(Z)V</li></div><div><li>expected:&lt;TABLE&gt; but was:&lt;TEXT&gt;</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>expected:&lt;[你好]
&gt; but was:&lt;[??]
&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Failed to bind to [10200]</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>org.apache.hadoop.yarn.api.records.LocalResource.setShouldBeUploadedToSharedCache(Z)V</li></div><div><li>org.apache.hadoop.yarn.api.records.LocalResource.setShouldBeUploadedToSharedCache(Z)V</li></div><div><li>expected:&lt;TABLE&gt; but was:&lt;TEXT&gt;</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>java.lang.NullPointerException: null
	at org.sonatype.aether.impl.internal.DefaultRepositorySystem.resolveDependencies(DefaultRepositorySystem.java:352)
	at org.apache.zeppelin.spark.dep.SparkDependencyContext.fetchArtifactWithDep(SparkDependencyContext.java:171)
	at org.apache.zeppelin.spark.dep.SparkDependencyContext.fetch(SparkDependencyContext.java:121)
	at org.apache.zeppelin.spark.DepInterpr</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.zeppelin.notebook.NotebookTest.testClearParagraphOutput</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.zeppelin.interpreter.InterpreterOutputChangeWatcherTest.test</div></li><li><div>org.apache.zeppelin.notebook.NotebookTest.testAbortParagraphStatusOnInterpreterRestart</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="zookeeper"><div style="font-weight:bold;" class="panel-heading">ZOOKEEPER<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Pravin)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>fbd21329d32f185d198344f2d394175c92a82f9e</div><div><b>Last Run: </b>13-04-2018 03:51 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th><th>PPC RHEL7.5</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 1131</div><div>Failed Count : 0</div><div>Skipped Count : 1</div></td><td><div>Total Count : 1131</div><div>Failed Count : 2</div><div>Skipped Count : 1</div></td><td><div>Total Count : 1127</div><div>Failed Count : 4</div><div>Skipped Count : 1</div></td><td><div>Total Count : 1128</div><div>Failed Count : 5</div><div>Skipped Count : 1</div></td><td><div>Total Count : 1120</div><div>Failed Count : 2</div><div>Skipped Count : 1</div></td></tr><tr><td>Result</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.zookeeper.server.quorum.Zab1_0Test.testNormalFollowerRunWithDiff</li></div><div><li>org.apache.zookeeper.test.SaslAuthFailNotifyTest.testBadSaslAuthNotifiesWatch</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.zookeeper.server.quorum.StandaloneDisabledTest.startSingleServerTest</li></div><div><li>junit.framework.TestSuite.org.apache.zookeeper.server.quorum.auth.MiniKdcTest</li></div><div><li>junit.framework.TestSuite.org.apache.zookeeper.server.quorum.auth.QuorumKerberosAuthTest</li></div><div><li>junit.framework.TestSuite.org.apache.zookeeper.server.quorum.auth.QuorumKerberosHostBasedAuthTest</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.zookeeper.server.quorum.QuorumPeerMainTest.testFailedTxnAsPartOfQuorumLoss</li></div><div><li>org.apache.zookeeper.server.quorum.RaceConditionTest.testRaceConditionBetweenLeaderAndAckRequestProcessor</li></div><div><li>junit.framework.TestSuite.org.apache.zookeeper.server.quorum.auth.MiniKdcTest</li></div><div><li>junit.framework.TestSuite.org.apache.zookeeper.server.quorum.auth.QuorumKerberosAuthTest</li></div><div><li>junit.framework.TestSuite.org.apache.zookeeper.server.quorum.auth.QuorumKerberosHostBasedAuthTest</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.zookeeper.server.quorum.Zab1_0Test.testNormalObserverRun</li></div><div><li>org.apache.zookeeper.test.WatchEventWhenAutoResetTest.testNodeDataChanged</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;4294967298&gt; but was:&lt;0&gt;</li></div><div><li>Timeout occurred. Please note the time in the report does not reflect the time until the timeout.</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Timeout occurred. Please note the time in the report does not reflect the time until the timeout.</li></div><div><li>Unable to parse:includedir /etc/krb5.conf.d/</li></div><div><li>Unable to parse:includedir /etc/krb5.conf.d/</li></div><div><li>Unable to parse:includedir /etc/krb5.conf.d/</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;1&gt; but was:&lt;2&gt;</li></div><div><li>Leader failed to transition to new state. Current state is leading</li></div><div><li>Unable to parse:includedir /etc/krb5.conf.d/</li></div><div><li>Unable to parse:includedir /etc/krb5.conf.d/</li></div><div><li>Unable to parse:includedir /etc/krb5.conf.d/</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Timeout occurred. Please note the time in the report does not reflect the time until the timeout.</li></div><div><li>expected:&lt;NodeDataChanged&gt; but was:&lt;NodeDeleted&gt;</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.zookeeper.server.quorum.Zab1_0Test.testNormalFollowerRunWithDiff</div></li><li><div>org.apache.zookeeper.test.SaslAuthFailNotifyTest.testBadSaslAuthNotifiesWatch</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.zookeeper.server.quorum.StandaloneDisabledTest.startSingleServerTest</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.zookeeper.server.quorum.QuorumPeerMainTest.testFailedTxnAsPartOfQuorumLoss</div></li><li><div>org.apache.zookeeper.server.quorum.RaceConditionTest.testRaceConditionBetweenLeaderAndAckRequestProcessor</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div id="ppcubuntu16" style="font-weight:bold;font-size:12;display:none" class="panel panel-info" name="summary"><div class="panel-heading"><div class="panel-title">PPC UBUNTU16 SUMMARY</div></div><div class="bs-callout bs-callout-info"><div><b>OS: </b>UBUNTU 16.04</div></div><table style="font-size:14" class="table table-striped"><tbody><tr><th>Package Name</th><th>Result</th><th></th><th></th></tr><tr><td><a href="#" id="anchor_accumulo" onclick="showme(this.id);">ACCUMULO</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_ambari" onclick="showme(this.id);">AMBARI</a></td><td><img src="resources/red.png" align="top" style="width: 16px; height: 16px;" title="FAILURE"></img>0</td></tr><tr><td><a href="#" id="anchor_atlas" onclick="showme(this.id);">ATLAS</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td></tr><tr><td><a href="#" id="anchor_falcon" onclick="showme(this.id);">FALCON</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_flume" onclick="showme(this.id);">FLUME</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_hadooptmp" onclick="showme(this.id);">HADOOP</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>34 (9)</td></tr><tr><td><a href="#" id="anchor_hbase" onclick="showme(this.id);">HBASE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (1)</td></tr><tr><td><a href="#" id="anchor_hive" onclick="showme(this.id);">HIVE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>13 (2)</td></tr><tr><td><a href="#" id="anchor_kafka" onclick="showme(this.id);">KAFKA</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_knox" onclick="showme(this.id);">KNOX</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_metron" onclick="showme(this.id);">METRON</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_oozie" onclick="showme(this.id);">OOZIE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>30 (28)</td></tr><tr><td><a href="#" id="anchor_phoenix" onclick="showme(this.id);">PHOENIX</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_pig" onclick="showme(this.id);">PIG</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>20</td></tr><tr><td><a href="#" id="anchor_ranger" onclick="showme(this.id);">RANGER</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_slider" onclick="showme(this.id);">SLIDER</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>24</td></tr><tr><td><a href="#" id="anchor_spark" onclick="showme(this.id);">SPARK</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (1)</td></tr><tr><td><a href="#" id="anchor_sqoop" onclick="showme(this.id);">SQOOP</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_storm" onclick="showme(this.id);">STORM</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_tez" onclick="showme(this.id);">TEZ</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (1)</td></tr><tr><td><a href="#" id="anchor_zeppelin" onclick="showme(this.id);">ZEPPELIN</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>9 (1)</td></tr><tr><td><a href="#" id="anchor_zookeeper" onclick="showme(this.id);">ZOOKEEPER</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr></tbody></table></div><div id="x86ubuntu16" style="font-weight:bold;font-size:12;display:none" class="panel panel-info" name="summary"><div class="panel-heading"><div class="panel-title">X86 UBUNTU16 SUMMARY</div></div><div class="bs-callout bs-callout-info"><div><b>OS: </b>UBUNTU 16.04</div></div><table style="font-size:14" class="table table-striped"><tbody><tr><th>Package Name</th><th>Result</th><th></th><th></th></tr><tr><td><a href="#" id="anchor_accumulo" onclick="showme(this.id);">ACCUMULO</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (2)</td></tr><tr><td><a href="#" id="anchor_ambari" onclick="showme(this.id);">AMBARI</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td></tr><tr><td><a href="#" id="anchor_atlas" onclick="showme(this.id);">ATLAS</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_falcon" onclick="showme(this.id);">FALCON</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3 (3)</td></tr><tr><td><a href="#" id="anchor_flume" onclick="showme(this.id);">FLUME</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_hadooptmp" onclick="showme(this.id);">HADOOP</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>35 (10)</td></tr><tr><td><a href="#" id="anchor_hbase" onclick="showme(this.id);">HBASE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>6 (5)</td></tr><tr><td><a href="#" id="anchor_hive" onclick="showme(this.id);">HIVE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>12 (1)</td></tr><tr><td><a href="#" id="anchor_kafka" onclick="showme(this.id);">KAFKA</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_knox" onclick="showme(this.id);">KNOX</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td></tr><tr><td><a href="#" id="anchor_metron" onclick="showme(this.id);">METRON</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_oozie" onclick="showme(this.id);">OOZIE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>6 (4)</td></tr><tr><td><a href="#" id="anchor_phoenix" onclick="showme(this.id);">PHOENIX</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_pig" onclick="showme(this.id);">PIG</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>20</td></tr><tr><td><a href="#" id="anchor_ranger" onclick="showme(this.id);">RANGER</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_slider" onclick="showme(this.id);">SLIDER</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>24</td></tr><tr><td><a href="#" id="anchor_spark" onclick="showme(this.id);">SPARK</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td></tr><tr><td><a href="#" id="anchor_sqoop" onclick="showme(this.id);">SQOOP</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_storm" onclick="showme(this.id);">STORM</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (2)</td></tr><tr><td><a href="#" id="anchor_tez" onclick="showme(this.id);">TEZ</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>7 (6)</td></tr><tr><td><a href="#" id="anchor_zeppelin" onclick="showme(this.id);">ZEPPELIN</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>8</td></tr><tr><td><a href="#" id="anchor_zookeeper" onclick="showme(this.id);">ZOOKEEPER</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (2)</td></tr></tbody></table></div><div id="ppcrhel75" style="font-weight:bold;font-size:12;display:none" class="panel panel-info" name="summary"><div class="panel-heading"><div class="panel-title">PPC RHEL75 SUMMARY</div></div><div class="bs-callout bs-callout-info"><div><b>OS: </b>RHEL 7.5</div></div><table style="font-size:14" class="table table-striped"><tbody><tr><th>Package Name</th><th>Result</th><th></th><th></th></tr><tr><td><a href="#" id="anchor_accumulo" onclick="showme(this.id);">ACCUMULO</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_ambari" onclick="showme(this.id);">AMBARI</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>5</td></tr><tr><td><a href="#" id="anchor_atlas" onclick="showme(this.id);">ATLAS</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_falcon" onclick="showme(this.id);">FALCON</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_flume" onclick="showme(this.id);">FLUME</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td></tr><tr><td><a href="#" id="anchor_hadooptmp" onclick="showme(this.id);">HADOOP</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>99</td></tr><tr><td><a href="#" id="anchor_hbase" onclick="showme(this.id);">HBASE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>5</td></tr><tr><td><a href="#" id="anchor_hive" onclick="showme(this.id);">HIVE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>4</td></tr><tr><td><a href="#" id="anchor_kafka" onclick="showme(this.id);">KAFKA</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td></tr><tr><td><a href="#" id="anchor_knox" onclick="showme(this.id);">KNOX</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_metron" onclick="showme(this.id);">METRON</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_oozie" onclick="showme(this.id);">OOZIE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>16</td></tr><tr><td><a href="#" id="anchor_phoenix" onclick="showme(this.id);">PHOENIX</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_pig" onclick="showme(this.id);">PIG</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_ranger" onclick="showme(this.id);">RANGER</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_slider" onclick="showme(this.id);">SLIDER</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_spark" onclick="showme(this.id);">SPARK</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>404</td></tr><tr><td><a href="#" id="anchor_sqoop" onclick="showme(this.id);">SQOOP</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_storm" onclick="showme(this.id);">STORM</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td></tr><tr><td><a href="#" id="anchor_tez" onclick="showme(this.id);">TEZ</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>11</td></tr><tr><td><a href="#" id="anchor_zeppelin" onclick="showme(this.id);">ZEPPELIN</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>10</td></tr><tr><td><a href="#" id="anchor_zookeeper" onclick="showme(this.id);">ZOOKEEPER</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2</td></tr></tbody></table></div><div id="ppcrhel7" style="font-weight:bold;font-size:12;display:none" class="panel panel-info" name="summary"><div class="panel-heading"><div class="panel-title">PPC RHEL7 SUMMARY</div></div><div class="bs-callout bs-callout-info"><div><b>OS: </b>RHEL 7.2</div></div><table style="font-size:14" class="table table-striped"><tbody><tr><th>Package Name</th><th>Result</th><th></th><th></th></tr><tr><td><a href="#" id="anchor_accumulo" onclick="showme(this.id);">ACCUMULO</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_ambari" onclick="showme(this.id);">AMBARI</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (2)</td></tr><tr><td><a href="#" id="anchor_atlas" onclick="showme(this.id);">ATLAS</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_falcon" onclick="showme(this.id);">FALCON</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_flume" onclick="showme(this.id);">FLUME</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td></tr><tr><td><a href="#" id="anchor_hadooptmp" onclick="showme(this.id);">HADOOP</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>15 (13)</td></tr><tr><td><a href="#" id="anchor_hbase" onclick="showme(this.id);">HBASE</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_hive" onclick="showme(this.id);">HIVE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3 (2)</td></tr><tr><td><a href="#" id="anchor_kafka" onclick="showme(this.id);">KAFKA</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_knox" onclick="showme(this.id);">KNOX</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_metron" onclick="showme(this.id);">METRON</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_oozie" onclick="showme(this.id);">OOZIE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td></tr><tr><td><a href="#" id="anchor_phoenix" onclick="showme(this.id);">PHOENIX</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_pig" onclick="showme(this.id);">PIG</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_ranger" onclick="showme(this.id);">RANGER</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_slider" onclick="showme(this.id);">SLIDER</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_spark" onclick="showme(this.id);">SPARK</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3 (2)</td></tr><tr><td><a href="#" id="anchor_sqoop" onclick="showme(this.id);">SQOOP</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_storm" onclick="showme(this.id);">STORM</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (1)</td></tr><tr><td><a href="#" id="anchor_tez" onclick="showme(this.id);">TEZ</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (1)</td></tr><tr><td><a href="#" id="anchor_zeppelin" onclick="showme(this.id);">ZEPPELIN</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>10 (2)</td></tr><tr><td><a href="#" id="anchor_zookeeper" onclick="showme(this.id);">ZOOKEEPER</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>4 (1)</td></tr></tbody></table></div><div id="x86rhel7" style="font-weight:bold;font-size:12;display:none" class="panel panel-info" name="summary"><div class="panel-heading"><div class="panel-title">X86 RHEL7 SUMMARY</div></div><div class="bs-callout bs-callout-info"><div><b>OS: </b>RHEL 7.2</div></div><table style="font-size:14" class="table table-striped"><tbody><tr><th>Package Name</th><th>Result</th><th></th><th></th></tr><tr><td><a href="#" id="anchor_accumulo" onclick="showme(this.id);">ACCUMULO</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_ambari" onclick="showme(this.id);">AMBARI</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_atlas" onclick="showme(this.id);">ATLAS</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_falcon" onclick="showme(this.id);">FALCON</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_flume" onclick="showme(this.id);">FLUME</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td></tr><tr><td><a href="#" id="anchor_hadooptmp" onclick="showme(this.id);">HADOOP</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>33 (31)</td></tr><tr><td><a href="#" id="anchor_hbase" onclick="showme(this.id);">HBASE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td></tr><tr><td><a href="#" id="anchor_hive" onclick="showme(this.id);">HIVE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (1)</td></tr><tr><td><a href="#" id="anchor_kafka" onclick="showme(this.id);">KAFKA</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (2)</td></tr><tr><td><a href="#" id="anchor_knox" onclick="showme(this.id);">KNOX</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_metron" onclick="showme(this.id);">METRON</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_oozie" onclick="showme(this.id);">OOZIE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (1)</td></tr><tr><td><a href="#" id="anchor_phoenix" onclick="showme(this.id);">PHOENIX</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_pig" onclick="showme(this.id);">PIG</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_ranger" onclick="showme(this.id);">RANGER</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_slider" onclick="showme(this.id);">SLIDER</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_spark" onclick="showme(this.id);">SPARK</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td></tr><tr><td><a href="#" id="anchor_sqoop" onclick="showme(this.id);">SQOOP</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_storm" onclick="showme(this.id);">STORM</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (1)</td></tr><tr><td><a href="#" id="anchor_tez" onclick="showme(this.id);">TEZ</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td></tr><tr><td><a href="#" id="anchor_zeppelin" onclick="showme(this.id);">ZEPPELIN</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>8</td></tr><tr><td><a href="#" id="anchor_zookeeper" onclick="showme(this.id);">ZOOKEEPER</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>5 (2)</td></tr></tbody></table></div><div id="ppcx86" style="display:block;font-weight:bold" class="panel panel-info" name="summary"><div class="panel-heading"><div class="panel-title">FULL SUMMARY</div></div><table style="font-size:15" class="table table-striped"><tbody><tr><th></th></tr><tr><th>Package Name</th><th>PPC UBUNTU16</th><th>x86 UBUNTU16</th><th>PPC RHEL7</th><th>x86 RHEL7</th><th>PPC RHEL75</th></tr><tr><td><a href="#" id="anchor_accumulo" onclick="showme(this.id);">ACCUMULO</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (2)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_ambari" onclick="showme(this.id);">AMBARI</a></td><td><img src="resources/red.png" align="top" style="width: 16px; height: 16px;" title="FAILURE"></img>0</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (2)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>5</td></tr><tr><td><a href="#" id="anchor_atlas" onclick="showme(this.id);">ATLAS</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_falcon" onclick="showme(this.id);">FALCON</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3 (3)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_flume" onclick="showme(this.id);">FLUME</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td></tr><tr><td><a href="#" id="anchor_hadooptmp" onclick="showme(this.id);">HADOOP</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>34 (9)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>35 (10)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>15 (13)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>33 (31)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>99</td></tr><tr><td><a href="#" id="anchor_hbase" onclick="showme(this.id);">HBASE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>6 (5)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>5</td></tr><tr><td><a href="#" id="anchor_hive" onclick="showme(this.id);">HIVE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>13 (2)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>12 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3 (2)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>4</td></tr><tr><td><a href="#" id="anchor_kafka" onclick="showme(this.id);">KAFKA</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (2)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td></tr><tr><td><a href="#" id="anchor_knox" onclick="showme(this.id);">KNOX</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_metron" onclick="showme(this.id);">METRON</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_oozie" onclick="showme(this.id);">OOZIE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>30 (28)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>6 (4)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>16</td></tr><tr><td><a href="#" id="anchor_phoenix" onclick="showme(this.id);">PHOENIX</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_pig" onclick="showme(this.id);">PIG</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>20</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>20</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_ranger" onclick="showme(this.id);">RANGER</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_slider" onclick="showme(this.id);">SLIDER</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>24</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>24</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_spark" onclick="showme(this.id);">SPARK</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3 (2)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>404</td></tr><tr><td><a href="#" id="anchor_sqoop" onclick="showme(this.id);">SQOOP</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_storm" onclick="showme(this.id);">STORM</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (2)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td></tr><tr><td><a href="#" id="anchor_tez" onclick="showme(this.id);">TEZ</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>7 (6)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>11</td></tr><tr><td><a href="#" id="anchor_zeppelin" onclick="showme(this.id);">ZEPPELIN</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>9 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>8</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>10 (2)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>8</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>10</td></tr><tr><td><a href="#" id="anchor_zookeeper" onclick="showme(this.id);">ZOOKEEPER</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (2)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>4 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>5 (2)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2</td></tr></tbody></table></div></div></body></html>