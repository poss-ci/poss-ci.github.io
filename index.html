<html><head><script src="resources/jquery.min.js"></script><link href="resources/bootstrap.min.css" rel="stylesheet"></link><link href="resources/bootstrap-theme.min.css" rel="stylesheet"></link><script src="resources/bootstrap.min.js"></script><script src="helper.js">function hideAll(){console.log("hideAll")}function showme(e){console.log("showme");var l,n=e.substring(7),o=document.getElementsByName("data");for(l=0;l&lt;o.length;l++)o[l].style.display="none";var t=document.getElementsByName("summary");for(l=0;l&lt;t.length;l++)t[l].style.display="none";document.getElementById(n).style.display="block"}</script><style>table, th, td { vertical-align:top; padding: 3px} table {table-layout:fixed} td {word-wrap:break-word} .bs-callout { padding: 5px; margin: 5px 0; border: 1px solid #eee; border-left-width: 5px; border-radius: 3px; font-weight:normal; }.bs-callout-info {border-left-color: #5bc0de;}</style></head><body><nav style="background-color: #F0F8FF;" class="navbar navbar-light"><div class="container-fluid"><ul class="nav nav-pills"><li role="presentation"><a style="font-weight:bold" href="#" id="anchor_ppcx86" onclick="showme(this.id);">FULL SUMMARY</a></li><li role="presentation"><a style="font-weight:bold" href="#" id="anchor_ppcubuntu16" onclick="showme(this.id);">PPC UBUNTU16</a></li><li role="presentation"><a style="font-weight:bold" href="#" id="anchor_x86ubuntu16" onclick="showme(this.id);">X86 UBUNTU16</a></li><li role="presentation"><a style="font-weight:bold" href="#" id="anchor_ppcrhel7" onclick="showme(this.id);">PPC RHEL7</a></li><li role="presentation"><a style="font-weight:bold" href="#" id="anchor_x86rhel7" onclick="showme(this.id);">X86 RHEL7</a></li><li role="presentation"><a style="font-weight:bold" href="#" id="anchor_ppcrhel75" onclick="showme(this.id);">PPC RHEL75</a></li><li role="presentation"><a style="font-weight:bold" href="#" id="anchor_developers" onclick="showme(this.id);">DEVELOPERS</a></li><p align="right" role="presentation" style="color:grey;padding-top:10px">Report Date: 09-05-2018 11:30 UTC</p></ul><div align="right" style="color:grey;font-size:12">Notations:<img src="resources/red.png" style="width: 16px; height: 16px;">Build failed </img><img src="resources/blue.png" style="width: 16px; height: 16px;">Build success with no failure </img><img src="resources/yellow.png" style="width: 16px; height: 16px;">N (M) Build success with N test failures &amp; M unique failures </img></div></div></nav><div style="table-cell" class="col-sm-2 col-md-2 sidebar"><div class="list-group"><a href="#" class="list-group-item list-group-item-action active" onclick="showme(this.id);" id="anchor_ppcx86">Packages</a><a class="list-group-item list-group-item-action" href="#" id="anchor_accumulo" onclick="showme(this.id);" title="Owned by Sonia">ACCUMULO</a><a class="list-group-item list-group-item-action" href="#" id="anchor_ambari" onclick="showme(this.id);" title="Owned by Valencia">AMBARI</a><a class="list-group-item list-group-item-action" href="#" id="anchor_atlas" onclick="showme(this.id);" title="Owned by Yussuf">ATLAS</a><a class="list-group-item list-group-item-action" href="#" id="anchor_falcon" onclick="showme(this.id);" title="Owned by Sonia">FALCON</a><a class="list-group-item list-group-item-action" href="#" id="anchor_flume" onclick="showme(this.id);" title="Owned by Pravin">FLUME</a><a class="list-group-item list-group-item-action" href="#" id="anchor_hadoop" onclick="showme(this.id);" title="Owned by Parita">HADOOP</a><a class="list-group-item list-group-item-action" href="#" id="anchor_hbase" onclick="showme(this.id);" title="Owned by Valencia">HBASE</a><a class="list-group-item list-group-item-action" href="#" id="anchor_hive" onclick="showme(this.id);" title="Owned by Alisha/Pravin">HIVE</a><a class="list-group-item list-group-item-action" href="#" id="anchor_kafka" onclick="showme(this.id);" title="Owned by Sonia">KAFKA</a><a class="list-group-item list-group-item-action" href="#" id="anchor_knox" onclick="showme(this.id);" title="Owned by Yussuf">KNOX</a><a class="list-group-item list-group-item-action" href="#" id="anchor_metron" onclick="showme(this.id);" title="Owned by Pravin">METRON</a><a class="list-group-item list-group-item-action" href="#" id="anchor_oozie" onclick="showme(this.id);" title="Owned by Alisha">OOZIE</a><a class="list-group-item list-group-item-action" href="#" id="anchor_phoenix" onclick="showme(this.id);" title="Owned by Valencia">PHOENIX</a><a class="list-group-item list-group-item-action" href="#" id="anchor_pig" onclick="showme(this.id);" title="Owned by Yussuf">PIG</a><a class="list-group-item list-group-item-action" href="#" id="anchor_ranger" onclick="showme(this.id);" title="Owned by Sneha">RANGER</a><a class="list-group-item list-group-item-action" href="#" id="anchor_slider" onclick="showme(this.id);" title="Owned by Yussuf">SLIDER</a><a class="list-group-item list-group-item-action" href="#" id="anchor_spark" onclick="showme(this.id);" title="Owned by Parita">SPARK</a><a class="list-group-item list-group-item-action" href="#" id="anchor_sqoop" onclick="showme(this.id);" title="Owned by Talat">SQOOP</a><a class="list-group-item list-group-item-action" href="#" id="anchor_storm" onclick="showme(this.id);" title="Owned by Parita">STORM</a><a class="list-group-item list-group-item-action" href="#" id="anchor_tez" onclick="showme(this.id);" title="Owned by Valencia">TEZ</a><a class="list-group-item list-group-item-action" href="#" id="anchor_zeppelin" onclick="showme(this.id);" title="Owned by Sneha">ZEPPELIN</a><a class="list-group-item list-group-item-action" href="#" id="anchor_zookeeper" onclick="showme(this.id);" title="Owned by Pravin">ZOOKEEPER</a></div></div><div style="display: table-cell"><div id="developers" style="display:block;font-weight:bold;display:none;" class="panel panel-info" name="summary"><div class="panel-heading"><div class="panel-title">DEVELOPERS</div></div><div class="panel-body"><table style="font-size:15" id="summarytable" class="table table-striped"><tr><td style="width: 100px;font-weight:bold">ALISHA</td><td><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_oozie">OOZIE </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_hive">HIVE </button></td></tr><tr><td style="width: 100px;font-weight:bold">PARITA</td><td><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_hadoop">HADOOP </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_storm">STORM </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_spark">SPARK </button></td></tr><tr><td style="width: 100px;font-weight:bold">PRAVIN</td><td><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_metron">METRON </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_zookeeper">ZOOKEEPER </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_flume">FLUME </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_hive">HIVE </button></td></tr><tr><td style="width: 100px;font-weight:bold">SONIA</td><td><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_kafka">KAFKA </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_accumulo">ACCUMULO </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_falcon">FALCON </button></td></tr><tr><td style="width: 100px;font-weight:bold">SNEHA</td><td><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_zeppelin">ZEPPELIN </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_ranger">RANGER </button></td></tr><tr><td style="width: 100px;font-weight:bold">TALAT</td><td><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_sqoop">SQOOP </button></td></tr><tr><td style="width: 100px;font-weight:bold">VALENCIA</td><td><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_tez">TEZ </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_hbase">HBASE </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_phoenix">PHOENIX </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_ambari">AMBARI </button></td></tr><tr><td style="width: 100px;font-weight:bold">YUSSUF</td><td><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_knox">KNOX </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_atlas">ATLAS </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_slider">SLIDER </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_pig">PIG </button></td></tr></table></div></div></div><div style="display: table-cell"><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="accumulo"><div style="font-weight:bold;" class="panel-heading">ACCUMULO<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Sonia)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>92cf0ba395964d41c5cc76c44b1168d1c01fba65</div><div><b>Last Run: </b>09-05-2018 05:07 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th><th>PPC RHEL7.5</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 1722</div><div>Failed Count : 0</div><div>Skipped Count : 6</div></td><td><div>Total Count : 1722</div><div>Failed Count : 2</div><div>Skipped Count : 6</div></td><td><div>Total Count : 1722</div><div>Failed Count : 2</div><div>Skipped Count : 6</div></td><td><div>Total Count : 1722</div><div>Failed Count : 0</div><div>Skipped Count : 6</div></td><td><div>Total Count : 1705</div><div>Failed Count : 0</div><div>Skipped Count : 6</div></td></tr><tr><td>Result</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.accumulo.minicluster.MiniAccumuloClusterTest.testPerTableClasspath</li></div><div><li>org.apache.accumulo.minicluster.MiniAccumuloClusterTest.test</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.accumulo.core.client.impl.NamespaceTest.testCacheDecreasesAfterGC</li></div><div><li>org.apache.accumulo.core.client.impl.TableTest.testCacheDecreasesAfterGC</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>test timed out after 60000 milliseconds</li></div><div><li>test timed out after 30000 milliseconds</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Cache did not decrease with GC.</li></div><div><li>Cache did not decrease with GC.</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.accumulo.minicluster.MiniAccumuloClusterTest.testPerTableClasspath</div></li><li><div>org.apache.accumulo.minicluster.MiniAccumuloClusterTest.test</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.accumulo.core.client.impl.NamespaceTest.testCacheDecreasesAfterGC</div></li><li><div>org.apache.accumulo.core.client.impl.TableTest.testCacheDecreasesAfterGC</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="ambari"><div style="font-weight:bold;" class="panel-heading">AMBARI<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Valencia)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> origin/trunk</div><div><b>Last Revision: </b>c49e7704b69b758ec315748b39307fec426f593d</div><div><b>Last Run: </b>02-05-2018 00:31 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th><th>PPC RHEL7.5</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 0</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 5486</div><div>Failed Count : 2</div><div>Skipped Count : 82</div></td><td><div>Total Count : 5486</div><div>Failed Count : 1</div><div>Skipped Count : 82</div></td><td><div>Total Count : 5486</div><div>Failed Count : 2</div><div>Skipped Count : 82</div></td><td><div>Total Count : 5491</div><div>Failed Count : 2</div><div>Skipped Count : 84</div></td></tr><tr><td>Result</td><td><img src="resources/red.png" align="top" style="width: 16px; height: 16px;"></img>FAILURE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.ambari.server.security.encryption.CredentialStoreTest.testInMemoryCredentialStoreService_CredentialExpired</li></div><div><li>org.apache.ambari.server.state.cluster.ClusterDeadlockTest.testDeadlockWhileRestartingComponents</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.ambari.server.controller.AmbariCustomCommandExecutionHelperTest.testIsTopologyRefreshRequired</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.ambari.server.api.services.AmbariMetaInfoTest.testLatestVdf</li></div><div><li>org.apache.ambari.server.api.services.AmbariMetaInfoTest.testAlertDefinitionMerging</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.ambari.server.controller.metrics.RestMetricsPropertyProviderTest.testRestMetricsPropertyProviderAsAdministrator</li></div><div><li>org.apache.ambari.server.security.encryption.CredentialStoreTest.testInMemoryCredentialStoreService_CredentialExpired</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>java.lang.NullPointerException
	at org.apache.ambari.server.security.encryption.CredentialStoreTest.getExpiredCredentialTest(CredentialStoreTest.java:169)
	at org.apache.ambari.server.security.encryption.CredentialStoreTest.testInMemoryCredentialStoreService_CredentialExpired(CredentialStoreTest.java:90)
</li></div><div><li>[Deadlocked Thread:
------------------
"Thread-22" Id=54 WAITING on java.util.concurrent.locks.ReentrantReadWriteLock$NonfairSync@1b8e1a54
 at sun.misc.Unsafe.park(Native Method)
 -  waiting on java.util.concurrent.locks.ReentrantReadWriteLock$NonfairSync@1b8e1a54
 at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
 at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAnd</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>missing behavior definition for the preceding method call:
ConfigHelper.getCredentialStoreEnabledProperties(HDP-2.0.6, org.apache.ambari.server.state.ServiceImpl$$EnhancerByGuice$$e4706bd8@172153ef)
Usage is: expect(a.foo()).andXXX()</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>java.lang.AssertionError
	at org.apache.ambari.server.api.services.AmbariMetaInfoTest.testLatestVdf(AmbariMetaInfoTest.java:1529)
</li></div><div><li>expected:&lt;13&gt; but was:&lt;26&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;400&gt; but was:&lt;349&gt;</li></div><div><li>java.lang.NullPointerException
	at org.apache.ambari.server.security.encryption.CredentialStoreTest.getExpiredCredentialTest(CredentialStoreTest.java:169)
	at org.apache.ambari.server.security.encryption.CredentialStoreTest.testInMemoryCredentialStoreService_CredentialExpired(CredentialStoreTest.java:90)
</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.ambari.server.security.encryption.CredentialStoreTest.testInMemoryCredentialStoreService_CredentialExpired</div></li><li><div>org.apache.ambari.server.state.cluster.ClusterDeadlockTest.testDeadlockWhileRestartingComponents</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.ambari.server.controller.AmbariCustomCommandExecutionHelperTest.testIsTopologyRefreshRequired</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.ambari.server.api.services.AmbariMetaInfoTest.testLatestVdf</div></li><li><div>org.apache.ambari.server.api.services.AmbariMetaInfoTest.testAlertDefinitionMerging</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="atlas"><div style="font-weight:bold;" class="panel-heading">ATLAS<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Yussuf)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>9e3af41bf87f4c1b1c6181fd28ac219edbf49883</div><div><b>Last Run: </b>08-05-2018 11:55 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th><th>PPC RHEL7.5</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 892</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 892</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 892</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 892</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 891</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td></tr><tr><td>Result</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="falcon"><div style="font-weight:bold;" class="panel-heading">FALCON<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Sonia)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>411d90c2ffb59559770d23d4bd2f7675e46392e6</div><div><b>Last Run: </b>09-05-2018 03:27 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th><th>PPC RHEL7.5</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 1000</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 1000</div><div>Failed Count : 1</div><div>Skipped Count : 7</div></td><td><div>Total Count : 1006</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 1003</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 1003</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td></tr><tr><td>Result</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.falcon.entity.v0.EntityGraphTest.initConfigStore</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>Unable to restore configurations for entity type PROCESS</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.falcon.entity.v0.EntityGraphTest.initConfigStore</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="flume"><div style="font-weight:bold;" class="panel-heading">FLUME<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Pravin)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/trunk</div><div><b>Last Revision: </b>62c59a68dad22b3ce6145c1a4ae26ffa3418fe32</div><div><b>Last Run: </b>08-05-2018 11:54 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th><th>PPC RHEL7.5</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 1208</div><div>Failed Count : 0</div><div>Skipped Count : 6</div></td><td><div>Total Count : 1208</div><div>Failed Count : 0</div><div>Skipped Count : 6</div></td><td><div>Total Count : 1208</div><div>Failed Count : 1</div><div>Skipped Count : 6</div></td><td><div>Total Count : 1208</div><div>Failed Count : 1</div><div>Skipped Count : 6</div></td><td><div>Total Count : 1207</div><div>Failed Count : 1</div><div>Skipped Count : 6</div></td></tr><tr><td>Result</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.flume.channel.file.TestLog.testMinimumRequiredSpaceTooSmallForPut</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.flume.channel.file.TestLog.testMinimumRequiredSpaceTooSmallForPut</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.flume.channel.file.TestLog.testMinimumRequiredSpaceTooSmallForPut</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>java.lang.AssertionError
	at org.apache.flume.channel.file.TestLog.doTestMinimumRequiredSpaceTooSmallForPut(TestLog.java:241)
	at org.apache.flume.channel.file.TestLog.testMinimumRequiredSpaceTooSmallForPut(TestLog.java:210)
</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>java.lang.AssertionError
	at org.apache.flume.channel.file.TestLog.doTestMinimumRequiredSpaceTooSmallForPut(TestLog.java:241)
	at org.apache.flume.channel.file.TestLog.testMinimumRequiredSpaceTooSmallForPut(TestLog.java:210)
</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>java.lang.AssertionError
	at org.apache.flume.channel.file.TestLog.doTestMinimumRequiredSpaceTooSmallForPut(TestLog.java:241)
	at org.apache.flume.channel.file.TestLog.testMinimumRequiredSpaceTooSmallForPut(TestLog.java:210)
</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="hadoop"><div style="font-weight:bold;" class="panel-heading">HADOOP<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Parita)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/trunk</div><div><b>Last Revision: </b>5b11b9fd413470e134ecdc7c50468f8c7b39fa50</div><div><b>Last Run: </b>07-05-2018 15:31 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th><th>PPC RHEL7.5</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 19443</div><div>Failed Count : 19</div><div>Skipped Count : 1160</div></td><td><div>Total Count : 19287</div><div>Failed Count : 43</div><div>Skipped Count : 1160</div></td><td><div>Total Count : 19508</div><div>Failed Count : 17</div><div>Skipped Count : 1160</div></td><td><div>Total Count : 19544</div><div>Failed Count : 11</div><div>Skipped Count : 1160</div></td><td><div>Total Count : 19370</div><div>Failed Count : 416</div><div>Skipped Count : 1158</div></td></tr><tr><td>Result</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.util.TestShell.testEnvVarsWithInheritance</li></div><div><li>org.apache.hadoop.hdfs.server.blockmanagement.TestBlockStatsMXBean.testStorageTypeStatsWhenStorageFailed</li></div><div><li>org.apache.hadoop.hdfs.server.blockmanagement.TestUnderReplicatedBlocks.testSetRepIncWithUnderReplicatedBlocks</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting.testDataNodeReconfigureWithVolumeFailures</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting.testVolFailureStatsPreservedOnNNRestart</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testAuthUrlConnectTimeout[timeoutSource=ConnectionFactory]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testConnectTimeout[timeoutSource=ConnectionFactory]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testRedirectConnectTimeout[timeoutSource=ConnectionFactory]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testTwoStepWriteConnectTimeout[timeoutSource=ConnectionFactory]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testAuthUrlConnectTimeout[timeoutSource=Configuration]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testConnectTimeout[timeoutSource=Configuration]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testRedirectConnectTimeout[timeoutSource=Configuration]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testTwoStepWriteConnectTimeout[timeoutSource=Configuration]</li></div><div><li>org.apache.hadoop.yarn.service.monitor.probe.TestDefaultProbe.testDefaultProbe[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMProxy.testAMRMProxyTokenRenewal</li></div><div><li>org.apache.hadoop.yarn.util.TestProcfsBasedProcessTree.testProcessTree</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.testContainerUpgradeSuccessExplicitRollback</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.TestRMRestart.testRMRestartRecoveringNodeLabelManager[CAPACITY]</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestQueueManagementDynamicEditPolicy.testEditSchedule</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.security.TestGroupsCaching.testThreadNotBlockedWhenExpiredEntryExistsWithBackgroundRefresh</li></div><div><li>org.apache.hadoop.util.TestShell.testEnvVarsWithInheritance</li></div><div><li>org.apache.hadoop.fs.http.client.TestHttpFSFWithWebhdfsFileSystem.testOperation[0]</li></div><div><li>org.apache.hadoop.fs.http.client.TestHttpFSFWithWebhdfsFileSystem.testOperationDoAs[0]</li></div><div><li>org.apache.hadoop.fs.contract.router.web.TestRouterWebHDFSContractAppend.testRenameFileBeingAppended</li></div><div><li>org.apache.hadoop.hdfs.TestDFSClientRetries.testLeaseRenewAndDFSOutputStreamDeadLock</li></div><div><li>org.apache.hadoop.hdfs.TestDFSClientRetries.testLeaseRenewSocketTimeout</li></div><div><li>org.apache.hadoop.hdfs.TestDFSInotifyEventInputStreamKerberized.testWithKerberizedCluster</li></div><div><li>org.apache.hadoop.hdfs.TestDFSStripedInputStreamWithRandomECPolicy.testPread</li></div><div><li>org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureWithRandomECPolicy.testCloseWithExceptionsInStreamer</li></div><div><li>org.apache.hadoop.hdfs.TestReadStripedFileWithMissingBlocks.testReadFileWithMissingBlocks</li></div><div><li>org.apache.hadoop.hdfs.client.impl.TestBlockReaderLocal.testStatisticsForErasureCodingRead</li></div><div><li>org.apache.hadoop.hdfs.server.blockmanagement.TestBlockStatsMXBean.testStorageTypeStatsWhenStorageFailed</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.TestDataNodeMultipleRegistrations.testClusterIdMismatchAtStartupWithHA</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.TestDataNodeMultipleRegistrations.test2NNRegistration</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailure.testUnderReplicationAfterVolFailure</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting.testSuccessiveVolumeFailures</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting.testHotSwapOutFailedVolumeAndReporting</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting.testMultipleVolFailuresOnNode</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestLazyPersistReplicaRecovery.testDnRestartWithSavedReplicas</li></div><div><li>org.apache.hadoop.hdfs.server.namenode.TestDecommissioningStatus.testDecommissionStatus</li></div><div><li>org.apache.hadoop.hdfs.server.namenode.TestNamenodeCapacityReport.testXceiverCount</li></div><div><li>org.apache.hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA.testUpdatePipeline</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testAuthUrlConnectTimeout[timeoutSource=ConnectionFactory]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testConnectTimeout[timeoutSource=ConnectionFactory]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testRedirectConnectTimeout[timeoutSource=ConnectionFactory]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testTwoStepWriteConnectTimeout[timeoutSource=ConnectionFactory]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testAuthUrlConnectTimeout[timeoutSource=Configuration]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testConnectTimeout[timeoutSource=Configuration]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testRedirectConnectTimeout[timeoutSource=Configuration]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testTwoStepWriteConnectTimeout[timeoutSource=Configuration]</li></div><div><li>org.apache.hadoop.hdfs.TestNNBench.testNNBenchCreateReadAndDelete</li></div><div><li>org.apache.hadoop.mapred.TestMRTimelineEventHandling.testMRTimelineEventHandling</li></div><div><li>org.apache.hadoop.mapred.TestMRTimelineEventHandling.testMapreduceJobTimelineServiceEnabled</li></div><div><li>org.apache.hadoop.tools.TestDistCpSystem.testPreserveUseNonEmptyDir</li></div><div><li>org.apache.hadoop.tools.TestDistCpSystem.testRecursiveChunkCopy</li></div><div><li>org.apache.hadoop.tools.TestDistCpSystem.testChunkCopyOneFile</li></div><div><li>org.apache.hadoop.tools.TestDistCpSystem.testDistcpLargeFile</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithContainerResourceChange[1]</li></div><div><li>org.apache.hadoop.yarn.util.TestProcfsBasedProcessTree.testProcessTree</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.TestSchedulingWithAllocationRequestId.testMultipleAllocationRequestDiffPriority[FAIR]</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacitySchedulerNodeLabelUpdate.testResourceUsageWhenNodeUpdatesPartition</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestQueueManagementDynamicEditPolicy.testEditSchedule</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.io.nativeio.TestNativeIO.testStat</li></div><div><li>org.apache.hadoop.io.nativeio.TestNativeIO.testMultiThreadedStat</li></div><div><li>org.apache.hadoop.util.TestShell.testEnvVarsWithInheritance</li></div><div><li>org.apache.hadoop.hdfs.TestDFSShell.testCopyCommandsWithPreserveOption</li></div><div><li>org.apache.hadoop.hdfs.server.blockmanagement.TestUnderReplicatedBlocks.testSetRepIncWithUnderReplicatedBlocks</li></div><div><li>org.apache.hadoop.hdfs.server.namenode.ha.TestEditLogTailer.testNN0TriggersLogRolls[1]</li></div><div><li>org.apache.hadoop.hdfs.server.namenode.ha.TestEditLogTailer.testTriggersLogRollsForAllStandbyNN[1]</li></div><div><li>org.apache.hadoop.hdfs.server.namenode.ha.TestEditLogTailer.testTailer[1]</li></div><div><li>org.apache.hadoop.hdfs.server.namenode.ha.TestHAAppend.testMultipleAppendsDuringCatchupTailing</li></div><div><li>org.apache.hadoop.yarn.sls.TestSLSRunner.testSimulatorRunning[Testing with: SYNTH, org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler, (nodeFile null)]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithContainerResourceChange[1]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMProxy.testAMRMProxyTokenRenewal</li></div><div><li>org.apache.hadoop.yarn.util.TestProcfsBasedProcessTree.testProcessTree</li></div><div><li>TestDockerUtil.test_add_rw_mounts</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.TestRMRestart.testRMRestartRecoveringNodeLabelManager[CAPACITY]</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.metrics.TestSystemMetricsPublisher.testPublishApplicationMetrics</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestIncreaseAllocationExpirer.testDecreaseAfterIncreaseWithAllocationExpiration</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.util.TestShell.testEnvVarsWithInheritance</li></div><div><li>org.apache.hadoop.fs.contract.router.web.TestRouterWebHDFSContractAppend.testRenameFileBeingAppended</li></div><div><li>org.apache.hadoop.hdfs.client.impl.TestBlockReaderLocal.testStatisticsForErasureCodingRead</li></div><div><li>org.apache.hadoop.hdfs.server.blockmanagement.TestUnderReplicatedBlocks.testSetRepIncWithUnderReplicatedBlocks</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestLazyPersistReplicaRecovery.testDnRestartWithSavedReplicas</li></div><div><li>org.apache.hadoop.hdfs.server.namenode.TestReencryptionWithKMS.testReencryptRaceRename</li></div><div><li>org.apache.hadoop.yarn.sls.TestSLSStreamAMSynth.testSimulatorRunning[Testing with: SYNTH, org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler, (nodeFile src/test/resources/nodes.json)]</li></div><div><li>org.apache.hadoop.yarn.service.monitor.probe.TestDefaultProbe.testDefaultProbe[2]</li></div><div><li>org.apache.hadoop.yarn.util.TestProcfsBasedProcessTree.testProcessTree</li></div><div><li>TestDockerUtil.test_add_rw_mounts</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestContainerResizing.testIncreaseContainerUnreservedWhenContainerCompleted</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.crypto.key.kms.TestKMSClientProvider.testSelectTokenLegacyService</li></div><div><li>org.apache.hadoop.crypto.key.kms.TestKMSClientProvider.testNotCopyFromLegacyToken</li></div><div><li>org.apache.hadoop.crypto.key.kms.TestKMSClientProvider.testSelectTokenWhenBothExist</li></div><div><li>org.apache.hadoop.crypto.key.kms.TestKMSClientProvider.testCopyFromToken</li></div><div><li>org.apache.hadoop.io.nativeio.TestNativeIO.testStat</li></div><div><li>org.apache.hadoop.io.nativeio.TestNativeIO.testMultiThreadedStat</li></div><div><li>org.apache.hadoop.util.TestKMSUtil.testCreateKeyProviderFromTokenService</li></div><div><li>org.apache.hadoop.crypto.key.kms.server.TestKMS.testDelegationTokensOpsHttpKerberized</li></div><div><li>org.apache.hadoop.crypto.key.kms.server.TestKMS.testDelegationTokensOpsHttpPseudo</li></div><div><li>org.apache.hadoop.crypto.key.kms.server.TestKMS.testDelegationTokensOpsHttpsKerberized</li></div><div><li>org.apache.hadoop.crypto.key.kms.server.TestKMS.testStartStopHttpKerberos</li></div><div><li>org.apache.hadoop.crypto.key.kms.server.TestKMS.testStartStopHttpPseudo</li></div><div><li>org.apache.hadoop.crypto.key.kms.server.TestKMS.testDelegationTokensUpdatedInUGI</li></div><div><li>org.apache.hadoop.crypto.key.kms.server.TestKMS.testStartStopHttpsPseudo</li></div><div><li>org.apache.hadoop.crypto.key.kms.server.TestKMS.testStartStopHttpsKerberos</li></div><div><li>org.apache.hadoop.crypto.key.kms.server.TestKMS.testDelegationTokensOpsHttpsPseudo</li></div><div><li>org.apache.hadoop.fs.contract.router.web.TestRouterWebHDFSContractMkdir.testMkdirSlashHandling</li></div><div><li>org.apache.hadoop.fs.contract.router.web.TestRouterWebHDFSContractMkdir.testMkdirOverParentFile</li></div><div><li>org.apache.hadoop.fs.contract.router.web.TestRouterWebHDFSContractMkdir.testMkDirRmDir</li></div><div><li>org.apache.hadoop.fs.contract.router.web.TestRouterWebHDFSContractMkdir.testMkDirRmRfDir</li></div><div><li>org.apache.hadoop.hdfs.TestDFSShell.testCopyCommandsWithPreserveOption</li></div><div><li>org.apache.hadoop.hdfs.TestDFSStripedInputStreamWithRandomECPolicy.testPread</li></div><div><li>org.apache.hadoop.hdfs.client.impl.TestBlockReaderLocal.testStatisticsForErasureCodingRead</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.TestBatchIbr.testIbr</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.TestDataNodeUUID.testUUIDRegeneration</li></div><div><li>org.apache.hadoop.hdfs.server.namenode.TestDecommissioningStatus.testDecommissionStatus</li></div><div><li>org.apache.hadoop.mapreduce.v2.hs.TestJobHistoryEvents.testAssignedQueue</li></div><div><li>org.apache.hadoop.conf.TestNoDefaultsJobConf.testNoDefaults</li></div><div><li>org.apache.hadoop.fs.TestDFSIO.org.apache.hadoop.fs.TestDFSIO</li></div><div><li>org.apache.hadoop.fs.TestFileSystem.testFs</li></div><div><li>org.apache.hadoop.fs.slive.TestSlive.testCreateOp</li></div><div><li>org.apache.hadoop.fs.slive.TestSlive.testMkdir</li></div><div><li>org.apache.hadoop.fs.slive.TestSlive.testList</li></div><div><li>org.apache.hadoop.fs.slive.TestSlive.testRead</li></div><div><li>org.apache.hadoop.fs.slive.TestSlive.testDelete</li></div><div><li>org.apache.hadoop.fs.slive.TestSlive.testMRFlow</li></div><div><li>org.apache.hadoop.fs.slive.TestSlive.testRename</li></div><div><li>org.apache.hadoop.fs.slive.TestSlive.testAppendOp</li></div><div><li>org.apache.hadoop.fs.slive.TestSlive.testTruncateOp</li></div><div><li>org.apache.hadoop.hdfs.TestNNBench.testNNBenchCreateReadAndDelete</li></div><div><li>org.apache.hadoop.hdfs.TestNNBench.testNNBenchCreateAndRename</li></div><div><li>org.apache.hadoop.io.TestSequenceFileMergeProgress.testMergeProgressWithBlockCompression</li></div><div><li>org.apache.hadoop.io.TestSequenceFileMergeProgress.testMergeProgressWithRecordCompression</li></div><div><li>org.apache.hadoop.io.TestSequenceFileMergeProgress.testMergeProgressWithNoCompression</li></div><div><li>org.apache.hadoop.ipc.TestMRCJCSocketFactory.testSocketFactory</li></div><div><li>org.apache.hadoop.mapred.TestClusterMRNotification.testMR</li></div><div><li>org.apache.hadoop.mapred.TestClusterMapReduceTestCase.testMapReduceRestarting</li></div><div><li>org.apache.hadoop.mapred.TestClusterMapReduceTestCase.testMRConfig</li></div><div><li>org.apache.hadoop.mapred.TestClusterMapReduceTestCase.testDFSRestart</li></div><div><li>org.apache.hadoop.mapred.TestClusterMapReduceTestCase.testMapReduce</li></div><div><li>org.apache.hadoop.mapred.TestCollect.testCollect</li></div><div><li>org.apache.hadoop.mapred.TestComparators.testDefaultMRComparator</li></div><div><li>org.apache.hadoop.mapred.TestComparators.testUserMRComparator</li></div><div><li>org.apache.hadoop.mapred.TestComparators.testUserValueGroupingComparator</li></div><div><li>org.apache.hadoop.mapred.TestComparators.testBakedUserComparator</li></div><div><li>org.apache.hadoop.mapred.TestComparators.testAllUserComparators</li></div><div><li>org.apache.hadoop.mapred.TestFieldSelection.testFieldSelection</li></div><div><li>org.apache.hadoop.mapred.TestFileInputFormatPathFilter.testWithoutPathFilterWithGlob</li></div><div><li>org.apache.hadoop.mapred.TestFileInputFormatPathFilter.testWithPathFilterWithoutGlob</li></div><div><li>org.apache.hadoop.mapred.TestFileInputFormatPathFilter.testWithoutPathFilterWithoutGlob</li></div><div><li>org.apache.hadoop.mapred.TestFileInputFormatPathFilter.testWithPathFilterWithGlob</li></div><div><li>org.apache.hadoop.mapred.TestFileOutputFormat.testCustomFile</li></div><div><li>org.apache.hadoop.mapred.TestJavaSerialization.testMapReduceJob</li></div><div><li>org.apache.hadoop.mapred.TestJavaSerialization.testWriteToSequencefile</li></div><div><li>org.apache.hadoop.mapred.TestJobCleanup.org.apache.hadoop.mapred.TestJobCleanup</li></div><div><li>org.apache.hadoop.mapred.TestJobCounters.org.apache.hadoop.mapred.TestJobCounters</li></div><div><li>org.apache.hadoop.mapred.TestJobCounters.org.apache.hadoop.mapred.TestJobCounters</li></div><div><li>org.apache.hadoop.mapred.TestJobName.testComplexNameWithRegex</li></div><div><li>org.apache.hadoop.mapred.TestJobName.testComplexName</li></div><div><li>org.apache.hadoop.mapred.TestJobSysDirWithDFS.testWithDFS</li></div><div><li>org.apache.hadoop.mapred.TestKeyValueTextInputFormat.testGzip</li></div><div><li>org.apache.hadoop.mapred.TestKeyValueTextInputFormat.testFormat</li></div><div><li>org.apache.hadoop.mapred.TestLazyOutput.testLazyOutput</li></div><div><li>org.apache.hadoop.mapred.TestLineRecordReaderJobs.testDefaultRecordDelimiters</li></div><div><li>org.apache.hadoop.mapred.TestLineRecordReaderJobs.testCustomRecordDelimiters</li></div><div><li>org.apache.hadoop.mapred.TestLocalJobSubmission.testLocalJobFilesOption</li></div><div><li>org.apache.hadoop.mapred.TestLocalJobSubmission.testLocalJobEncryptedIntermediateData</li></div><div><li>org.apache.hadoop.mapred.TestLocalJobSubmission.testLocalJobArchivesOption</li></div><div><li>org.apache.hadoop.mapred.TestLocalJobSubmission.testJobMaxMapConfig</li></div><div><li>org.apache.hadoop.mapred.TestLocalJobSubmission.testLocalJobLibjarsOption</li></div><div><li>org.apache.hadoop.mapred.TestLocalMRNotification.testMR</li></div><div><li>org.apache.hadoop.mapred.TestMRCJCFileOutputCommitter.testAbort</li></div><div><li>org.apache.hadoop.mapred.TestMRCJCFileOutputCommitter.testCommitter</li></div><div><li>org.apache.hadoop.mapred.TestMRIntermediateDataEncryption.testUberMode</li></div><div><li>org.apache.hadoop.mapred.TestMRIntermediateDataEncryption.testMultipleMapsPerNode</li></div><div><li>org.apache.hadoop.mapred.TestMRIntermediateDataEncryption.testMultipleReducers</li></div><div><li>org.apache.hadoop.mapred.TestMRIntermediateDataEncryption.testSingleReducer</li></div><div><li>org.apache.hadoop.mapred.TestMROpportunisticMaps.testHalfOpportunisticMaps</li></div><div><li>org.apache.hadoop.mapred.TestMROpportunisticMaps.testAllOpportunisticMaps</li></div><div><li>org.apache.hadoop.mapred.TestMRTimelineEventHandling.testMRTimelineEventHandling</li></div><div><li>org.apache.hadoop.mapred.TestMRTimelineEventHandling.testMRNewTimelineServiceEventHandling</li></div><div><li>org.apache.hadoop.mapred.TestMRTimelineEventHandling.testTimelineServiceStartInMiniCluster</li></div><div><li>org.apache.hadoop.mapred.TestMRTimelineEventHandling.testMapreduceJobTimelineServiceEnabled</li></div><div><li>org.apache.hadoop.mapred.TestMapOutputType.testNoMismatch</li></div><div><li>org.apache.hadoop.mapred.TestMapOutputType.testValueMismatch</li></div><div><li>org.apache.hadoop.mapred.TestMapOutputType.testKeyMismatch</li></div><div><li>org.apache.hadoop.mapred.TestMapProgress.testMapProgress</li></div><div><li>org.apache.hadoop.mapred.TestMapRed.testNullKeys</li></div><div><li>org.apache.hadoop.mapred.TestMapRed.testSmallInput</li></div><div><li>org.apache.hadoop.mapred.TestMapRed.testBiggerInput</li></div><div><li>org.apache.hadoop.mapred.TestMapRed.testCompression</li></div><div><li>org.apache.hadoop.mapred.TestMapRed.testMapred</li></div><div><li>org.apache.hadoop.mapred.TestMerge.testMerge</li></div><div><li>org.apache.hadoop.mapred.TestMiniMRBringup.testBringUp</li></div><div><li>org.apache.hadoop.mapred.TestMiniMRChildTask.org.apache.hadoop.mapred.TestMiniMRChildTask</li></div><div><li>org.apache.hadoop.mapred.TestMiniMRClasspath.testClassPath</li></div><div><li>org.apache.hadoop.mapred.TestMiniMRClasspath.testExternalWritable</li></div><div><li>org.apache.hadoop.mapred.TestMiniMRClientCluster.org.apache.hadoop.mapred.TestMiniMRClientCluster</li></div><div><li>org.apache.hadoop.mapred.TestMiniMRClientCluster.org.apache.hadoop.mapred.TestMiniMRClientCluster</li></div><div><li>org.apache.hadoop.mapred.TestMiniMRWithDFSWithDistinctUsers.testMultipleSpills</li></div><div><li>org.apache.hadoop.mapred.TestMiniMRWithDFSWithDistinctUsers.testDistinctUsers</li></div><div><li>org.apache.hadoop.mapred.TestMultiFileInputFormat.testFormatWithLessPathsThanSplits</li></div><div><li>org.apache.hadoop.mapred.TestMultiFileInputFormat.testFormat</li></div><div><li>org.apache.hadoop.mapred.TestMultiFileSplit.testgetLocations</li></div><div><li>org.apache.hadoop.mapred.TestMultipleTextOutputFormat.testFormat</li></div><div><li>org.apache.hadoop.mapred.TestNetworkedJob.testGetJobStatus</li></div><div><li>org.apache.hadoop.mapred.TestNetworkedJob.testJobQueueClient</li></div><div><li>org.apache.hadoop.mapred.TestNetworkedJob.testNetworkedJob</li></div><div><li>org.apache.hadoop.mapred.TestOldCombinerGrouping.testCombiner</li></div><div><li>org.apache.hadoop.mapred.TestReduceFetch.testReduceFromMem</li></div><div><li>org.apache.hadoop.mapred.TestReduceFetch.testReduceFromDisk</li></div><div><li>org.apache.hadoop.mapred.TestReduceFetch.testReduceFromPartialMem</li></div><div><li>org.apache.hadoop.mapred.TestReduceFetchFromPartialMem.testReduceFromPartialMem</li></div><div><li>org.apache.hadoop.mapred.TestReporter.testReporterProgressForMRJob</li></div><div><li>org.apache.hadoop.mapred.TestReporter.testStatusLimit</li></div><div><li>org.apache.hadoop.mapred.TestReporter.testReporterProgressForMapOnlyJob</li></div><div><li>org.apache.hadoop.mapred.TestSequenceFileAsBinaryInputFormat.testBinary</li></div><div><li>org.apache.hadoop.mapred.TestSequenceFileAsBinaryOutputFormat.testBinary</li></div><div><li>org.apache.hadoop.mapred.TestSequenceFileAsBinaryOutputFormat.testcheckOutputSpecsForbidRecordCompression</li></div><div><li>org.apache.hadoop.mapred.TestSequenceFileAsTextInputFormat.testFormat</li></div><div><li>org.apache.hadoop.mapred.TestSequenceFileInputFilter.testPercentFilter</li></div><div><li>org.apache.hadoop.mapred.TestSequenceFileInputFilter.testMD5Filter</li></div><div><li>org.apache.hadoop.mapred.TestSequenceFileInputFilter.testRegexFilter</li></div><div><li>org.apache.hadoop.mapred.TestSequenceFileInputFormat.testFormat</li></div><div><li>org.apache.hadoop.mapred.TestSpecialCharactersInOutputPath.testJobWithDFS</li></div><div><li>org.apache.hadoop.mapred.TestTaskCommit.testCommitFail</li></div><div><li>org.apache.hadoop.mapred.TestTaskCommit.testTaskCleanupDoesNotCommit</li></div><div><li>org.apache.hadoop.mapred.TestTextOutputFormat.testCompress</li></div><div><li>org.apache.hadoop.mapred.TestTextOutputFormat.testFormatWithCustomSeparator</li></div><div><li>org.apache.hadoop.mapred.TestTextOutputFormat.testFormat</li></div><div><li>org.apache.hadoop.mapred.TestUserDefinedCounters.testMapReduceJob</li></div><div><li>org.apache.hadoop.mapred.TestYARNRunner.testNodeLabelExp</li></div><div><li>org.apache.hadoop.mapred.TestYARNRunner.testResourceRequestLocalityMultipleNodesDifferentRack</li></div><div><li>org.apache.hadoop.mapred.TestYARNRunner.testResourceRequestLocalityMultipleNodesDefaultRack</li></div><div><li>org.apache.hadoop.mapred.TestYARNRunner.testResourceRequestLocalityAny</li></div><div><li>org.apache.hadoop.mapred.TestYARNRunner.testAMProfiler</li></div><div><li>org.apache.hadoop.mapred.TestYARNRunner.testAMStandardEnvWithCustomLibPath</li></div><div><li>org.apache.hadoop.mapred.TestYARNRunner.testAMStandardEnvWithDefaultLibPath</li></div><div><li>org.apache.hadoop.mapred.TestYARNRunner.testWarnCommandOpts</li></div><div><li>org.apache.hadoop.mapred.TestYARNRunner.testJobPriority</li></div><div><li>org.apache.hadoop.mapred.TestYARNRunner.testCustomAMRMResourceType</li></div><div><li>org.apache.hadoop.mapred.TestYARNRunner.testAMAdminCommandOpts</li></div><div><li>org.apache.hadoop.mapred.TestYARNRunner.testResourceRequestLocalityNode</li></div><div><li>org.apache.hadoop.mapred.TestYARNRunner.testResourceRequestLocalityRack</li></div><div><li>org.apache.hadoop.mapred.TestYARNRunner.testResourceRequestLocalityInvalid</li></div><div><li>org.apache.hadoop.mapred.TestYARNRunner.testAMRMemoryRequestOverriding</li></div><div><li>org.apache.hadoop.mapred.TestYARNRunner.testResourceRequestLocalityMultipleNodes</li></div><div><li>org.apache.hadoop.mapred.TestYARNRunner.testAMStandardEnvWithCustomLibPathWithSeparateEnvProps</li></div><div><li>org.apache.hadoop.mapred.TestYARNRunner.testResourceRequestLocalityNodeDefaultRack</li></div><div><li>org.apache.hadoop.mapred.TestYARNRunner.testSendJobConf</li></div><div><li>org.apache.hadoop.mapred.TestYARNRunner.testGetHSDelegationToken</li></div><div><li>org.apache.hadoop.mapred.TestYARNRunner.testAMRMemoryRequest</li></div><div><li>org.apache.hadoop.mapred.jobcontrol.TestJobControl.testJobControl</li></div><div><li>org.apache.hadoop.mapred.jobcontrol.TestJobControl.testAddingDependingJob</li></div><div><li>org.apache.hadoop.mapred.jobcontrol.TestJobControl.testJobState</li></div><div><li>org.apache.hadoop.mapred.jobcontrol.TestLocalJobControl.testLocalJobControlDataCopy</li></div><div><li>org.apache.hadoop.mapred.join.TestDatamerge.testNestedJoin</li></div><div><li>org.apache.hadoop.mapred.join.TestDatamerge.testEmptyJoin</li></div><div><li>org.apache.hadoop.mapred.join.TestDatamerge.testSimpleOuterJoin</li></div><div><li>org.apache.hadoop.mapred.join.TestDatamerge.testSimpleOverride</li></div><div><li>org.apache.hadoop.mapred.join.TestDatamerge.testSimpleInnerJoin</li></div><div><li>org.apache.hadoop.mapred.lib.TestChainMapReduce.testChain</li></div><div><li>org.apache.hadoop.mapred.lib.TestKeyFieldBasedComparator.testBasicUnixComparator</li></div><div><li>org.apache.hadoop.mapred.lib.TestLineInputFormat.testFormat</li></div><div><li>org.apache.hadoop.mapred.lib.TestMultipleOutputs.testWithCounters</li></div><div><li>org.apache.hadoop.mapred.lib.TestMultipleOutputs.testWithCounters</li></div><div><li>org.apache.hadoop.mapred.lib.TestMultipleOutputs.testWithoutCounters</li></div><div><li>org.apache.hadoop.mapred.lib.TestMultipleOutputs.testWithoutCounters</li></div><div><li>org.apache.hadoop.mapred.lib.TestMultithreadedMapRunner.testRuntimeExRun</li></div><div><li>org.apache.hadoop.mapred.lib.TestMultithreadedMapRunner.testOKRun</li></div><div><li>org.apache.hadoop.mapred.lib.TestMultithreadedMapRunner.testIOExRun</li></div><div><li>org.apache.hadoop.mapred.lib.aggregate.TestAggregates.testAggregates</li></div><div><li>org.apache.hadoop.mapred.pipes.TestPipeApplication.testSubmitter</li></div><div><li>org.apache.hadoop.mapred.pipes.TestPipesNonJavaInputFormat.testFormat</li></div><div><li>org.apache.hadoop.mapreduce.TestChild.testChild</li></div><div><li>org.apache.hadoop.mapreduce.TestLargeSort.testLargeSort</li></div><div><li>org.apache.hadoop.mapreduce.TestMROutputFormat.testJobSubmission</li></div><div><li>org.apache.hadoop.mapreduce.TestMapCollection.testSingleRecord</li></div><div><li>org.apache.hadoop.mapreduce.TestMapCollection.testPostSpillMeta</li></div><div><li>org.apache.hadoop.mapreduce.TestMapCollection.testLowSpill</li></div><div><li>org.apache.hadoop.mapreduce.TestMapCollection.testSplitMetaSpill</li></div><div><li>org.apache.hadoop.mapreduce.TestMapCollection.testZeroVal</li></div><div><li>org.apache.hadoop.mapreduce.TestMapCollection.testLargeRecConcurrent</li></div><div><li>org.apache.hadoop.mapreduce.TestMapCollection.testLargeRecords</li></div><div><li>org.apache.hadoop.mapreduce.TestMapCollection.testRandomCompress</li></div><div><li>org.apache.hadoop.mapreduce.TestMapCollection.testSpillPer2B</li></div><div><li>org.apache.hadoop.mapreduce.TestMapCollection.testValLastByte</li></div><div><li>org.apache.hadoop.mapreduce.TestMapCollection.testRandom</li></div><div><li>org.apache.hadoop.mapreduce.TestMapReduce.testMapred</li></div><div><li>org.apache.hadoop.mapreduce.TestMapReduceLazyOutput.testLazyOutput</li></div><div><li>org.apache.hadoop.mapreduce.TestNewCombinerGrouping.testCombiner</li></div><div><li>org.apache.hadoop.mapreduce.v2.TestMRJobsWithHistoryService.testJobHistoryData</li></div><div><li>org.apache.hadoop.mapreduce.v2.TestMRJobsWithProfiler.testDefaultProfiler</li></div><div><li>org.apache.hadoop.mapreduce.v2.TestMRJobsWithProfiler.testDifferentProfilers</li></div><div><li>org.apache.hadoop.mapreduce.v2.TestRMNMInfo.testRMNMInfo</li></div><div><li>org.apache.hadoop.fs.s3a.s3guard.TestDynamoDBMetadataStore.org.apache.hadoop.fs.s3a.s3guard.TestDynamoDBMetadataStore</li></div><div><li>org.apache.hadoop.mapred.gridmix.TestDistCacheEmulation.testGenerateDistCacheData</li></div><div><li>org.apache.hadoop.streaming.TestSymLink.testSymLink</li></div><div><li>org.apache.hadoop.yarn.applications.distributedshell.TestDistributedShellWithNodeLabels.testDSShellWithNodeLabelExpression</li></div><div><li>org.apache.hadoop.yarn.service.monitor.probe.TestDefaultProbe.testDefaultProbe[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientMatchingFitExecType[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientMatchingFitExecType[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientMatchingFit[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientMatchingFit[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClient[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClient[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithContainerDemotion[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithContainerDemotion[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithBlacklist[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithBlacklist[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAskWithInvalidNodeLabels[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAskWithInvalidNodeLabels[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientNoMatchingRequests[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientNoMatchingRequests[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientOnAMRMTokenRollOver[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientOnAMRMTokenRollOver[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientAllocReqId[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientAllocReqId[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientMatchingFitInferredRack[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientMatchingFitInferredRack[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testWaitFor[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testWaitFor[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAskWithNodeLabels[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAskWithNodeLabels[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithContainerPromotion[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithContainerPromotion[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientMatchStorage[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientMatchStorage[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMContainerPromotionAndDemotionWithAutoUpdate[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMContainerPromotionAndDemotionWithAutoUpdate[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithContainerResourceChange[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithContainerResourceChange[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAllocationWithBlacklist[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAllocationWithBlacklist[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithSaslEncryption[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithSaslEncryption[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testGetMatchingFitWithProfiles[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testGetMatchingFitWithProfiles[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientMatchingFitExecType[1]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientMatchingFitExecType[1]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientMatchingFit[1]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientMatchingFit[1]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClient[1]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClient[1]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithContainerDemotion[1]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithContainerDemotion[1]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithBlacklist[1]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithBlacklist[1]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAskWithInvalidNodeLabels[1]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAskWithInvalidNodeLabels[1]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientNoMatchingRequests[1]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientNoMatchingRequests[1]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientOnAMRMTokenRollOver[1]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientOnAMRMTokenRollOver[1]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientAllocReqId[1]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientAllocReqId[1]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientMatchingFitInferredRack[1]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientMatchingFitInferredRack[1]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testWaitFor[1]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testWaitFor[1]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAskWithNodeLabels[1]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAskWithNodeLabels[1]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithContainerPromotion[1]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithContainerPromotion[1]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientMatchStorage[1]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientMatchStorage[1]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMContainerPromotionAndDemotionWithAutoUpdate[1]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMContainerPromotionAndDemotionWithAutoUpdate[1]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithContainerResourceChange[1]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithContainerResourceChange[1]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAllocationWithBlacklist[1]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAllocationWithBlacklist[1]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithSaslEncryption[1]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithSaslEncryption[1]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testGetMatchingFitWithProfiles[1]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testGetMatchingFitWithProfiles[1]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientMatchingFitExecType[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientMatchingFitExecType[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientMatchingFit[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientMatchingFit[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClient[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClient[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithContainerDemotion[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithContainerDemotion[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithBlacklist[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithBlacklist[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAskWithInvalidNodeLabels[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAskWithInvalidNodeLabels[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientNoMatchingRequests[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientNoMatchingRequests[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientOnAMRMTokenRollOver[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientOnAMRMTokenRollOver[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientAllocReqId[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientAllocReqId[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientMatchingFitInferredRack[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientMatchingFitInferredRack[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testWaitFor[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testWaitFor[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAskWithNodeLabels[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAskWithNodeLabels[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithContainerPromotion[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithContainerPromotion[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientMatchStorage[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientMatchStorage[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMContainerPromotionAndDemotionWithAutoUpdate[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMContainerPromotionAndDemotionWithAutoUpdate[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithContainerResourceChange[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithContainerResourceChange[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAllocationWithBlacklist[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAllocationWithBlacklist[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithSaslEncryption[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithSaslEncryption[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testGetMatchingFitWithProfiles[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testGetMatchingFitWithProfiles[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClientPlacementConstraints.testAMRMClientWithPlacementConstraints</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClientPlacementConstraints.testAMRMClientWithPlacementConstraints</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMProxy.testAMRMProxyE2E</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMProxy.testAMRMProxyTokenRenewal</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMProxy.testE2ETokenSwap</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestNMClient.testNMClient</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestNMClient.testNMClientNoCleanupOnStop</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestOpportunisticContainerAllocationE2E.org.apache.hadoop.yarn.client.api.impl.TestOpportunisticContainerAllocationE2E</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestYarnClientWithReservation.testListReservationsByReservationId[CAPACITY]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestYarnClientWithReservation.testReservationDelete[CAPACITY]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestYarnClientWithReservation.testListReservationsByInvalidTimeInterval[CAPACITY]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestYarnClientWithReservation.testUpdateReservation[CAPACITY]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestYarnClientWithReservation.testListReservationsByTimeIntervalContainingNoReservations[CAPACITY]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestYarnClientWithReservation.testCreateReservation[CAPACITY]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestYarnClientWithReservation.testListReservationsByTimeInterval[CAPACITY]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestYarnClientWithReservation.testListReservationsByReservationId[FAIR]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestYarnClientWithReservation.testReservationDelete[FAIR]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestYarnClientWithReservation.testListReservationsByInvalidTimeInterval[FAIR]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestYarnClientWithReservation.testUpdateReservation[FAIR]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestYarnClientWithReservation.testListReservationsByTimeIntervalContainingNoReservations[FAIR]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestYarnClientWithReservation.testCreateReservation[FAIR]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestYarnClientWithReservation.testListReservationsByTimeInterval[FAIR]</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.TestLinuxContainerExecutorWithMocks.testStartLocalizer</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.TestLinuxContainerExecutorWithMocks.testNoExitCodeFromPrivilegedOperation</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.TestLocalDirsHandlerService.testDirStructure</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.TestNodeHealthService.testNodeHealthService</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerReboot.testClearLocalDirWhenNodeReboot</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerResync.testPreserveContainersOnResyncKeepingContainers</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerResync.testContainerResourceIncreaseIsSynchronizedWithRMResync</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerResync.testKillContainersOnResync</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerShutdown.testKillContainersOnShutdown</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.testContainerLaunchAndExitFailure</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.testContainerUpgradeLocalizationFailure</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.testContainerUpgradeRollbackDueToFailure</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.testChangeContainerResource</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.testContainerUpgradeSuccessExplicitCommit</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.testForcefulShutdownSignal</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.testContainerSetup</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.testContainerUpgradeSuccessExplicitRollback</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.testLocalingResourceWhileContainerRunning</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.testGracefulShutdownSignal</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.testContainerUpgradeSuccessAutoCommit</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.testLocalFilesCleanup</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.testOutputThreadDumpSignal</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.testContainerUpgradeProcessFailure</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.testContainerLaunchAndExitSuccess</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.testContainerRestart</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.TestContainerLaunch.testImmediateKill</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.TestContainerLaunch.testErrorLogOnContainerExitWithMultipleFiles</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.TestContainerLaunch.testContainerEnvVariables</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.TestContainerLaunch.testContainerLaunchOnConfigurationError</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.TestContainerLaunch.testDelayedKill</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.TestContainerLaunch.testErrorLogOnContainerExitWithCustomPattern</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.TestContainerLaunch.testErrorLogOnContainerExitForCase</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.TestContainerLaunch.testKillProcessGroup</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.TestContainerLaunch.testErrorLogOnContainerExit</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.TestContainerLaunch.testErrorLogOnContainerExitForExt</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestResourceLocalizationService.testDirectoryCleanupOnNewlyCreatedStateStore</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestResourceLocalizationService.testLocalizerHeartbeatWhenAppCleaningUp</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestResourceLocalizationService.testRecovery</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestResourceLocalizationService.testLocalizationHeartbeat</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestResourceLocalizationService.testLocalizationInit</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestResourceLocalizationService.testDownloadingResourcesOnContainerKill</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestResourceLocalizationService.testLocalResourcePath</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestResourceLocalizationService.testParallelDownloadAttemptsForPrivateResource</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestResourceLocalizationService.testPublicResourceInitializesLocalDir</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestResourceLocalizationService.testFailedPublicResource</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestResourceLocalizationService.testParallelDownloadAttemptsForPublicResource</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestLogAggregationService.testLocalFileDeletionOnDiskFull</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.TestContainersMonitor.testContainerKillOnMemoryOverflow</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.TestContainersMonitor.testContainerMonitor</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.TestContainerSchedulerQueuing.testStopQueuedContainer</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.TestContainerSchedulerQueuing.testStartMultipleContainers</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.TestContainerSchedulerQueuing.testStartAndQueueMultipleContainers</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.TestContainerSchedulerQueuing.testQueueShedding</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.TestContainerSchedulerQueuing.testKillOnlyRequiredOpportunisticContainers</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.TestContainerSchedulerQueuing.testStartOpportunistcsWhenOppQueueIsFull</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.TestContainerSchedulerQueuing.testKillMultipleOpportunisticContainers</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.TestContainerSchedulerQueuing.testPromotionOfOpportunisticContainers</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.TestContainerSchedulerQueuing.testPauseOpportunisticForGuaranteedContainer</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.TestContainerSchedulerQueuing.testContainerDeQueuedAfterAMKill</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.TestContainerSchedulerQueuing.testContainerUpdateExecTypeGuaranteedToOpportunistic</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.TestContainerSchedulerQueuing.testKillOpportunisticForGuaranteedContainer</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServices.testContainerLogsWithNewAPI</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServices.testContainerLogsWithOldAPI</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.TestRMRestart.testRMRestartRecoveringNodeLabelManager[FAIR]</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestContainerResizing.testIncreaseContainerUnreservedWhenContainerCompleted</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestContainerResizing.testExcessiveReservationWhenDecreaseSameContainer</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.security.TestDelegationTokenRenewer.testRMRestartWithExpiredToken</li></div><div><li>org.apache.hadoop.yarn.server.TestDiskFailures.testLocalDirsFailures</li></div><div><li>org.apache.hadoop.yarn.server.TestDiskFailures.testDirFailuresOnStartup</li></div><div><li>org.apache.hadoop.yarn.server.TestDiskFailures.testLogDirsFailures</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"><div><li>String index out of range: -1</li></div><div><li>test timed out after 300000 milliseconds</li></div><div><li>test timed out after 120000 milliseconds</li></div><div><li>Timed out waiting for /test1 to reach 2 replicas</li></div><div><li>test timed out after 120000 milliseconds</li></div><div><li> Expected to find 'localhost:44124: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:44124: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li> Expected to find 'localhost:33687: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:33687: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li> Expected to find 'localhost:46459: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:46459: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li>expected timeout</li></div><div><li> Expected to find 'localhost:36398: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:36398: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li> Expected to find 'localhost:39876: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:39876: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li> Expected to find 'localhost:43434: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:43434: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li>expected timeout</li></div><div><li>Expected success for Probe Status, time="Tue May 08 03:39:38 UTC 2018", outcome="failure", message="Failure in Default probe: IP presence with DNS checking and DNS server address 8.8.8.8", exception="java.io.IOException: comp-0: DNS checking is enabled, but lookup for example.com is not available yet"</li></div><div><li>Application attempt appattempt_1525740364249_0001_000001 doesn't exist in ApplicationMasterService cache.
 at org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.allocate(ApplicationMasterService.java:404)
 at org.apache.hadoop.yarn.server.nodemanager.amrmproxy.DefaultRequestInterceptor$3.allocate(DefaultRequestInterceptor.java:224)
 at org.apache.hadoop.yarn.server.nodemanager.</li></div><div><li>Child process owned by init escaped process tree.</li></div><div><li>Process is still alive!</li></div><div><li>expected:&lt;2&gt; but was:&lt;3&gt;</li></div><div><li>expected:&lt;0.5&gt; but was:&lt;0.0&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;2&gt; but was:&lt;1&gt;</li></div><div><li>String index out of range: -1</li></div><div><li>Address already in use</li></div><div><li>Could not stop embedded servlet container, Server is not running</li></div><div><li>Failed to append to non-existent file /test/test/target for client 127.0.0.1
 at org.apache.hadoop.hdfs.server.namenode.FSDirAppendOp.appendFile(FSDirAppendOp.java:104)
 at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.appendFile(FSNamesystem.java:2621)
 at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.append(NameNodeRpcServer.java:805)
 at org.apache.hadoop.hdfs.protocolPB.Client</li></div><div><li>test timed out after 120000 milliseconds</li></div><div><li>Lease should be empty.</li></div><div><li>DestHost:destPort localhost:13976 , LocalHost:localPort d98b9e29c6f8/172.17.0.3:0. Failed on local exception: java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]</li></div><div><li>test timed out after 300000 milliseconds</li></div><div><li>Failed: the number of failed blocks = 2 &gt; the number of parity blocks = 1</li></div><div><li>test timed out after 300000 milliseconds</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.hdfs.client.impl.TestBlockReaderLocal.testStatisticsForErasureCodingRead(TestBlockReaderLocal.java:842)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke</li></div><div><li>test timed out after 300000 milliseconds</li></div><div><li>test timed out after 20000 milliseconds</li></div><div><li>Test resulted in an unexpected exit</li></div><div><li>test timed out after 120000 milliseconds</li></div><div><li>test timed out after 120000 milliseconds</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div><div><li>test timed out after 120000 milliseconds</li></div><div><li>Timed out waiting for condition. Thread diagnostics:
Timestamp: 2018-05-07 09:10:00,959

"qtp443688145-29" daemon prio=5 tid=29 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos</li></div><div><li>Unexpected num under-replicated blocks expected:&lt;3&gt; but was:&lt;4&gt;</li></div><div><li>expected:&lt;14.0&gt; but was:&lt;16.0&gt;</li></div><div><li>After waiting the operation updatePipeline still has not taken effect on NN yet</li></div><div><li> Expected to find 'localhost:39175: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:39175: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li> Expected to find 'localhost:45235: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:45235: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li> Expected to find 'localhost:33344: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:33344: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li>expected timeout</li></div><div><li> Expected to find 'localhost:43532: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:43532: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li> Expected to find 'localhost:42401: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:42401: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li> Expected to find 'localhost:32933: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:32933: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li>expected timeout</li></div><div><li>create_write should create the file</li></div><div><li>Job didn't finish in 30 seconds</li></div><div><li>Job didn't finish in 30 seconds</li></div><div><li>test timed out after 30000 milliseconds</li></div><div><li>test timed out after 30000 milliseconds</li></div><div><li>src and dst file does not match at 0 between HdfsNamedFileStatus{path=hdfs://localhost:39481/testdir/srcdat/file0; isDirectory=false; length=102400; replication=2; blocksize=1024; modification_time=1525772231698; access_time=1525772229670; owner=u1; group=g1; permission=rw-r--r--; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false} and HdfsNamedFileStatus{path=hdfs://localhost:</li></div><div><li>File group ownership should match expected:&lt;[g0]&gt; but was:&lt;[supergroup]&gt;</li></div><div><li>expected:&lt;1&gt; but was:&lt;0&gt;</li></div><div><li>Child process owned by init escaped process tree.</li></div><div><li>test timed out after 10000 milliseconds</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacitySchedulerNodeLabelUpdate.checkUserUsedResource(TestCapacitySchedulerNodeLabelUpdate.java:191)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacitySchedulerNodeLabelUpdate.testResourceUsageWhenNodeUpdatesPartition(TestCapacitySchedulerNodeLabelUpdate.java:410)</li></div><div><li>expected:&lt;0.5&gt; but was:&lt;0.0&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>length != 10(unixSymbolicPermission=-rw-r--r--.)</li></div><div><li>java.lang.IllegalArgumentException: length != 10(unixSymbolicPermission=-rw-r--r--.)</li></div><div><li>String index out of range: -1</li></div><div><li>expected:&lt;1525735831449&gt; but was:&lt;1525735832555&gt;</li></div><div><li>test timed out after 120000 milliseconds</li></div><div><li>Test resulted in an unexpected exit</li></div><div><li>Test resulted in an unexpected exit</li></div><div><li>Test resulted in an unexpected exit</li></div><div><li>inode should complete in ~60000 ms.
Expected: is &lt;true&gt;
     but: was &lt;false&gt;</li></div><div><li>TestSLSRunner catched exception from child thread (TaskRunner.TaskDefinition): [java.lang.reflect.UndeclaredThrowableException]</li></div><div><li>expected:&lt;1&gt; but was:&lt;0&gt;</li></div><div><li>Application attempt appattempt_1525745343900_0001_000001 doesn't exist in ApplicationMasterService cache.
 at org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.allocate(ApplicationMasterService.java:404)
 at org.apache.hadoop.yarn.server.nodemanager.amrmproxy.DefaultRequestInterceptor$3.allocate(DefaultRequestInterceptor.java:224)
 at org.apache.hadoop.yarn.server.nodemanager.</li></div><div><li>Child process owned by init escaped process tree.</li></div><div><li>/var/lib/jenkins/workspace/hadoop/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/test/utils/test_docker_util.cc:932
      Expected: 0
To be equal to: ret
      Which is: 14</li></div><div><li>expected:&lt;2&gt; but was:&lt;3&gt;</li></div><div><li>expected:&lt;uers1,user2&gt; but was:&lt;null&gt;</li></div><div><li>expected:&lt;3072&gt; but was:&lt;4096&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>String index out of range: -1</li></div><div><li>Failed to append to non-existent file /test/test/target for client 127.0.0.1
 at org.apache.hadoop.hdfs.server.namenode.FSDirAppendOp.appendFile(FSDirAppendOp.java:104)
 at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.appendFile(FSNamesystem.java:2621)
 at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.append(NameNodeRpcServer.java:805)
 at org.apache.hadoop.hdfs.protocolPB.Client</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.hdfs.client.impl.TestBlockReaderLocal.testStatisticsForErasureCodingRead(TestBlockReaderLocal.java:842)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke</li></div><div><li>test timed out after 120000 milliseconds</li></div><div><li>Timed out waiting for condition. Thread diagnostics:
Timestamp: 2018-05-07 05:31:28,400

"IPC Server handler 7 on 41535" daemon prio=5 tid=202 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionOb</li></div><div><li>Timed out waiting for condition. Thread diagnostics:
Timestamp: 2018-05-07 07:02:19,383

"org.apache.hadoop.crypto.key.kms.ValueQueue_thread" daemon prio=5 tid=3906 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.Abst</li></div><div><li>TestSLSRunner catched exception from child thread (TaskRunner.TaskDefinition): [java.lang.reflect.UndeclaredThrowableException]</li></div><div><li>Expected success for Probe Status, time="Tue May 08 02:17:01 UTC 2018", outcome="failure", message="Failure in Default probe: IP presence with DNS checking and DNS server address 8.8.8.8", exception="java.io.IOException: comp-0: DNS checking is enabled, but lookup for example.com is not available yet"</li></div><div><li>Child process owned by init escaped process tree.</li></div><div><li>/var/lib/jenkins/workspace/hadoop/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/test/utils/test_docker_util.cc:932
      Expected: 0
To be equal to: ret
      Which is: 14</li></div><div><li>expected:&lt;1024&gt; but was:&lt;7168&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>tried to access field org.apache.hadoop.crypto.key.kms.KMSClientProvider.LOG from class org.apache.hadoop.crypto.key.kms.TestKMSClientProvider</li></div><div><li>tried to access field org.apache.hadoop.crypto.key.kms.KMSClientProvider.LOG from class org.apache.hadoop.crypto.key.kms.TestKMSClientProvider</li></div><div><li>tried to access field org.apache.hadoop.crypto.key.kms.KMSClientProvider.LOG from class org.apache.hadoop.crypto.key.kms.TestKMSClientProvider</li></div><div><li>tried to access field org.apache.hadoop.crypto.key.kms.KMSClientProvider.LOG from class org.apache.hadoop.crypto.key.kms.TestKMSClientProvider</li></div><div><li>length != 10(unixSymbolicPermission=-rw-r--r--.)</li></div><div><li>java.lang.IllegalArgumentException: length != 10(unixSymbolicPermission=-rw-r--r--.)</li></div><div><li>org.apache.hadoop.util.KMSUtil.createKeyProviderFromTokenService(Lorg/apache/hadoop/conf/Configuration;Ljava/lang/String;)Lorg/apache/hadoop/crypto/key/KeyProvider;</li></div><div><li>expected:&lt;kms-dt&gt; but was:&lt;KMS_DELEGATION_TOKEN&gt;</li></div><div><li>expected:&lt;kms-dt&gt; but was:&lt;KMS_DELEGATION_TOKEN&gt;</li></div><div><li>expected:&lt;kms-dt&gt; but was:&lt;KMS_DELEGATION_TOKEN&gt;</li></div><div><li>expected:&lt;[kms-dt]&gt; but was:&lt;[KMS_DELEGATION_TOKEN]&gt;</li></div><div><li>expected:&lt;[kms-dt]&gt; but was:&lt;[KMS_DELEGATION_TOKEN]&gt;</li></div><div><li>expected:&lt;kms-dt&gt; but was:&lt;KMS_DELEGATION_TOKEN&gt;</li></div><div><li>expected:&lt;[kms-dt]&gt; but was:&lt;[KMS_DELEGATION_TOKEN]&gt;</li></div><div><li>expected:&lt;[kms-dt]&gt; but was:&lt;[KMS_DELEGATION_TOKEN]&gt;</li></div><div><li>expected:&lt;kms-dt&gt; but was:&lt;KMS_DELEGATION_TOKEN&gt;</li></div><div><li>No namenode available under nameservice ns0</li></div><div><li>No namenode available under nameservice ns0</li></div><div><li>No namenode available under nameservice ns0</li></div><div><li>No namenode available under nameservice ns0</li></div><div><li>expected:&lt;1525800414905&gt; but was:&lt;1525800415925&gt;</li></div><div><li>test timed out after 300000 milliseconds</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.hdfs.client.impl.TestBlockReaderLocal.testStatisticsForErasureCodingRead(TestBlockReaderLocal.java:842)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke</li></div><div><li>org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /dir/-255907890695930339_4 could only be written to 0 of the 1 minReplication nodes. There are 4 datanode(s) running and no node(s) are excluded in this operation.
 at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2117)
 at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.ch</li></div><div><li>test timed out after 10000 milliseconds</li></div><div><li>Unexpected num under-replicated blocks expected:&lt;3&gt; but was:&lt;4&gt;</li></div><div><li>QueueName not correct expected:&lt;[assignedQueue]&gt; but was:&lt;[default]&gt;</li></div><div><li>Could not create staging directory. </li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.fs.slive.TestSlive.runOperationOk(TestSlive.java:354)
	at org.apache.hadoop.fs.slive.TestSlive.testCreateOp(TestSlive.java:290)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeM</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.fs.slive.TestSlive.runOperationOk(TestSlive.java:354)
	at org.apache.hadoop.fs.slive.TestSlive.testMkdir(TestSlive.java:503)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMeth</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.fs.slive.TestSlive.runOperationOk(TestSlive.java:354)
	at org.apache.hadoop.fs.slive.TestSlive.testList(TestSlive.java:460)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMetho</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.fs.slive.TestSlive.runOperationOk(TestSlive.java:354)
	at org.apache.hadoop.fs.slive.TestSlive.testRead(TestSlive.java:434)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMetho</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.fs.slive.TestSlive.runOperationOk(TestSlive.java:354)
	at org.apache.hadoop.fs.slive.TestSlive.testDelete(TestSlive.java:381)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMet</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.fs.slive.TestSlive.testMRFlow(TestSlive.java:416)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.Del</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.fs.slive.TestSlive.runOperationOk(TestSlive.java:354)
	at org.apache.hadoop.fs.slive.TestSlive.testRename(TestSlive.java:402)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMet</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.fs.slive.TestSlive.runOperationOk(TestSlive.java:354)
	at org.apache.hadoop.fs.slive.TestSlive.testAppendOp(TestSlive.java:532)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeM</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.fs.slive.TestSlive.runOperationOk(TestSlive.java:354)
	at org.apache.hadoop.fs.slive.TestSlive.testTruncateOp(TestSlive.java:554)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.Nativ</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Could not create staging directory. </li></div><div><li>Could not create staging directory. </li></div><div><li>Could not create staging directory. </li></div><div><li>Could not create staging directory. </li></div><div><li>Could not create staging directory. </li></div><div><li>Could not create staging directory. </li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Wrong FS: file:/var/lib/jenkins/workspace/hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/target/test-dir/input, expected: hdfs://localhost:36811</li></div><div><li>Wrong FS: file:/var/lib/jenkins/workspace/hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/target/test-dir/input, expected: hdfs://localhost:36811</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Could not create staging directory. </li></div><div><li>Could not create staging directory. </li></div><div><li>Could not create staging directory. </li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Could not create staging directory. </li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Job failed</li></div><div><li>Job failed</li></div><div><li>Job failed</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Job failed</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Could not create staging directory. </li></div><div><li>Could not create staging directory. </li></div><div><li>Could not create staging directory. </li></div><div><li>Could not create staging directory. </li></div><div><li>Could not create staging directory. </li></div><div><li>Could not create staging directory. </li></div><div><li>Could not create staging directory. </li></div><div><li>Could not create staging directory. </li></div><div><li>Could not create staging directory. </li></div><div><li>Could not create staging directory. </li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Threw exception:java.net.ConnectException: Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Threw exception:java.net.ConnectException: Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Could not create staging directory. </li></div><div><li>Could not create staging directory. </li></div><div><li>Could not create staging directory. </li></div><div><li>Could not create staging directory. </li></div><div><li>Could not create staging directory. </li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Could not create staging directory. </li></div><div><li>Could not create staging directory. </li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Could not create staging directory. </li></div><div><li>Could not create staging directory. </li></div><div><li>Could not create staging directory. </li></div><div><li>Could not create staging directory. </li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Block compression should be allowed for SequenceFileAsBinaryOutputFormat:Caught java.net.ConnectException</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Could not create staging directory. </li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Wrong FS: file:/var/lib/jenkins/workspace/hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/target/test-dir/org.apache.hadoop.mapred.TestUserDefinedCounters/input, expected: hdfs://localhost:36811</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.mapred.TestYARNRunner.testResourceRequestLocalityInvalid(TestYARNRunner.java:765)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorI</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>
mRClientProtocol.getDelegationToken(&lt;any&gt;);
Never wanted here:
-&gt; at org.apache.hadoop.mapred.TestYARNRunner.testGetHSDelegationToken(TestYARNRunner.java:450)
But invoked here:
-&gt; at org.apache.hadoop.mapred.YARNRunner.getDelegationTokenFromHS(YARNRunner.java:250)
</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Could not create staging directory. </li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Could not create staging directory. </li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:36811 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>test timed out after 90000 milliseconds</li></div><div><li>test timed out after 150000 milliseconds</li></div><div><li>test timed out after 150000 milliseconds</li></div><div><li>Node "pts00449-vm36.persistent.co.in:34157" should be RUNNING</li></div><div><li>initTable on TestDynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.model.AmazonDynamoDBException: The request processing has failed because of an unknown error, exception or failure. (Service: AmazonDynamoDBv2; Status Code: 500; Error Code: InternalFailure; Request ID: 22df30eb-e0a0-45de-850b-e8e98685ec9a): The request processing has failed because of an unknown error, exception or failure.</li></div><div><li>/ by zero</li></div><div><li>test timed out after 120000 milliseconds</li></div><div><li>test timed out after 90000 milliseconds</li></div><div><li>Expected success for Probe Status, time="Wed May 09 01:01:16 UTC 2018", outcome="failure", message="Failure in Default probe: IP presence with DNS checking and DNS server address 8.8.8.8", exception="java.io.IOException: comp-0: DNS checking is enabled, but lookup for example.com is not available yet"</li></div><div><li>Not all node managers were reported running expected:&lt;3&gt; but was:&lt;0&gt;</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invo</li></div><div><li>Not all node managers were reported running expected:&lt;3&gt; but was:&lt;0&gt;</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invo</li></div><div><li>Not all node managers were reported running expected:&lt;3&gt; but was:&lt;0&gt;</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invo</li></div><div><li>Not all node managers were reported running expected:&lt;3&gt; but was:&lt;0&gt;</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invo</li></div><div><li>Not all node managers were reported running expected:&lt;3&gt; but was:&lt;0&gt;</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invo</li></div><div><li>Not all node managers were reported running expected:&lt;3&gt; but was:&lt;0&gt;</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invo</li></div><div><li>Not all node managers were reported running expected:&lt;3&gt; but was:&lt;0&gt;</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invo</li></div><div><li>Not all node managers were reported running expected:&lt;3&gt; but was:&lt;0&gt;</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invo</li></div><div><li>Not all node managers were reported running expected:&lt;3&gt; but was:&lt;0&gt;</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invo</li></div><div><li>Not all node managers were reported running expected:&lt;3&gt; but was:&lt;0&gt;</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invo</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invo</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invo</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invo</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invo</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invo</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invo</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor45.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor45.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor45.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor45.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor45.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor45.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor45.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor45.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor45.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor45.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor45.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor45.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor45.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor45.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor45.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor45.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor45.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor45.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor45.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor45.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor45.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor45.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor45.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor45.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor45.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor45.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor45.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor45.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor45.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor45.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor45.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor45.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor45.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor45.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor45.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor45.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor45.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor45.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor45.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor45.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor45.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>Not all node managers were reported running expected:&lt;3&gt; but was:&lt;0&gt;</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invo</li></div><div><li>test timed out after 120000 milliseconds</li></div><div><li>java.net.BindException: Problem binding to [0.0.0.0:8049] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException</li></div><div><li>java.net.BindException: Problem binding to [0.0.0.0:8049] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException</li></div><div><li>Index: 0, Size: 0</li></div><div><li>Index: 0, Size: 0</li></div><div><li>Index: 0, Size: 0</li></div><div><li>Timed out waiting for condition. Thread diagnostics:
Timestamp: 2018-05-08 08:05:05,743

"IPC Server handler 8 on 33138" daemon prio=5 tid=82 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObj</li></div><div><li>Timed out waiting for condition. Thread diagnostics:
Timestamp: 2018-05-08 08:05:17,455

"IPC Server handler 32 on 33138" daemon prio=5 tid=106 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionO</li></div><div><li>Timed out waiting for condition. Thread diagnostics:
Timestamp: 2018-05-08 08:05:29,207

"IPC Server handler 32 on 33138" daemon prio=5 tid=106 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionO</li></div><div><li>Timed out waiting for condition. Thread diagnostics:
Timestamp: 2018-05-08 08:05:40,653

"IPC Server handler 32 on 33138" daemon prio=5 tid=106 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionO</li></div><div><li>Timed out waiting for condition. Thread diagnostics:
Timestamp: 2018-05-08 08:05:52,009

"IPC Server handler 29 on 43898" daemon prio=5 tid=1396 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$Condition</li></div><div><li>Timed out waiting for condition. Thread diagnostics:
Timestamp: 2018-05-08 08:06:03,369

"IPC Server handler 29 on 43898" daemon prio=5 tid=1396 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$Condition</li></div><div><li>Timed out waiting for condition. Thread diagnostics:
Timestamp: 2018-05-08 08:06:14,680

"IPC Server handler 29 on 43898" daemon prio=5 tid=1396 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$Condition</li></div><div><li>Timed out waiting for condition. Thread diagnostics:
Timestamp: 2018-05-08 08:06:26,693

"IPC Server handler 29 on 43898" daemon prio=5 tid=1396 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$Condition</li></div><div><li>Timed out waiting for condition. Thread diagnostics:
Timestamp: 2018-05-08 08:06:38,636

"IPC Server handler 7 on 44450" daemon prio=5 tid=2725 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionO</li></div><div><li>Timed out waiting for condition. Thread diagnostics:
Timestamp: 2018-05-08 08:06:50,026

"IPC Server handler 7 on 44450" daemon prio=5 tid=2725 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionO</li></div><div><li>Timed out waiting for condition. Thread diagnostics:
Timestamp: 2018-05-08 08:07:01,460

"IPC Server handler 7 on 44450" daemon prio=5 tid=2725 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionO</li></div><div><li>Timed out waiting for condition. Thread diagnostics:
Timestamp: 2018-05-08 08:07:13,296

"IPC Server handler 7 on 44450" daemon prio=5 tid=2725 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionO</li></div><div><li>unable to create new native thread</li></div><div><li>unable to create new native thread</li></div><div><li>No space available in any of the local directories.</li></div><div><li>Unexpected exception org.apache.hadoop.util.DiskChecker$DiskErrorException: No space available in any of the local directories.</li></div><div><li>expected:&lt;1&gt; but was:&lt;0&gt;</li></div><div><li>Node health status reported unhealthy</li></div><div><li>The container should create a subDir named currentUser: jenkinsunder localDir/usercache</li></div><div><li>expected:&lt;2&gt; but was:&lt;1&gt;</li></div><div><li>ContainerState is not correct (timedout)</li></div><div><li>Metrics source NodeManagerMetrics already exists!</li></div><div><li>/var/lib/jenkins/workspace/hadoop/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/target/org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerShutdown/tmpDir/start_file.txt (No such file or directory)</li></div><div><li>expected:&lt;50&gt; but was:&lt;-1000&gt;</li></div><div><li>ProcessStartFile doesn't exist!</li></div><div><li>ProcessStartFile doesn't exist!</li></div><div><li>ContainerState is not correct (timedout)</li></div><div><li>ProcessStartFile doesn't exist!</li></div><div><li>ProcessStartFile doesn't exist!</li></div><div><li>/var/lib/jenkins/workspace/hadoop/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/target/TestContainerManager-localDir/nmPrivate doesn't exist!!</li></div><div><li>ProcessStartFile doesn't exist!</li></div><div><li>ContainerState is not correct (timedout)</li></div><div><li>ProcessStartFile doesn't exist!</li></div><div><li>ProcessStartFile doesn't exist!</li></div><div><li>AppDir /var/lib/jenkins/workspace/hadoop/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/target/TestContainerManager-localDir/usercache/nobody/appcache/application_0_0000 doesn't exist!!</li></div><div><li>ProcessStartFile doesn't exist!</li></div><div><li>ProcessStartFile doesn't exist!</li></div><div><li>expected:&lt;0&gt; but was:&lt;-1000&gt;</li></div><div><li>ProcessStartFile doesn't exist!</li></div><div><li>ProcessStartFile doesn't exist!</li></div><div><li>No space available in any of the local directories.</li></div><div><li>Timed out waiting for condition. Thread diagnostics:
Timestamp: 2018-05-08 06:17:03,874

"Public Localizer"  prio=5 tid=133 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.aw</li></div><div><li>
Wanted but not invoked:
nodeStatusUpdater.reportException(&lt;any&gt;);
-&gt; at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.TestContainerLaunch.testContainerLaunchOnConfigurationError(TestContainerLaunch.java:1709)
Actually, there were zero interactions with this mock.
</li></div><div><li>ProcessStartFile doesn't exist!</li></div><div><li>No space available in any of the local directories.</li></div><div><li>No space available in any of the local directories.</li></div><div><li>ProcessStartFile doesn't exist!</li></div><div><li>No space available in any of the local directories.</li></div><div><li>No space available in any of the local directories.</li></div><div><li>
Wanted but not invoked:
localFs.mkdir(
    /var/lib/jenkins/workspace/hadoop/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/target/org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestResourceLocalizationService/0/usercache,
    rwxr-xr-x,
    true
);
-&gt; at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestResourceLocal</li></div><div><li>test timed out after 20000 milliseconds</li></div><div><li>No space available in any of the local directories.</li></div><div><li>
Wanted but not invoked:
containerExecutor.startLocalizer(
    &lt;Capturing argument&gt;
);
-&gt; at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestResourceLocalizationService.testLocalizationHeartbeat(TestResourceLocalizationService.java:956)
Actually, there were zero interactions with this mock.
</li></div><div><li>
Wanted but not invoked:
localFs.mkdir(
    /var/lib/jenkins/workspace/hadoop/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/target/org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestResourceLocalizationService/0/usercache,
    rwxr-xr-x,
    true
);
-&gt; at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestResourceLocal</li></div><div><li>test timed out after 20000 milliseconds</li></div><div><li>test timed out after 10000 milliseconds</li></div><div><li>Index: 0, Size: 0</li></div><div><li>
Wanted but not invoked:
localFs.mkdir(
    /var/lib/jenkins/workspace/hadoop/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/target/org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestResourceLocalizationService/0/filecache,
    rwxr-xr-x,
    true
);
-&gt; at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestResourceLocal</li></div><div><li>test timed out after 20000 milliseconds</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestResourceLocalizationService.testParallelDownloadAttemptsForPublicResource(TestResourceLocalizationService.java:2360)
	at sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>
deletionService.delete(
    &lt;File deletion matcher&gt;
);
Wanted 2 times:
-&gt; at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestLogAggregationService.verifyLocalFileDeletion(TestLogAggregationService.java:228)
But was 1 time:
-&gt; at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl.doAppLogAggregationPostCleanUp(AppLogAggregat</li></div><div><li>ProcessStartFile doesn't exist!</li></div><div><li>ContainerState is not correct (timedout)</li></div><div><li>expected:&lt;1&gt; but was:&lt;0&gt;</li></div><div><li>ContainerState is not correct (timedout)</li></div><div><li>expected:&lt;RUNNING&gt; but was:&lt;DONE&gt;</li></div><div><li>expected:&lt;6&gt; but was:&lt;0&gt;</li></div><div><li>expected:&lt;2&gt; but was:&lt;0&gt;</li></div><div><li>ContainerState is not correct (timedout)</li></div><div><li>expected:&lt;2&gt; but was:&lt;1&gt;</li></div><div><li>expected:&lt;RUNNING&gt; but was:&lt;DONE&gt;</li></div><div><li>ContainerState is not correct (timedout)</li></div><div><li>expected:&lt;2&gt; but was:&lt;0&gt;</li></div><div><li>ContainerState is not correct (timedout)</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.TestContainerSchedulerQueuing.testKillOpportunisticForGuaranteedContainer(TestContainerSchedulerQueuing.java:544)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(N</li></div><div><li>No space available in any of the local directories.</li></div><div><li>No space available in any of the local directories.</li></div><div><li>expected:&lt;2&gt; but was:&lt;0&gt;</li></div><div><li>expected:&lt;1024&gt; but was:&lt;7168&gt;</li></div><div><li>expected:&lt;1024&gt; but was:&lt;2048&gt;</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertNotNull(Assert.java:621)
	at org.junit.Assert.assertNotNull(Assert.java:631)
	at org.apache.hadoop.yarn.server.resourcemanager.security.TestDelegationTokenRenewer.testRMRestartWithExpiredToken(TestDelegationTokenRenewer.java:1067)
	at sun.reflect.NativeMetho</li></div><div><li>Number of nm-local-dirs is wrong. expected:&lt;4&gt; but was:&lt;0&gt;</li></div><div><li>expected:&lt;1&gt; but was:&lt;0&gt;</li></div><div><li>Number of nm-log-dirs is wrong. expected:&lt;4&gt; but was:&lt;0&gt;</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.hdfs.server.blockmanagement.TestUnderReplicatedBlocks.testSetRepIncWithUnderReplicatedBlocks</div></li><li><div>org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting.testDataNodeReconfigureWithVolumeFailures</div></li><li><div>org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting.testVolFailureStatsPreservedOnNNRestart</div></li><li><div>org.apache.hadoop.yarn.service.monitor.probe.TestDefaultProbe.testDefaultProbe[2]</div></li><li><div>org.apache.hadoop.yarn.client.api.impl.TestAMRMProxy.testAMRMProxyTokenRenewal</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.testContainerUpgradeSuccessExplicitRollback</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.TestRMRestart.testRMRestartRecoveringNodeLabelManager[CAPACITY]</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.security.TestGroupsCaching.testThreadNotBlockedWhenExpiredEntryExistsWithBackgroundRefresh</div></li><li><div>org.apache.hadoop.fs.http.client.TestHttpFSFWithWebhdfsFileSystem.testOperation[0]</div></li><li><div>org.apache.hadoop.fs.http.client.TestHttpFSFWithWebhdfsFileSystem.testOperationDoAs[0]</div></li><li><div>org.apache.hadoop.fs.contract.router.web.TestRouterWebHDFSContractAppend.testRenameFileBeingAppended</div></li><li><div>org.apache.hadoop.hdfs.TestDFSClientRetries.testLeaseRenewAndDFSOutputStreamDeadLock</div></li><li><div>org.apache.hadoop.hdfs.TestDFSClientRetries.testLeaseRenewSocketTimeout</div></li><li><div>org.apache.hadoop.hdfs.TestDFSInotifyEventInputStreamKerberized.testWithKerberizedCluster</div></li><li><div>org.apache.hadoop.hdfs.TestDFSStripedInputStreamWithRandomECPolicy.testPread</div></li><li><div>org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureWithRandomECPolicy.testCloseWithExceptionsInStreamer</div></li><li><div>org.apache.hadoop.hdfs.TestReadStripedFileWithMissingBlocks.testReadFileWithMissingBlocks</div></li><li><div>org.apache.hadoop.hdfs.client.impl.TestBlockReaderLocal.testStatisticsForErasureCodingRead</div></li><li><div>org.apache.hadoop.hdfs.server.datanode.TestDataNodeMultipleRegistrations.testClusterIdMismatchAtStartupWithHA</div></li><li><div>org.apache.hadoop.hdfs.server.datanode.TestDataNodeMultipleRegistrations.test2NNRegistration</div></li><li><div>org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailure.testUnderReplicationAfterVolFailure</div></li><li><div>org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting.testSuccessiveVolumeFailures</div></li><li><div>org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting.testHotSwapOutFailedVolumeAndReporting</div></li><li><div>org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting.testMultipleVolFailuresOnNode</div></li><li><div>org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestLazyPersistReplicaRecovery.testDnRestartWithSavedReplicas</div></li><li><div>org.apache.hadoop.hdfs.server.namenode.TestDecommissioningStatus.testDecommissionStatus</div></li><li><div>org.apache.hadoop.hdfs.server.namenode.TestNamenodeCapacityReport.testXceiverCount</div></li><li><div>org.apache.hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA.testUpdatePipeline</div></li><li><div>org.apache.hadoop.hdfs.TestNNBench.testNNBenchCreateReadAndDelete</div></li><li><div>org.apache.hadoop.mapred.TestMRTimelineEventHandling.testMRTimelineEventHandling</div></li><li><div>org.apache.hadoop.mapred.TestMRTimelineEventHandling.testMapreduceJobTimelineServiceEnabled</div></li><li><div>org.apache.hadoop.tools.TestDistCpSystem.testPreserveUseNonEmptyDir</div></li><li><div>org.apache.hadoop.tools.TestDistCpSystem.testRecursiveChunkCopy</div></li><li><div>org.apache.hadoop.tools.TestDistCpSystem.testChunkCopyOneFile</div></li><li><div>org.apache.hadoop.tools.TestDistCpSystem.testDistcpLargeFile</div></li><li><div>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithContainerResourceChange[1]</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.scheduler.TestSchedulingWithAllocationRequestId.testMultipleAllocationRequestDiffPriority[FAIR]</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacitySchedulerNodeLabelUpdate.testResourceUsageWhenNodeUpdatesPartition</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.io.nativeio.TestNativeIO.testStat</div></li><li><div>org.apache.hadoop.io.nativeio.TestNativeIO.testMultiThreadedStat</div></li><li><div>org.apache.hadoop.hdfs.TestDFSShell.testCopyCommandsWithPreserveOption</div></li><li><div>org.apache.hadoop.hdfs.server.namenode.ha.TestEditLogTailer.testNN0TriggersLogRolls[1]</div></li><li><div>org.apache.hadoop.hdfs.server.namenode.ha.TestEditLogTailer.testTriggersLogRollsForAllStandbyNN[1]</div></li><li><div>org.apache.hadoop.hdfs.server.namenode.ha.TestEditLogTailer.testTailer[1]</div></li><li><div>org.apache.hadoop.hdfs.server.namenode.ha.TestHAAppend.testMultipleAppendsDuringCatchupTailing</div></li><li><div>org.apache.hadoop.yarn.sls.TestSLSRunner.testSimulatorRunning[Testing with: SYNTH, org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler, (nodeFile null)]</div></li><li><div>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithContainerResourceChange[1]</div></li><li><div>org.apache.hadoop.yarn.client.api.impl.TestAMRMProxy.testAMRMProxyTokenRenewal</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.TestRMRestart.testRMRestartRecoveringNodeLabelManager[CAPACITY]</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.metrics.TestSystemMetricsPublisher.testPublishApplicationMetrics</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestIncreaseAllocationExpirer.testDecreaseAfterIncreaseWithAllocationExpiration</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.fs.contract.router.web.TestRouterWebHDFSContractAppend.testRenameFileBeingAppended</div></li><li><div>org.apache.hadoop.hdfs.client.impl.TestBlockReaderLocal.testStatisticsForErasureCodingRead</div></li><li><div>org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestLazyPersistReplicaRecovery.testDnRestartWithSavedReplicas</div></li><li><div>org.apache.hadoop.hdfs.server.namenode.TestReencryptionWithKMS.testReencryptRaceRename</div></li><li><div>org.apache.hadoop.yarn.sls.TestSLSStreamAMSynth.testSimulatorRunning[Testing with: SYNTH, org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler, (nodeFile src/test/resources/nodes.json)]</div></li><li><div>org.apache.hadoop.yarn.service.monitor.probe.TestDefaultProbe.testDefaultProbe[2]</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestContainerResizing.testIncreaseContainerUnreservedWhenContainerCompleted</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="hbase"><div style="font-weight:bold;" class="panel-heading">HBASE<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Valencia)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>8e6ff689e806138e602a46730886d8c5f524fdcd</div><div><b>Last Run: </b>07-05-2018 14:09 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th><th>PPC RHEL7.5</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 4580</div><div>Failed Count : 0</div><div>Skipped Count : 38</div></td><td><div>Total Count : 4592</div><div>Failed Count : 5</div><div>Skipped Count : 38</div></td><td><div>Total Count : 4580</div><div>Failed Count : 5</div><div>Skipped Count : 38</div></td><td><div>Total Count : 4592</div><div>Failed Count : 1</div><div>Skipped Count : 38</div></td><td><div>Total Count : 4573</div><div>Failed Count : 0</div><div>Skipped Count : 39</div></td></tr><tr><td>Result</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.hbase.master.cleaner.TestHFileLinkCleaner.org.apache.hadoop.hbase.master.cleaner.TestHFileLinkCleaner</li></div><div><li>org.apache.hadoop.hbase.master.procedure.TestProcedureAdmin.testAbortProcedureInterruptedNotAllowed</li></div><div><li>org.apache.hadoop.hbase.regionserver.TestScannerHeartbeatMessages.testHeartbeatWithSparseRowFilter</li></div><div><li>org.apache.hadoop.hbase.replication.multiwal.TestReplicationEndpointWithMultipleAsyncWAL.testWALEntryFilterFromReplicationEndpoint</li></div><div><li>org.apache.hadoop.hbase.replication.multiwal.TestReplicationEndpointWithMultipleAsyncWAL.testWALEntryFilterUpdateValidation</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.hbase.TestClientOperationTimeout.testGetTimeout</li></div><div><li>org.apache.hadoop.hbase.TestClientOperationTimeout.testScanTimeout</li></div><div><li>org.apache.hadoop.hbase.client.TestClientPushback.testClientTracksServerPushback</li></div><div><li>org.apache.hadoop.hbase.io.asyncfs.TestFanOutOneBlockAsyncDFSOutput.testHeartbeat</li></div><div><li>org.apache.hadoop.hbase.master.TestAssignmentManagerMetrics.testRITAssignmentManagerMetrics</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.hbase.master.cleaner.TestHFileLinkCleaner.org.apache.hadoop.hbase.master.cleaner.TestHFileLinkCleaner</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>test timed out after 780 seconds</li></div><div><li>expected procedure result</li></div><div><li>Heartbeat messages are enabled, exceptions should NOT be thrown. Exception trace:org.apache.hadoop.hbase.client.RetriesExhaustedException: Failed after attempts=16, exceptions:
Mon May 07 18:05:36 UTC 2018, null, java.net.SocketTimeoutException: callTimeout=1000, callDuration=1066: Call to fb0225835501/172.17.0.4:40559 failed on local exception: org.apache.hadoop.hbase.ipc.CallTimeoutException: Ca</li></div><div><li>Call to fb0225835501/172.17.0.4:39320 failed on connection exception: org.apache.hbase.thirdparty.io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: fb0225835501/172.17.0.4:39320</li></div><div><li>Call to fb0225835501/172.17.0.4:39320 failed on connection exception: org.apache.hbase.thirdparty.io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: fb0225835501/172.17.0.4:39320</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>callTimeout=500, callDuration=612: Call to 12037faf78e6/172.17.0.6:35971 failed on local exception: org.apache.hadoop.hbase.ipc.CallTimeoutException: Call id=22, waitTime=504, rpcTimeout=494 </li></div><div><li>callTimeout=500, callDuration=622: Call to 12037faf78e6/172.17.0.6:35971 failed on local exception: org.apache.hadoop.hbase.ipc.CallTimeoutException: Call id=24, waitTime=504, rpcTimeout=496 </li></div><div><li>We did not find some load on the memstore expected:&lt;20&gt; but was:&lt;40&gt;</li></div><div><li>Timeout(2000ms) waiting for response</li></div><div><li>Metrics Should be equal expected:&lt;1&gt; but was:&lt;2&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>test timed out after 780 seconds</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.hbase.master.cleaner.TestHFileLinkCleaner.org.apache.hadoop.hbase.master.cleaner.TestHFileLinkCleaner</div></li><li><div>org.apache.hadoop.hbase.master.procedure.TestProcedureAdmin.testAbortProcedureInterruptedNotAllowed</div></li><li><div>org.apache.hadoop.hbase.regionserver.TestScannerHeartbeatMessages.testHeartbeatWithSparseRowFilter</div></li><li><div>org.apache.hadoop.hbase.replication.multiwal.TestReplicationEndpointWithMultipleAsyncWAL.testWALEntryFilterFromReplicationEndpoint</div></li><li><div>org.apache.hadoop.hbase.replication.multiwal.TestReplicationEndpointWithMultipleAsyncWAL.testWALEntryFilterUpdateValidation</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.hbase.TestClientOperationTimeout.testGetTimeout</div></li><li><div>org.apache.hadoop.hbase.TestClientOperationTimeout.testScanTimeout</div></li><li><div>org.apache.hadoop.hbase.client.TestClientPushback.testClientTracksServerPushback</div></li><li><div>org.apache.hadoop.hbase.io.asyncfs.TestFanOutOneBlockAsyncDFSOutput.testHeartbeat</div></li><li><div>org.apache.hadoop.hbase.master.TestAssignmentManagerMetrics.testRITAssignmentManagerMetrics</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.hbase.master.cleaner.TestHFileLinkCleaner.org.apache.hadoop.hbase.master.cleaner.TestHFileLinkCleaner</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="hive"><div style="font-weight:bold;" class="panel-heading">HIVE<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Alisha/Pravin)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>1121ab46ef4594c260312a813966637c5bff91c3</div><div><b>Last Run: </b>08-05-2018 15:00 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th><th>PPC RHEL7.5</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 7031</div><div>Failed Count : 6</div><div>Skipped Count : 226</div></td><td><div>Total Count : 7031</div><div>Failed Count : 12</div><div>Skipped Count : 226</div></td><td><div>Total Count : 7031</div><div>Failed Count : 5</div><div>Skipped Count : 226</div></td><td><div>Total Count : 7031</div><div>Failed Count : 10</div><div>Skipped Count : 226</div></td><td><div>Total Count : 7019</div><div>Failed Count : 5</div><div>Skipped Count : 226</div></td></tr><tr><td>Result</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskCommunicator.testFinishableStateUpdateFailure</li></div><div><li>org.apache.hadoop.hive.ql.TestTxnCommandsForMmTable.testInsertOverwriteForPartitionedMmTable</li></div><div><li>org.apache.hadoop.hive.ql.TestTxnCommandsForOrcMmTable.testInsertOverwriteForPartitionedMmTable</li></div><div><li>org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.testStringLikeMultiByte</li></div><div><li>org.apache.hive.spark.client.rpc.TestRpc.testServerPort</li></div><div><li>org.apache.hadoop.hive.metastore.TestOldSchema.testPartitionOps</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.hive.llap.daemon.impl.comparator.TestAMReporter.testMultipleAM</li></div><div><li>org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskCommunicator.testFinishableStateUpdateFailure</li></div><div><li>org.apache.hadoop.hive.ql.TestTxnCommands2.testDeleteIn</li></div><div><li>org.apache.hadoop.hive.ql.TestTxnCommands2.testInsertOverwrite1</li></div><div><li>org.apache.hadoop.hive.ql.TestTxnCommands2.testInsertOverwrite2</li></div><div><li>org.apache.hadoop.hive.ql.TestTxnCommands2WithSplitUpdateAndVectorization.testDeleteIn</li></div><div><li>org.apache.hadoop.hive.ql.TestTxnCommands2WithSplitUpdateAndVectorization.testInsertOverwrite1</li></div><div><li>org.apache.hadoop.hive.ql.TestTxnCommands2WithSplitUpdateAndVectorization.testInsertOverwrite2</li></div><div><li>org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.testStringLikeMultiByte</li></div><div><li>org.apache.hadoop.hive.ql.plan.mapping.TestReOptimization.testStatCachingMetaStore</li></div><div><li>org.apache.hive.spark.client.rpc.TestRpc.testServerPort</li></div><div><li>org.apache.hadoop.hive.metastore.TestOldSchema.testPartitionOps</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskCommunicator.testFinishableStateUpdateFailure</li></div><div><li>org.apache.hadoop.hive.ql.TestTxnCommandsForMmTable.testInsertOverwriteForPartitionedMmTable</li></div><div><li>org.apache.hadoop.hive.ql.TestTxnCommandsForOrcMmTable.testInsertOverwriteForPartitionedMmTable</li></div><div><li>org.apache.hive.spark.client.rpc.TestRpc.testServerPort</li></div><div><li>org.apache.hadoop.hive.metastore.TestOldSchema.testPartitionOps</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskCommunicator.testFinishableStateUpdateFailure</li></div><div><li>org.apache.hadoop.hive.ql.TestTxnCommands2.testDeleteIn</li></div><div><li>org.apache.hadoop.hive.ql.TestTxnCommands2.testInsertOverwrite1</li></div><div><li>org.apache.hadoop.hive.ql.TestTxnCommands2.testInsertOverwrite2</li></div><div><li>org.apache.hadoop.hive.ql.TestTxnCommands2WithSplitUpdateAndVectorization.testDeleteIn</li></div><div><li>org.apache.hadoop.hive.ql.TestTxnCommands2WithSplitUpdateAndVectorization.testInsertOverwrite1</li></div><div><li>org.apache.hadoop.hive.ql.TestTxnCommands2WithSplitUpdateAndVectorization.testInsertOverwrite2</li></div><div><li>org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.testStringLikeMultiByte</li></div><div><li>org.apache.hive.spark.client.rpc.TestRpc.testServerPort</li></div><div><li>org.apache.hadoop.hive.metastore.TestOldSchema.testPartitionOps</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.hive.llap.daemon.services.impl.TestLlapWebServices.testContextRootUrlRewrite</li></div><div><li>org.apache.hadoop.hive.ql.TestTxnCommands2.testNonAcidToAcidConversion1</li></div><div><li>org.apache.hadoop.hive.ql.TestTxnCommands2WithSplitUpdateAndVectorization.testNonAcidToAcidConversion1</li></div><div><li>org.apache.hadoop.hive.ql.TestTxnCommandsForMmTable.testInsertOverwriteForPartitionedMmTable</li></div><div><li>org.apache.hadoop.hive.ql.TestTxnCommandsForOrcMmTable.testInsertOverwriteForPartitionedMmTable</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"><div><li>java.lang.NullPointerException
	at org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos$SignableVertexSpec$Builder.setUser(LlapDaemonProtocolProtos.java:4899)
	at org.apache.hadoop.hive.llap.tez.Converters.constructSignableVertexSpec(Converters.java:135)
	at org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.constructSubmitWorkRequest(LlapTaskCommunicator.java:806)
	at org.apac</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.hive.ql.TestTxnCommandsForMmTable.testInsertOverwriteForPartitionedMmTable(TestTxnCommandsForMmTable.java:296)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.hive.ql.TestTxnCommandsForMmTable.testInsertOverwriteForPartitionedMmTable(TestTxnCommandsForMmTable.java:296)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl</li></div><div><li>expected:&lt;1&gt; but was:&lt;0&gt;</li></div><div><li>Malformed configuration value for hive.spark.client.rpc.server.port</li></div><div><li>SQL is not operational</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>test timed out after 5000 milliseconds</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos$SignableVertexSpec$Builder.setUser(LlapDaemonProtocolProtos.java:4899)
	at org.apache.hadoop.hive.llap.tez.Converters.constructSignableVertexSpec(Converters.java:135)
	at org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.constructSubmitWorkRequest(LlapTaskCommunicator.java:806)
	at org.apac</li></div><div><li>insert into acidTbl(a,b) select a,b from nonAcidOrcTbl failed: (responseCode = -101, errorMessage = FAILED: Execution Error, return code -101 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask. file:/var/lib/jenkins/workspace/hive/ql/target/tmp/org.apache.hadoop.hive.ql.TestTxnCommands2-1525816160867/warehouse/nonacidorctbl: PartitionDesc [tableDesc=TableDesc [inputFileFormatClass=class org.apache.</li></div><div><li>insert overwrite table acidTbl select a,b from nonAcidOrcTbl failed: (responseCode = -101, errorMessage = FAILED: Execution Error, return code -101 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask. file:/var/lib/jenkins/workspace/hive/ql/target/tmp/org.apache.hadoop.hive.ql.TestTxnCommands2-1525816160867/warehouse/nonacidorctbl: PartitionDesc [tableDesc=TableDesc [inputFileFormatClass=class org.a</li></div><div><li>insert overwrite table acidTbl select a,b from nonAcidOrcTbl failed: (responseCode = -101, errorMessage = FAILED: Execution Error, return code -101 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask. file:/var/lib/jenkins/workspace/hive/ql/target/tmp/org.apache.hadoop.hive.ql.TestTxnCommands2-1525816160867/warehouse/nonacidorctbl: PartitionDesc [tableDesc=TableDesc [inputFileFormatClass=class org.a</li></div><div><li>insert into acidTbl(a,b) select a,b from nonAcidOrcTbl failed: (responseCode = -101, errorMessage = FAILED: Execution Error, return code -101 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask. file:/var/lib/jenkins/workspace/hive/ql/target/tmp/org.apache.hadoop.hive.ql.TestTxnCommands2-1525817670743/warehouse/nonacidorctbl: PartitionDesc [tableDesc=TableDesc [inputFileFormatClass=class org.apache.</li></div><div><li>insert overwrite table acidTbl select a,b from nonAcidOrcTbl failed: (responseCode = -101, errorMessage = FAILED: Execution Error, return code -101 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask. file:/var/lib/jenkins/workspace/hive/ql/target/tmp/org.apache.hadoop.hive.ql.TestTxnCommands2-1525817670743/warehouse/nonacidorctbl: PartitionDesc [tableDesc=TableDesc [inputFileFormatClass=class org.a</li></div><div><li>insert overwrite table acidTbl select a,b from nonAcidOrcTbl failed: (responseCode = -101, errorMessage = FAILED: Execution Error, return code -101 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask. file:/var/lib/jenkins/workspace/hive/ql/target/tmp/org.apache.hadoop.hive.ql.TestTxnCommands2-1525817670743/warehouse/nonacidorctbl: PartitionDesc [tableDesc=TableDesc [inputFileFormatClass=class org.a</li></div><div><li>expected:&lt;1&gt; but was:&lt;0&gt;</li></div><div><li>expectation of the usage of runtime stats doesn't match expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>Malformed configuration value for hive.spark.client.rpc.server.port</li></div><div><li>SQL is not operational</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>java.lang.NullPointerException
	at org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos$SignableVertexSpec$Builder.setUser(LlapDaemonProtocolProtos.java:4899)
	at org.apache.hadoop.hive.llap.tez.Converters.constructSignableVertexSpec(Converters.java:135)
	at org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.constructSubmitWorkRequest(LlapTaskCommunicator.java:806)
	at org.apac</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.hive.ql.TestTxnCommandsForMmTable.testInsertOverwriteForPartitionedMmTable(TestTxnCommandsForMmTable.java:296)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.hive.ql.TestTxnCommandsForMmTable.testInsertOverwriteForPartitionedMmTable(TestTxnCommandsForMmTable.java:296)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl</li></div><div><li>Malformed configuration value for hive.spark.client.rpc.server.port</li></div><div><li>SQL is not operational</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>java.lang.NullPointerException
	at org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos$SignableVertexSpec$Builder.setUser(LlapDaemonProtocolProtos.java:4899)
	at org.apache.hadoop.hive.llap.tez.Converters.constructSignableVertexSpec(Converters.java:135)
	at org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.constructSubmitWorkRequest(LlapTaskCommunicator.java:806)
	at org.apac</li></div><div><li>insert into acidTbl(a,b) select a,b from nonAcidOrcTbl failed: (responseCode = -101, errorMessage = FAILED: Execution Error, return code -101 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask. file:/var/lib/jenkins/workspace/hive/ql/target/tmp/org.apache.hadoop.hive.ql.TestTxnCommands2-1525812872484/warehouse/nonacidorctbl: PartitionDesc [tableDesc=TableDesc [inputFileFormatClass=class org.apache.</li></div><div><li>insert overwrite table acidTbl select a,b from nonAcidOrcTbl failed: (responseCode = -101, errorMessage = FAILED: Execution Error, return code -101 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask. file:/var/lib/jenkins/workspace/hive/ql/target/tmp/org.apache.hadoop.hive.ql.TestTxnCommands2-1525812872484/warehouse/nonacidorctbl: PartitionDesc [tableDesc=TableDesc [inputFileFormatClass=class org.a</li></div><div><li>insert overwrite table acidTbl select a,b from nonAcidOrcTbl failed: (responseCode = -101, errorMessage = FAILED: Execution Error, return code -101 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask. file:/var/lib/jenkins/workspace/hive/ql/target/tmp/org.apache.hadoop.hive.ql.TestTxnCommands2-1525812872484/warehouse/nonacidorctbl: PartitionDesc [tableDesc=TableDesc [inputFileFormatClass=class org.a</li></div><div><li>insert into acidTbl(a,b) select a,b from nonAcidOrcTbl failed: (responseCode = -101, errorMessage = FAILED: Execution Error, return code -101 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask. file:/var/lib/jenkins/workspace/hive/ql/target/tmp/org.apache.hadoop.hive.ql.TestTxnCommands2-1525809310709/warehouse/nonacidorctbl: PartitionDesc [tableDesc=TableDesc [inputFileFormatClass=class org.apache.</li></div><div><li>insert overwrite table acidTbl select a,b from nonAcidOrcTbl failed: (responseCode = -101, errorMessage = FAILED: Execution Error, return code -101 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask. file:/var/lib/jenkins/workspace/hive/ql/target/tmp/org.apache.hadoop.hive.ql.TestTxnCommands2-1525809310709/warehouse/nonacidorctbl: PartitionDesc [tableDesc=TableDesc [inputFileFormatClass=class org.a</li></div><div><li>insert overwrite table acidTbl select a,b from nonAcidOrcTbl failed: (responseCode = -101, errorMessage = FAILED: Execution Error, return code -101 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask. file:/var/lib/jenkins/workspace/hive/ql/target/tmp/org.apache.hadoop.hive.ql.TestTxnCommands2-1525809310709/warehouse/nonacidorctbl: PartitionDesc [tableDesc=TableDesc [inputFileFormatClass=class org.a</li></div><div><li>expected:&lt;1&gt; but was:&lt;0&gt;</li></div><div><li>Malformed configuration value for hive.spark.client.rpc.server.port</li></div><div><li>SQL is not operational</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;200&gt; but was:&lt;500&gt;</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.hive.ql.TestTxnCommands2.testNonAcidToAcidConversion1(TestTxnCommands2.java:499)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorIm</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.hive.ql.TestTxnCommands2.testNonAcidToAcidConversion1(TestTxnCommands2.java:499)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorIm</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.hive.ql.TestTxnCommandsForMmTable.testInsertOverwriteForPartitionedMmTable(TestTxnCommandsForMmTable.java:296)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.hive.ql.TestTxnCommandsForMmTable.testInsertOverwriteForPartitionedMmTable(TestTxnCommandsForMmTable.java:296)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.hive.ql.TestTxnCommandsForMmTable.testInsertOverwriteForPartitionedMmTable</div></li><li><div>org.apache.hadoop.hive.ql.TestTxnCommandsForOrcMmTable.testInsertOverwriteForPartitionedMmTable</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.hive.llap.daemon.impl.comparator.TestAMReporter.testMultipleAM</div></li><li><div>org.apache.hadoop.hive.ql.TestTxnCommands2.testDeleteIn</div></li><li><div>org.apache.hadoop.hive.ql.TestTxnCommands2.testInsertOverwrite1</div></li><li><div>org.apache.hadoop.hive.ql.TestTxnCommands2.testInsertOverwrite2</div></li><li><div>org.apache.hadoop.hive.ql.TestTxnCommands2WithSplitUpdateAndVectorization.testDeleteIn</div></li><li><div>org.apache.hadoop.hive.ql.TestTxnCommands2WithSplitUpdateAndVectorization.testInsertOverwrite1</div></li><li><div>org.apache.hadoop.hive.ql.TestTxnCommands2WithSplitUpdateAndVectorization.testInsertOverwrite2</div></li><li><div>org.apache.hadoop.hive.ql.plan.mapping.TestReOptimization.testStatCachingMetaStore</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.hive.ql.TestTxnCommandsForMmTable.testInsertOverwriteForPartitionedMmTable</div></li><li><div>org.apache.hadoop.hive.ql.TestTxnCommandsForOrcMmTable.testInsertOverwriteForPartitionedMmTable</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.hive.ql.TestTxnCommands2.testDeleteIn</div></li><li><div>org.apache.hadoop.hive.ql.TestTxnCommands2.testInsertOverwrite1</div></li><li><div>org.apache.hadoop.hive.ql.TestTxnCommands2.testInsertOverwrite2</div></li><li><div>org.apache.hadoop.hive.ql.TestTxnCommands2WithSplitUpdateAndVectorization.testDeleteIn</div></li><li><div>org.apache.hadoop.hive.ql.TestTxnCommands2WithSplitUpdateAndVectorization.testInsertOverwrite1</div></li><li><div>org.apache.hadoop.hive.ql.TestTxnCommands2WithSplitUpdateAndVectorization.testInsertOverwrite2</div></li><li><div>org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.testStringLikeMultiByte</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="kafka"><div style="font-weight:bold;" class="panel-heading">KAFKA<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Sonia)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/trunk</div><div><b>Last Revision: </b>2b5a59406622affa1e333e073546f075f59b4ac9</div><div><b>Last Run: </b>08-05-2018 11:58 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th><th>PPC RHEL7.5</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 8909</div><div>Failed Count : 0</div><div>Skipped Count : 7</div></td><td><div>Total Count : 8909</div><div>Failed Count : 1</div><div>Skipped Count : 7</div></td><td><div>Total Count : 8909</div><div>Failed Count : 1</div><div>Skipped Count : 7</div></td><td><div>Total Count : 8909</div><div>Failed Count : 0</div><div>Skipped Count : 7</div></td><td><div>Total Count : 8930</div><div>Failed Count : 1</div><div>Skipped Count : 6</div></td></tr><tr><td>Result</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.kafka.common.network.SslSelectorTest.testCloseConnectionInClosingState</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.kafka.clients.producer.internals.BufferPoolTest.testBlockTimeout</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>kafka.api.SaslSslAdminClientIntegrationTest.testLogStartOffsetCheckpoint</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>java.lang.AssertionError: Channel not expired expected null, but was:&lt;org.apache.kafka.common.network.KafkaChannel@4f&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>java.lang.AssertionError: available memory8</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: Aborted due to timeout.</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.kafka.common.network.SslSelectorTest.testCloseConnectionInClosingState</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.kafka.clients.producer.internals.BufferPoolTest.testBlockTimeout</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="knox"><div style="font-weight:bold;" class="panel-heading">KNOX<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Yussuf)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>069c028cbf0135e56b29df84ee77ad4b2cf19a82</div><div><b>Last Run: </b>08-05-2018 11:22 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th><th>PPC RHEL7.5</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 1004</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1002</div><div>Failed Count : 2</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1004</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1004</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1004</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td></tr><tr><td>Result</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.knox.gateway.GatewayLdapGroupFuncTest.org.apache.knox.gateway.GatewayLdapGroupFuncTest</li></div><div><li>org.apache.knox.gateway.Knox242FuncTest.org.apache.knox.gateway.Knox242FuncTest</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>Timed out 10000 waiting for URL http://localhost:46224/gateway/test-cluster/test-service-path/test-service-resource</li></div><div><li>Timed out 10000 waiting for URL http://localhost:40546/gateway/testdg-cluster/test-service-path/test-service-resource</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.knox.gateway.GatewayLdapGroupFuncTest.org.apache.knox.gateway.GatewayLdapGroupFuncTest</div></li><li><div>org.apache.knox.gateway.Knox242FuncTest.org.apache.knox.gateway.Knox242FuncTest</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="metron"><div style="font-weight:bold;" class="panel-heading">METRON<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Pravin)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>ac05638160288b32535b986adbeb8f14e594e740</div><div><b>Last Run: </b>08-05-2018 11:53 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th><th>PPC RHEL7.5</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 1724</div><div>Failed Count : 1</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1724</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1724</div><div>Failed Count : 1</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1724</div><div>Failed Count : 1</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1724</div><div>Failed Count : 1</div><div>Skipped Count : 2</div></td></tr><tr><td>Result</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"><div><li>org.apache.metron.stellar.common.CachingStellarProcessorTest.testCaching</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.metron.stellar.common.CachingStellarProcessorTest.testCaching</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.metron.stellar.common.CachingStellarProcessorTest.testCaching</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.metron.stellar.common.CachingStellarProcessorTest.testCaching</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;6&gt; but was:&lt;5&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;6&gt; but was:&lt;3&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;6&gt; but was:&lt;4&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;6&gt; but was:&lt;4&gt;</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.metron.stellar.common.CachingStellarProcessorTest.testCaching</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="oozie"><div style="font-weight:bold;" class="panel-heading">OOZIE<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Alisha)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>553d5d4dbe50d73133443b1bf4bc44f841e19dbd</div><div><b>Last Run: </b>08-05-2018 11:59 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th><th>PPC RHEL7.5</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 2129</div><div>Failed Count : 30</div><div>Skipped Count : 2</div></td><td><div>Total Count : 2129</div><div>Failed Count : 31</div><div>Skipped Count : 2</div></td><td><div>Total Count : 2129</div><div>Failed Count : 3</div><div>Skipped Count : 2</div></td><td><div>Total Count : 2129</div><div>Failed Count : 2</div><div>Skipped Count : 2</div></td><td><div>Total Count : 2125</div><div>Failed Count : 4</div><div>Skipped Count : 2</div></td></tr><tr><td>Result</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"><div><li>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testCredentialsSkip</li></div><div><li>org.apache.oozie.jms.TestDefaultConnectionContext.testThreadLocalSession</li></div><div><li>org.apache.oozie.jms.TestHCatMessageHandler.testDropEventTypeMessage</li></div><div><li>org.apache.oozie.jms.TestHCatMessageHandler.testCacheUpdateByMessage</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testCoordinatorActionSelectors</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testCoordinatorActionSelectorsNegative</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectors</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorJobSuccessEvent</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobSuspendEvent</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectorsAnd</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorJobFailureEvent</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testConnectionDrop</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectorsNegative</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectorsOr</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobSuccessEvent</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorActionWaitingEvent</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobFailureEvent</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorActionStartEvent</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobStartedEvent</li></div><div><li>org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLADurationMetEvent</li></div><div><li>org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAStartMetEvent</li></div><div><li>org.apache.oozie.jms.TestJMSSLAEventListener.testSLAJobSelectorsNegative</li></div><div><li>org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAEndMetEvent</li></div><div><li>org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAStartMissEvent</li></div><div><li>org.apache.oozie.jms.TestJMSSLAEventListener.testSLAJobSelectors</li></div><div><li>org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAEndMissEvent</li></div><div><li>org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLADurationMissEvent</li></div><div><li>org.apache.oozie.service.TestJMSAccessorService.testConnectionRetry</li></div><div><li>org.apache.oozie.service.TestPartitionDependencyManagerEhcache.testPartitionDependency</li></div><div><li>org.apache.oozie.tools.TestOozieDBCLI.testOozieDBCLI</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testCredentialsSkip</li></div><div><li>org.apache.oozie.command.coord.TestCoordChangeXCommand.testCoordChangeEndTime3</li></div><div><li>org.apache.oozie.jms.TestDefaultConnectionContext.testThreadLocalSession</li></div><div><li>org.apache.oozie.jms.TestHCatMessageHandler.testDropEventTypeMessage</li></div><div><li>org.apache.oozie.jms.TestHCatMessageHandler.testCacheUpdateByMessage</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testCoordinatorActionSelectors</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testCoordinatorActionSelectorsNegative</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectors</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorJobSuccessEvent</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobSuspendEvent</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectorsAnd</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorJobFailureEvent</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testConnectionDrop</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectorsNegative</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectorsOr</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobSuccessEvent</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorActionWaitingEvent</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobFailureEvent</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorActionStartEvent</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobStartedEvent</li></div><div><li>org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLADurationMetEvent</li></div><div><li>org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAStartMetEvent</li></div><div><li>org.apache.oozie.jms.TestJMSSLAEventListener.testSLAJobSelectorsNegative</li></div><div><li>org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAEndMetEvent</li></div><div><li>org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAStartMissEvent</li></div><div><li>org.apache.oozie.jms.TestJMSSLAEventListener.testSLAJobSelectors</li></div><div><li>org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAEndMissEvent</li></div><div><li>org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLADurationMissEvent</li></div><div><li>org.apache.oozie.service.TestJMSAccessorService.testConnectionRetry</li></div><div><li>org.apache.oozie.service.TestPartitionDependencyManagerService.testPartitionDependency</li></div><div><li>org.apache.oozie.tools.TestOozieDBCLI.testOozieDBCLI</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.oozie.service.TestStatusTransitService.testBundleStatusCoordSubmitFails</li></div><div><li>org.apache.oozie.service.TestStatusTransitService.testCoordStatusTransitWithLock</li></div><div><li>org.apache.oozie.tools.TestOozieDBCLI.testOozieDBCLI</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.oozie.command.coord.TestCoordActionsKillXCommand.testActionKillCommandDate</li></div><div><li>org.apache.oozie.tools.TestOozieDBCLI.testOozieDBCLI</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.oozie.command.coord.TestCoordPushDependencyCheckXCommand.testTimeOutWithException1</li></div><div><li>org.apache.oozie.service.TestStatusTransitService.testCoordStatusTransitWithLock</li></div><div><li>org.apache.oozie.service.TestZKLocksService.testTimeoutTimingOutWriteLockOozies</li></div><div><li>org.apache.oozie.tools.TestOozieDBCLI.testOozieDBCLI</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"><div><li>JA020: Could not load credentials of type [abc] with name [abcname]]; perhaps it was not defined in oozie-site.xml?</li></div><div><li>java.lang.NullPointerException
	at org.apache.oozie.jms.TestDefaultConnectionContext.testThreadLocalSession(TestDefaultConnectionContext.java:74)
</li></div><div><li>Could not create Transport. Reason: javax.management.InstanceAlreadyExistsException: org.apache.activemq:type=Broker,brokerName=localhost</li></div><div><li>Could not create Transport. Reason: javax.management.InstanceAlreadyExistsException: org.apache.activemq:type=Broker,brokerName=localhost</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testCoordinatorActionSelectors(TestJMSJobEventListener.java:544)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testCoordinatorActionSelectorsNegative(TestJMSJobEventListener.java:568)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectors(TestJMSJobEventListener.java:239)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorJobSuccessEvent(TestJMSJobEventListener.java:477)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobSuspendEvent(TestJMSJobEventListener.java:214)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectorsAnd(TestJMSJobEventListener.java:316)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorJobFailureEvent(TestJMSJobEventListener.java:517)
</li></div><div><li>org.apache.activemq:type=Broker,brokerName=localhost</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectorsNegative(TestJMSJobEventListener.java:262)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectorsOr(TestJMSJobEventListener.java:289)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobSuccessEvent(TestJMSJobEventListener.java:143)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorActionWaitingEvent(TestJMSJobEventListener.java:403)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobFailureEvent(TestJMSJobEventListener.java:180)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorActionStartEvent(TestJMSJobEventListener.java:439)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobStartedEvent(TestJMSJobEventListener.java:108)
</li></div><div><li>java.lang.NullPointerException
	at org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLADurationMetEvent(TestJMSSLAEventListener.java:382)
</li></div><div><li>java.lang.NullPointerException
	at org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAStartMetEvent(TestJMSSLAEventListener.java:292)
</li></div><div><li>java.lang.NullPointerException
	at org.apache.oozie.jms.TestJMSSLAEventListener.testSLAJobSelectorsNegative(TestJMSSLAEventListener.java:261)
</li></div><div><li>java.lang.NullPointerException
	at org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAEndMetEvent(TestJMSSLAEventListener.java:332)
</li></div><div><li>java.lang.NullPointerException
	at org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAStartMissEvent(TestJMSSLAEventListener.java:103)
</li></div><div><li>java.lang.NullPointerException
	at org.apache.oozie.jms.TestJMSSLAEventListener.testSLAJobSelectors(TestJMSSLAEventListener.java:231)
</li></div><div><li>java.lang.NullPointerException
	at org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAEndMissEvent(TestJMSSLAEventListener.java:143)
</li></div><div><li>java.lang.NullPointerException
	at org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLADurationMissEvent(TestJMSSLAEventListener.java:191)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.service.TestJMSAccessorService.testConnectionRetry(TestJMSAccessorService.java:183)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.service.TestPartitionDependencyManagerEhcache.testPartitionDependency(TestPartitionDependencyManagerEhcache.java:47)
</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>JA020: Could not load credentials of type [abc] with name [abcname]]; perhaps it was not defined in oozie-site.xml?</li></div><div><li>expected:&lt;SUCCEEDED&gt; but was:&lt;RUNNING&gt;</li></div><div><li>java.lang.NullPointerException
	at org.apache.oozie.jms.TestDefaultConnectionContext.testThreadLocalSession(TestDefaultConnectionContext.java:74)
</li></div><div><li>Could not create Transport. Reason: javax.management.InstanceAlreadyExistsException: org.apache.activemq:type=Broker,brokerName=localhost</li></div><div><li>Could not create Transport. Reason: javax.management.InstanceAlreadyExistsException: org.apache.activemq:type=Broker,brokerName=localhost</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testCoordinatorActionSelectors(TestJMSJobEventListener.java:544)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testCoordinatorActionSelectorsNegative(TestJMSJobEventListener.java:568)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectors(TestJMSJobEventListener.java:239)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorJobSuccessEvent(TestJMSJobEventListener.java:477)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobSuspendEvent(TestJMSJobEventListener.java:214)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectorsAnd(TestJMSJobEventListener.java:316)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorJobFailureEvent(TestJMSJobEventListener.java:517)
</li></div><div><li>org.apache.activemq:type=Broker,brokerName=localhost</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectorsNegative(TestJMSJobEventListener.java:262)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectorsOr(TestJMSJobEventListener.java:289)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobSuccessEvent(TestJMSJobEventListener.java:143)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorActionWaitingEvent(TestJMSJobEventListener.java:403)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobFailureEvent(TestJMSJobEventListener.java:180)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorActionStartEvent(TestJMSJobEventListener.java:439)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobStartedEvent(TestJMSJobEventListener.java:108)
</li></div><div><li>java.lang.NullPointerException
	at org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLADurationMetEvent(TestJMSSLAEventListener.java:382)
</li></div><div><li>java.lang.NullPointerException
	at org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAStartMetEvent(TestJMSSLAEventListener.java:292)
</li></div><div><li>java.lang.NullPointerException
	at org.apache.oozie.jms.TestJMSSLAEventListener.testSLAJobSelectorsNegative(TestJMSSLAEventListener.java:261)
</li></div><div><li>java.lang.NullPointerException
	at org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAEndMetEvent(TestJMSSLAEventListener.java:332)
</li></div><div><li>java.lang.NullPointerException
	at org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAStartMissEvent(TestJMSSLAEventListener.java:103)
</li></div><div><li>java.lang.NullPointerException
	at org.apache.oozie.jms.TestJMSSLAEventListener.testSLAJobSelectors(TestJMSSLAEventListener.java:231)
</li></div><div><li>java.lang.NullPointerException
	at org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAEndMissEvent(TestJMSSLAEventListener.java:143)
</li></div><div><li>java.lang.NullPointerException
	at org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLADurationMissEvent(TestJMSSLAEventListener.java:191)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.service.TestJMSAccessorService.testConnectionRetry(TestJMSAccessorService.java:183)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.service.TestPartitionDependencyManagerService.testPartitionDependency(TestPartitionDependencyManagerService.java:95)
</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;FAILED&gt; but was:&lt;RUNNING&gt;</li></div><div><li>expected:&lt;RUNNINGWITHERROR&gt; but was:&lt;RUNNING&gt;</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;RUNNING&gt; but was:&lt;KILLED&gt;</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;TIMEDOUT&gt; but was:&lt;WAITING&gt;</li></div><div><li>expected:&lt;RUNNING&gt; but was:&lt;RUNNINGWITHERROR&gt;</li></div><div><li>expected:&lt;a:1-L a:2-N[ a:1-U]&gt; but was:&lt;a:1-L a:2-N[]&gt;</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.oozie.service.TestPartitionDependencyManagerEhcache.testPartitionDependency</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.oozie.command.coord.TestCoordChangeXCommand.testCoordChangeEndTime3</div></li><li><div>org.apache.oozie.service.TestPartitionDependencyManagerService.testPartitionDependency</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.oozie.service.TestStatusTransitService.testBundleStatusCoordSubmitFails</div></li><li><div>org.apache.oozie.service.TestStatusTransitService.testCoordStatusTransitWithLock</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.oozie.command.coord.TestCoordActionsKillXCommand.testActionKillCommandDate</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="phoenix"><div style="font-weight:bold;" class="panel-heading">PHOENIX<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Valencia)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>0675fe5459dae5bab27934d8d35ec257fe733f3c</div><div><b>Last Run: </b>08-05-2018 11:22 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th><th>PPC RHEL7.5</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 1694</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 1694</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 1694</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 1694</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 1694</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td></tr><tr><td>Result</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="pig"><div style="font-weight:bold;" class="panel-heading">PIG<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Yussuf)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/trunk</div><div><b>Last Revision: </b>84889e7852c98c67b40d12116e3bb6dd311a3363</div><div><b>Last Run: </b>08-05-2018 11:59 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th><th>PPC RHEL7.5</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 723</div><div>Failed Count : 6</div><div>Skipped Count : 0</div></td><td><div>Total Count : 895</div><div>Failed Count : 20</div><div>Skipped Count : 0</div></td><td><div>Total Count : 895</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 895</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 895</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td></tr><tr><td>Result</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"><div><li>org.apache.pig.test.TestBuiltin.testRANDOMWithJob</li></div><div><li>org.apache.pig.test.TestLoad.testCommaSeparatedString3</li></div><div><li>org.apache.pig.test.TestLocalRearrange.testMultiQueryJiraPig1194</li></div><div><li>org.apache.pig.test.TestPigServer.testRegisterRemoteScript</li></div><div><li>org.apache.pig.test.TestSchema.testEnabledDisambiguationPassesForDupeAliases</li></div><div><li>org.apache.pig.test.TestStore.testStore</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.pig.test.TestBuiltin.testRANDOMWithJob</li></div><div><li>org.apache.pig.test.TestBuiltin.testSFPig</li></div><div><li>org.apache.pig.test.TestBuiltin.testUniqueID</li></div><div><li>org.apache.pig.test.TestLoad.testCommaSeparatedString3</li></div><div><li>org.apache.pig.test.TestLoad.testCommaSeparatedString5</li></div><div><li>org.apache.pig.test.TestLoad.testLoadRemoteAbsScheme</li></div><div><li>org.apache.pig.test.TestLocalRearrange.testMultiQueryJiraPig1194</li></div><div><li>org.apache.pig.test.TestPigServer.testRegisterRemoteScript</li></div><div><li>org.apache.pig.test.TestSchema.testEnabledDisambiguationPassesForDupeAliases</li></div><div><li>org.apache.pig.test.TestSchema.testSchemaSerialization</li></div><div><li>org.apache.pig.test.TestSchema.testDisabledDisambiguationContainsNoColonsForNestedSchema</li></div><div><li>org.apache.pig.test.TestSchema.testDisabledDisambiguationContainsNoColons</li></div><div><li>org.apache.pig.test.TestStore.testStore</li></div><div><li>org.apache.pig.test.TestStore.testStoreComplexDataWithNull</li></div><div><li>org.apache.pig.test.TestStore.testBinStorageGetSchema</li></div><div><li>org.apache.pig.test.TestStore.testStoreComplexData</li></div><div><li>org.apache.pig.test.TestStore.testSetStoreSchema</li></div><div><li>org.apache.pig.test.TestStore.testSuccessFileCreation1</li></div><div><li>org.apache.pig.test.TestStore.testCleanupOnFailureMultiStore</li></div><div><li>org.apache.pig.test.TestStore.testEmptyPartFileCreation</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"><div><li>Timeout occurred. Please note the time in the report does not reflect the time until the timeout.</li></div><div><li>Timeout occurred. Please note the time in the report does not reflect the time until the timeout.</li></div><div><li>Timeout occurred. Please note the time in the report does not reflect the time until the timeout.</li></div><div><li>Timeout occurred. Please note the time in the report does not reflect the time until the timeout.</li></div><div><li>Timeout occurred. Please note the time in the report does not reflect the time until the timeout.</li></div><div><li>Timeout occurred. Please note the time in the report does not reflect the time until the timeout.</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Unable to open iterator for alias B</li></div><div><li>Input path does not exist: hdfs://localhost:41860/user/jenkins/testSFPig-output.txt</li></div><div><li>Unable to open iterator for alias B</li></div><div><li>Unable to open iterator for alias a</li></div><div><li>Unable to open iterator for alias a</li></div><div><li>Unable to open iterator for alias a</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.pig.test.TestLocalRearrange.testMultiQueryJiraPig1194(TestLocalRearrange.java:231)
</li></div><div><li>Unable to open iterator for alias b</li></div><div><li>Unable to open iterator for alias C</li></div><div><li>Unable to open iterator for alias c</li></div><div><li>Unable to open iterator for alias F</li></div><div><li>Unable to open iterator for alias E</li></div><div><li>File /tmp/TestStore/TestStore-output-7633889282329631457.txt does not exist.</li></div><div><li>File /tmp/TestStore/TestStore-output--6393187357910495654.txt does not exist.</li></div><div><li>Checking binstorage getSchema output</li></div><div><li>File /tmp/TestStore/TestStore-output--6660124898629015874.txt does not exist.</li></div><div><li>Checking if file /tmp/TestStore/_commitJob_called does  exists in MAPREDUCE mode expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>Checking if _SUCCESS file exists in MAPREDUCE mode expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>Checking if file /tmp/TestStore/_setupTask_called1 does  exists in MAPREDUCE mode expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>File /tmp/TestStore/TestStore-output--3863060909626205378.txt_1 does not exist.</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.pig.test.TestBuiltin.testSFPig</div></li><li><div>org.apache.pig.test.TestBuiltin.testUniqueID</div></li><li><div>org.apache.pig.test.TestLoad.testCommaSeparatedString5</div></li><li><div>org.apache.pig.test.TestLoad.testLoadRemoteAbsScheme</div></li><li><div>org.apache.pig.test.TestSchema.testSchemaSerialization</div></li><li><div>org.apache.pig.test.TestSchema.testDisabledDisambiguationContainsNoColonsForNestedSchema</div></li><li><div>org.apache.pig.test.TestSchema.testDisabledDisambiguationContainsNoColons</div></li><li><div>org.apache.pig.test.TestStore.testStoreComplexDataWithNull</div></li><li><div>org.apache.pig.test.TestStore.testBinStorageGetSchema</div></li><li><div>org.apache.pig.test.TestStore.testStoreComplexData</div></li><li><div>org.apache.pig.test.TestStore.testSetStoreSchema</div></li><li><div>org.apache.pig.test.TestStore.testSuccessFileCreation1</div></li><li><div>org.apache.pig.test.TestStore.testCleanupOnFailureMultiStore</div></li><li><div>org.apache.pig.test.TestStore.testEmptyPartFileCreation</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="ranger"><div style="font-weight:bold;" class="panel-heading">RANGER<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Sneha)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>f21e2b4cfa61b9e2d4f28e35e0a965b99090bf6e</div><div><b>Last Run: </b>08-05-2018 12:01 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th><th>PPC RHEL7.5</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 1077</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1077</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1077</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1077</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1077</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td></tr><tr><td>Result</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="slider"><div style="font-weight:bold;" class="panel-heading">SLIDER<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Yussuf)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/develop</div><div><b>Last Revision: </b>4032999f35db4877b6b8ffc5e97a59837e22365b</div><div><b>Last Run: </b>08-05-2018 12:01 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th><th>PPC RHEL7.5</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 607</div><div>Failed Count : 24</div><div>Skipped Count : 13</div></td><td><div>Total Count : 607</div><div>Failed Count : 25</div><div>Skipped Count : 13</div></td><td><div>Total Count : 607</div><div>Failed Count : 0</div><div>Skipped Count : 13</div></td><td><div>Total Count : 607</div><div>Failed Count : 0</div><div>Skipped Count : 13</div></td><td><div>Total Count : 607</div><div>Failed Count : 0</div><div>Skipped Count : 13</div></td></tr><tr><td>Result</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"><div><li>org.apache.slider.agent.actions.TestActionExists.testExistsLiveCluster</li></div><div><li>org.apache.slider.agent.actions.TestActionList.testActionListSuite</li></div><div><li>org.apache.slider.agent.actions.TestActionStatus.testSuite</li></div><div><li>org.apache.slider.agent.freezethaw.TestFreezeCommands.testFreezeCommands</li></div><div><li>org.apache.slider.agent.freezethaw.TestFreezeThawFlexStandaloneAM.testFreezeThawFlexStandaloneAM</li></div><div><li>org.apache.slider.agent.rest.TestStandaloneREST.testStandaloneREST</li></div><div><li>org.apache.slider.agent.standalone.TestBuildStandaloneAM.testBuildCluster</li></div><div><li>org.apache.slider.agent.standalone.TestBuildStandaloneAM.testUpdateCluster</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneAMDestroy.testStandaloneAMDestroy</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneAMKill.testKillStandaloneAM</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneAMRestart.testStandaloneAMRestartWithDefaultRetryWindow</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneAMRestart.testStandaloneAMRestart</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneAMRestart.testStandaloneAMRestartWithRetryWindow</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneAgentAM.testStandaloneAgentAM</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneYarnRegistryAM.testStandaloneYarnRegistryAM</li></div><div><li>org.apache.slider.client.TestDiagnostics.testContainerDiagsNoAppContainer</li></div><div><li>org.apache.slider.client.TestDiagnostics.testContainerDiagsWithAppPackage</li></div><div><li>org.apache.slider.client.TestUpgradeCommandOptions.testAll</li></div><div><li>org.apache.slider.providers.agent.TestAddonPackage.testEchoApplicationAddPackage</li></div><div><li>org.apache.slider.providers.agent.TestAgentAAEcho.testAgentEcho</li></div><div><li>org.apache.slider.providers.agent.TestAgentAMManagementWS.testAgentAMManagementWS</li></div><div><li>org.apache.slider.providers.agent.TestAgentEcho.testAgentEcho</li></div><div><li>org.apache.slider.server.appmaster.TestDelayInContainerLaunch.testDelayInContainerLaunch</li></div><div><li>org.apache.slider.server.appmaster.web.rest.publisher.TestPublisherRestResources.testRestURIs</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.slider.agent.actions.TestActionExists.testExistsLiveCluster</li></div><div><li>org.apache.slider.agent.actions.TestActionList.testActionListSuite</li></div><div><li>org.apache.slider.agent.actions.TestActionStatus.testSuite</li></div><div><li>org.apache.slider.agent.freezethaw.TestFreezeCommands.testFreezeCommands</li></div><div><li>org.apache.slider.agent.freezethaw.TestFreezeThawFlexStandaloneAM.testFreezeThawFlexStandaloneAM</li></div><div><li>org.apache.slider.agent.rest.TestStandaloneREST.testStandaloneREST</li></div><div><li>org.apache.slider.agent.standalone.TestBuildStandaloneAM.testBuildCluster</li></div><div><li>org.apache.slider.agent.standalone.TestBuildStandaloneAM.testUpdateCluster</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneAMDestroy.testStandaloneAMDestroy</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneAMKill.testKillStandaloneAM</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneAMRestart.testStandaloneAMRestartWithDefaultRetryWindow</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneAMRestart.testStandaloneAMRestart</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneAMRestart.testStandaloneAMRestartWithRetryWindow</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneAgentAM.testStandaloneAgentAM</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneYarnRegistryAM.testStandaloneYarnRegistryAM</li></div><div><li>org.apache.slider.client.TestDiagnostics.testContainerDiagsNoAppContainer</li></div><div><li>org.apache.slider.client.TestDiagnostics.testContainerDiagsWithAppPackage</li></div><div><li>org.apache.slider.client.TestUpgradeCommandOptions.testAll</li></div><div><li>org.apache.slider.providers.agent.TestAddonPackage.testEchoApplicationAddPackage</li></div><div><li>org.apache.slider.providers.agent.TestAgentAAEcho.testAgentEcho</li></div><div><li>org.apache.slider.providers.agent.TestAgentAMManagementWS.testAgentAMManagementWS</li></div><div><li>org.apache.slider.providers.agent.TestAgentEcho.testAgentEcho</li></div><div><li>org.apache.slider.providers.agent.TestHeartbeatMonitor.testRegularHeartbeat</li></div><div><li>org.apache.slider.server.appmaster.TestDelayInContainerLaunch.testDelayInContainerLaunch</li></div><div><li>org.apache.slider.server.appmaster.web.rest.publisher.TestPublisherRestResources.testRestURIs</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"><div><li>Launch failed with exit code -1</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Application not running: application_1525806011663_0001 state=FAILED </li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Failed on local exception: java.io.FileNotFoundException: http://4d433166a45e:35614/cluster/app/application_1525805559984_0001; Host Details : local host is: "localhost"; destination host is: "http://4d433166a45e:35614/cluster/app/application_1525805559984_0001":35614; </li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>assert report.yarnApplicationState == YarnApplicationState.RUNNING
       |      |                                            |
       |      FAILED                                       RUNNING
       applicationId { id: 1 cluster_timestamp: 1525805952168 } user: "jenkins" queue: "default" name: "testkillstandaloneam" host: "N/A" rpc_port: -1 yarn_application_state: FAILED trackingUrl: "http://4d</li></div><div><li>Cluster teststandaloneamrestartwithdefaultretrywindow not live after 30000 ms</li></div><div><li>Cluster teststandaloneamrestart not live after 30000 ms</li></div><div><li>Cluster teststandaloneamrestartwithretrywindow not live after 30000 ms</li></div><div><li>assert uri.port in 60000..60010
       |   |            |
       |   36857        [60000, 60001, 60002, 60003, 60004, 60005, 60006, 60007, 60008, 60009, 60010]
       http://4d433166a45e:36857/cluster/app/application_1525805808892_0001</li></div><div><li>Application not running: application_1525805832370_0001 state=FAILED </li></div><div><li>assert 0 == status
         |  |
         |  -1
         false</li></div><div><li>Launch failed with exit code -1</li></div><div><li>Upgrade command should have failed</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Launch failed with exit code -1</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Application not running: application_1525807779128_0001 state=FAILED </li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Failed on local exception: java.io.FileNotFoundException: http://2578affd298a:44744/cluster/app/application_1525807812455_0001; Host Details : local host is: "localhost"; destination host is: "http://2578affd298a:44744/cluster/app/application_1525807812455_0001":44744; </li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>assert report.yarnApplicationState == YarnApplicationState.RUNNING
       |      |                                            |
       |      FAILED                                       RUNNING
       applicationId { id: 1 cluster_timestamp: 1525807384134 } user: "jenkins" queue: "default" name: "testkillstandaloneam" host: "N/A" rpc_port: -1 yarn_application_state: FAILED trackingUrl: "http://25</li></div><div><li>Cluster teststandaloneamrestartwithdefaultretrywindow not live after 30000 ms</li></div><div><li>Cluster teststandaloneamrestart not live after 30000 ms</li></div><div><li>Cluster teststandaloneamrestartwithretrywindow not live after 30000 ms</li></div><div><li>assert uri.port in 60000..60010
       |   |            |
       |   33152        [60000, 60001, 60002, 60003, 60004, 60005, 60006, 60007, 60008, 60009, 60010]
       http://2578affd298a:33152/cluster/app/application_1525807355590_0001</li></div><div><li>Application not running: application_1525807442266_0001 state=FAILED </li></div><div><li>assert 0 == status
         |  |
         |  -1
         false</li></div><div><li>Launch failed with exit code -1</li></div><div><li>Upgrade command should have failed</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>java.lang.AssertionError: null
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertFalse(Assert.java:64)
	at org.junit.Assert.assertFalse(Assert.java:74)
	at org.apache.slider.providers.agent.TestHeartbeatMonitor.testRegularHeartbeat(TestHeartbeatMonitor.java:51)
</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.slider.providers.agent.TestHeartbeatMonitor.testRegularHeartbeat</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="spark"><div style="font-weight:bold;" class="panel-heading">SPARK<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Parita)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>8dbf56c055218ff0f3fabae84b63c022f43afbfd</div><div><b>Last Run: </b>02-05-2018 18:59 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th><th>PPC RHEL7.5</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 13828</div><div>Failed Count : 0</div><div>Skipped Count : 670</div></td><td><div>Total Count : 15810</div><div>Failed Count : 2</div><div>Skipped Count : 678</div></td><td><div>Total Count : 13828</div><div>Failed Count : 1</div><div>Skipped Count : 670</div></td><td><div>Total Count : 15810</div><div>Failed Count : 1</div><div>Skipped Count : 678</div></td><td><div>Total Count : 13799</div><div>Failed Count : 3</div><div>Skipped Count : 670</div></td></tr><tr><td>Result</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.spark.deploy.yarn.YarnClusterSuite.timeout to get SparkContext in cluster mode triggers failure</li></div><div><li>org.apache.spark.sql.hive.HiveSparkSubmitSuite.SPARK-8020: set sql conf in spark conf</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.spark.sql.DataFrameRangeSuite.Cancelling stage in a query with Range.</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.spark.sql.hive.HiveSparkSubmitSuite.SPARK-8020: set sql conf in spark conf</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.spark.sql.DataFrameRangeSuite.Cancelling stage in a query with Range.</li></div><div><li>org.apache.spark.sql.execution.streaming.sources.RateSourceSuite.microbatch - basic</li></div><div><li>org.apache.spark.sql.execution.ui.SQLAppStatusListenerSuite.driver side SQL metrics</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>The code passed to eventually never returned normally. Attempted 130 times over 2.0061177331 minutes. Last failure message: handle.getState().isFinal() was false.</li></div><div><li>Timeout of './bin/spark-submit' '--class' 'org.apache.spark.sql.hive.SparkSQLConfTest' '--name' 'SparkSQLConfTest' '--master' 'local-cluster[2,1,1024]' '--conf' 'spark.ui.enabled=false' '--conf' 'spark.master.rest.enabled=false' '--conf' 'spark.sql.hive.metastore.version=0.12' '--conf' 'spark.sql.hive.metastore.jars=maven' '--driver-java-options' '-Dderby.system.durability=test' 'file:/var/lib/jen</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Expected exception org.apache.spark.SparkException to be thrown, but no exception was thrown</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Timeout of './bin/spark-submit' '--class' 'org.apache.spark.sql.hive.SparkSQLConfTest' '--name' 'SparkSQLConfTest' '--master' 'local-cluster[2,1,1024]' '--conf' 'spark.ui.enabled=false' '--conf' 'spark.master.rest.enabled=false' '--conf' 'spark.sql.hive.metastore.version=0.12' '--conf' 'spark.sql.hive.metastore.jars=maven' '--driver-java-options' '-Dderby.system.durability=test' 'file:/var/lib/jen</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Expected exception org.apache.spark.SparkException to be thrown, but no exception was thrown</li></div><div><li>Query [id = 8fb54b4e-2e49-4f02-8fcc-de73bfd723ff, runId = c0491696-ae0c-494a-bc1d-136ab684d365] terminated with exception: assertion failed: startSeconds(1) &gt; endSeconds(0)</li></div><div><li>java.util.NoSuchElementException was thrown.</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.spark.deploy.yarn.YarnClusterSuite.timeout to get SparkContext in cluster mode triggers failure</div></li><li><div>org.apache.spark.sql.hive.HiveSparkSubmitSuite.SPARK-8020: set sql conf in spark conf</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.spark.sql.DataFrameRangeSuite.Cancelling stage in a query with Range.</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.spark.sql.hive.HiveSparkSubmitSuite.SPARK-8020: set sql conf in spark conf</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="sqoop"><div style="font-weight:bold;" class="panel-heading">SQOOP<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Talat)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/trunk</div><div><b>Last Revision: </b>11c83f68386add243762929ecf7f6f25a99efbf4</div><div><b>Last Run: </b>09-05-2018 06:36 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th><th>PPC RHEL7.5</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 776</div><div>Failed Count : 3</div><div>Skipped Count : 0</div></td><td><div>Total Count : 776</div><div>Failed Count : 4</div><div>Skipped Count : 0</div></td><td><div>Total Count : 776</div><div>Failed Count : 3</div><div>Skipped Count : 0</div></td><td><div>Total Count : 776</div><div>Failed Count : 3</div><div>Skipped Count : 0</div></td><td><div>Total Count : 776</div><div>Failed Count : 3</div><div>Skipped Count : 0</div></td></tr><tr><td>Result</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"><div><li>org.apache.sqoop.hive.TestHiveImport.testAppendHiveImportAsParquet</li></div><div><li>org.apache.sqoop.hive.TestHiveImport.testNormalHiveImportAsParquet</li></div><div><li>org.apache.sqoop.hive.TestHiveImport.testCreateOverwriteHiveImportAsParquet</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.sqoop.hive.TestHiveImport.testAppendHiveImportAsParquet</li></div><div><li>org.apache.sqoop.hive.TestHiveImport.testNormalHiveImportAsParquet</li></div><div><li>org.apache.sqoop.hive.TestHiveImport.testCreateOverwriteHiveImportAsParquet</li></div><div><li>org.apache.sqoop.hive.TestHiveMiniCluster.testInsertedRowCanBeReadFromTable[config = org.apache.sqoop.hive.minicluster.KerberosAuthenticationConfiguration@731a74c]</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.sqoop.hive.TestHiveImport.testAppendHiveImportAsParquet</li></div><div><li>org.apache.sqoop.hive.TestHiveImport.testNormalHiveImportAsParquet</li></div><div><li>org.apache.sqoop.hive.TestHiveImport.testCreateOverwriteHiveImportAsParquet</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.sqoop.hive.TestHiveImport.testAppendHiveImportAsParquet</li></div><div><li>org.apache.sqoop.hive.TestHiveImport.testNormalHiveImportAsParquet</li></div><div><li>org.apache.sqoop.hive.TestHiveImport.testCreateOverwriteHiveImportAsParquet</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.sqoop.hive.TestHiveImport.testAppendHiveImportAsParquet</li></div><div><li>org.apache.sqoop.hive.TestHiveImport.testNormalHiveImportAsParquet</li></div><div><li>org.apache.sqoop.hive.TestHiveImport.testCreateOverwriteHiveImportAsParquet</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"><div><li>java.io.FileNotFoundException: File /var/lib/jenkins/workspace/sqoop/build/test/data/sqoop/warehouse/APPEND_HIVE_IMPORT_AS_PARQUET does not exist</li></div><div><li>java.io.FileNotFoundException: File /var/lib/jenkins/workspace/sqoop/build/test/data/sqoop/warehouse/NORMAL_HIVE_IMPORT_AS_PARQUET does not exist</li></div><div><li>java.io.FileNotFoundException: File /var/lib/jenkins/workspace/sqoop/build/test/data/sqoop/warehouse/CREATE_OVERWRITE_HIVE_IMPORT_AS_PARQUET does not exist</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>java.io.FileNotFoundException: File /var/lib/jenkins/workspace/sqoop/build/test/data/sqoop/warehouse/APPEND_HIVE_IMPORT_AS_PARQUET does not exist</li></div><div><li>java.io.FileNotFoundException: File /var/lib/jenkins/workspace/sqoop/build/test/data/sqoop/warehouse/NORMAL_HIVE_IMPORT_AS_PARQUET does not exist</li></div><div><li>java.io.FileNotFoundException: File /var/lib/jenkins/workspace/sqoop/build/test/data/sqoop/warehouse/CREATE_OVERWRITE_HIVE_IMPORT_AS_PARQUET does not exist</li></div><div><li>Establishing connection failed!</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>java.io.FileNotFoundException: File /var/lib/jenkins/workspace/sqoop/build/test/data/sqoop/warehouse/APPEND_HIVE_IMPORT_AS_PARQUET does not exist</li></div><div><li>java.io.FileNotFoundException: File /var/lib/jenkins/workspace/sqoop/build/test/data/sqoop/warehouse/NORMAL_HIVE_IMPORT_AS_PARQUET does not exist</li></div><div><li>java.io.FileNotFoundException: File /var/lib/jenkins/workspace/sqoop/build/test/data/sqoop/warehouse/CREATE_OVERWRITE_HIVE_IMPORT_AS_PARQUET does not exist</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>java.io.FileNotFoundException: File /var/lib/jenkins/workspace/sqoop/build/test/data/sqoop/warehouse/APPEND_HIVE_IMPORT_AS_PARQUET does not exist</li></div><div><li>java.io.FileNotFoundException: File /var/lib/jenkins/workspace/sqoop/build/test/data/sqoop/warehouse/NORMAL_HIVE_IMPORT_AS_PARQUET does not exist</li></div><div><li>java.io.FileNotFoundException: File /var/lib/jenkins/workspace/sqoop/build/test/data/sqoop/warehouse/CREATE_OVERWRITE_HIVE_IMPORT_AS_PARQUET does not exist</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>java.io.FileNotFoundException: File /var/lib/jenkins/workspace/sqoop/build/test/data/sqoop/warehouse/APPEND_HIVE_IMPORT_AS_PARQUET does not exist</li></div><div><li>java.io.FileNotFoundException: File /var/lib/jenkins/workspace/sqoop/build/test/data/sqoop/warehouse/NORMAL_HIVE_IMPORT_AS_PARQUET does not exist</li></div><div><li>java.io.FileNotFoundException: File /var/lib/jenkins/workspace/sqoop/build/test/data/sqoop/warehouse/CREATE_OVERWRITE_HIVE_IMPORT_AS_PARQUET does not exist</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.sqoop.hive.TestHiveMiniCluster.testInsertedRowCanBeReadFromTable[config = org.apache.sqoop.hive.minicluster.KerberosAuthenticationConfiguration@731a74c]</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="storm"><div style="font-weight:bold;" class="panel-heading">STORM<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Parita)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>57658f7fb83149cd16acbad5b089b8c268b729db</div><div><b>Last Run: </b>03-05-2018 01:18 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th><th>PPC RHEL7.5</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 1200</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 1200</div><div>Failed Count : 3</div><div>Skipped Count : 5</div></td><td><div>Total Count : 1200</div><div>Failed Count : 1</div><div>Skipped Count : 5</div></td><td><div>Total Count : 1200</div><div>Failed Count : 1</div><div>Skipped Count : 5</div></td><td><div>Total Count : 1200</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td></tr><tr><td>Result</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithoutKey</li></div><div><li>org.apache.storm.kafka.KafkaUtilsTest.generateTuplesWithKeyAndKeyValueScheme</li></div><div><li>org.apache.storm.TickTupleTest.testTickTupleWorksWithSystemBolt</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.storm.utils.JCQueueTest.testFirstMessageFirst</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.storm.flux.multilang.MultilangEnvironmentTest.testInvokeNode</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>
Wanted but not invoked:
collector.ack(
    source: null:1, stream: , id: {}, [value-234] PROC_START_TIME(sampled): null EXEC_START_TIME(sampled): null
);
-&gt; at org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithoutKey(KafkaBoltTest.java:291)

However, there were exactly 2 interactions with this mock:
collector.reportError(
    java.util.concurrent.ExecutionException: org.apache.kafka.common.err</li></div><div><li>org.apache.kafka.common.errors.TimeoutException: Failed to update metadata after 72 ms.</li></div><div><li>took over 110000 ms of simulated time to get a message back...</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Unable to send halt interrupt</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Cannot run program "node": error=2, No such file or directory</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.storm.kafka.bolt.KafkaBoltTest.executeWithoutKey</div></li><li><div>org.apache.storm.kafka.KafkaUtilsTest.generateTuplesWithKeyAndKeyValueScheme</div></li><li><div>org.apache.storm.TickTupleTest.testTickTupleWorksWithSystemBolt</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.storm.utils.JCQueueTest.testFirstMessageFirst</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.storm.flux.multilang.MultilangEnvironmentTest.testInvokeNode</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="tez"><div style="font-weight:bold;" class="panel-heading">TEZ<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Valencia)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>c5c26c655d43c1c6602e50d1bdf3a69e9b40fd50</div><div><b>Last Run: </b>03-05-2018 03:53 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th><th>PPC RHEL7.5</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 1797</div><div>Failed Count : 2</div><div>Skipped Count : 14</div></td><td><div>Total Count : 1797</div><div>Failed Count : 4</div><div>Skipped Count : 14</div></td><td><div>Total Count : 1797</div><div>Failed Count : 2</div><div>Skipped Count : 14</div></td><td><div>Total Count : 1797</div><div>Failed Count : 2</div><div>Skipped Count : 14</div></td><td><div>Total Count : 1794</div><div>Failed Count : 2</div><div>Skipped Count : 14</div></td></tr><tr><td>Result</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"><div><li>org.apache.tez.dag.app.TestMockDAGAppMaster.testInternalPreemption</li></div><div><li>org.apache.tez.tests.TestExtServicesWithLocalMode.test1</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.tez.tests.TestExtServicesWithLocalMode.test1</li></div><div><li>org.apache.tez.history.TestHistoryParser.testParserWithFailedJob</li></div><div><li>org.apache.tez.history.TestHistoryParser.testParserWithSuccessfulJob</li></div><div><li>org.apache.tez.analyzer.TestAnalyzer.testWithATS</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.tez.tests.TestExtServicesWithLocalMode.test1</li></div><div><li>org.apache.tez.test.TestExceptionPropagation.testExceptionPropagationSession</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.tez.tests.TestExtServicesWithLocalMode.test1</li></div><div><li>org.apache.tez.test.TestExceptionPropagation.testExceptionPropagationSession</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.tez.test.TestExceptionPropagation.testExceptionPropagationSession</li></div><div><li>org.apache.tez.analyzer.TestAnalyzer.testWithATS</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;KILLED&gt; but was:&lt;FAILED&gt;</li></div><div><li>org.apache.tez.dag.api.TezUncheckedException: java.lang.reflect.InvocationTargetException</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.tez.dag.api.TezUncheckedException: java.lang.reflect.InvocationTargetException</li></div><div><li>java.lang.AssertionError: null
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.tez.history.TestHistoryParser.testParserWithFailedJob(TestHistoryParser.java:383)
</li></div><div><li>java.lang.AssertionError: null
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.tez.history.TestHistoryParser.testParserWithSuccessfulJob(TestHistoryParser.java:207)
</li></div><div><li>java.lang.AssertionError: null
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.tez.analyzer.TestAnalyzer.getDagInfo(TestAnalyzer.java:264)
	at org.apache.tez.analyzer.TestAnalyzer.verify(TestAnalyzer.java:251)
	at org.apache.tez.analyzer.TestAnalyzer.runTests(TestAnalyzer.java:390)
	at org.apac</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.tez.dag.api.TezUncheckedException: java.lang.reflect.InvocationTargetException</li></div><div><li>Application not running, applicationId=application_1525323680552_0001, yarnApplicationState=FAILED, finalApplicationStatus=FAILED, trackingUrl=N/A, diagnostics=[DAG completed with an ERROR state. Shutting down AM, Session stats:submittedDAGs=10, successfulDAGs=0, failedDAGs=11, killedDAGs=0]</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.tez.dag.api.TezUncheckedException: java.lang.reflect.InvocationTargetException</li></div><div><li>Application not running, applicationId=application_1525320902014_0001, yarnApplicationState=FAILED, finalApplicationStatus=FAILED, trackingUrl=N/A, diagnostics=[DAG completed with an ERROR state. Shutting down AM, Session stats:submittedDAGs=16, successfulDAGs=0, failedDAGs=17, killedDAGs=0]</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Application not running, applicationId=application_1525325982549_0001, yarnApplicationState=FAILED, finalApplicationStatus=FAILED, trackingUrl=N/A, diagnostics=[DAG completed with an ERROR state. Shutting down AM, Session stats:submittedDAGs=7, successfulDAGs=0, failedDAGs=8, killedDAGs=0]</li></div><div><li>java.lang.AssertionError: null
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.tez.analyzer.TestAnalyzer.getDagInfo(TestAnalyzer.java:264)
	at org.apache.tez.analyzer.TestAnalyzer.verify(TestAnalyzer.java:251)
	at org.apache.tez.analyzer.TestAnalyzer.runTests(TestAnalyzer.java:390)
	at org.apac</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.tez.dag.app.TestMockDAGAppMaster.testInternalPreemption</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.tez.history.TestHistoryParser.testParserWithFailedJob</div></li><li><div>org.apache.tez.history.TestHistoryParser.testParserWithSuccessfulJob</div></li><li><div>org.apache.tez.analyzer.TestAnalyzer.testWithATS</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="zeppelin"><div style="font-weight:bold;" class="panel-heading">ZEPPELIN<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Sneha)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>b9222381bfe4a9e8a055076a05b50fa6e13a0e05</div><div><b>Last Run: </b>07-05-2018 13:54 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th><th>PPC RHEL7.5</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 812</div><div>Failed Count : 11</div><div>Skipped Count : 5</div></td><td><div>Total Count : 812</div><div>Failed Count : 16</div><div>Skipped Count : 5</div></td><td><div>Total Count : 812</div><div>Failed Count : 12</div><div>Skipped Count : 5</div></td><td><div>Total Count : 812</div><div>Failed Count : 17</div><div>Skipped Count : 5</div></td><td><div>Total Count : 812</div><div>Failed Count : 9</div><div>Skipped Count : 5</div></td></tr><tr><td>Result</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"><div><li>org.apache.zeppelin.pig.PigInterpreterTest.testIncludeJobStats</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTest.testBasics</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTezTest.testIncludeJobStats</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTezTest.testBasics</li></div><div><li>org.apache.zeppelin.pig.PigQueryInterpreterTest.testMaxResult</li></div><div><li>org.apache.zeppelin.pig.PigQueryInterpreterTest.testBasics</li></div><div><li>org.apache.zeppelin.python.IPythonInterpreterTest.testIPythonPlotting</li></div><div><li>org.apache.zeppelin.spark.IPySparkInterpreterTest.testIPythonPlotting</li></div><div><li>org.apache.zeppelin.spark.IPySparkInterpreterTest.testPythonBasics</li></div><div><li>org.apache.zeppelin.spark.PySparkInterpreterTest.testPythonBasics</li></div><div><li>org.apache.zeppelin.rest.KnoxRestApiTest.testThatOtherUserCanAccessNoteIfPermissionNotSet</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.zeppelin.pig.PigInterpreterTest.testIncludeJobStats</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTest.testBasics</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTezTest.testIncludeJobStats</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTezTest.testBasics</li></div><div><li>org.apache.zeppelin.pig.PigQueryInterpreterTest.testMaxResult</li></div><div><li>org.apache.zeppelin.pig.PigQueryInterpreterTest.testBasics</li></div><div><li>org.apache.zeppelin.python.IPythonInterpreterTest.testIPythonPlotting</li></div><div><li>org.apache.zeppelin.spark.IPySparkInterpreterTest.testIPythonPlotting</li></div><div><li>org.apache.zeppelin.spark.IPySparkInterpreterTest.testPythonBasics</li></div><div><li>org.apache.zeppelin.spark.PySparkInterpreterTest.testPythonBasics</li></div><div><li>org.apache.zeppelin.rest.KnoxRestApiTest.testThatOtherUserCanAccessNoteIfPermissionNotSet</li></div><div><li>org.apache.zeppelin.security.SecurityUtilsTest.canGetPrincipalName</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClusterMode[0]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testLocalMode[0]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClusterMode[2]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testLocalMode[2]</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.zeppelin.pig.PigInterpreterTest.testIncludeJobStats</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTest.testBasics</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTezTest.testIncludeJobStats</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTezTest.testBasics</li></div><div><li>org.apache.zeppelin.pig.PigQueryInterpreterTest.testMaxResult</li></div><div><li>org.apache.zeppelin.pig.PigQueryInterpreterTest.testBasics</li></div><div><li>org.apache.zeppelin.python.IPythonInterpreterTest.testIPythonPlotting</li></div><div><li>org.apache.zeppelin.spark.IPySparkInterpreterTest.testIPythonPlotting</li></div><div><li>org.apache.zeppelin.spark.IPySparkInterpreterTest.testPythonBasics</li></div><div><li>org.apache.zeppelin.spark.PySparkInterpreterTest.testPythonBasics</li></div><div><li>org.apache.zeppelin.interpreter.InterpreterOutputChangeWatcherTest.test</li></div><div><li>org.apache.zeppelin.rest.KnoxRestApiTest.testThatOtherUserCanAccessNoteIfPermissionNotSet</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.zeppelin.pig.PigInterpreterTest.testIncludeJobStats</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTest.testBasics</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTezTest.testIncludeJobStats</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTezTest.testBasics</li></div><div><li>org.apache.zeppelin.pig.PigQueryInterpreterTest.testMaxResult</li></div><div><li>org.apache.zeppelin.pig.PigQueryInterpreterTest.testBasics</li></div><div><li>org.apache.zeppelin.python.IPythonInterpreterTest.testIPythonPlotting</li></div><div><li>org.apache.zeppelin.spark.IPySparkInterpreterTest.testIPythonPlotting</li></div><div><li>org.apache.zeppelin.spark.IPySparkInterpreterTest.testPythonBasics</li></div><div><li>org.apache.zeppelin.spark.PySparkInterpreterTest.testPythonBasics</li></div><div><li>org.apache.zeppelin.rest.KnoxRestApiTest.testThatOtherUserCanAccessNoteIfPermissionNotSet</li></div><div><li>org.apache.zeppelin.helium.HeliumBundleFactoryTest.downloadPackage</li></div><div><li>org.apache.zeppelin.helium.HeliumBundleFactoryTest.bundleErrorPropagation</li></div><div><li>org.apache.zeppelin.helium.HeliumBundleFactoryTest.bundlePackage</li></div><div><li>org.apache.zeppelin.helium.HeliumBundleFactoryTest.testInstallNpm</li></div><div><li>org.apache.zeppelin.helium.HeliumBundleFactoryTest.bundleLocalPackage</li></div><div><li>org.apache.zeppelin.helium.HeliumBundleFactoryTest.switchVersion</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.zeppelin.pig.PigInterpreterTest.testIncludeJobStats</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTest.testBasics</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTezTest.testIncludeJobStats</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTezTest.testBasics</li></div><div><li>org.apache.zeppelin.pig.PigQueryInterpreterTest.testMaxResult</li></div><div><li>org.apache.zeppelin.pig.PigQueryInterpreterTest.testBasics</li></div><div><li>org.apache.zeppelin.python.IPythonInterpreterTest.testIPythonPlotting</li></div><div><li>org.apache.zeppelin.spark.IPySparkInterpreterTest.testIPythonPlotting</li></div><div><li>org.apache.zeppelin.rest.KnoxRestApiTest.testThatOtherUserCanAccessNoteIfPermissionNotSet</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>org.apache.hadoop.yarn.api.records.LocalResource.setShouldBeUploadedToSharedCache(Z)V</li></div><div><li>org.apache.hadoop.yarn.api.records.LocalResource.setShouldBeUploadedToSharedCache(Z)V</li></div><div><li>expected:&lt;TABLE&gt; but was:&lt;TEXT&gt;</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>No Figure Text</li></div><div><li>No Figure Text</li></div><div><li>expected:&lt;[]
&gt; but was:&lt;[??]
&gt;</li></div><div><li>expected:&lt;[]
&gt; but was:&lt;[??]
&gt;</li></div><div><li>java.lang.NullPointerException: null
	at org.apache.zeppelin.rest.KnoxRestApiTest.testThatOtherUserCanAccessNoteIfPermissionNotSet(KnoxRestApiTest.java:80)
</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>org.apache.hadoop.yarn.api.records.LocalResource.setShouldBeUploadedToSharedCache(Z)V</li></div><div><li>org.apache.hadoop.yarn.api.records.LocalResource.setShouldBeUploadedToSharedCache(Z)V</li></div><div><li>expected:&lt;TABLE&gt; but was:&lt;TEXT&gt;</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>No Figure Text</li></div><div><li>No Figure Text</li></div><div><li>expected:&lt;[]
&gt; but was:&lt;[??]
&gt;</li></div><div><li>expected:&lt;[]
&gt; but was:&lt;[??]
&gt;</li></div><div><li>java.lang.NullPointerException: null
	at org.apache.zeppelin.rest.KnoxRestApiTest.testThatOtherUserCanAccessNoteIfPermissionNotSet(KnoxRestApiTest.java:80)
</li></div><div><li>expected:&lt;java.security.[Principal.getN]ame()&gt; but was:&lt;java.security.[principal.getn]ame()&gt;</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>org.apache.hadoop.yarn.api.records.LocalResource.setShouldBeUploadedToSharedCache(Z)V</li></div><div><li>org.apache.hadoop.yarn.api.records.LocalResource.setShouldBeUploadedToSharedCache(Z)V</li></div><div><li>expected:&lt;TABLE&gt; but was:&lt;TEXT&gt;</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>No Figure Text</li></div><div><li>No Figure Text</li></div><div><li>expected:&lt;[]
&gt; but was:&lt;[??]
&gt;</li></div><div><li>expected:&lt;[]
&gt; but was:&lt;[??]
&gt;</li></div><div><li>java.lang.AssertionError: null
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertNotNull(Assert.java:712)
	at org.junit.Assert.assertNotNull(Assert.java:722)
	at org.apache.zeppelin.interpreter.InterpreterOutputChangeWatcherTest.test(InterpreterOutputChangeWatcherTest.java:99)
</li></div><div><li>java.lang.NullPointerException: null
	at org.apache.zeppelin.rest.KnoxRestApiTest.testThatOtherUserCanAccessNoteIfPermissionNotSet(KnoxRestApiTest.java:80)
</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>org.apache.hadoop.yarn.api.records.LocalResource.setShouldBeUploadedToSharedCache(Z)V</li></div><div><li>org.apache.hadoop.yarn.api.records.LocalResource.setShouldBeUploadedToSharedCache(Z)V</li></div><div><li>expected:&lt;TABLE&gt; but was:&lt;TEXT&gt;</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>No Figure Text</li></div><div><li>No Figure Text</li></div><div><li>expected:&lt;[]
&gt; but was:&lt;[??]
&gt;</li></div><div><li>expected:&lt;[]
&gt; but was:&lt;[??]
&gt;</li></div><div><li>java.lang.NullPointerException: null
	at org.apache.zeppelin.rest.KnoxRestApiTest.testThatOtherUserCanAccessNoteIfPermissionNotSet(KnoxRestApiTest.java:80)
</li></div><div><li>'npm install lodash@3.9.3 --fetch-retries=2 --fetch-retry-factor=1 --fetch-retry-mintimeout=5000 --registry=http://registry.npmjs.org/' failed. (error code 1)</li></div><div><li>java.lang.AssertionError: null
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.zeppelin.helium.HeliumBundleFactoryTest.bundleErrorPropagation(HeliumBundleFactoryTest.java:164)
</li></div><div><li>com.github.eirslett.maven.plugins.frontend.lib.TaskRunnerException: 'npm pack zeppelin-bubblechart@0.0.3 --registry=http://registry.npmjs.org/' failed. (error code 1)</li></div><div><li>java.lang.AssertionError: null
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.zeppelin.helium.HeliumBundleFactoryTest.testInstallNpm(HeliumBundleFactoryTest.java:79)
</li></div><div><li>com.github.eirslett.maven.plugins.frontend.lib.TaskRunnerException: 'yarn install --fetch-retries=2 --fetch-retry-factor=1 --fetch-retry-mintimeout=5000 --registry=http://registry.npmjs.org/' failed.</li></div><div><li>com.github.eirslett.maven.plugins.frontend.lib.TaskRunnerException: 'npm pack zeppelin-bubblechart@0.0.3 --registry=http://registry.npmjs.org/' failed. (error code 1)</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>org.apache.hadoop.yarn.api.records.LocalResource.setShouldBeUploadedToSharedCache(Z)V</li></div><div><li>org.apache.hadoop.yarn.api.records.LocalResource.setShouldBeUploadedToSharedCache(Z)V</li></div><div><li>expected:&lt;TABLE&gt; but was:&lt;TEXT&gt;</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>No Figure Text</li></div><div><li>No Figure Text</li></div><div><li>java.lang.NullPointerException: null
	at org.apache.zeppelin.rest.KnoxRestApiTest.testThatOtherUserCanAccessNoteIfPermissionNotSet(KnoxRestApiTest.java:80)
</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.zeppelin.security.SecurityUtilsTest.canGetPrincipalName</div></li><li><div>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClusterMode[0]</div></li><li><div>org.apache.zeppelin.interpreter.SparkIntegrationTest.testLocalMode[0]</div></li><li><div>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClusterMode[2]</div></li><li><div>org.apache.zeppelin.interpreter.SparkIntegrationTest.testLocalMode[2]</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.zeppelin.interpreter.InterpreterOutputChangeWatcherTest.test</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.zeppelin.helium.HeliumBundleFactoryTest.downloadPackage</div></li><li><div>org.apache.zeppelin.helium.HeliumBundleFactoryTest.bundleErrorPropagation</div></li><li><div>org.apache.zeppelin.helium.HeliumBundleFactoryTest.bundlePackage</div></li><li><div>org.apache.zeppelin.helium.HeliumBundleFactoryTest.testInstallNpm</div></li><li><div>org.apache.zeppelin.helium.HeliumBundleFactoryTest.bundleLocalPackage</div></li><li><div>org.apache.zeppelin.helium.HeliumBundleFactoryTest.switchVersion</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="zookeeper"><div style="font-weight:bold;" class="panel-heading">ZOOKEEPER<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Pravin)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>2022766ec907f63b1b43fc5455e9e7761cd332f6</div><div><b>Last Run: </b>08-05-2018 11:55 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th><th>PPC RHEL7.5</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 1140</div><div>Failed Count : 1</div><div>Skipped Count : 1</div></td><td><div>Total Count : 1140</div><div>Failed Count : 2</div><div>Skipped Count : 1</div></td><td><div>Total Count : 1137</div><div>Failed Count : 5</div><div>Skipped Count : 1</div></td><td><div>Total Count : 1137</div><div>Failed Count : 3</div><div>Skipped Count : 1</div></td><td><div>Total Count : 1129</div><div>Failed Count : 5</div><div>Skipped Count : 1</div></td></tr><tr><td>Result</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"><div><li>org.apache.zookeeper.test.WatchEventWhenAutoResetTest.testNodeDataChanged</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.zookeeper.server.quorum.Zab1_0Test.testNormalFollowerRunWithDiff</li></div><div><li>org.apache.zookeeper.test.WatchEventWhenAutoResetTest.testNodeDataChanged</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.zookeeper.server.quorum.Zab1_0Test.testNormalFollowerRunWithDiff</li></div><div><li>junit.framework.TestSuite.org.apache.zookeeper.server.quorum.auth.MiniKdcTest</li></div><div><li>junit.framework.TestSuite.org.apache.zookeeper.server.quorum.auth.QuorumKerberosAuthTest</li></div><div><li>junit.framework.TestSuite.org.apache.zookeeper.server.quorum.auth.QuorumKerberosHostBasedAuthTest</li></div><div><li>org.apache.zookeeper.test.ReadOnlyModeTest.testMultiTransaction</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>junit.framework.TestSuite.org.apache.zookeeper.server.quorum.auth.MiniKdcTest</li></div><div><li>junit.framework.TestSuite.org.apache.zookeeper.server.quorum.auth.QuorumKerberosAuthTest</li></div><div><li>junit.framework.TestSuite.org.apache.zookeeper.server.quorum.auth.QuorumKerberosHostBasedAuthTest</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.zookeeper.server.quorum.QuorumPeerMainTest.testFailedTxnAsPartOfQuorumLoss</li></div><div><li>org.apache.zookeeper.server.quorum.ReconfigRecoveryTest.testCurrentServersAreObserversInNextConfig</li></div><div><li>org.apache.zookeeper.server.quorum.Zab1_0Test.testNormalObserverRun</li></div><div><li>org.apache.zookeeper.test.FLETest.testJoin</li></div><div><li>org.apache.zookeeper.test.WatchEventWhenAutoResetTest.testNodeDataChanged</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;NodeDataChanged&gt; but was:&lt;NodeDeleted&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.jute.BinaryInputArchive.readInt(BinaryInputArchive.java:63)
	at org.apache.zookeeper.server.persistence.FileHeader.deserialize(FileHeader.java:66)
	at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.inStreamCreated(FileTxnLog.java:620)
	at org.apache.zookeeper.server.persistence.Fil</li></div><div><li>expected:&lt;NodeDataChanged&gt; but was:&lt;NodeDeleted&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;4294967298&gt; but was:&lt;0&gt;</li></div><div><li>Unable to parse:includedir /etc/krb5.conf.d/</li></div><div><li>Unable to parse:includedir /etc/krb5.conf.d/</li></div><div><li>Unable to parse:includedir /etc/krb5.conf.d/</li></div><div><li>KeeperErrorCode = Not a read-only call for /tnode1</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Unable to parse:includedir /etc/krb5.conf.d/</li></div><div><li>Unable to parse:includedir /etc/krb5.conf.d/</li></div><div><li>Unable to parse:includedir /etc/krb5.conf.d/</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;1&gt; but was:&lt;3&gt;</li></div><div><li>waiting for server 4 being up</li></div><div><li>Timeout occurred. Please note the time in the report does not reflect the time until the timeout.</li></div><div><li>Incorrect LEADING state for peer 2</li></div><div><li>expected:&lt;NodeDataChanged&gt; but was:&lt;NodeDeleted&gt;</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.zookeeper.server.quorum.Zab1_0Test.testNormalFollowerRunWithDiff</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.zookeeper.server.quorum.Zab1_0Test.testNormalFollowerRunWithDiff</div></li><li><div>org.apache.zookeeper.test.ReadOnlyModeTest.testMultiTransaction</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div id="ppcubuntu16" style="font-weight:bold;font-size:12;display:none" class="panel panel-info" name="summary"><div class="panel-heading"><div class="panel-title">PPC UBUNTU16 SUMMARY</div></div><div class="bs-callout bs-callout-info"><div><b>OS: </b>UBUNTU 16.04</div></div><table style="font-size:14" class="table table-striped"><tbody><tr><th>Package Name</th><th>Result</th><th></th><th></th></tr><tr><td><a href="#" id="anchor_accumulo" onclick="showme(this.id);">ACCUMULO</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_ambari" onclick="showme(this.id);">AMBARI</a></td><td><img src="resources/red.png" align="top" style="width: 16px; height: 16px;" title="FAILURE"></img>0</td></tr><tr><td><a href="#" id="anchor_atlas" onclick="showme(this.id);">ATLAS</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_falcon" onclick="showme(this.id);">FALCON</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_flume" onclick="showme(this.id);">FLUME</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_hadoop" onclick="showme(this.id);">HADOOP</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>19 (7)</td></tr><tr><td><a href="#" id="anchor_hbase" onclick="showme(this.id);">HBASE</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_hive" onclick="showme(this.id);">HIVE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>6 (2)</td></tr><tr><td><a href="#" id="anchor_kafka" onclick="showme(this.id);">KAFKA</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_knox" onclick="showme(this.id);">KNOX</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_metron" onclick="showme(this.id);">METRON</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td></tr><tr><td><a href="#" id="anchor_oozie" onclick="showme(this.id);">OOZIE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>30 (1)</td></tr><tr><td><a href="#" id="anchor_phoenix" onclick="showme(this.id);">PHOENIX</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_pig" onclick="showme(this.id);">PIG</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>6</td></tr><tr><td><a href="#" id="anchor_ranger" onclick="showme(this.id);">RANGER</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_slider" onclick="showme(this.id);">SLIDER</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>24</td></tr><tr><td><a href="#" id="anchor_spark" onclick="showme(this.id);">SPARK</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_sqoop" onclick="showme(this.id);">SQOOP</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3</td></tr><tr><td><a href="#" id="anchor_storm" onclick="showme(this.id);">STORM</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_tez" onclick="showme(this.id);">TEZ</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (1)</td></tr><tr><td><a href="#" id="anchor_zeppelin" onclick="showme(this.id);">ZEPPELIN</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>11</td></tr><tr><td><a href="#" id="anchor_zookeeper" onclick="showme(this.id);">ZOOKEEPER</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td></tr></tbody></table></div><div id="x86ubuntu16" style="font-weight:bold;font-size:12;display:none" class="panel panel-info" name="summary"><div class="panel-heading"><div class="panel-title">X86 UBUNTU16 SUMMARY</div></div><div class="bs-callout bs-callout-info"><div><b>OS: </b>UBUNTU 16.04</div></div><table style="font-size:14" class="table table-striped"><tbody><tr><th>Package Name</th><th>Result</th><th></th><th></th></tr><tr><td><a href="#" id="anchor_accumulo" onclick="showme(this.id);">ACCUMULO</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (2)</td></tr><tr><td><a href="#" id="anchor_ambari" onclick="showme(this.id);">AMBARI</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (2)</td></tr><tr><td><a href="#" id="anchor_atlas" onclick="showme(this.id);">ATLAS</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_falcon" onclick="showme(this.id);">FALCON</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td></tr><tr><td><a href="#" id="anchor_flume" onclick="showme(this.id);">FLUME</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_hadoop" onclick="showme(this.id);">HADOOP</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>43 (31)</td></tr><tr><td><a href="#" id="anchor_hbase" onclick="showme(this.id);">HBASE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>5 (5)</td></tr><tr><td><a href="#" id="anchor_hive" onclick="showme(this.id);">HIVE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>12 (8)</td></tr><tr><td><a href="#" id="anchor_kafka" onclick="showme(this.id);">KAFKA</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td></tr><tr><td><a href="#" id="anchor_knox" onclick="showme(this.id);">KNOX</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (2)</td></tr><tr><td><a href="#" id="anchor_metron" onclick="showme(this.id);">METRON</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_oozie" onclick="showme(this.id);">OOZIE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>31 (2)</td></tr><tr><td><a href="#" id="anchor_phoenix" onclick="showme(this.id);">PHOENIX</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_pig" onclick="showme(this.id);">PIG</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>20 (14)</td></tr><tr><td><a href="#" id="anchor_ranger" onclick="showme(this.id);">RANGER</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_slider" onclick="showme(this.id);">SLIDER</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>25 (1)</td></tr><tr><td><a href="#" id="anchor_spark" onclick="showme(this.id);">SPARK</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (2)</td></tr><tr><td><a href="#" id="anchor_sqoop" onclick="showme(this.id);">SQOOP</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>4 (1)</td></tr><tr><td><a href="#" id="anchor_storm" onclick="showme(this.id);">STORM</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3 (3)</td></tr><tr><td><a href="#" id="anchor_tez" onclick="showme(this.id);">TEZ</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>4 (3)</td></tr><tr><td><a href="#" id="anchor_zeppelin" onclick="showme(this.id);">ZEPPELIN</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>16 (5)</td></tr><tr><td><a href="#" id="anchor_zookeeper" onclick="showme(this.id);">ZOOKEEPER</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (1)</td></tr></tbody></table></div><div id="ppcrhel75" style="font-weight:bold;font-size:12;display:none" class="panel panel-info" name="summary"><div class="panel-heading"><div class="panel-title">PPC RHEL75 SUMMARY</div></div><div class="bs-callout bs-callout-info"><div><b>OS: </b>RHEL 7.5</div></div><table style="font-size:14" class="table table-striped"><tbody><tr><th>Package Name</th><th>Result</th><th></th><th></th></tr><tr><td><a href="#" id="anchor_accumulo" onclick="showme(this.id);">ACCUMULO</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_ambari" onclick="showme(this.id);">AMBARI</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2</td></tr><tr><td><a href="#" id="anchor_atlas" onclick="showme(this.id);">ATLAS</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_falcon" onclick="showme(this.id);">FALCON</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_flume" onclick="showme(this.id);">FLUME</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td></tr><tr><td><a href="#" id="anchor_hadoop" onclick="showme(this.id);">HADOOP</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>416</td></tr><tr><td><a href="#" id="anchor_hbase" onclick="showme(this.id);">HBASE</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_hive" onclick="showme(this.id);">HIVE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>5</td></tr><tr><td><a href="#" id="anchor_kafka" onclick="showme(this.id);">KAFKA</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td></tr><tr><td><a href="#" id="anchor_knox" onclick="showme(this.id);">KNOX</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_metron" onclick="showme(this.id);">METRON</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td></tr><tr><td><a href="#" id="anchor_oozie" onclick="showme(this.id);">OOZIE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>4</td></tr><tr><td><a href="#" id="anchor_phoenix" onclick="showme(this.id);">PHOENIX</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_pig" onclick="showme(this.id);">PIG</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_ranger" onclick="showme(this.id);">RANGER</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_slider" onclick="showme(this.id);">SLIDER</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_spark" onclick="showme(this.id);">SPARK</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3</td></tr><tr><td><a href="#" id="anchor_sqoop" onclick="showme(this.id);">SQOOP</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3</td></tr><tr><td><a href="#" id="anchor_storm" onclick="showme(this.id);">STORM</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_tez" onclick="showme(this.id);">TEZ</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2</td></tr><tr><td><a href="#" id="anchor_zeppelin" onclick="showme(this.id);">ZEPPELIN</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>9</td></tr><tr><td><a href="#" id="anchor_zookeeper" onclick="showme(this.id);">ZOOKEEPER</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>5</td></tr></tbody></table></div><div id="ppcrhel7" style="font-weight:bold;font-size:12;display:none" class="panel panel-info" name="summary"><div class="panel-heading"><div class="panel-title">PPC RHEL7 SUMMARY</div></div><div class="bs-callout bs-callout-info"><div><b>OS: </b>RHEL 7.2</div></div><table style="font-size:14" class="table table-striped"><tbody><tr><th>Package Name</th><th>Result</th><th></th><th></th></tr><tr><td><a href="#" id="anchor_accumulo" onclick="showme(this.id);">ACCUMULO</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (2)</td></tr><tr><td><a href="#" id="anchor_ambari" onclick="showme(this.id);">AMBARI</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td></tr><tr><td><a href="#" id="anchor_atlas" onclick="showme(this.id);">ATLAS</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_falcon" onclick="showme(this.id);">FALCON</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_flume" onclick="showme(this.id);">FLUME</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td></tr><tr><td><a href="#" id="anchor_hadoop" onclick="showme(this.id);">HADOOP</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>17 (13)</td></tr><tr><td><a href="#" id="anchor_hbase" onclick="showme(this.id);">HBASE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>5 (5)</td></tr><tr><td><a href="#" id="anchor_hive" onclick="showme(this.id);">HIVE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>5 (2)</td></tr><tr><td><a href="#" id="anchor_kafka" onclick="showme(this.id);">KAFKA</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td></tr><tr><td><a href="#" id="anchor_knox" onclick="showme(this.id);">KNOX</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_metron" onclick="showme(this.id);">METRON</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td></tr><tr><td><a href="#" id="anchor_oozie" onclick="showme(this.id);">OOZIE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3 (2)</td></tr><tr><td><a href="#" id="anchor_phoenix" onclick="showme(this.id);">PHOENIX</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_pig" onclick="showme(this.id);">PIG</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_ranger" onclick="showme(this.id);">RANGER</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_slider" onclick="showme(this.id);">SLIDER</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_spark" onclick="showme(this.id);">SPARK</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td></tr><tr><td><a href="#" id="anchor_sqoop" onclick="showme(this.id);">SQOOP</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3</td></tr><tr><td><a href="#" id="anchor_storm" onclick="showme(this.id);">STORM</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td></tr><tr><td><a href="#" id="anchor_tez" onclick="showme(this.id);">TEZ</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2</td></tr><tr><td><a href="#" id="anchor_zeppelin" onclick="showme(this.id);">ZEPPELIN</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>12 (1)</td></tr><tr><td><a href="#" id="anchor_zookeeper" onclick="showme(this.id);">ZOOKEEPER</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>5 (2)</td></tr></tbody></table></div><div id="x86rhel7" style="font-weight:bold;font-size:12;display:none" class="panel panel-info" name="summary"><div class="panel-heading"><div class="panel-title">X86 RHEL7 SUMMARY</div></div><div class="bs-callout bs-callout-info"><div><b>OS: </b>RHEL 7.2</div></div><table style="font-size:14" class="table table-striped"><tbody><tr><th>Package Name</th><th>Result</th><th></th><th></th></tr><tr><td><a href="#" id="anchor_accumulo" onclick="showme(this.id);">ACCUMULO</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_ambari" onclick="showme(this.id);">AMBARI</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (2)</td></tr><tr><td><a href="#" id="anchor_atlas" onclick="showme(this.id);">ATLAS</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_falcon" onclick="showme(this.id);">FALCON</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_flume" onclick="showme(this.id);">FLUME</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td></tr><tr><td><a href="#" id="anchor_hadoop" onclick="showme(this.id);">HADOOP</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>11 (7)</td></tr><tr><td><a href="#" id="anchor_hbase" onclick="showme(this.id);">HBASE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td></tr><tr><td><a href="#" id="anchor_hive" onclick="showme(this.id);">HIVE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>10 (7)</td></tr><tr><td><a href="#" id="anchor_kafka" onclick="showme(this.id);">KAFKA</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_knox" onclick="showme(this.id);">KNOX</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_metron" onclick="showme(this.id);">METRON</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td></tr><tr><td><a href="#" id="anchor_oozie" onclick="showme(this.id);">OOZIE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (1)</td></tr><tr><td><a href="#" id="anchor_phoenix" onclick="showme(this.id);">PHOENIX</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_pig" onclick="showme(this.id);">PIG</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_ranger" onclick="showme(this.id);">RANGER</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_slider" onclick="showme(this.id);">SLIDER</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_spark" onclick="showme(this.id);">SPARK</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td></tr><tr><td><a href="#" id="anchor_sqoop" onclick="showme(this.id);">SQOOP</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3</td></tr><tr><td><a href="#" id="anchor_storm" onclick="showme(this.id);">STORM</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td></tr><tr><td><a href="#" id="anchor_tez" onclick="showme(this.id);">TEZ</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2</td></tr><tr><td><a href="#" id="anchor_zeppelin" onclick="showme(this.id);">ZEPPELIN</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>17 (6)</td></tr><tr><td><a href="#" id="anchor_zookeeper" onclick="showme(this.id);">ZOOKEEPER</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3</td></tr></tbody></table></div><div id="ppcx86" style="display:block;font-weight:bold" class="panel panel-info" name="summary"><div class="panel-heading"><div class="panel-title">FULL SUMMARY</div></div><table style="font-size:15" id="summarytable" class="table table-striped"><tbody><tr><th></th></tr><tr><th>Package Name</th><th>PPC UBUNTU16</th><th>x86 UBUNTU16</th><th>PPC RHEL7</th><th>x86 RHEL7</th><th>PPC RHEL7.5</th><th>Owner</th></tr><tr><td><a href="#" id="anchor_accumulo" onclick="showme(this.id);">ACCUMULO</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (2)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (2)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td>Sonia</td></tr><tr><td><a href="#" id="anchor_ambari" onclick="showme(this.id);">AMBARI</a></td><td><img src="resources/red.png" align="top" style="width: 16px; height: 16px;" title="FAILURE"></img>0</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (2)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (2)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2</td><td>Valencia</td></tr><tr><td><a href="#" id="anchor_atlas" onclick="showme(this.id);">ATLAS</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td>Yussuf</td></tr><tr><td><a href="#" id="anchor_falcon" onclick="showme(this.id);">FALCON</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td>Sonia</td></tr><tr><td><a href="#" id="anchor_flume" onclick="showme(this.id);">FLUME</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td><td>Pravin</td></tr><tr><td><a href="#" id="anchor_hadoop" onclick="showme(this.id);">HADOOP</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>19 (7)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>43 (31)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>17 (13)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>11 (7)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>416</td><td>Parita</td></tr><tr><td><a href="#" id="anchor_hbase" onclick="showme(this.id);">HBASE</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>5 (5)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>5 (5)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td>Valencia</td></tr><tr><td><a href="#" id="anchor_hive" onclick="showme(this.id);">HIVE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>6 (2)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>12 (8)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>5 (2)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>10 (7)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>5</td><td>Alisha/Pravin</td></tr><tr><td><a href="#" id="anchor_kafka" onclick="showme(this.id);">KAFKA</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td><td>Sonia</td></tr><tr><td><a href="#" id="anchor_knox" onclick="showme(this.id);">KNOX</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (2)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td>Yussuf</td></tr><tr><td><a href="#" id="anchor_metron" onclick="showme(this.id);">METRON</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td><td>Pravin</td></tr><tr><td><a href="#" id="anchor_oozie" onclick="showme(this.id);">OOZIE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>30 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>31 (2)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3 (2)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>4</td><td>Alisha</td></tr><tr><td><a href="#" id="anchor_phoenix" onclick="showme(this.id);">PHOENIX</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td>Valencia</td></tr><tr><td><a href="#" id="anchor_pig" onclick="showme(this.id);">PIG</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>6</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>20 (14)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td>Yussuf</td></tr><tr><td><a href="#" id="anchor_ranger" onclick="showme(this.id);">RANGER</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td>Sneha</td></tr><tr><td><a href="#" id="anchor_slider" onclick="showme(this.id);">SLIDER</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>24</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>25 (1)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td>Yussuf</td></tr><tr><td><a href="#" id="anchor_spark" onclick="showme(this.id);">SPARK</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (2)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3</td><td>Parita</td></tr><tr><td><a href="#" id="anchor_sqoop" onclick="showme(this.id);">SQOOP</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>4 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3</td><td>Talat</td></tr><tr><td><a href="#" id="anchor_storm" onclick="showme(this.id);">STORM</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3 (3)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td>Parita</td></tr><tr><td><a href="#" id="anchor_tez" onclick="showme(this.id);">TEZ</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>4 (3)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2</td><td>Valencia</td></tr><tr><td><a href="#" id="anchor_zeppelin" onclick="showme(this.id);">ZEPPELIN</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>11</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>16 (5)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>12 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>17 (6)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>9</td><td>Sneha</td></tr><tr><td><a href="#" id="anchor_zookeeper" onclick="showme(this.id);">ZOOKEEPER</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>5 (2)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>5</td><td>Pravin</td></tr></tbody></table></div></div></body></html>