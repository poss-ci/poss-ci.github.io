<html><head><script src="resources/jquery.min.js"></script><link href="resources/bootstrap.min.css" rel="stylesheet"></link><link href="resources/bootstrap-theme.min.css" rel="stylesheet"></link><script src="resources/bootstrap.min.js"></script><script src="helper.js">function hideAll(){console.log("hideAll")}function showme(e){console.log("showme");var l,n=e.substring(7),o=document.getElementsByName("data");for(l=0;l&lt;o.length;l++)o[l].style.display="none";var t=document.getElementsByName("summary");for(l=0;l&lt;t.length;l++)t[l].style.display="none";document.getElementById(n).style.display="block"}</script><style>table, th, td { vertical-align:top; padding: 3px} table {table-layout:fixed} td {word-wrap:break-word} .bs-callout { padding: 5px; margin: 5px 0; border: 1px solid #eee; border-left-width: 5px; border-radius: 3px; font-weight:normal; }.bs-callout-info {border-left-color: #5bc0de;}</style></head><body><nav style="background-color: #F0F8FF;" class="navbar navbar-light"><div class="container-fluid"><ul class="nav nav-pills"><li role="presentation"><a style="font-weight:bold" href="#" id="anchor_ppcx86" onclick="showme(this.id);">FULL SUMMARY</a></li><li role="presentation"><a style="font-weight:bold" href="#" id="anchor_ppcubuntu16" onclick="showme(this.id);">PPC UBUNTU16</a></li><li role="presentation"><a style="font-weight:bold" href="#" id="anchor_x86ubuntu16" onclick="showme(this.id);">X86 UBUNTU16</a></li><li role="presentation"><a style="font-weight:bold" href="#" id="anchor_ppcrhel7" onclick="showme(this.id);">PPC RHEL7</a></li><li role="presentation"><a style="font-weight:bold" href="#" id="anchor_x86rhel7" onclick="showme(this.id);">X86 RHEL7</a></li><li role="presentation"><a style="font-weight:bold" href="#" id="anchor_ppcrhel75" onclick="showme(this.id);">PPC RHEL75</a></li><li role="presentation"><a style="font-weight:bold" href="#" id="anchor_developers" onclick="showme(this.id);">DEVELOPERS</a></li><p align="right" role="presentation" style="color:grey;padding-top:10px">Report Date: 29-04-2018 04:30 UTC</p></ul><div align="right" style="color:grey;font-size:12">Notations:<img src="resources/red.png" style="width: 16px; height: 16px;">Build failed </img><img src="resources/blue.png" style="width: 16px; height: 16px;">Build success with no failure </img><img src="resources/yellow.png" style="width: 16px; height: 16px;">N (M) Build success with N test failures &amp; M unique failures </img></div></div></nav><div style="table-cell" class="col-sm-2 col-md-2 sidebar"><div class="list-group"><a href="#" class="list-group-item list-group-item-action active" onclick="showme(this.id);" id="anchor_ppcx86">Packages</a><a class="list-group-item list-group-item-action" href="#" id="anchor_accumulo" onclick="showme(this.id);" title="Owned by Sonia">ACCUMULO</a><a class="list-group-item list-group-item-action" href="#" id="anchor_ambari" onclick="showme(this.id);" title="Owned by Valencia">AMBARI</a><a class="list-group-item list-group-item-action" href="#" id="anchor_atlas" onclick="showme(this.id);" title="Owned by Yussuf">ATLAS</a><a class="list-group-item list-group-item-action" href="#" id="anchor_falcon" onclick="showme(this.id);" title="Owned by Sonia">FALCON</a><a class="list-group-item list-group-item-action" href="#" id="anchor_flume" onclick="showme(this.id);" title="Owned by Pravin">FLUME</a><a class="list-group-item list-group-item-action" href="#" id="anchor_hadoop" onclick="showme(this.id);" title="Owned by Parita">HADOOP</a><a class="list-group-item list-group-item-action" href="#" id="anchor_hbase" onclick="showme(this.id);" title="Owned by Valencia">HBASE</a><a class="list-group-item list-group-item-action" href="#" id="anchor_hive" onclick="showme(this.id);" title="Owned by Alisha/Pravin">HIVE</a><a class="list-group-item list-group-item-action" href="#" id="anchor_kafka" onclick="showme(this.id);" title="Owned by Sonia">KAFKA</a><a class="list-group-item list-group-item-action" href="#" id="anchor_knox" onclick="showme(this.id);" title="Owned by Yussuf">KNOX</a><a class="list-group-item list-group-item-action" href="#" id="anchor_metron" onclick="showme(this.id);" title="Owned by Pravin">METRON</a><a class="list-group-item list-group-item-action" href="#" id="anchor_oozie" onclick="showme(this.id);" title="Owned by Alisha">OOZIE</a><a class="list-group-item list-group-item-action" href="#" id="anchor_phoenix" onclick="showme(this.id);" title="Owned by Valencia">PHOENIX</a><a class="list-group-item list-group-item-action" href="#" id="anchor_pig" onclick="showme(this.id);" title="Owned by Yussuf">PIG</a><a class="list-group-item list-group-item-action" href="#" id="anchor_ranger" onclick="showme(this.id);" title="Owned by Sneha">RANGER</a><a class="list-group-item list-group-item-action" href="#" id="anchor_slider" onclick="showme(this.id);" title="Owned by Yussuf">SLIDER</a><a class="list-group-item list-group-item-action" href="#" id="anchor_spark" onclick="showme(this.id);" title="Owned by Parita">SPARK</a><a class="list-group-item list-group-item-action" href="#" id="anchor_sqoop" onclick="showme(this.id);" title="Owned by Talat">SQOOP</a><a class="list-group-item list-group-item-action" href="#" id="anchor_storm" onclick="showme(this.id);" title="Owned by Parita">STORM</a><a class="list-group-item list-group-item-action" href="#" id="anchor_tez" onclick="showme(this.id);" title="Owned by Valencia">TEZ</a><a class="list-group-item list-group-item-action" href="#" id="anchor_zeppelin" onclick="showme(this.id);" title="Owned by Sneha">ZEPPELIN</a><a class="list-group-item list-group-item-action" href="#" id="anchor_zookeeper" onclick="showme(this.id);" title="Owned by Pravin">ZOOKEEPER</a></div></div><div style="display: table-cell"><div id="developers" style="display:block;font-weight:bold;display:none;" class="panel panel-info" name="summary"><div class="panel-heading"><div class="panel-title">DEVELOPERS</div></div><div class="panel-body"><table style="font-size:15" id="summarytable" class="table table-striped"><tr><td style="width: 100px;font-weight:bold">ALISHA</td><td><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_oozie">OOZIE </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_hive">HIVE </button></td></tr><tr><td style="width: 100px;font-weight:bold">PARITA</td><td><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_hadoop">HADOOP </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_storm">STORM </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_spark">SPARK </button></td></tr><tr><td style="width: 100px;font-weight:bold">PRAVIN</td><td><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_metron">METRON </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_zookeeper">ZOOKEEPER </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_flume">FLUME </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_hive">HIVE </button></td></tr><tr><td style="width: 100px;font-weight:bold">SONIA</td><td><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_kafka">KAFKA </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_accumulo">ACCUMULO </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_falcon">FALCON </button></td></tr><tr><td style="width: 100px;font-weight:bold">SNEHA</td><td><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_zeppelin">ZEPPELIN </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_ranger">RANGER </button></td></tr><tr><td style="width: 100px;font-weight:bold">TALAT</td><td><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_sqoop">SQOOP </button></td></tr><tr><td style="width: 100px;font-weight:bold">VALENCIA</td><td><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_tez">TEZ </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_hbase">HBASE </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_phoenix">PHOENIX </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_ambari">AMBARI </button></td></tr><tr><td style="width: 100px;font-weight:bold">YUSSUF</td><td><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_knox">KNOX </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_atlas">ATLAS </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_slider">SLIDER </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_pig">PIG </button></td></tr></table></div></div></div><div style="display: table-cell"><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="accumulo"><div style="font-weight:bold;" class="panel-heading">ACCUMULO<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Sonia)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>bdd54edae805ed842973720a4e15aef92e8e8579</div><div><b>Last Run: </b>24-04-2018 20:26 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th><th>PPC RHEL7.5</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 1706</div><div>Failed Count : 0</div><div>Skipped Count : 6</div></td><td><div>Total Count : 1706</div><div>Failed Count : 2</div><div>Skipped Count : 6</div></td><td><div>Total Count : 1706</div><div>Failed Count : 0</div><div>Skipped Count : 6</div></td><td><div>Total Count : 1706</div><div>Failed Count : 0</div><div>Skipped Count : 6</div></td><td><div>Total Count : 1706</div><div>Failed Count : 1</div><div>Skipped Count : 6</div></td></tr><tr><td>Result</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.accumulo.minicluster.MiniAccumuloClusterTest.testPerTableClasspath</li></div><div><li>org.apache.accumulo.minicluster.MiniAccumuloClusterTest.test</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.accumulo.core.client.impl.NamespaceTest.testCacheDecreasesAfterGC</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>test timed out after 60000 milliseconds</li></div><div><li>test timed out after 30000 milliseconds</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>Cache did not decrease with GC.</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.accumulo.minicluster.MiniAccumuloClusterTest.testPerTableClasspath</div></li><li><div>org.apache.accumulo.minicluster.MiniAccumuloClusterTest.test</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="ambari"><div style="font-weight:bold;" class="panel-heading">AMBARI<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Valencia)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> origin/trunk</div><div><b>Last Revision: </b>a0643687a337925f991a1a6c5933b93513373244</div><div><b>Last Run: </b>25-04-2018 00:31 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th><th>PPC RHEL7.5</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 0</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 5485</div><div>Failed Count : 0</div><div>Skipped Count : 82</div></td><td><div>Total Count : 5485</div><div>Failed Count : 0</div><div>Skipped Count : 82</div></td><td><div>Total Count : 5485</div><div>Failed Count : 0</div><div>Skipped Count : 82</div></td><td><div>Total Count : 5485</div><div>Failed Count : 5</div><div>Skipped Count : 82</div></td></tr><tr><td>Result</td><td><img src="resources/red.png" align="top" style="width: 16px; height: 16px;"></img>FAILURE</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.ambari.server.controller.metrics.RestMetricsPropertyProviderTest.testRestMetricsPropertyProviderAsAdministrator</li></div><div><li>org.apache.ambari.server.controller.metrics.RestMetricsPropertyProviderTest.testRestMetricsPropertyProviderAsServiceAdministrator</li></div><div><li>org.apache.ambari.server.controller.test.BufferedThreadPoolExecutorCompletionServiceTest.testMaxPoolSizeThreadsLaunched</li></div><div><li>org.apache.ambari.server.controller.test.BufferedThreadPoolExecutorCompletionServiceTest.testScalingThreadPoolExecutor</li></div><div><li>org.apache.ambari.server.security.encryption.CredentialStoreTest.testInMemoryCredentialStoreService_CredentialExpired</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;400&gt; but was:&lt;121&gt;</li></div><div><li>expected:&lt;1&gt; but was:&lt;0&gt;</li></div><div><li>expected:&lt;10&gt; but was:&lt;8&gt;</li></div><div><li>expected:&lt;10&gt; but was:&lt;7&gt;</li></div><div><li>java.lang.NullPointerException
	at org.apache.ambari.server.security.encryption.CredentialStoreTest.getExpiredCredentialTest(CredentialStoreTest.java:169)
	at org.apache.ambari.server.security.encryption.CredentialStoreTest.testInMemoryCredentialStoreService_CredentialExpired(CredentialStoreTest.java:90)
</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="atlas"><div style="font-weight:bold;" class="panel-heading">ATLAS<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Yussuf)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>54c31d5c8e601757e19e26d3c30f2414532e2f8f</div><div><b>Last Run: </b>11-04-2018 15:22 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th><th>PPC RHEL7.5</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 839</div><div>Failed Count : 1</div><div>Skipped Count : 6</div></td><td><div>Total Count : 838</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 838</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 838</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 873</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td></tr><tr><td>Result</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"><div><li>org.apache.atlas.repository.audit.CassandraAuditRepositoryTest.setup</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"><div><li>com.datastax.driver.core.exceptions.NoHostAvailableException: All host(s) tried for query failed (tried: localhost/127.0.0.1:9042 (com.datastax.driver.core.exceptions.TransportException: [localhost/127.0.0.1:9042] Cannot connect))</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.atlas.repository.audit.CassandraAuditRepositoryTest.setup</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="falcon"><div style="font-weight:bold;" class="panel-heading">FALCON<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Sonia)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>411d90c2ffb59559770d23d4bd2f7675e46392e6</div><div><b>Last Run: </b>18-04-2018 09:04 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th><th>PPC RHEL7.5</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 1003</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 1002</div><div>Failed Count : 1</div><div>Skipped Count : 7</div></td><td><div>Total Count : 1004</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 1005</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 1002</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td></tr><tr><td>Result</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.falcon.entity.v0.EntityGraphTest.initConfigStore</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>Unable to restore configurations for entity type PROCESS</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.falcon.entity.v0.EntityGraphTest.initConfigStore</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="flume"><div style="font-weight:bold;" class="panel-heading">FLUME<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Pravin)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/trunk</div><div><b>Last Revision: </b>0d437810dc850192b48fa3b31608ffcd23b1f1e9</div><div><b>Last Run: </b>24-04-2018 01:48 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th><th>PPC RHEL7.5</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 1182</div><div>Failed Count : 0</div><div>Skipped Count : 6</div></td><td><div>Total Count : 1208</div><div>Failed Count : 0</div><div>Skipped Count : 6</div></td><td><div>Total Count : 1182</div><div>Failed Count : 1</div><div>Skipped Count : 6</div></td><td><div>Total Count : 1208</div><div>Failed Count : 1</div><div>Skipped Count : 6</div></td><td><div>Total Count : 1182</div><div>Failed Count : 1</div><div>Skipped Count : 6</div></td></tr><tr><td>Result</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.flume.channel.file.TestLog.testMinimumRequiredSpaceTooSmallForPut</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.flume.channel.file.TestLog.testMinimumRequiredSpaceTooSmallForPut</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.flume.channel.file.TestLog.testMinimumRequiredSpaceTooSmallForPut</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>java.lang.AssertionError
	at org.apache.flume.channel.file.TestLog.doTestMinimumRequiredSpaceTooSmallForPut(TestLog.java:241)
	at org.apache.flume.channel.file.TestLog.testMinimumRequiredSpaceTooSmallForPut(TestLog.java:210)
</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>java.lang.AssertionError
	at org.apache.flume.channel.file.TestLog.doTestMinimumRequiredSpaceTooSmallForPut(TestLog.java:241)
	at org.apache.flume.channel.file.TestLog.testMinimumRequiredSpaceTooSmallForPut(TestLog.java:210)
</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>java.lang.AssertionError
	at org.apache.flume.channel.file.TestLog.doTestMinimumRequiredSpaceTooSmallForPut(TestLog.java:241)
	at org.apache.flume.channel.file.TestLog.testMinimumRequiredSpaceTooSmallForPut(TestLog.java:210)
</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="hadoop"><div style="font-weight:bold;" class="panel-heading">HADOOP<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Parita)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/trunk</div><div><b>Last Revision: </b>9b5375e0c1ee8c634a5accb7415ec27440543a60</div><div><b>Last Run: </b>24-04-2018 00:39 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th><th>PPC RHEL7.5</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 19375</div><div>Failed Count : 21</div><div>Skipped Count : 1160</div></td><td><div>Total Count : 19212</div><div>Failed Count : 31</div><div>Skipped Count : 1160</div></td><td><div>Total Count : 19440</div><div>Failed Count : 18</div><div>Skipped Count : 1161</div></td><td><div>Total Count : 19477</div><div>Failed Count : 20</div><div>Skipped Count : 1161</div></td><td><div>Total Count : 19320</div><div>Failed Count : 639</div><div>Skipped Count : 1159</div></td></tr><tr><td>Result</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.util.TestShell.testEnvVarsWithInheritance</li></div><div><li>org.apache.hadoop.fs.contract.router.web.TestRouterWebHDFSContractAppend.testRenameFileBeingAppended</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting.testMultipleVolFailuresOnNode</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting.testVolFailureStatsPreservedOnNNRestart</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureToleration.testConfigureMinValidVolumes</li></div><div><li>org.apache.hadoop.hdfs.server.namenode.ha.TestFailureToReadEdits.testCheckpointStartingMidEditsFile[0]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testAuthUrlConnectTimeout[timeoutSource=ConnectionFactory]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testConnectTimeout[timeoutSource=ConnectionFactory]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testRedirectConnectTimeout[timeoutSource=ConnectionFactory]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testTwoStepWriteConnectTimeout[timeoutSource=ConnectionFactory]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testAuthUrlConnectTimeout[timeoutSource=Configuration]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testConnectTimeout[timeoutSource=Configuration]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testRedirectConnectTimeout[timeoutSource=Configuration]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testTwoStepWriteConnectTimeout[timeoutSource=Configuration]</li></div><div><li>org.apache.hadoop.mapred.TestMRTimelineEventHandling.testMapreduceJobTimelineServiceEnabled</li></div><div><li>org.apache.hadoop.yarn.service.monitor.probe.TestDefaultProbe.testDefaultProbe[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMProxy.testAMRMProxyTokenRenewal</li></div><div><li>org.apache.hadoop.yarn.api.resource.TestPlacementConstraintTransformations.testCardinalityConstraint</li></div><div><li>org.apache.hadoop.yarn.util.TestProcfsBasedProcessTree.testProcessTree</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestIncreaseAllocationExpirer.testContainerIncreaseAllocationExpiration</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestQueueManagementDynamicEditPolicy.testEditSchedule</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.util.TestShell.testEnvVarsWithInheritance</li></div><div><li>org.apache.hadoop.fs.contract.router.web.TestRouterWebHDFSContractAppend.testRenameFileBeingAppended</li></div><div><li>org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureWithRandomECPolicy.testCloseWithExceptionsInStreamer</li></div><div><li>org.apache.hadoop.hdfs.TestMissingBlocksAlert.testMissingBlocksAlert</li></div><div><li>org.apache.hadoop.hdfs.TestRollingUpgrade.testCheckpointWithMultipleNN</li></div><div><li>org.apache.hadoop.hdfs.TestRollingUpgrade.testRollBackImage</li></div><div><li>org.apache.hadoop.hdfs.TestRollingUpgrade.testDFSAdminRollingUpgradeCommands</li></div><div><li>org.apache.hadoop.hdfs.TestRollingUpgrade.testRollback</li></div><div><li>org.apache.hadoop.hdfs.qjournal.server.TestJournalNodeSync.testSyncForDiscontinuousMissingLogs</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting.testSuccessiveVolumeFailures</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting.testHotSwapOutFailedVolumeAndReporting</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting.testMultipleVolFailuresOnNode</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting.testDataNodeReconfigureWithVolumeFailures</li></div><div><li>org.apache.hadoop.hdfs.server.namenode.TestReencryptionWithKMS.testReencryptNestedZones</li></div><div><li>org.apache.hadoop.hdfs.server.namenode.ha.TestDFSUpgradeWithHA.testRollbackWithJournalNodes</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testAuthUrlConnectTimeout[timeoutSource=ConnectionFactory]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testConnectTimeout[timeoutSource=ConnectionFactory]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testRedirectConnectTimeout[timeoutSource=ConnectionFactory]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testTwoStepWriteConnectTimeout[timeoutSource=ConnectionFactory]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testAuthUrlConnectTimeout[timeoutSource=Configuration]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testConnectTimeout[timeoutSource=Configuration]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testRedirectConnectTimeout[timeoutSource=Configuration]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testTwoStepWriteConnectTimeout[timeoutSource=Configuration]</li></div><div><li>org.apache.hadoop.mapreduce.v2.hs.webapp.TestHsWebServicesJobsQuery.testJobsQueryFinishTimeBegin</li></div><div><li>org.apache.hadoop.mapred.TestMRTimelineEventHandling.testMapreduceJobTimelineServiceEnabled</li></div><div><li>org.apache.hadoop.yarn.api.resource.TestPlacementConstraintTransformations.testCardinalityConstraint</li></div><div><li>org.apache.hadoop.yarn.util.TestProcfsBasedProcessTree.testProcessTree</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.TestContainerSchedulerQueuing.testKillOnlyRequiredOpportunisticContainers</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestContainerResizing.testIncreaseContainerUnreservedWhenContainerCompleted</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestQueueManagementDynamicEditPolicy.testEditSchedule</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.TestPlacementProcessor.testRePlacementAfterSchedulerRejection</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.io.nativeio.TestNativeIO.testStat</li></div><div><li>org.apache.hadoop.io.nativeio.TestNativeIO.testMultiThreadedStat</li></div><div><li>org.apache.hadoop.util.TestShell.testEnvVarsWithInheritance</li></div><div><li>org.apache.hadoop.hdfs.TestDFSClientRetries.testLeaseRenewAndDFSOutputStreamDeadLock</li></div><div><li>org.apache.hadoop.hdfs.TestDFSClientRetries.testLeaseRenewSocketTimeout</li></div><div><li>org.apache.hadoop.hdfs.TestReadStripedFileWithMissingBlocks.testReadFileWithMissingBlocks</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.TestNNHandlesCombinedBlockReport.blockReport_08</li></div><div><li>org.apache.hadoop.hdfs.server.namenode.TestReencryption.testReencryptNestedZones</li></div><div><li>org.apache.hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA.testUpdatePipeline</li></div><div><li>org.apache.hadoop.mapred.TestMRTimelineEventHandling.testMapreduceJobTimelineServiceEnabled</li></div><div><li>org.apache.hadoop.yarn.api.resource.TestPlacementConstraintTransformations.testCardinalityConstraint</li></div><div><li>org.apache.hadoop.yarn.util.TestProcfsBasedProcessTree.testProcessTree</li></div><div><li>TestDockerUtil.test_add_rw_mounts</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.TestApplicationMasterLauncher.testRetriesOnFailures</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.TestRMRestart.testRMRestartRecoveringNodeLabelManager[CAPACITY]</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.recovery.TestLeveldbRMStateStore.testRemoveAttempt</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestContainerResizing.testIncreaseContainerUnreservedWhenApplicationCompleted</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.security.TestDelegationTokenRenewer.testRMRestartWithExpiredToken</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.util.TestShell.testEnvVarsWithInheritance</li></div><div><li>org.apache.hadoop.fs.contract.router.web.TestRouterWebHDFSContractAppend.testAppendNonexistentFile</li></div><div><li>org.apache.hadoop.fs.contract.router.web.TestRouterWebHDFSContractAppend.testBuilderAppendToExistingFile</li></div><div><li>org.apache.hadoop.fs.contract.router.web.TestRouterWebHDFSContractRootDirectory.testRecursiveRootListing</li></div><div><li>org.apache.hadoop.fs.contract.router.web.TestRouterWebHDFSContractRootDirectory.testRmEmptyRootDirNonRecursive</li></div><div><li>org.apache.hadoop.fs.contract.router.web.TestRouterWebHDFSContractRootDirectory.testRmRootRecursive</li></div><div><li>org.apache.hadoop.fs.contract.router.web.TestRouterWebHDFSContractRootDirectory.testListEmptyRootDirectory</li></div><div><li>org.apache.hadoop.fs.contract.router.web.TestRouterWebHDFSContractRootDirectory.testSimpleRootListing</li></div><div><li>org.apache.hadoop.fs.contract.router.web.TestRouterWebHDFSContractRootDirectory.testMkDirDepth1</li></div><div><li>org.apache.hadoop.fs.contract.router.web.TestRouterWebHDFSContractRootDirectory.testRmNonEmptyRootDirNonRecursive</li></div><div><li>org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureWithRandomECPolicy.testCloseWithExceptionsInStreamer</li></div><div><li>org.apache.hadoop.hdfs.TestDecommission.testBlocksPerInterval</li></div><div><li>org.apache.hadoop.hdfs.TestDecommission.testBlocksPerInterval</li></div><div><li>org.apache.hadoop.hdfs.TestDistributedFileSystem.testDFSClientPeerWriteTimeout</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting.testMultipleVolFailuresOnNode</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithContainerResourceChange[1]</li></div><div><li>org.apache.hadoop.yarn.api.resource.TestPlacementConstraintTransformations.testCardinalityConstraint</li></div><div><li>org.apache.hadoop.yarn.util.TestProcfsBasedProcessTree.testProcessTree</li></div><div><li>TestDockerUtil.test_add_rw_mounts</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestQueueManagementDynamicEditPolicy.testEditSchedule</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.io.nativeio.TestNativeIO.testStat</li></div><div><li>org.apache.hadoop.io.nativeio.TestNativeIO.testMultiThreadedStat</li></div><div><li>org.apache.hadoop.fs.contract.router.web.TestRouterWebHDFSContractAppend.testRenameFileBeingAppended</li></div><div><li>org.apache.hadoop.fs.viewfs.TestViewFsHdfs.testGetBlockLocations</li></div><div><li>org.apache.hadoop.hdfs.TestDFSStripedInputStreamWithRandomECPolicy.testStatefulRead</li></div><div><li>org.apache.hadoop.hdfs.TestDistributedFileSystem.testDFSClientPeerWriteTimeout</li></div><div><li>org.apache.hadoop.hdfs.TestLeaseRecovery2.testHardLeaseRecoveryWithRenameAfterNameNodeRestart</li></div><div><li>org.apache.hadoop.hdfs.server.blockmanagement.TestPendingReconstruction.testPendingAndInvalidate</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting.testDataNodeReconfigureWithVolumeFailures</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.TestNNHandlesBlockReportPerStorage.blockReport_08</li></div><div><li>org.apache.hadoop.conf.TestNoDefaultsJobConf.testNoDefaults</li></div><div><li>org.apache.hadoop.fs.TestDFSIO.org.apache.hadoop.fs.TestDFSIO</li></div><div><li>org.apache.hadoop.fs.TestFileSystem.testFs</li></div><div><li>org.apache.hadoop.fs.slive.TestSlive.testCreateOp</li></div><div><li>org.apache.hadoop.fs.slive.TestSlive.testMkdir</li></div><div><li>org.apache.hadoop.fs.slive.TestSlive.testList</li></div><div><li>org.apache.hadoop.fs.slive.TestSlive.testRead</li></div><div><li>org.apache.hadoop.fs.slive.TestSlive.testDelete</li></div><div><li>org.apache.hadoop.fs.slive.TestSlive.testMRFlow</li></div><div><li>org.apache.hadoop.fs.slive.TestSlive.testRename</li></div><div><li>org.apache.hadoop.fs.slive.TestSlive.testAppendOp</li></div><div><li>org.apache.hadoop.fs.slive.TestSlive.testTruncateOp</li></div><div><li>org.apache.hadoop.hdfs.TestNNBench.testNNBenchCreateReadAndDelete</li></div><div><li>org.apache.hadoop.hdfs.TestNNBench.testNNBenchCreateAndRename</li></div><div><li>org.apache.hadoop.io.TestSequenceFileMergeProgress.testMergeProgressWithBlockCompression</li></div><div><li>org.apache.hadoop.io.TestSequenceFileMergeProgress.testMergeProgressWithRecordCompression</li></div><div><li>org.apache.hadoop.io.TestSequenceFileMergeProgress.testMergeProgressWithNoCompression</li></div><div><li>org.apache.hadoop.ipc.TestMRCJCSocketFactory.testSocketFactory</li></div><div><li>org.apache.hadoop.mapred.TestClusterMRNotification.testMR</li></div><div><li>org.apache.hadoop.mapred.TestClusterMapReduceTestCase.testMapReduceRestarting</li></div><div><li>org.apache.hadoop.mapred.TestClusterMapReduceTestCase.testMRConfig</li></div><div><li>org.apache.hadoop.mapred.TestClusterMapReduceTestCase.testDFSRestart</li></div><div><li>org.apache.hadoop.mapred.TestClusterMapReduceTestCase.testMapReduce</li></div><div><li>org.apache.hadoop.mapred.TestCollect.testCollect</li></div><div><li>org.apache.hadoop.mapred.TestComparators.testDefaultMRComparator</li></div><div><li>org.apache.hadoop.mapred.TestComparators.testUserMRComparator</li></div><div><li>org.apache.hadoop.mapred.TestComparators.testUserValueGroupingComparator</li></div><div><li>org.apache.hadoop.mapred.TestComparators.testBakedUserComparator</li></div><div><li>org.apache.hadoop.mapred.TestComparators.testAllUserComparators</li></div><div><li>org.apache.hadoop.mapred.TestFieldSelection.testFieldSelection</li></div><div><li>org.apache.hadoop.mapred.TestFileInputFormatPathFilter.testWithoutPathFilterWithGlob</li></div><div><li>org.apache.hadoop.mapred.TestFileInputFormatPathFilter.testWithPathFilterWithoutGlob</li></div><div><li>org.apache.hadoop.mapred.TestFileInputFormatPathFilter.testWithoutPathFilterWithoutGlob</li></div><div><li>org.apache.hadoop.mapred.TestFileInputFormatPathFilter.testWithPathFilterWithGlob</li></div><div><li>org.apache.hadoop.mapred.TestFileOutputFormat.testCustomFile</li></div><div><li>org.apache.hadoop.mapred.TestJavaSerialization.testMapReduceJob</li></div><div><li>org.apache.hadoop.mapred.TestJavaSerialization.testWriteToSequencefile</li></div><div><li>org.apache.hadoop.mapred.TestJobCleanup.org.apache.hadoop.mapred.TestJobCleanup</li></div><div><li>org.apache.hadoop.mapred.TestJobCounters.org.apache.hadoop.mapred.TestJobCounters</li></div><div><li>org.apache.hadoop.mapred.TestJobCounters.org.apache.hadoop.mapred.TestJobCounters</li></div><div><li>org.apache.hadoop.mapred.TestJobName.testComplexNameWithRegex</li></div><div><li>org.apache.hadoop.mapred.TestJobName.testComplexName</li></div><div><li>org.apache.hadoop.mapred.TestJobSysDirWithDFS.testWithDFS</li></div><div><li>org.apache.hadoop.mapred.TestKeyValueTextInputFormat.testGzip</li></div><div><li>org.apache.hadoop.mapred.TestKeyValueTextInputFormat.testFormat</li></div><div><li>org.apache.hadoop.mapred.TestLazyOutput.testLazyOutput</li></div><div><li>org.apache.hadoop.mapred.TestLineRecordReaderJobs.testDefaultRecordDelimiters</li></div><div><li>org.apache.hadoop.mapred.TestLineRecordReaderJobs.testCustomRecordDelimiters</li></div><div><li>org.apache.hadoop.mapred.TestLocalJobSubmission.testLocalJobFilesOption</li></div><div><li>org.apache.hadoop.mapred.TestLocalJobSubmission.testLocalJobEncryptedIntermediateData</li></div><div><li>org.apache.hadoop.mapred.TestLocalJobSubmission.testLocalJobArchivesOption</li></div><div><li>org.apache.hadoop.mapred.TestLocalJobSubmission.testJobMaxMapConfig</li></div><div><li>org.apache.hadoop.mapred.TestLocalJobSubmission.testLocalJobLibjarsOption</li></div><div><li>org.apache.hadoop.mapred.TestLocalMRNotification.testMR</li></div><div><li>org.apache.hadoop.mapred.TestMRCJCFileOutputCommitter.testAbort</li></div><div><li>org.apache.hadoop.mapred.TestMRCJCFileOutputCommitter.testCommitter</li></div><div><li>org.apache.hadoop.mapred.TestMRIntermediateDataEncryption.testUberMode</li></div><div><li>org.apache.hadoop.mapred.TestMRIntermediateDataEncryption.testMultipleMapsPerNode</li></div><div><li>org.apache.hadoop.mapred.TestMRIntermediateDataEncryption.testMultipleReducers</li></div><div><li>org.apache.hadoop.mapred.TestMRIntermediateDataEncryption.testSingleReducer</li></div><div><li>org.apache.hadoop.mapred.TestMROpportunisticMaps.testHalfOpportunisticMaps</li></div><div><li>org.apache.hadoop.mapred.TestMROpportunisticMaps.testAllOpportunisticMaps</li></div><div><li>org.apache.hadoop.mapred.TestMRTimelineEventHandling.testMRTimelineEventHandling</li></div><div><li>org.apache.hadoop.mapred.TestMRTimelineEventHandling.testMRNewTimelineServiceEventHandling</li></div><div><li>org.apache.hadoop.mapred.TestMRTimelineEventHandling.testTimelineServiceStartInMiniCluster</li></div><div><li>org.apache.hadoop.mapred.TestMRTimelineEventHandling.testMapreduceJobTimelineServiceEnabled</li></div><div><li>org.apache.hadoop.mapred.TestMapOutputType.testNoMismatch</li></div><div><li>org.apache.hadoop.mapred.TestMapOutputType.testValueMismatch</li></div><div><li>org.apache.hadoop.mapred.TestMapOutputType.testKeyMismatch</li></div><div><li>org.apache.hadoop.mapred.TestMapProgress.testMapProgress</li></div><div><li>org.apache.hadoop.mapred.TestMapRed.testNullKeys</li></div><div><li>org.apache.hadoop.mapred.TestMapRed.testSmallInput</li></div><div><li>org.apache.hadoop.mapred.TestMapRed.testBiggerInput</li></div><div><li>org.apache.hadoop.mapred.TestMapRed.testCompression</li></div><div><li>org.apache.hadoop.mapred.TestMapRed.testMapred</li></div><div><li>org.apache.hadoop.mapred.TestMerge.testMerge</li></div><div><li>org.apache.hadoop.mapred.TestMiniMRBringup.testBringUp</li></div><div><li>org.apache.hadoop.mapred.TestMiniMRChildTask.org.apache.hadoop.mapred.TestMiniMRChildTask</li></div><div><li>org.apache.hadoop.mapred.TestMiniMRClasspath.testClassPath</li></div><div><li>org.apache.hadoop.mapred.TestMiniMRClasspath.testExternalWritable</li></div><div><li>org.apache.hadoop.mapred.TestMiniMRClientCluster.org.apache.hadoop.mapred.TestMiniMRClientCluster</li></div><div><li>org.apache.hadoop.mapred.TestMiniMRClientCluster.org.apache.hadoop.mapred.TestMiniMRClientCluster</li></div><div><li>org.apache.hadoop.mapred.TestMiniMRWithDFSWithDistinctUsers.testMultipleSpills</li></div><div><li>org.apache.hadoop.mapred.TestMiniMRWithDFSWithDistinctUsers.testDistinctUsers</li></div><div><li>org.apache.hadoop.mapred.TestMultiFileInputFormat.testFormatWithLessPathsThanSplits</li></div><div><li>org.apache.hadoop.mapred.TestMultiFileInputFormat.testFormat</li></div><div><li>org.apache.hadoop.mapred.TestMultiFileSplit.testgetLocations</li></div><div><li>org.apache.hadoop.mapred.TestMultipleTextOutputFormat.testFormat</li></div><div><li>org.apache.hadoop.mapred.TestNetworkedJob.testGetJobStatus</li></div><div><li>org.apache.hadoop.mapred.TestNetworkedJob.testJobQueueClient</li></div><div><li>org.apache.hadoop.mapred.TestNetworkedJob.testNetworkedJob</li></div><div><li>org.apache.hadoop.mapred.TestOldCombinerGrouping.testCombiner</li></div><div><li>org.apache.hadoop.mapred.TestReduceFetch.testReduceFromMem</li></div><div><li>org.apache.hadoop.mapred.TestReduceFetch.testReduceFromDisk</li></div><div><li>org.apache.hadoop.mapred.TestReduceFetch.testReduceFromPartialMem</li></div><div><li>org.apache.hadoop.mapred.TestReduceFetchFromPartialMem.testReduceFromPartialMem</li></div><div><li>org.apache.hadoop.mapred.TestReporter.testReporterProgressForMRJob</li></div><div><li>org.apache.hadoop.mapred.TestReporter.testStatusLimit</li></div><div><li>org.apache.hadoop.mapred.TestReporter.testReporterProgressForMapOnlyJob</li></div><div><li>org.apache.hadoop.mapred.TestSequenceFileAsBinaryInputFormat.testBinary</li></div><div><li>org.apache.hadoop.mapred.TestSequenceFileAsBinaryOutputFormat.testBinary</li></div><div><li>org.apache.hadoop.mapred.TestSequenceFileAsBinaryOutputFormat.testcheckOutputSpecsForbidRecordCompression</li></div><div><li>org.apache.hadoop.mapred.TestSequenceFileAsTextInputFormat.testFormat</li></div><div><li>org.apache.hadoop.mapred.TestSequenceFileInputFilter.testPercentFilter</li></div><div><li>org.apache.hadoop.mapred.TestSequenceFileInputFilter.testMD5Filter</li></div><div><li>org.apache.hadoop.mapred.TestSequenceFileInputFilter.testRegexFilter</li></div><div><li>org.apache.hadoop.mapred.TestSequenceFileInputFormat.testFormat</li></div><div><li>org.apache.hadoop.mapred.TestSpecialCharactersInOutputPath.testJobWithDFS</li></div><div><li>org.apache.hadoop.mapred.TestTaskCommit.testCommitFail</li></div><div><li>org.apache.hadoop.mapred.TestTaskCommit.testTaskCleanupDoesNotCommit</li></div><div><li>org.apache.hadoop.mapred.TestTextOutputFormat.testCompress</li></div><div><li>org.apache.hadoop.mapred.TestTextOutputFormat.testFormatWithCustomSeparator</li></div><div><li>org.apache.hadoop.mapred.TestTextOutputFormat.testFormat</li></div><div><li>org.apache.hadoop.mapred.TestUserDefinedCounters.testMapReduceJob</li></div><div><li>org.apache.hadoop.mapred.TestYARNRunner.testNodeLabelExp</li></div><div><li>org.apache.hadoop.mapred.TestYARNRunner.testResourceRequestLocalityMultipleNodesDifferentRack</li></div><div><li>org.apache.hadoop.mapred.TestYARNRunner.testResourceRequestLocalityMultipleNodesDefaultRack</li></div><div><li>org.apache.hadoop.mapred.TestYARNRunner.testResourceRequestLocalityAny</li></div><div><li>org.apache.hadoop.mapred.TestYARNRunner.testAMProfiler</li></div><div><li>org.apache.hadoop.mapred.TestYARNRunner.testAMStandardEnvWithCustomLibPath</li></div><div><li>org.apache.hadoop.mapred.TestYARNRunner.testAMStandardEnvWithDefaultLibPath</li></div><div><li>org.apache.hadoop.mapred.TestYARNRunner.testWarnCommandOpts</li></div><div><li>org.apache.hadoop.mapred.TestYARNRunner.testJobPriority</li></div><div><li>org.apache.hadoop.mapred.TestYARNRunner.testCustomAMRMResourceType</li></div><div><li>org.apache.hadoop.mapred.TestYARNRunner.testAMAdminCommandOpts</li></div><div><li>org.apache.hadoop.mapred.TestYARNRunner.testResourceRequestLocalityNode</li></div><div><li>org.apache.hadoop.mapred.TestYARNRunner.testResourceRequestLocalityRack</li></div><div><li>org.apache.hadoop.mapred.TestYARNRunner.testResourceRequestLocalityInvalid</li></div><div><li>org.apache.hadoop.mapred.TestYARNRunner.testAMRMemoryRequestOverriding</li></div><div><li>org.apache.hadoop.mapred.TestYARNRunner.testResourceRequestLocalityMultipleNodes</li></div><div><li>org.apache.hadoop.mapred.TestYARNRunner.testAMStandardEnvWithCustomLibPathWithSeparateEnvProps</li></div><div><li>org.apache.hadoop.mapred.TestYARNRunner.testResourceRequestLocalityNodeDefaultRack</li></div><div><li>org.apache.hadoop.mapred.TestYARNRunner.testSendJobConf</li></div><div><li>org.apache.hadoop.mapred.TestYARNRunner.testGetHSDelegationToken</li></div><div><li>org.apache.hadoop.mapred.TestYARNRunner.testAMRMemoryRequest</li></div><div><li>org.apache.hadoop.mapred.jobcontrol.TestJobControl.testJobControl</li></div><div><li>org.apache.hadoop.mapred.jobcontrol.TestJobControl.testAddingDependingJob</li></div><div><li>org.apache.hadoop.mapred.jobcontrol.TestJobControl.testJobState</li></div><div><li>org.apache.hadoop.mapred.jobcontrol.TestLocalJobControl.testLocalJobControlDataCopy</li></div><div><li>org.apache.hadoop.mapred.join.TestDatamerge.testNestedJoin</li></div><div><li>org.apache.hadoop.mapred.join.TestDatamerge.testEmptyJoin</li></div><div><li>org.apache.hadoop.mapred.join.TestDatamerge.testSimpleOuterJoin</li></div><div><li>org.apache.hadoop.mapred.join.TestDatamerge.testSimpleOverride</li></div><div><li>org.apache.hadoop.mapred.join.TestDatamerge.testSimpleInnerJoin</li></div><div><li>org.apache.hadoop.mapred.lib.TestChainMapReduce.testChain</li></div><div><li>org.apache.hadoop.mapred.lib.TestKeyFieldBasedComparator.testBasicUnixComparator</li></div><div><li>org.apache.hadoop.mapred.lib.TestLineInputFormat.testFormat</li></div><div><li>org.apache.hadoop.mapred.lib.TestMultipleOutputs.testWithCounters</li></div><div><li>org.apache.hadoop.mapred.lib.TestMultipleOutputs.testWithCounters</li></div><div><li>org.apache.hadoop.mapred.lib.TestMultipleOutputs.testWithoutCounters</li></div><div><li>org.apache.hadoop.mapred.lib.TestMultipleOutputs.testWithoutCounters</li></div><div><li>org.apache.hadoop.mapred.lib.TestMultithreadedMapRunner.testRuntimeExRun</li></div><div><li>org.apache.hadoop.mapred.lib.TestMultithreadedMapRunner.testOKRun</li></div><div><li>org.apache.hadoop.mapred.lib.TestMultithreadedMapRunner.testIOExRun</li></div><div><li>org.apache.hadoop.mapred.lib.aggregate.TestAggregates.testAggregates</li></div><div><li>org.apache.hadoop.mapred.pipes.TestPipeApplication.testSubmitter</li></div><div><li>org.apache.hadoop.mapred.pipes.TestPipesNonJavaInputFormat.testFormat</li></div><div><li>org.apache.hadoop.mapreduce.TestChild.testChild</li></div><div><li>org.apache.hadoop.mapreduce.TestLargeSort.testLargeSort</li></div><div><li>org.apache.hadoop.mapreduce.TestLocalRunner.testGcCounter</li></div><div><li>org.apache.hadoop.mapreduce.TestLocalRunner.testInvalidMultiMapParallelism</li></div><div><li>org.apache.hadoop.mapreduce.TestLocalRunner.testOneMapMultiReduce</li></div><div><li>org.apache.hadoop.mapreduce.TestLocalRunner.testEmptyMaps</li></div><div><li>org.apache.hadoop.mapreduce.TestLocalRunner.testMultiMaps</li></div><div><li>org.apache.hadoop.mapreduce.TestLocalRunner.testMultiMapMultiReduce</li></div><div><li>org.apache.hadoop.mapreduce.TestLocalRunner.testOneMapMultiParallelReduce</li></div><div><li>org.apache.hadoop.mapreduce.TestLocalRunner.testMultiMapOneReduce</li></div><div><li>org.apache.hadoop.mapreduce.TestMRJobClient.testJobName</li></div><div><li>org.apache.hadoop.mapreduce.TestMRJobClient.testJobSubmissionSpecsAndFiles</li></div><div><li>org.apache.hadoop.mapreduce.TestMRJobClient.testJobClient</li></div><div><li>org.apache.hadoop.mapreduce.TestMROutputFormat.testJobSubmission</li></div><div><li>org.apache.hadoop.mapreduce.TestMapCollection.testSingleRecord</li></div><div><li>org.apache.hadoop.mapreduce.TestMapCollection.testPostSpillMeta</li></div><div><li>org.apache.hadoop.mapreduce.TestMapCollection.testLowSpill</li></div><div><li>org.apache.hadoop.mapreduce.TestMapCollection.testSplitMetaSpill</li></div><div><li>org.apache.hadoop.mapreduce.TestMapCollection.testZeroVal</li></div><div><li>org.apache.hadoop.mapreduce.TestMapCollection.testLargeRecConcurrent</li></div><div><li>org.apache.hadoop.mapreduce.TestMapCollection.testLargeRecords</li></div><div><li>org.apache.hadoop.mapreduce.TestMapCollection.testRandomCompress</li></div><div><li>org.apache.hadoop.mapreduce.TestMapCollection.testSpillPer2B</li></div><div><li>org.apache.hadoop.mapreduce.TestMapCollection.testValLastByte</li></div><div><li>org.apache.hadoop.mapreduce.TestMapCollection.testRandom</li></div><div><li>org.apache.hadoop.mapreduce.TestMapReduce.testMapred</li></div><div><li>org.apache.hadoop.mapreduce.TestMapReduceLazyOutput.testLazyOutput</li></div><div><li>org.apache.hadoop.mapreduce.TestMapperReducerCleanup.testMapCleanup</li></div><div><li>org.apache.hadoop.mapreduce.TestMapperReducerCleanup.testJobSuccessCleanup</li></div><div><li>org.apache.hadoop.mapreduce.TestMapperReducerCleanup.testReduceCleanup</li></div><div><li>org.apache.hadoop.mapreduce.TestNewCombinerGrouping.testCombiner</li></div><div><li>org.apache.hadoop.mapreduce.TestValueIterReset.testValueIterReset</li></div><div><li>org.apache.hadoop.mapreduce.lib.aggregate.TestMapReduceAggregates.testAggregates</li></div><div><li>org.apache.hadoop.mapreduce.lib.chain.TestChainErrors.testChainMapNoOuptut</li></div><div><li>org.apache.hadoop.mapreduce.lib.chain.TestChainErrors.testChainSubmission</li></div><div><li>org.apache.hadoop.mapreduce.lib.chain.TestChainErrors.testReducerFail</li></div><div><li>org.apache.hadoop.mapreduce.lib.chain.TestChainErrors.testChainReduceNoOuptut</li></div><div><li>org.apache.hadoop.mapreduce.lib.chain.TestChainErrors.testChainFail</li></div><div><li>org.apache.hadoop.mapreduce.lib.chain.TestMapReduceChain.testChain</li></div><div><li>org.apache.hadoop.mapreduce.lib.chain.TestSingleElementChain.testNoChain</li></div><div><li>org.apache.hadoop.mapreduce.lib.db.TestDataDrivenDBInputFormat.testDateSplits</li></div><div><li>org.apache.hadoop.mapreduce.lib.fieldsel.TestMRFieldSelection.testFieldSelection</li></div><div><li>org.apache.hadoop.mapreduce.lib.input.TestCombineFileInputFormat.testForEmptyFile</li></div><div><li>org.apache.hadoop.mapreduce.lib.input.TestLineRecordReaderJobs.testDefaultRecordDelimiters</li></div><div><li>org.apache.hadoop.mapreduce.lib.input.TestLineRecordReaderJobs.testCustomRecordDelimiters</li></div><div><li>org.apache.hadoop.mapreduce.lib.input.TestMRCJCFileInputFormat.testForEmptyFile</li></div><div><li>org.apache.hadoop.mapreduce.lib.input.TestMRSequenceFileAsBinaryInputFormat.testBinary</li></div><div><li>org.apache.hadoop.mapreduce.lib.input.TestMRSequenceFileAsTextInputFormat.testFormat</li></div><div><li>org.apache.hadoop.mapreduce.lib.input.TestMRSequenceFileInputFilter.testPercentFilter</li></div><div><li>org.apache.hadoop.mapreduce.lib.input.TestMRSequenceFileInputFilter.testMD5Filter</li></div><div><li>org.apache.hadoop.mapreduce.lib.input.TestMRSequenceFileInputFilter.testRegexFilter</li></div><div><li>org.apache.hadoop.mapreduce.lib.input.TestMultipleInputs.testDoMultipleInputs</li></div><div><li>org.apache.hadoop.mapreduce.lib.input.TestMultipleInputs.testAddInputPathWithFormat</li></div><div><li>org.apache.hadoop.mapreduce.lib.input.TestMultipleInputs.testAddInputPathWithMapper</li></div><div><li>org.apache.hadoop.mapreduce.lib.input.TestNLineInputFormat.testFormat</li></div><div><li>org.apache.hadoop.mapreduce.lib.jobcontrol.TestMapReduceJobControl.testJobControl</li></div><div><li>org.apache.hadoop.mapreduce.lib.jobcontrol.TestMapReduceJobControl.testControlledJob</li></div><div><li>org.apache.hadoop.mapreduce.lib.jobcontrol.TestMapReduceJobControl.testJobControlWithFailJob</li></div><div><li>org.apache.hadoop.mapreduce.lib.jobcontrol.TestMapReduceJobControl.testJobControlWithKillJob</li></div><div><li>org.apache.hadoop.mapreduce.lib.join.TestJoinDatamerge.testNestedJoin</li></div><div><li>org.apache.hadoop.mapreduce.lib.join.TestJoinDatamerge.testEmptyJoin</li></div><div><li>org.apache.hadoop.mapreduce.lib.join.TestJoinDatamerge.testSimpleOuterJoin</li></div><div><li>org.apache.hadoop.mapreduce.lib.join.TestJoinDatamerge.testSimpleOverride</li></div><div><li>org.apache.hadoop.mapreduce.lib.join.TestJoinDatamerge.testSimpleInnerJoin</li></div><div><li>org.apache.hadoop.mapreduce.lib.map.TestMultithreadedMapper.testRuntimeExRun</li></div><div><li>org.apache.hadoop.mapreduce.lib.map.TestMultithreadedMapper.testOKRun</li></div><div><li>org.apache.hadoop.mapreduce.lib.map.TestMultithreadedMapper.testIOExRun</li></div><div><li>org.apache.hadoop.mapreduce.lib.output.TestJobOutputCommitter.testCustomCleanup</li></div><div><li>org.apache.hadoop.mapreduce.lib.output.TestJobOutputCommitter.testCustomCleanup</li></div><div><li>org.apache.hadoop.mapreduce.lib.output.TestJobOutputCommitter.testDefaultCleanupAndAbort</li></div><div><li>org.apache.hadoop.mapreduce.lib.output.TestJobOutputCommitter.testDefaultCleanupAndAbort</li></div><div><li>org.apache.hadoop.mapreduce.lib.output.TestJobOutputCommitter.testCustomAbort</li></div><div><li>org.apache.hadoop.mapreduce.lib.output.TestJobOutputCommitter.testCustomAbort</li></div><div><li>org.apache.hadoop.mapreduce.lib.output.TestMRCJCFileOutputCommitter.testAbort</li></div><div><li>org.apache.hadoop.mapreduce.lib.output.TestMRCJCFileOutputCommitter.testAbort</li></div><div><li>org.apache.hadoop.mapreduce.lib.output.TestMRCJCFileOutputCommitter.testCommitter</li></div><div><li>org.apache.hadoop.mapreduce.lib.output.TestMRCJCFileOutputCommitter.testCommitter</li></div><div><li>org.apache.hadoop.mapreduce.lib.output.TestMRCJCFileOutputCommitter.testFailAbort</li></div><div><li>org.apache.hadoop.mapreduce.lib.output.TestMRCJCFileOutputCommitter.testFailAbort</li></div><div><li>org.apache.hadoop.mapreduce.lib.output.TestMRCJCFileOutputCommitter.testEmptyOutput</li></div><div><li>org.apache.hadoop.mapreduce.lib.output.TestMRCJCFileOutputCommitter.testEmptyOutput</li></div><div><li>org.apache.hadoop.mapreduce.lib.output.TestMRMultipleOutputs.testWithCounters</li></div><div><li>org.apache.hadoop.mapreduce.lib.output.TestMRMultipleOutputs.testWithCounters</li></div><div><li>org.apache.hadoop.mapreduce.lib.output.TestMRMultipleOutputs.testWithoutCounters</li></div><div><li>org.apache.hadoop.mapreduce.lib.output.TestMRMultipleOutputs.testWithoutCounters</li></div><div><li>org.apache.hadoop.mapreduce.lib.output.TestMRSequenceFileAsBinaryOutputFormat.testBinary</li></div><div><li>org.apache.hadoop.mapreduce.lib.output.TestMRSequenceFileAsBinaryOutputFormat.testcheckOutputSpecsForbidRecordCompression</li></div><div><li>org.apache.hadoop.mapreduce.lib.partition.TestMRKeyFieldBasedComparator.testBasicUnixComparator</li></div><div><li>org.apache.hadoop.mapreduce.security.TestBinaryTokenFile.org.apache.hadoop.mapreduce.security.TestBinaryTokenFile</li></div><div><li>org.apache.hadoop.mapreduce.security.TestMRCredentials.org.apache.hadoop.mapreduce.security.TestMRCredentials</li></div><div><li>org.apache.hadoop.mapreduce.v2.TestMRAMWithNonNormalizedCapabilities.testJobWithNonNormalizedCapabilities</li></div><div><li>org.apache.hadoop.mapreduce.v2.TestMRAppWithCombiner.org.apache.hadoop.mapreduce.v2.TestMRAppWithCombiner</li></div><div><li>org.apache.hadoop.mapreduce.v2.TestMRJobs.org.apache.hadoop.mapreduce.v2.TestMRJobs</li></div><div><li>org.apache.hadoop.mapreduce.v2.TestMRJobsWithHistoryService.testJobHistoryData</li></div><div><li>org.apache.hadoop.mapreduce.v2.TestMRJobsWithProfiler.org.apache.hadoop.mapreduce.v2.TestMRJobsWithProfiler</li></div><div><li>org.apache.hadoop.mapreduce.v2.TestMROldApiJobs.org.apache.hadoop.mapreduce.v2.TestMROldApiJobs</li></div><div><li>org.apache.hadoop.mapreduce.v2.TestMiniMRProxyUser.___testInvalidProxyUser</li></div><div><li>org.apache.hadoop.mapreduce.v2.TestMiniMRProxyUser.__testCurrentUser</li></div><div><li>org.apache.hadoop.mapreduce.v2.TestMiniMRProxyUser.testValidProxyUser</li></div><div><li>org.apache.hadoop.mapreduce.v2.TestNonExistentJob.testGetInvalidJob</li></div><div><li>org.apache.hadoop.mapreduce.v2.TestRMNMInfo.org.apache.hadoop.mapreduce.v2.TestRMNMInfo</li></div><div><li>org.apache.hadoop.mapreduce.v2.TestSpeculativeExecution.org.apache.hadoop.mapreduce.v2.TestSpeculativeExecution</li></div><div><li>org.apache.hadoop.mapreduce.v2.TestSpeculativeExecutionWithMRApp.testSepculateSuccessfulWithUpdateEvents</li></div><div><li>org.apache.hadoop.mapreduce.v2.TestSpeculativeExecutionWithMRApp.testSpeculateSuccessfulWithoutUpdateEvents</li></div><div><li>org.apache.hadoop.mapreduce.v2.TestUberAM.org.apache.hadoop.mapreduce.v2.TestUberAM</li></div><div><li>org.apache.hadoop.examples.TestWordStats.testGetTheMean</li></div><div><li>org.apache.hadoop.examples.TestWordStats.testGetTheMedian</li></div><div><li>org.apache.hadoop.examples.TestWordStats.testGetTheStandardDeviation</li></div><div><li>org.apache.hadoop.examples.terasort.TestTeraSort.testTeraSort</li></div><div><li>org.apache.hadoop.fs.s3a.s3guard.TestDynamoDBMetadataStore.org.apache.hadoop.fs.s3a.s3guard.TestDynamoDBMetadataStore</li></div><div><li>org.apache.hadoop.contrib.utils.join.TestDataJoin.testDataJoin</li></div><div><li>org.apache.hadoop.tools.TestDistCpSync.testSync9</li></div><div><li>org.apache.hadoop.tools.TestDistCpSystem.testSourceRoot</li></div><div><li>org.apache.hadoop.tools.TestDistCpSystem.testPreserveUseNonEmptyDir</li></div><div><li>org.apache.hadoop.tools.TestDistCpSystem.testPreserveUserEmptyDir</li></div><div><li>org.apache.hadoop.tools.TestDistCpSystem.testPreserveUserNonEmptyDirWithUpdate</li></div><div><li>org.apache.hadoop.tools.TestDistCpSystem.testRecursiveChunkCopy</li></div><div><li>org.apache.hadoop.tools.TestDistCpSystem.testPreserveUserSingleFile</li></div><div><li>org.apache.hadoop.tools.TestDistCpSystem.testChunkCopyOneFile</li></div><div><li>org.apache.hadoop.tools.TestDistCpSystem.testDistcpLargeFile</li></div><div><li>org.apache.hadoop.tools.TestDistCpViewFs.testMultiDirTargetMissing</li></div><div><li>org.apache.hadoop.tools.TestDistCpViewFs.testUpdateGlobTargetMissingSingleLevel</li></div><div><li>org.apache.hadoop.tools.TestDistCpViewFs.testGlobTargetMissingMultiLevel</li></div><div><li>org.apache.hadoop.tools.TestDistCpViewFs.testSingleDirTargetPresent</li></div><div><li>org.apache.hadoop.tools.TestDistCpViewFs.testUpdateMultiDirTargetPresent</li></div><div><li>org.apache.hadoop.tools.TestDistCpViewFs.testSingleFileTargetFile</li></div><div><li>org.apache.hadoop.tools.TestDistCpViewFs.testMultiFileTargetPresent</li></div><div><li>org.apache.hadoop.tools.TestDistCpViewFs.testGlobTargetMissingSingleLevel</li></div><div><li>org.apache.hadoop.tools.TestDistCpViewFs.testSingleFileMissingTarget</li></div><div><li>org.apache.hadoop.tools.TestDistCpViewFs.testSingleFileTargetDir</li></div><div><li>org.apache.hadoop.tools.TestDistCpViewFs.testSingleDirTargetMissing</li></div><div><li>org.apache.hadoop.tools.TestDistCpViewFs.testUpdateMultiDirTargetMissing</li></div><div><li>org.apache.hadoop.tools.TestDistCpViewFs.testUpdateSingleDirTargetPresent</li></div><div><li>org.apache.hadoop.tools.TestDistCpViewFs.testMultiFileTargetMissing</li></div><div><li>org.apache.hadoop.tools.TestDistCpViewFs.testMultiDirTargetPresent</li></div><div><li>org.apache.hadoop.tools.TestDistCpViewFs.testUpdateGlobTargetMissingMultiLevel</li></div><div><li>org.apache.hadoop.tools.TestDistCpWithAcls.testPreserveAcls</li></div><div><li>org.apache.hadoop.tools.TestDistCpWithRawXAttrs.testPreserveRawXAttrs1</li></div><div><li>org.apache.hadoop.tools.TestDistCpWithRawXAttrs.testPreserveRawXAttrs2</li></div><div><li>org.apache.hadoop.tools.TestDistCpWithRawXAttrs.testPreserveRawXAttrs3</li></div><div><li>org.apache.hadoop.tools.TestDistCpWithRawXAttrs.testPreserveRawXAttrs4</li></div><div><li>org.apache.hadoop.tools.TestDistCpWithXAttrs.testPreserveXAttrs</li></div><div><li>org.apache.hadoop.tools.TestExternalCall.testCleanupTestViaToolRunner</li></div><div><li>org.apache.hadoop.tools.TestIntegration.testMultiDirTargetMissing[0]</li></div><div><li>org.apache.hadoop.tools.TestIntegration.testUpdateGlobTargetMissingSingleLevel[0]</li></div><div><li>org.apache.hadoop.tools.TestIntegration.testGlobTargetMissingMultiLevel[0]</li></div><div><li>org.apache.hadoop.tools.TestIntegration.testSingleDirTargetPresent[0]</li></div><div><li>org.apache.hadoop.tools.TestIntegration.testUpdateMultiDirTargetPresent[0]</li></div><div><li>org.apache.hadoop.tools.TestIntegration.testSingleFileTargetFile[0]</li></div><div><li>org.apache.hadoop.tools.TestIntegration.testMultiFileTargetPresent[0]</li></div><div><li>org.apache.hadoop.tools.TestIntegration.testGlobTargetMissingSingleLevel[0]</li></div><div><li>org.apache.hadoop.tools.TestIntegration.testDeleteMissingInDestination[0]</li></div><div><li>org.apache.hadoop.tools.TestIntegration.testSingleFileMissingTarget[0]</li></div><div><li>org.apache.hadoop.tools.TestIntegration.testSingleFileTargetDir[0]</li></div><div><li>org.apache.hadoop.tools.TestIntegration.testSingleDirTargetMissing[0]</li></div><div><li>org.apache.hadoop.tools.TestIntegration.testUpdateMultiDirTargetMissing[0]</li></div><div><li>org.apache.hadoop.tools.TestIntegration.testOverwrite[0]</li></div><div><li>org.apache.hadoop.tools.TestIntegration.testUpdateSingleDirTargetPresent[0]</li></div><div><li>org.apache.hadoop.tools.TestIntegration.testMultiFileTargetMissing[0]</li></div><div><li>org.apache.hadoop.tools.TestIntegration.testMultiDirTargetPresent[0]</li></div><div><li>org.apache.hadoop.tools.TestIntegration.testUpdateGlobTargetMissingMultiLevel[0]</li></div><div><li>org.apache.hadoop.tools.TestIntegration.testMultiDirTargetMissing[1]</li></div><div><li>org.apache.hadoop.tools.TestIntegration.testUpdateGlobTargetMissingSingleLevel[1]</li></div><div><li>org.apache.hadoop.tools.TestIntegration.testGlobTargetMissingMultiLevel[1]</li></div><div><li>org.apache.hadoop.tools.TestIntegration.testSingleDirTargetPresent[1]</li></div><div><li>org.apache.hadoop.tools.TestIntegration.testUpdateMultiDirTargetPresent[1]</li></div><div><li>org.apache.hadoop.tools.TestIntegration.testSingleFileTargetFile[1]</li></div><div><li>org.apache.hadoop.tools.TestIntegration.testMultiFileTargetPresent[1]</li></div><div><li>org.apache.hadoop.tools.TestIntegration.testGlobTargetMissingSingleLevel[1]</li></div><div><li>org.apache.hadoop.tools.TestIntegration.testDeleteMissingInDestination[1]</li></div><div><li>org.apache.hadoop.tools.TestIntegration.testSingleFileMissingTarget[1]</li></div><div><li>org.apache.hadoop.tools.TestIntegration.testSingleFileTargetDir[1]</li></div><div><li>org.apache.hadoop.tools.TestIntegration.testSingleDirTargetMissing[1]</li></div><div><li>org.apache.hadoop.tools.TestIntegration.testUpdateMultiDirTargetMissing[1]</li></div><div><li>org.apache.hadoop.tools.TestIntegration.testOverwrite[1]</li></div><div><li>org.apache.hadoop.tools.TestIntegration.testUpdateSingleDirTargetPresent[1]</li></div><div><li>org.apache.hadoop.tools.TestIntegration.testMultiFileTargetMissing[1]</li></div><div><li>org.apache.hadoop.tools.TestIntegration.testMultiDirTargetPresent[1]</li></div><div><li>org.apache.hadoop.tools.TestIntegration.testUpdateGlobTargetMissingMultiLevel[1]</li></div><div><li>org.apache.hadoop.tools.TestIntegration.testMultiDirTargetMissing[2]</li></div><div><li>org.apache.hadoop.tools.TestIntegration.testUpdateGlobTargetMissingSingleLevel[2]</li></div><div><li>org.apache.hadoop.tools.TestIntegration.testGlobTargetMissingMultiLevel[2]</li></div><div><li>org.apache.hadoop.tools.TestIntegration.testSingleDirTargetPresent[2]</li></div><div><li>org.apache.hadoop.tools.TestIntegration.testUpdateMultiDirTargetPresent[2]</li></div><div><li>org.apache.hadoop.tools.TestIntegration.testSingleFileTargetFile[2]</li></div><div><li>org.apache.hadoop.tools.TestIntegration.testMultiFileTargetPresent[2]</li></div><div><li>org.apache.hadoop.tools.TestIntegration.testGlobTargetMissingSingleLevel[2]</li></div><div><li>org.apache.hadoop.tools.TestIntegration.testDeleteMissingInDestination[2]</li></div><div><li>org.apache.hadoop.tools.TestIntegration.testSingleFileMissingTarget[2]</li></div><div><li>org.apache.hadoop.tools.TestIntegration.testSingleFileTargetDir[2]</li></div><div><li>org.apache.hadoop.tools.TestIntegration.testSingleDirTargetMissing[2]</li></div><div><li>org.apache.hadoop.tools.TestIntegration.testUpdateMultiDirTargetMissing[2]</li></div><div><li>org.apache.hadoop.tools.TestIntegration.testOverwrite[2]</li></div><div><li>org.apache.hadoop.tools.TestIntegration.testUpdateSingleDirTargetPresent[2]</li></div><div><li>org.apache.hadoop.tools.TestIntegration.testMultiFileTargetMissing[2]</li></div><div><li>org.apache.hadoop.tools.TestIntegration.testMultiDirTargetPresent[2]</li></div><div><li>org.apache.hadoop.tools.TestIntegration.testUpdateGlobTargetMissingMultiLevel[2]</li></div><div><li>org.apache.hadoop.tools.contract.TestLocalContractDistCp.testUpdateDeepDirectoryStructureNoChange</li></div><div><li>org.apache.hadoop.tools.contract.TestLocalContractDistCp.testTrackDeepDirectoryStructureToRemote</li></div><div><li>org.apache.hadoop.tools.contract.TestLocalContractDistCp.largeFilesToRemote</li></div><div><li>org.apache.hadoop.tools.contract.TestLocalContractDistCp.testLargeFilesFromRemote</li></div><div><li>org.apache.hadoop.tools.contract.TestLocalContractDistCp.testUpdateDeepDirectoryStructureToRemote</li></div><div><li>org.apache.hadoop.tools.contract.TestLocalContractDistCp.testDeepDirectoryStructureFromRemote</li></div><div><li>org.apache.hadoop.tools.util.TestDistCpUtils.testReplFactorNotPreservedOnErasureCodedFile</li></div><div><li>org.apache.hadoop.tools.TestDistCh.testDistCh</li></div><div><li>org.apache.hadoop.mapred.gridmix.TestCompressionEmulationUtils.testCompressionRatios</li></div><div><li>org.apache.hadoop.mapred.gridmix.TestCompressionEmulationUtils.testRandomCompressedTextDataGenerator</li></div><div><li>org.apache.hadoop.mapred.gridmix.TestDistCacheEmulation.org.apache.hadoop.mapred.gridmix.TestDistCacheEmulation</li></div><div><li>org.apache.hadoop.mapred.gridmix.TestGridMixClasses.testSerialReaderThread</li></div><div><li>org.apache.hadoop.mapred.gridmix.TestGridmixSubmission.org.apache.hadoop.mapred.gridmix.TestGridmixSubmission</li></div><div><li>org.apache.hadoop.mapred.gridmix.TestGridmixSummary.testDataStatistics</li></div><div><li>org.apache.hadoop.mapred.gridmix.TestGridmixSummary.testExecutionSummarizer</li></div><div><li>org.apache.hadoop.mapred.gridmix.TestLoadJob.org.apache.hadoop.mapred.gridmix.TestLoadJob</li></div><div><li>org.apache.hadoop.mapred.gridmix.TestSleepJob.org.apache.hadoop.mapred.gridmix.TestSleepJob</li></div><div><li>org.apache.hadoop.yarn.sls.scheduler.TestTaskRunner.testPreStartQueueing</li></div><div><li>org.apache.hadoop.streaming.TestAutoInputFormat.testFormat</li></div><div><li>org.apache.hadoop.streaming.TestFileArgs.testCommandLine</li></div><div><li>org.apache.hadoop.streaming.TestGzipInput.testCommandLine</li></div><div><li>org.apache.hadoop.streaming.TestMultipleArchiveFiles.testCommandLine</li></div><div><li>org.apache.hadoop.streaming.TestMultipleCachefiles.testMultipleCachefiles</li></div><div><li>org.apache.hadoop.streaming.TestRawBytesStreaming.testCommandLine</li></div><div><li>org.apache.hadoop.streaming.TestStreamAggregate.testCommandLine</li></div><div><li>org.apache.hadoop.streaming.TestStreamDataProtocol.testCommandLine</li></div><div><li>org.apache.hadoop.streaming.TestStreamReduceNone.testCommandLine</li></div><div><li>org.apache.hadoop.streaming.TestStreamXmlMultipleRecords.testStreamXmlMultiInnerFast</li></div><div><li>org.apache.hadoop.streaming.TestStreamXmlMultipleRecords.testStreamXmlMultiInnerSlow</li></div><div><li>org.apache.hadoop.streaming.TestStreamXmlMultipleRecords.testStreamXmlMultiOuterFast</li></div><div><li>org.apache.hadoop.streaming.TestStreamXmlMultipleRecords.testStreamXmlMultiOuterSlow</li></div><div><li>org.apache.hadoop.streaming.TestStreamXmlRecordReader.testCommandLine</li></div><div><li>org.apache.hadoop.streaming.TestStreaming.testCommandLine</li></div><div><li>org.apache.hadoop.streaming.TestStreamingBackground.testBackgroundSubmitOk</li></div><div><li>org.apache.hadoop.streaming.TestStreamingBadRecords.testNoOp</li></div><div><li>org.apache.hadoop.streaming.TestStreamingCombiner.testCommandLine</li></div><div><li>org.apache.hadoop.streaming.TestStreamingCounters.testCommandLine</li></div><div><li>org.apache.hadoop.streaming.TestStreamingExitStatus.testReduceFailNotOk</li></div><div><li>org.apache.hadoop.streaming.TestStreamingExitStatus.testReduceFailOk</li></div><div><li>org.apache.hadoop.streaming.TestStreamingExitStatus.testMapFailNotOk</li></div><div><li>org.apache.hadoop.streaming.TestStreamingExitStatus.testMapFailOk</li></div><div><li>org.apache.hadoop.streaming.TestStreamingFailure.testCommandLine</li></div><div><li>org.apache.hadoop.streaming.TestStreamingKeyValue.testCommandLineWithKey</li></div><div><li>org.apache.hadoop.streaming.TestStreamingKeyValue.testCommandLineWithoutKey</li></div><div><li>org.apache.hadoop.streaming.TestStreamingOutputKeyValueTypes.testJavaMapperWithReduceNone</li></div><div><li>org.apache.hadoop.streaming.TestStreamingOutputKeyValueTypes.testDefaultToIdentityReducer</li></div><div><li>org.apache.hadoop.streaming.TestStreamingOutputKeyValueTypes.testJavaMapperAndJavaReducer</li></div><div><li>org.apache.hadoop.streaming.TestStreamingOutputKeyValueTypes.testCommandMapperAndCommandReducer</li></div><div><li>org.apache.hadoop.streaming.TestStreamingOutputKeyValueTypes.testJavaMapperAndJavaReducerAndZeroReduces</li></div><div><li>org.apache.hadoop.streaming.TestStreamingOutputKeyValueTypes.testJavaMapperAndCommandReducer</li></div><div><li>org.apache.hadoop.streaming.TestStreamingOutputKeyValueTypes.testJavaMapperAndCommandReducerAndZeroReduces</li></div><div><li>org.apache.hadoop.streaming.TestStreamingOutputKeyValueTypes.testCommandMapperWithReduceNone</li></div><div><li>org.apache.hadoop.streaming.TestStreamingOutputKeyValueTypes.testCommandMapperAndJavaReducer</li></div><div><li>org.apache.hadoop.streaming.TestStreamingOutputKeyValueTypes.testCommandMapperAndJavaReducerAndZeroReduces</li></div><div><li>org.apache.hadoop.streaming.TestStreamingOutputKeyValueTypes.testCommandMapperAndCommandReducerAndZeroReduces</li></div><div><li>org.apache.hadoop.streaming.TestStreamingOutputOnlyKeys.testOutputOnlyKeys</li></div><div><li>org.apache.hadoop.streaming.TestStreamingSeparator.testCommandLine</li></div><div><li>org.apache.hadoop.streaming.TestStreamingStderr.testStderrCountsAsProgress</li></div><div><li>org.apache.hadoop.streaming.TestStreamingStderr.testStderrNoInput</li></div><div><li>org.apache.hadoop.streaming.TestStreamingStderr.testStderrAfterOutput</li></div><div><li>org.apache.hadoop.streaming.TestSymLink.testSymLink</li></div><div><li>org.apache.hadoop.streaming.TestTypedBytesStreaming.testCommandLine</li></div><div><li>org.apache.hadoop.streaming.TestUnconsumedInput.testUnconsumedInput</li></div><div><li>org.apache.hadoop.streaming.mapreduce.TestStreamXmlRecordReader.testStreamXmlRecordReader</li></div><div><li>org.apache.hadoop.streaming.mapreduce.TestStreamXmlRecordReader.testStreamXmlRecordReader</li></div><div><li>org.apache.hadoop.yarn.applications.distributedshell.TestDistributedShellWithNodeLabels.testDSShellWithNodeLabelExpression</li></div><div><li>org.apache.hadoop.yarn.service.client.TestSystemServiceImpl.testSystemServiceSubmission</li></div><div><li>org.apache.hadoop.yarn.service.monitor.probe.TestDefaultProbe.testDefaultProbe[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientMatchingFitExecType[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientMatchingFitExecType[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientMatchingFit[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientMatchingFit[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClient[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClient[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithContainerDemotion[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithContainerDemotion[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithBlacklist[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithBlacklist[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAskWithInvalidNodeLabels[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAskWithInvalidNodeLabels[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientNoMatchingRequests[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientNoMatchingRequests[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientOnAMRMTokenRollOver[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientOnAMRMTokenRollOver[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientAllocReqId[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientAllocReqId[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientMatchingFitInferredRack[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientMatchingFitInferredRack[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testWaitFor[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testWaitFor[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAskWithNodeLabels[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAskWithNodeLabels[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithContainerPromotion[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithContainerPromotion[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientMatchStorage[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientMatchStorage[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMContainerPromotionAndDemotionWithAutoUpdate[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMContainerPromotionAndDemotionWithAutoUpdate[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithContainerResourceChange[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithContainerResourceChange[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAllocationWithBlacklist[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAllocationWithBlacklist[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithSaslEncryption[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithSaslEncryption[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testGetMatchingFitWithProfiles[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testGetMatchingFitWithProfiles[0]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientMatchingFitExecType[1]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientMatchingFitExecType[1]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientMatchingFit[1]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientMatchingFit[1]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClient[1]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClient[1]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithContainerDemotion[1]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithContainerDemotion[1]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithBlacklist[1]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithBlacklist[1]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAskWithInvalidNodeLabels[1]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAskWithInvalidNodeLabels[1]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientNoMatchingRequests[1]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientNoMatchingRequests[1]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientOnAMRMTokenRollOver[1]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientOnAMRMTokenRollOver[1]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientAllocReqId[1]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientAllocReqId[1]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientMatchingFitInferredRack[1]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientMatchingFitInferredRack[1]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testWaitFor[1]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testWaitFor[1]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAskWithNodeLabels[1]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAskWithNodeLabels[1]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithContainerPromotion[1]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithContainerPromotion[1]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientMatchStorage[1]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientMatchStorage[1]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMContainerPromotionAndDemotionWithAutoUpdate[1]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMContainerPromotionAndDemotionWithAutoUpdate[1]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithContainerResourceChange[1]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithContainerResourceChange[1]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAllocationWithBlacklist[1]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAllocationWithBlacklist[1]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithSaslEncryption[1]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithSaslEncryption[1]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testGetMatchingFitWithProfiles[1]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testGetMatchingFitWithProfiles[1]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientMatchingFitExecType[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientMatchingFitExecType[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientMatchingFit[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientMatchingFit[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClient[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClient[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithContainerDemotion[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithContainerDemotion[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithBlacklist[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithBlacklist[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAskWithInvalidNodeLabels[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAskWithInvalidNodeLabels[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientNoMatchingRequests[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientNoMatchingRequests[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientOnAMRMTokenRollOver[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientOnAMRMTokenRollOver[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientAllocReqId[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientAllocReqId[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientMatchingFitInferredRack[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientMatchingFitInferredRack[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testWaitFor[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testWaitFor[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAskWithNodeLabels[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAskWithNodeLabels[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithContainerPromotion[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithContainerPromotion[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientMatchStorage[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientMatchStorage[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMContainerPromotionAndDemotionWithAutoUpdate[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMContainerPromotionAndDemotionWithAutoUpdate[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithContainerResourceChange[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithContainerResourceChange[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAllocationWithBlacklist[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAllocationWithBlacklist[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithSaslEncryption[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithSaslEncryption[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testGetMatchingFitWithProfiles[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testGetMatchingFitWithProfiles[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClientPlacementConstraints.testAMRMClientWithPlacementConstraints</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMClientPlacementConstraints.testAMRMClientWithPlacementConstraints</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMProxy.testAMRMProxyE2E</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMProxy.testAMRMProxyTokenRenewal</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMProxy.testE2ETokenSwap</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestNMClient.testNMClient</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestNMClient.testNMClientNoCleanupOnStop</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestOpportunisticContainerAllocationE2E.org.apache.hadoop.yarn.client.api.impl.TestOpportunisticContainerAllocationE2E</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestYarnClientWithReservation.testListReservationsByReservationId[CAPACITY]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestYarnClientWithReservation.testReservationDelete[CAPACITY]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestYarnClientWithReservation.testListReservationsByInvalidTimeInterval[CAPACITY]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestYarnClientWithReservation.testUpdateReservation[CAPACITY]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestYarnClientWithReservation.testListReservationsByTimeIntervalContainingNoReservations[CAPACITY]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestYarnClientWithReservation.testCreateReservation[CAPACITY]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestYarnClientWithReservation.testListReservationsByTimeInterval[CAPACITY]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestYarnClientWithReservation.testListReservationsByReservationId[FAIR]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestYarnClientWithReservation.testReservationDelete[FAIR]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestYarnClientWithReservation.testListReservationsByInvalidTimeInterval[FAIR]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestYarnClientWithReservation.testUpdateReservation[FAIR]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestYarnClientWithReservation.testListReservationsByTimeIntervalContainingNoReservations[FAIR]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestYarnClientWithReservation.testCreateReservation[FAIR]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestYarnClientWithReservation.testListReservationsByTimeInterval[FAIR]</li></div><div><li>org.apache.hadoop.yarn.api.resource.TestPlacementConstraintTransformations.testCardinalityConstraint</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.TestLinuxContainerExecutorWithMocks.testStartLocalizer</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.TestLinuxContainerExecutorWithMocks.testNoExitCodeFromPrivilegedOperation</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.TestLocalDirsHandlerService.testDirStructure</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.TestNodeHealthService.testNodeHealthService</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerReboot.testClearLocalDirWhenNodeReboot</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerResync.testContainerResourceIncreaseIsSynchronizedWithRMResync</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerResync.testKillContainersOnResync</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerShutdown.testKillContainersOnShutdown</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.testContainerLaunchAndExitFailure</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.testContainerUpgradeLocalizationFailure</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.testContainerUpgradeRollbackDueToFailure</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.testChangeContainerResource</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.testContainerUpgradeSuccessExplicitCommit</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.testForcefulShutdownSignal</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.testContainerSetup</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.testContainerUpgradeSuccessExplicitRollback</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.testLocalingResourceWhileContainerRunning</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.testGracefulShutdownSignal</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.testContainerUpgradeSuccessAutoCommit</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.testLocalFilesCleanup</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.testOutputThreadDumpSignal</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.testContainerUpgradeProcessFailure</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.testContainerLaunchAndExitSuccess</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.testContainerRestart</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.TestContainerLaunch.testImmediateKill</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.TestContainerLaunch.testErrorLogOnContainerExitWithMultipleFiles</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.TestContainerLaunch.testContainerEnvVariables</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.TestContainerLaunch.testContainerLaunchOnConfigurationError</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.TestContainerLaunch.testDelayedKill</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.TestContainerLaunch.testErrorLogOnContainerExitWithCustomPattern</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.TestContainerLaunch.testErrorLogOnContainerExitForCase</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.TestContainerLaunch.testKillProcessGroup</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.TestContainerLaunch.testErrorLogOnContainerExit</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.TestContainerLaunch.testErrorLogOnContainerExitForExt</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestResourceLocalizationService.testDirectoryCleanupOnNewlyCreatedStateStore</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestResourceLocalizationService.testLocalizerHeartbeatWhenAppCleaningUp</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestResourceLocalizationService.testRecovery</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestResourceLocalizationService.testLocalizationHeartbeat</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestResourceLocalizationService.testLocalizationInit</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestResourceLocalizationService.testDownloadingResourcesOnContainerKill</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestResourceLocalizationService.testLocalResourcePath</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestResourceLocalizationService.testParallelDownloadAttemptsForPrivateResource</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestResourceLocalizationService.testPublicResourceInitializesLocalDir</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestResourceLocalizationService.testFailedPublicResource</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestResourceLocalizationService.testParallelDownloadAttemptsForPublicResource</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestLogAggregationService.testLocalFileDeletionOnDiskFull</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.TestContainersMonitor.testContainerKillOnMemoryOverflow</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.TestContainersMonitor.testContainerMonitor</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.TestContainerSchedulerQueuing.testStopQueuedContainer</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.TestContainerSchedulerQueuing.testStartMultipleContainers</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.TestContainerSchedulerQueuing.testStartAndQueueMultipleContainers</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.TestContainerSchedulerQueuing.testQueueShedding</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.TestContainerSchedulerQueuing.testKillOnlyRequiredOpportunisticContainers</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.TestContainerSchedulerQueuing.testStartOpportunistcsWhenOppQueueIsFull</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.TestContainerSchedulerQueuing.testPromotionOfOpportunisticContainers</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.TestContainerSchedulerQueuing.testPauseOpportunisticForGuaranteedContainer</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.TestContainerSchedulerQueuing.testContainerDeQueuedAfterAMKill</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.TestContainerSchedulerQueuing.testContainerUpdateExecTypeGuaranteedToOpportunistic</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.TestContainerSchedulerQueuing.testKillOpportunisticForGuaranteedContainer</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServices.testContainerLogsWithNewAPI</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServices.testContainerLogsWithOldAPI</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService.testRefreshNodesResourceWithResourceReturnInRegistration</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestContainerResizing.testIncreaseContainerUnreservedWhenContainerCompleted</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestContainerResizing.testExcessiveReservationWhenDecreaseSameContainer</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestIncreaseAllocationExpirer.testDecreaseAfterIncreaseWithAllocationExpiration</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestIncreaseAllocationExpirer.testContainerIncreaseAllocationExpiration</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.security.TestDelegationTokenRenewer.testRMRestartWithExpiredToken</li></div><div><li>org.apache.hadoop.yarn.server.TestDiskFailures.testLocalDirsFailures</li></div><div><li>org.apache.hadoop.yarn.server.TestDiskFailures.testDirFailuresOnStartup</li></div><div><li>org.apache.hadoop.yarn.server.TestDiskFailures.testLogDirsFailures</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"><div><li>String index out of range: -1</li></div><div><li>Failed to append to non-existent file /test/test/target for client 127.0.0.1
 at org.apache.hadoop.hdfs.server.namenode.FSDirAppendOp.appendFile(FSDirAppendOp.java:104)
 at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.appendFile(FSNamesystem.java:2621)
 at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.append(NameNodeRpcServer.java:805)
 at org.apache.hadoop.hdfs.protocolPB.Client</li></div><div><li>Timed out waiting for /test1 to reach 3 replicas</li></div><div><li>test timed out after 120000 milliseconds</li></div><div><li>Timed out waiting for capacity. Live = 3 Expected = 2 Dead = 0 Expected = 1 Total capacity = 2533243453440 Expected = 2026594762752 Vol Fails = 1 Expected = 0</li></div><div><li>Expected non-empty /var/lib/jenkins/workspace/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-3/current/fsimage_0000000000000000005</li></div><div><li> Expected to find 'localhost:39717: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:39717: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li> Expected to find 'localhost:38227: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:38227: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li> Expected to find 'localhost:39637: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:39637: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li>expected timeout</li></div><div><li> Expected to find 'localhost:41306: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:41306: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li> Expected to find 'localhost:34961: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:34961: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li> Expected to find 'localhost:45289: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:45289: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li>expected timeout</li></div><div><li>Job didn't finish in 30 seconds</li></div><div><li>Expected success for Probe Status, time="Tue Apr 24 12:47:42 UTC 2018", outcome="failure", message="Failure in Default probe: IP presence with DNS checking and DNS server address 8.8.8.8", exception="java.io.IOException: comp-0: DNS checking is enabled, but lookup for example.com is not available yet"</li></div><div><li>Application attempt appattempt_1524563453599_0001_000001 doesn't exist in ApplicationMasterService cache.
 at org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.allocate(ApplicationMasterService.java:404)
 at org.apache.hadoop.yarn.server.nodemanager.amrmproxy.DefaultRequestInterceptor$3.allocate(DefaultRequestInterceptor.java:224)
 at org.apache.hadoop.yarn.server.nodemanager.</li></div><div><li>expected: java.util.HashSet&lt;[hb]&gt; but was: java.util.HashSet&lt;[hb]&gt;</li></div><div><li>Child process owned by init escaped process tree.</li></div><div><li>expected:&lt;2048&gt; but was:&lt;4096&gt;</li></div><div><li>expected:&lt;0.5&gt; but was:&lt;0.0&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>String index out of range: -1</li></div><div><li>Failed to append to non-existent file /test/test/target for client 127.0.0.1
 at org.apache.hadoop.hdfs.server.namenode.FSDirAppendOp.appendFile(FSDirAppendOp.java:104)
 at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.appendFile(FSNamesystem.java:2621)
 at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.append(NameNodeRpcServer.java:805)
 at org.apache.hadoop.hdfs.protocolPB.Client</li></div><div><li>Failed: the number of failed blocks = 2 &gt; the number of parity blocks = 1</li></div><div><li>expected:&lt;2&gt; but was:&lt;4&gt;</li></div><div><li>Test resulted in an unexpected exit</li></div><div><li>Test resulted in an unexpected exit</li></div><div><li>expected null, but was:&lt;javax.management.openmbean.CompositeDataSupport(compositeType=javax.management.openmbean.CompositeType(name=org.apache.hadoop.hdfs.protocol.RollingUpgradeInfo$Bean,items=((itemName=blockPoolId,itemType=javax.management.openmbean.SimpleType(name=java.lang.String)),(itemName=createdRollbackImages,itemType=javax.management.openmbean.SimpleType(name=java.lang.Boolean)),(itemNam</li></div><div><li>expected null, but was:&lt;javax.management.openmbean.CompositeDataSupport(compositeType=javax.management.openmbean.CompositeType(name=org.apache.hadoop.hdfs.protocol.RollingUpgradeInfo$Bean,items=((itemName=blockPoolId,itemType=javax.management.openmbean.SimpleType(name=java.lang.String)),(itemName=createdRollbackImages,itemType=javax.management.openmbean.SimpleType(name=java.lang.Boolean)),(itemNam</li></div><div><li>Test resulted in an unexpected exit</li></div><div><li>test timed out after 120000 milliseconds</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div><div><li>test timed out after 120000 milliseconds</li></div><div><li>File /test1 could only be written to 0 of the 1 minReplication nodes. There are 1 datanode(s) running and 1 node(s) are excluded in this operation.
 at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2117)
 at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:287)
 at org.apache.hadoop.hdfs.serve</li></div><div><li>Timed out waiting for condition. Thread diagnostics:
Timestamp: 2018-04-24 05:27:21,567

"qtp1303773067-3245" daemon prio=5 tid=3245 runnable
java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
        at </li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.hdfs.server.namenode.ha.TestDFSUpgradeWithHA.testRollbackWithJournalNodes(TestDFSUpgradeWithHA.java:687)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invok</li></div><div><li> Expected to find 'localhost:39096: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:39096: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li> Expected to find 'localhost:37488: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:37488: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li> Expected to find 'localhost:38061: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:38061: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li>expected timeout</li></div><div><li> Expected to find 'localhost:38754: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:38754: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li> Expected to find 'localhost:38936: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:38936: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li> Expected to find 'localhost:36345: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:36345: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li>expected timeout</li></div><div><li>java.net.BindException: Address already in use</li></div><div><li>Job didn't finish in 30 seconds</li></div><div><li>expected: java.util.HashSet&lt;[hb]&gt; but was: java.util.HashSet&lt;[hb]&gt;</li></div><div><li>Child process owned by init escaped process tree.</li></div><div><li>expected:&lt;2&gt; but was:&lt;0&gt;</li></div><div><li>expected:&lt;1024&gt; but was:&lt;7168&gt;</li></div><div><li>expected:&lt;0.5&gt; but was:&lt;0.0&gt;</li></div><div><li>expected:&lt;15360&gt; but was:&lt;20480&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>length != 10(unixSymbolicPermission=-rw-r--r--.)</li></div><div><li>java.lang.IllegalArgumentException: length != 10(unixSymbolicPermission=-rw-r--r--.)</li></div><div><li>String index out of range: -1</li></div><div><li>test timed out after 120000 milliseconds</li></div><div><li>Lease should be empty.</li></div><div><li>test timed out after 300000 milliseconds</li></div><div><li>Wrong number of PendingReplication blocks expected:&lt;2&gt; but was:&lt;0&gt;</li></div><div><li>Timed out waiting for condition. Thread diagnostics:
Timestamp: 2018-04-24 03:01:57,022

"IPC Server idle connection scanner for port 40305" daemon prio=5 tid=2756 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at java.util.TimerThread.mainLoop(Timer.java:552)
        at java.util.TimerThread.run(Timer.java:505)
"qtp881429460-2732" daemo</li></div><div><li>After waiting the operation updatePipeline still has not taken effect on NN yet</li></div><div><li>Job didn't finish in 30 seconds</li></div><div><li>expected: java.util.HashSet&lt;[hb]&gt; but was: java.util.HashSet&lt;[hb]&gt;</li></div><div><li>Child process owned by init escaped process tree.</li></div><div><li>/var/lib/jenkins/workspace/hadoop/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/test/utils/test_docker_util.cc:932
      Expected: 0
To be equal to: ret
      Which is: 14</li></div><div><li>Attempt state is not correct (timeout). expected:&lt;LAUNCHED&gt; but was:&lt;ALLOCATED&gt;</li></div><div><li>expected:&lt;2&gt; but was:&lt;3&gt;</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreTestBase.testRemoveAttempt(RMStateStoreTestBase.java:672)
	at org.apache.hadoop.yarn.server.resourcemanager.recovery.TestLeveldbRMStateStore.testRemoveAttempt(TestLev</li></div><div><li>expected null, but was:&lt;org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.UsersManager$User@588df31b&gt;</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertNotNull(Assert.java:621)
	at org.junit.Assert.assertNotNull(Assert.java:631)
	at org.apache.hadoop.yarn.server.resourcemanager.security.TestDelegationTokenRenewer.testRMRestartWithExpiredToken(TestDelegationTokenRenewer.java:1067)
	at sun.reflect.NativeMetho</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>String index out of range: -1</li></div><div><li>No namenode available under nameservice ns0</li></div><div><li>No namenode available under nameservice ns0</li></div><div><li>No namenode available under nameservice ns0</li></div><div><li>No namenode available under nameservice ns0</li></div><div><li>No namenode available under nameservice ns0</li></div><div><li>No namenode available under nameservice ns0</li></div><div><li>No namenode available under nameservice ns0</li></div><div><li>No namenode available under nameservice ns0</li></div><div><li>No namenode available under nameservice ns0</li></div><div><li>Failed: the number of failed blocks = 2 &gt; the number of parity blocks = 1</li></div><div><li>Unexpected # of nodes checked expected:&lt;1&gt; but was:&lt;2&gt;</li></div><div><li>Test resulted in an unexpected exit</li></div><div><li>write timedout too late in 1290 ms.</li></div><div><li>Timed out waiting for /test1 to reach 3 replicas</li></div><div><li>expected:&lt;1&gt; but was:&lt;0&gt;</li></div><div><li>expected: java.util.HashSet&lt;[hb]&gt; but was: java.util.HashSet&lt;[hb]&gt;</li></div><div><li>Child process owned by init escaped process tree.</li></div><div><li>/var/lib/jenkins/workspace/hadoop/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/test/utils/test_docker_util.cc:932
      Expected: 0
To be equal to: ret
      Which is: 14</li></div><div><li>expected:&lt;0.5&gt; but was:&lt;0.0&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>length != 10(unixSymbolicPermission=-rw-r--r--.)</li></div><div><li>java.lang.IllegalArgumentException: length != 10(unixSymbolicPermission=-rw-r--r--.)</li></div><div><li>Failed to append to non-existent file /test/test/target for client 127.0.0.1
 at org.apache.hadoop.hdfs.server.namenode.FSDirAppendOp.appendFile(FSDirAppendOp.java:104)
 at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.appendFile(FSNamesystem.java:2621)
 at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.append(NameNodeRpcServer.java:805)
 at org.apache.hadoop.hdfs.protocolPB.Client</li></div><div><li>expected:&lt;9216,1024,127.0.0.1[]&gt; but was:&lt;9216,1024,127.0.0.1[,127.0.0.1]&gt;</li></div><div><li>test timed out after 300000 milliseconds</li></div><div><li>write timedout too late in 1236 ms.</li></div><div><li>lease holder should now be the NN</li></div><div><li>expected:&lt;1&gt; but was:&lt;2&gt;</li></div><div><li>test timed out after 120000 milliseconds</li></div><div><li>Wrong number of PendingReplication blocks expected:&lt;2&gt; but was:&lt;1&gt;</li></div><div><li>Could not create staging directory. </li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:39576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:39576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.fs.slive.TestSlive.runOperationOk(TestSlive.java:354)
	at org.apache.hadoop.fs.slive.TestSlive.testCreateOp(TestSlive.java:290)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeM</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.fs.slive.TestSlive.runOperationOk(TestSlive.java:354)
	at org.apache.hadoop.fs.slive.TestSlive.testMkdir(TestSlive.java:503)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMeth</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.fs.slive.TestSlive.runOperationOk(TestSlive.java:354)
	at org.apache.hadoop.fs.slive.TestSlive.testList(TestSlive.java:460)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMetho</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.fs.slive.TestSlive.runOperationOk(TestSlive.java:354)
	at org.apache.hadoop.fs.slive.TestSlive.testRead(TestSlive.java:434)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMetho</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.fs.slive.TestSlive.runOperationOk(TestSlive.java:354)
	at org.apache.hadoop.fs.slive.TestSlive.testDelete(TestSlive.java:381)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMet</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.fs.slive.TestSlive.testMRFlow(TestSlive.java:416)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.Del</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.fs.slive.TestSlive.runOperationOk(TestSlive.java:354)
	at org.apache.hadoop.fs.slive.TestSlive.testRename(TestSlive.java:402)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMet</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.fs.slive.TestSlive.runOperationOk(TestSlive.java:354)
	at org.apache.hadoop.fs.slive.TestSlive.testAppendOp(TestSlive.java:532)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeM</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.fs.slive.TestSlive.runOperationOk(TestSlive.java:354)
	at org.apache.hadoop.fs.slive.TestSlive.testTruncateOp(TestSlive.java:554)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.Nativ</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:39576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:39576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:39576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:39576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:39576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Could not create staging directory. </li></div><div><li>Could not create staging directory. </li></div><div><li>Could not create staging directory. </li></div><div><li>Could not create staging directory. </li></div><div><li>Could not create staging directory. </li></div><div><li>Could not create staging directory. </li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Wrong FS: file:/var/lib/jenkins/workspace/hadooptmp/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/target/test-dir/input, expected: hdfs://localhost:34576</li></div><div><li>Wrong FS: file:/var/lib/jenkins/workspace/hadooptmp/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/target/test-dir/input, expected: hdfs://localhost:34576</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Could not create staging directory. </li></div><div><li>Could not create staging directory. </li></div><div><li>Could not create staging directory. </li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Could not create staging directory. </li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Job failed</li></div><div><li>Job failed</li></div><div><li>Job failed</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Job failed</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Could not create staging directory. </li></div><div><li>Could not create staging directory. </li></div><div><li>Could not create staging directory. </li></div><div><li>Could not create staging directory. </li></div><div><li>Could not create staging directory. </li></div><div><li>Could not create staging directory. </li></div><div><li>Could not create staging directory. </li></div><div><li>Could not create staging directory. </li></div><div><li>Could not create staging directory. </li></div><div><li>Could not create staging directory. </li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Threw exception:java.net.ConnectException: Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Threw exception:java.net.ConnectException: Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Could not create staging directory. </li></div><div><li>Could not create staging directory. </li></div><div><li>Could not create staging directory. </li></div><div><li>Could not create staging directory. </li></div><div><li>Could not create staging directory. </li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Could not create staging directory. </li></div><div><li>Could not create staging directory. </li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Could not create staging directory. </li></div><div><li>Could not create staging directory. </li></div><div><li>Could not create staging directory. </li></div><div><li>Could not create staging directory. </li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Block compression should be allowed for SequenceFileAsBinaryOutputFormat:Caught java.net.ConnectException</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Could not create staging directory. </li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Wrong FS: file:/var/lib/jenkins/workspace/hadooptmp/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/target/test-dir/org.apache.hadoop.mapred.TestUserDefinedCounters/input, expected: hdfs://localhost:34576</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.mapred.TestYARNRunner.testResourceRequestLocalityInvalid(TestYARNRunner.java:765)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorI</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>
mRClientProtocol.getDelegationToken(&lt;any&gt;);
Never wanted here:
-&gt; at org.apache.hadoop.mapred.TestYARNRunner.testGetHSDelegationToken(TestYARNRunner.java:450)
But invoked here:
-&gt; at org.apache.hadoop.mapred.YARNRunner.getDelegationTokenFromHS(YARNRunner.java:250)
</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Could not create staging directory. </li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:39576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Could not create staging directory. </li></div><div><li>Could not create staging directory. </li></div><div><li>Could not create staging directory. </li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:39576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:39576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:39576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:39576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:39576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:39576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:39576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:39576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:39576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:39576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:39576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:39576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:39576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Could not create staging directory. </li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:39576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:39576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:39576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:39576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.mapreduce.TestValueIterReset.testValueIterReset(TestValueIterReset.java:549)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.j</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Could not create staging directory. </li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.mapreduce.lib.output.TestJobOutputCommitter.tearDown(TestJobOutputCommitter.java:71)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Meth</li></div><div><li>Could not create staging directory. </li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.mapreduce.lib.output.TestJobOutputCommitter.tearDown(TestJobOutputCommitter.java:71)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Meth</li></div><div><li>Could not create staging directory. </li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.mapreduce.lib.output.TestJobOutputCommitter.tearDown(TestJobOutputCommitter.java:71)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Meth</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Block compression should be allowed for SequenceFileAsBinaryOutputFormat:Caught java.net.ConnectException</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Could not create staging directory. </li></div><div><li>Could not create staging directory. </li></div><div><li>Could not create staging directory. </li></div><div><li>Could not create staging directory. </li></div><div><li>Could not create staging directory. </li></div><div><li>Could not create staging directory. </li></div><div><li>Could not create staging directory. </li></div><div><li>Could not create staging directory. </li></div><div><li>Could not create staging directory. </li></div><div><li>Could not create staging directory. </li></div><div><li>Could not create staging directory. </li></div><div><li>Could not create staging directory. </li></div><div><li>Could not create staging directory. </li></div><div><li>Could not create staging directory. </li></div><div><li>Error creating staging dir</li></div><div><li>Error creating staging dir</li></div><div><li>Could not create staging directory. </li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>initTable on TestDynamoDBMetadataStore: com.amazonaws.services.dynamodbv2.model.AmazonDynamoDBException: The request processing has failed because of an unknown error, exception or failure. (Service: AmazonDynamoDBv2; Status Code: 500; Error Code: InternalFailure; Request ID: d30b70b9-27d0-46b2-a6bf-b21a1740161c): The request processing has failed because of an unknown error, exception or failure.</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>
Expected: is &lt;0&gt;
     but: was &lt;-999&gt;</li></div><div><li>File does not exist: /testdir.testPreserveUseNonEmptyDir/dstdat/srcdat</li></div><div><li>File does not exist: /testdir.testPreserveUserEmptyDir/dstdat/srcdat</li></div><div><li>File does not exist: /testdir.testPreserveUserNonEmptyDirWithUpdate/dstdat/a</li></div><div><li>File does not exist: /testdir/dstdat/srcdat/file0</li></div><div><li>File does not exist: /testdir.testPreserveUserSingleFile/dstdat/srcdat</li></div><div><li>File does not exist: /testdir/dstdat/srcdat/file0</li></div><div><li>File does not exist: /testdir/dstdat/srcdat/file</li></div><div><li>java.net.ConnectException: Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>java.net.ConnectException: Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>java.net.ConnectException: Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>java.net.ConnectException: Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>java.net.ConnectException: Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>java.net.ConnectException: Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>java.net.ConnectException: Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>java.net.ConnectException: Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>java.net.ConnectException: Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>java.net.ConnectException: Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>java.net.ConnectException: Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>java.net.ConnectException: Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>java.net.ConnectException: Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>java.net.ConnectException: Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>java.net.ConnectException: Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>java.net.ConnectException: Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>expected:&lt;0&gt; but was:&lt;-999&gt;</li></div><div><li>expected:&lt;0&gt; but was:&lt;-999&gt;</li></div><div><li>expected:&lt;0&gt; but was:&lt;-999&gt;</li></div><div><li>expected:&lt;0&gt; but was:&lt;-999&gt;</li></div><div><li>expected:&lt;0&gt; but was:&lt;-999&gt;</li></div><div><li>expected:&lt;0&gt; but was:&lt;-999&gt;</li></div><div><li>expected:&lt;-999&gt; but was:&lt;0&gt;</li></div><div><li>distcp failure</li></div><div><li>distcp failure</li></div><div><li>distcp failure</li></div><div><li>distcp failure</li></div><div><li>distcp failure</li></div><div><li>distcp failure</li></div><div><li>distcp failure</li></div><div><li>distcp failure</li></div><div><li>distcp failure</li></div><div><li>distcp failure</li></div><div><li>distcp failure</li></div><div><li>distcp failure</li></div><div><li>distcp failure</li></div><div><li>distcp failure</li></div><div><li>distcp failure</li></div><div><li>distcp failure</li></div><div><li>distcp failure</li></div><div><li>distcp failure</li></div><div><li>distcp failure</li></div><div><li>distcp failure</li></div><div><li>distcp failure</li></div><div><li>distcp failure</li></div><div><li>distcp failure</li></div><div><li>distcp failure</li></div><div><li>distcp failure</li></div><div><li>distcp failure</li></div><div><li>distcp failure</li></div><div><li>distcp failure</li></div><div><li>distcp failure</li></div><div><li>distcp failure</li></div><div><li>distcp failure</li></div><div><li>distcp failure</li></div><div><li>distcp failure</li></div><div><li>distcp failure</li></div><div><li>distcp failure</li></div><div><li>distcp failure</li></div><div><li>distcp failure</li></div><div><li>distcp failure</li></div><div><li>distcp failure</li></div><div><li>distcp failure</li></div><div><li>distcp failure</li></div><div><li>distcp failure</li></div><div><li>distcp failure</li></div><div><li>distcp failure</li></div><div><li>distcp failure</li></div><div><li>distcp failure</li></div><div><li>distcp failure</li></div><div><li>distcp failure</li></div><div><li>distcp failure</li></div><div><li>distcp failure</li></div><div><li>distcp failure</li></div><div><li>distcp failure</li></div><div><li>distcp failure</li></div><div><li>distcp failure</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>/tmp/dstReplDir/destReplFile replication factor should not be same as /tmp/srcECDir/srcECFile</li></div><div><li>Could not create staging directory. </li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Could not create staging directory. </li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Could not create staging directory. </li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Could not create staging directory. </li></div><div><li>Could not create staging directory. </li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.yarn.sls.scheduler.TestTaskRunner.testPreStartQueueing(TestTaskRunner.java:244)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImp</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Could not create staging directory. </li></div><div><li>expected:&lt;0&gt; but was:&lt;5&gt;</li></div><div><li>Could not create staging directory. </li></div><div><li>Could not create staging directory. </li></div><div><li>/var/lib/jenkins/workspace/hadooptmp/hadoop-tools/hadoop-streaming/target/out/part-00000 (No such file or directory)</li></div><div><li>/var/lib/jenkins/workspace/hadooptmp/hadoop-tools/hadoop-streaming/stream_aggregate_out/part-00000 (No such file or directory)</li></div><div><li>/var/lib/jenkins/workspace/hadooptmp/hadoop-tools/hadoop-streaming/out_for_data_protocol_test/part-00000 (No such file or directory)</li></div><div><li>/var/lib/jenkins/workspace/hadooptmp/hadoop-tools/hadoop-streaming/stream_reduce_none_out/part-00000 (No such file or directory)</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>expected:&lt;0&gt; but was:&lt;5&gt;</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Streaming Job expected to succeed expected:&lt;0&gt; but was:&lt;5&gt;</li></div><div><li>Could not create staging directory. </li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Streaming Job failure code expected expected:&lt;1&gt; but was:&lt;5&gt;</li></div><div><li>Streaming Job expected to succeed expected:&lt;0&gt; but was:&lt;5&gt;</li></div><div><li>Streaming Job failure code expected expected:&lt;1&gt; but was:&lt;5&gt;</li></div><div><li>Streaming Job expected to succeed expected:&lt;0&gt; but was:&lt;5&gt;</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>/var/lib/jenkins/workspace/hadooptmp/hadoop-tools/hadoop-streaming/target/stream_out/part-00000 (No such file or directory)</li></div><div><li>/var/lib/jenkins/workspace/hadooptmp/hadoop-tools/hadoop-streaming/target/stream_out/part-00000 (No such file or directory)</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>/var/lib/jenkins/workspace/hadooptmp/hadoop-tools/hadoop-streaming/TestStreamingSeparator.out/part-00000 (No such file or directory)</li></div><div><li>StreamJob success expected:&lt;0&gt; but was:&lt;5&gt;</li></div><div><li>StreamJob success expected:&lt;0&gt; but was:&lt;5&gt;</li></div><div><li>StreamJob success expected:&lt;0&gt; but was:&lt;5&gt;</li></div><div><li>Could not create staging directory. </li></div><div><li>/var/lib/jenkins/workspace/hadooptmp/hadoop-tools/hadoop-streaming/target/out/part-00000 (No such file or directory)</li></div><div><li>Job failed expected:&lt;0&gt; but was:&lt;5&gt;</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>Call From pts00449-vm36.persistent.co.in/10.88.67.159 to localhost:34576 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</li></div><div><li>test timed out after 90000 milliseconds</li></div><div><li>org.apache.hadoop.yarn.service.client.SystemServiceManagerImpl.getSkipCounter()I</li></div><div><li>Expected success for Probe Status, time="Tue Apr 24 17:37:52 UTC 2018", outcome="failure", message="Failure in Default probe: IP presence with DNS checking and DNS server address 8.8.8.8", exception="java.io.IOException: comp-0: DNS checking is enabled, but lookup for example.com is not available yet"</li></div><div><li>Not all node managers were reported running expected:&lt;3&gt; but was:&lt;0&gt;</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invo</li></div><div><li>Not all node managers were reported running expected:&lt;3&gt; but was:&lt;0&gt;</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invo</li></div><div><li>Not all node managers were reported running expected:&lt;3&gt; but was:&lt;0&gt;</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invo</li></div><div><li>Not all node managers were reported running expected:&lt;3&gt; but was:&lt;0&gt;</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invo</li></div><div><li>Not all node managers were reported running expected:&lt;3&gt; but was:&lt;0&gt;</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invo</li></div><div><li>Not all node managers were reported running expected:&lt;3&gt; but was:&lt;0&gt;</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invo</li></div><div><li>Not all node managers were reported running expected:&lt;3&gt; but was:&lt;0&gt;</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invo</li></div><div><li>Not all node managers were reported running expected:&lt;3&gt; but was:&lt;0&gt;</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invo</li></div><div><li>Not all node managers were reported running expected:&lt;3&gt; but was:&lt;0&gt;</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invo</li></div><div><li>Not all node managers were reported running expected:&lt;3&gt; but was:&lt;0&gt;</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invo</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invo</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invo</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invo</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invo</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invo</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invo</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor40.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor40.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor40.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor40.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor40.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor40.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor40.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor40.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor40.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor40.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor40.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor40.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor40.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor40.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor40.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor40.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor40.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor40.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor40.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor40.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor40.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor40.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor40.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor40.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor40.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor40.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor40.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor40.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor40.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor40.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor40.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor40.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor40.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor40.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor40.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor40.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor40.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor40.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor40.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor40.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>unable to create new native thread</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.GeneratedMethodAccessor40.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCa</li></div><div><li>Not all node managers were reported running expected:&lt;3&gt; but was:&lt;0&gt;</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.api.impl.BaseAMRMClientTest.teardown(BaseAMRMClientTest.java:199)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invo</li></div><div><li>test timed out after 120000 milliseconds</li></div><div><li>java.net.BindException: Problem binding to [0.0.0.0:8049] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException</li></div><div><li>java.net.BindException: Problem binding to [0.0.0.0:8049] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException</li></div><div><li>Index: 0, Size: 0</li></div><div><li>Index: 0, Size: 0</li></div><div><li>Index: 0, Size: 0</li></div><div><li>Timed out waiting for condition. Thread diagnostics:
Timestamp: 2018-04-24 03:14:46,292

"IPC Server handler 38 on 33556" daemon prio=5 tid=113 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionO</li></div><div><li>Timed out waiting for condition. Thread diagnostics:
Timestamp: 2018-04-24 03:14:57,757

"IPC Server handler 6 on 39009" daemon prio=5 tid=137 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionOb</li></div><div><li>Timed out waiting for condition. Thread diagnostics:
Timestamp: 2018-04-24 03:15:09,079

"IPC Server handler 6 on 39009" daemon prio=5 tid=137 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionOb</li></div><div><li>Timed out waiting for condition. Thread diagnostics:
Timestamp: 2018-04-24 03:15:20,395

"IPC Server handler 6 on 39009" daemon prio=5 tid=137 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionOb</li></div><div><li>Timed out waiting for condition. Thread diagnostics:
Timestamp: 2018-04-24 03:15:31,714

"IPC Server handler 2 on 37422" daemon prio=5 tid=1425 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionO</li></div><div><li>Timed out waiting for condition. Thread diagnostics:
Timestamp: 2018-04-24 03:15:42,988

"IPC Server handler 2 on 37422" daemon prio=5 tid=1425 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionO</li></div><div><li>Timed out waiting for condition. Thread diagnostics:
Timestamp: 2018-04-24 03:15:54,296

"IPC Server handler 2 on 37422" daemon prio=5 tid=1425 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionO</li></div><div><li>Timed out waiting for condition. Thread diagnostics:
Timestamp: 2018-04-24 03:16:05,755

"IPC Server handler 2 on 37422" daemon prio=5 tid=1425 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionO</li></div><div><li>Timed out waiting for condition. Thread diagnostics:
Timestamp: 2018-04-24 03:16:17,103

"IPC Server handler 37 on 36917" daemon prio=5 tid=2755 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$Condition</li></div><div><li>Timed out waiting for condition. Thread diagnostics:
Timestamp: 2018-04-24 03:16:28,405

"IPC Server handler 37 on 36917" daemon prio=5 tid=2755 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$Condition</li></div><div><li>Timed out waiting for condition. Thread diagnostics:
Timestamp: 2018-04-24 03:16:40,541

"IPC Server handler 37 on 36917" daemon prio=5 tid=2755 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$Condition</li></div><div><li>Timed out waiting for condition. Thread diagnostics:
Timestamp: 2018-04-24 03:16:52,156

"IPC Server handler 37 on 36917" daemon prio=5 tid=2755 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$Condition</li></div><div><li>unable to create new native thread</li></div><div><li>unable to create new native thread</li></div><div><li>expected: java.util.HashSet&lt;[hb]&gt; but was: java.util.HashSet&lt;[hb]&gt;</li></div><div><li>No space available in any of the local directories.</li></div><div><li>Unexpected exception org.apache.hadoop.util.DiskChecker$DiskErrorException: No space available in any of the local directories.</li></div><div><li>expected:&lt;1&gt; but was:&lt;0&gt;</li></div><div><li>Node health status reported unhealthy</li></div><div><li>The container should create a subDir named currentUser: jenkinsunder localDir/usercache</li></div><div><li>ContainerState is not correct (timedout)</li></div><div><li>Metrics source NodeManagerMetrics already exists!</li></div><div><li>/var/lib/jenkins/workspace/hadooptmp/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/target/org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerShutdown/tmpDir/start_file.txt (No such file or directory)</li></div><div><li>expected:&lt;50&gt; but was:&lt;-1000&gt;</li></div><div><li>ProcessStartFile doesn't exist!</li></div><div><li>ProcessStartFile doesn't exist!</li></div><div><li>ContainerState is not correct (timedout)</li></div><div><li>ProcessStartFile doesn't exist!</li></div><div><li>ProcessStartFile doesn't exist!</li></div><div><li>/var/lib/jenkins/workspace/hadooptmp/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/target/TestContainerManager-localDir/nmPrivate doesn't exist!!</li></div><div><li>ProcessStartFile doesn't exist!</li></div><div><li>ContainerState is not correct (timedout)</li></div><div><li>ProcessStartFile doesn't exist!</li></div><div><li>ProcessStartFile doesn't exist!</li></div><div><li>AppDir /var/lib/jenkins/workspace/hadooptmp/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/target/TestContainerManager-localDir/usercache/nobody/appcache/application_0_0000 doesn't exist!!</li></div><div><li>ProcessStartFile doesn't exist!</li></div><div><li>ProcessStartFile doesn't exist!</li></div><div><li>expected:&lt;0&gt; but was:&lt;-1000&gt;</li></div><div><li>ProcessStartFile doesn't exist!</li></div><div><li>ProcessStartFile doesn't exist!</li></div><div><li>No space available in any of the local directories.</li></div><div><li>Timed out waiting for condition. Thread diagnostics:
Timestamp: 2018-04-24 01:47:31,164

"Container Monitor"  prio=5 tid=129 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl$MonitoringThread.run(ContainersMonitorImpl.java:517)
"IPC Server handler 5</li></div><div><li>
Wanted but not invoked:
nodeStatusUpdater.reportException(&lt;any&gt;);
-&gt; at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.TestContainerLaunch.testContainerLaunchOnConfigurationError(TestContainerLaunch.java:1709)
Actually, there were zero interactions with this mock.
</li></div><div><li>ProcessStartFile doesn't exist!</li></div><div><li>No space available in any of the local directories.</li></div><div><li>No space available in any of the local directories.</li></div><div><li>ProcessStartFile doesn't exist!</li></div><div><li>No space available in any of the local directories.</li></div><div><li>No space available in any of the local directories.</li></div><div><li>
Wanted but not invoked:
localFs.mkdir(
    /var/lib/jenkins/workspace/hadooptmp/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/target/org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestResourceLocalizationService/0/usercache,
    rwxr-xr-x,
    true
);
-&gt; at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestResourceLo</li></div><div><li>test timed out after 20000 milliseconds</li></div><div><li>No space available in any of the local directories.</li></div><div><li>
Wanted but not invoked:
containerExecutor.startLocalizer(
    &lt;Capturing argument&gt;
);
-&gt; at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestResourceLocalizationService.testLocalizationHeartbeat(TestResourceLocalizationService.java:956)
Actually, there were zero interactions with this mock.
</li></div><div><li>
Wanted but not invoked:
localFs.mkdir(
    /var/lib/jenkins/workspace/hadooptmp/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/target/org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestResourceLocalizationService/0/usercache,
    rwxr-xr-x,
    true
);
-&gt; at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestResourceLo</li></div><div><li>test timed out after 20000 milliseconds</li></div><div><li>test timed out after 10000 milliseconds</li></div><div><li>Index: 0, Size: 0</li></div><div><li>
Wanted but not invoked:
localFs.mkdir(
    /var/lib/jenkins/workspace/hadooptmp/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/target/org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestResourceLocalizationService/0/filecache,
    rwxr-xr-x,
    true
);
-&gt; at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestResourceLo</li></div><div><li>test timed out after 20000 milliseconds</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestResourceLocalizationService.testParallelDownloadAttemptsForPublicResource(TestResourceLocalizationService.java:2360)
	at sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>
deletionService.delete(
    &lt;File deletion matcher&gt;
);
Wanted 2 times:
-&gt; at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestLogAggregationService.verifyLocalFileDeletion(TestLogAggregationService.java:228)
But was 1 time:
-&gt; at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl.doAppLogAggregationPostCleanUp(AppLogAggregat</li></div><div><li>ProcessStartFile doesn't exist!</li></div><div><li>ContainerState is not correct (timedout)</li></div><div><li>expected:&lt;1&gt; but was:&lt;0&gt;</li></div><div><li>ContainerState is not correct (timedout)</li></div><div><li>expected:&lt;RUNNING&gt; but was:&lt;DONE&gt;</li></div><div><li>expected:&lt;6&gt; but was:&lt;0&gt;</li></div><div><li>expected:&lt;2&gt; but was:&lt;0&gt;</li></div><div><li>ContainerState is not correct (timedout)</li></div><div><li>expected:&lt;RUNNING&gt; but was:&lt;DONE&gt;</li></div><div><li>ContainerState is not correct (timedout)</li></div><div><li>expected:&lt;2&gt; but was:&lt;0&gt;</li></div><div><li>ContainerState is not correct (timedout)</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.TestContainerSchedulerQueuing.testKillOpportunisticForGuaranteedContainer(TestContainerSchedulerQueuing.java:544)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(N</li></div><div><li>No space available in any of the local directories.</li></div><div><li>No space available in any of the local directories.</li></div><div><li>expected:&lt;&lt;memory:[4096, vCores:4]&gt;&gt; but was:&lt;&lt;memory:[2048, vCores:2]&gt;&gt;</li></div><div><li>expected:&lt;1024&gt; but was:&lt;7168&gt;</li></div><div><li>expected:&lt;1024&gt; but was:&lt;2048&gt;</li></div><div><li>expected:&lt;3072&gt; but was:&lt;4096&gt;</li></div><div><li>expected:&lt;2048&gt; but was:&lt;4096&gt;</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertNotNull(Assert.java:621)
	at org.junit.Assert.assertNotNull(Assert.java:631)
	at org.apache.hadoop.yarn.server.resourcemanager.security.TestDelegationTokenRenewer.testRMRestartWithExpiredToken(TestDelegationTokenRenewer.java:1067)
	at sun.reflect.NativeMetho</li></div><div><li>Number of nm-local-dirs is wrong. expected:&lt;4&gt; but was:&lt;0&gt;</li></div><div><li>expected:&lt;1&gt; but was:&lt;0&gt;</li></div><div><li>Number of nm-log-dirs is wrong. expected:&lt;4&gt; but was:&lt;0&gt;</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting.testVolFailureStatsPreservedOnNNRestart</div></li><li><div>org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureToleration.testConfigureMinValidVolumes</div></li><li><div>org.apache.hadoop.hdfs.server.namenode.ha.TestFailureToReadEdits.testCheckpointStartingMidEditsFile[0]</div></li><li><div>org.apache.hadoop.yarn.service.monitor.probe.TestDefaultProbe.testDefaultProbe[2]</div></li><li><div>org.apache.hadoop.yarn.client.api.impl.TestAMRMProxy.testAMRMProxyTokenRenewal</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestIncreaseAllocationExpirer.testContainerIncreaseAllocationExpiration</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureWithRandomECPolicy.testCloseWithExceptionsInStreamer</div></li><li><div>org.apache.hadoop.hdfs.TestMissingBlocksAlert.testMissingBlocksAlert</div></li><li><div>org.apache.hadoop.hdfs.TestRollingUpgrade.testCheckpointWithMultipleNN</div></li><li><div>org.apache.hadoop.hdfs.TestRollingUpgrade.testRollBackImage</div></li><li><div>org.apache.hadoop.hdfs.TestRollingUpgrade.testDFSAdminRollingUpgradeCommands</div></li><li><div>org.apache.hadoop.hdfs.TestRollingUpgrade.testRollback</div></li><li><div>org.apache.hadoop.hdfs.qjournal.server.TestJournalNodeSync.testSyncForDiscontinuousMissingLogs</div></li><li><div>org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting.testSuccessiveVolumeFailures</div></li><li><div>org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting.testHotSwapOutFailedVolumeAndReporting</div></li><li><div>org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting.testDataNodeReconfigureWithVolumeFailures</div></li><li><div>org.apache.hadoop.hdfs.server.namenode.TestReencryptionWithKMS.testReencryptNestedZones</div></li><li><div>org.apache.hadoop.hdfs.server.namenode.ha.TestDFSUpgradeWithHA.testRollbackWithJournalNodes</div></li><li><div>org.apache.hadoop.mapreduce.v2.hs.webapp.TestHsWebServicesJobsQuery.testJobsQueryFinishTimeBegin</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.TestContainerSchedulerQueuing.testKillOnlyRequiredOpportunisticContainers</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestContainerResizing.testIncreaseContainerUnreservedWhenContainerCompleted</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.TestPlacementProcessor.testRePlacementAfterSchedulerRejection</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.io.nativeio.TestNativeIO.testStat</div></li><li><div>org.apache.hadoop.io.nativeio.TestNativeIO.testMultiThreadedStat</div></li><li><div>org.apache.hadoop.hdfs.TestDFSClientRetries.testLeaseRenewAndDFSOutputStreamDeadLock</div></li><li><div>org.apache.hadoop.hdfs.TestDFSClientRetries.testLeaseRenewSocketTimeout</div></li><li><div>org.apache.hadoop.hdfs.TestReadStripedFileWithMissingBlocks.testReadFileWithMissingBlocks</div></li><li><div>org.apache.hadoop.hdfs.server.datanode.TestNNHandlesCombinedBlockReport.blockReport_08</div></li><li><div>org.apache.hadoop.hdfs.server.namenode.TestReencryption.testReencryptNestedZones</div></li><li><div>org.apache.hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA.testUpdatePipeline</div></li><li><div>org.apache.hadoop.mapred.TestMRTimelineEventHandling.testMapreduceJobTimelineServiceEnabled</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.TestApplicationMasterLauncher.testRetriesOnFailures</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.TestRMRestart.testRMRestartRecoveringNodeLabelManager[CAPACITY]</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.recovery.TestLeveldbRMStateStore.testRemoveAttempt</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestContainerResizing.testIncreaseContainerUnreservedWhenApplicationCompleted</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.security.TestDelegationTokenRenewer.testRMRestartWithExpiredToken</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.fs.contract.router.web.TestRouterWebHDFSContractAppend.testAppendNonexistentFile</div></li><li><div>org.apache.hadoop.fs.contract.router.web.TestRouterWebHDFSContractAppend.testBuilderAppendToExistingFile</div></li><li><div>org.apache.hadoop.fs.contract.router.web.TestRouterWebHDFSContractRootDirectory.testRecursiveRootListing</div></li><li><div>org.apache.hadoop.fs.contract.router.web.TestRouterWebHDFSContractRootDirectory.testRmEmptyRootDirNonRecursive</div></li><li><div>org.apache.hadoop.fs.contract.router.web.TestRouterWebHDFSContractRootDirectory.testRmRootRecursive</div></li><li><div>org.apache.hadoop.fs.contract.router.web.TestRouterWebHDFSContractRootDirectory.testListEmptyRootDirectory</div></li><li><div>org.apache.hadoop.fs.contract.router.web.TestRouterWebHDFSContractRootDirectory.testSimpleRootListing</div></li><li><div>org.apache.hadoop.fs.contract.router.web.TestRouterWebHDFSContractRootDirectory.testMkDirDepth1</div></li><li><div>org.apache.hadoop.fs.contract.router.web.TestRouterWebHDFSContractRootDirectory.testRmNonEmptyRootDirNonRecursive</div></li><li><div>org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureWithRandomECPolicy.testCloseWithExceptionsInStreamer</div></li><li><div>org.apache.hadoop.hdfs.TestDecommission.testBlocksPerInterval</div></li><li><div>org.apache.hadoop.hdfs.TestDecommission.testBlocksPerInterval</div></li><li><div>org.apache.hadoop.hdfs.TestDistributedFileSystem.testDFSClientPeerWriteTimeout</div></li><li><div>org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting.testMultipleVolFailuresOnNode</div></li><li><div>org.apache.hadoop.yarn.client.api.impl.TestAMRMClient.testAMRMClientWithContainerResourceChange[1]</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestQueueManagementDynamicEditPolicy.testEditSchedule</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="hbase"><div style="font-weight:bold;" class="panel-heading">HBASE<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Valencia)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>d4768114e860eb03418e6b08e5de20950824327e</div><div><b>Last Run: </b>26-04-2018 14:31 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th><th>PPC RHEL7.5</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 2614</div><div>Failed Count : 1</div><div>Skipped Count : 19</div></td><td><div>Total Count : 4580</div><div>Failed Count : 1</div><div>Skipped Count : 39</div></td><td><div>Total Count : 4568</div><div>Failed Count : 1</div><div>Skipped Count : 39</div></td><td><div>Total Count : 4580</div><div>Failed Count : 0</div><div>Skipped Count : 39</div></td><td><div>Total Count : 4568</div><div>Failed Count : 1</div><div>Skipped Count : 39</div></td></tr><tr><td>Result</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.hbase.master.TestCatalogJanitor.testParentCleanedEvenIfDaughterGoneFirst</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.hbase.master.cleaner.TestHFileLinkCleaner.org.apache.hadoop.hbase.master.cleaner.TestHFileLinkCleaner</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.hbase.TestClientOperationTimeout.testScanTimeout</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.hbase.master.TestAssignmentManagerMetrics.testRITAssignmentManagerMetrics</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"><div><li>java.lang.AssertionError
	at org.apache.hadoop.hbase.master.TestCatalogJanitor.parentWithSpecifiedEndKeyCleanedEvenIfDaughterGoneFirst(TestCatalogJanitor.java:267)
	at org.apache.hadoop.hbase.master.TestCatalogJanitor.testParentCleanedEvenIfDaughterGoneFirst(TestCatalogJanitor.java:156)
</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>test timed out after 780 seconds</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>callTimeout=500, callDuration=611: Call to 002dc9e79e56/172.17.0.5:34791 failed on local exception: org.apache.hadoop.hbase.ipc.CallTimeoutException: Call id=36, waitTime=506, rpcTimeout=498 </li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>Metrics Should be equal expected:&lt;1&gt; but was:&lt;2&gt;</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.hbase.master.TestCatalogJanitor.testParentCleanedEvenIfDaughterGoneFirst</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.hbase.master.cleaner.TestHFileLinkCleaner.org.apache.hadoop.hbase.master.cleaner.TestHFileLinkCleaner</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.hbase.TestClientOperationTimeout.testScanTimeout</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="hive"><div style="font-weight:bold;" class="panel-heading">HIVE<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Alisha/Pravin)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>6f9090c1de9bd6d2d98312a74b73802044cfa23e</div><div><b>Last Run: </b>13-04-2018 15:05 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th><th>PPC RHEL7.5</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 6912</div><div>Failed Count : 13</div><div>Skipped Count : 224</div></td><td><div>Total Count : 6912</div><div>Failed Count : 12</div><div>Skipped Count : 224</div></td><td><div>Total Count : 6912</div><div>Failed Count : 3</div><div>Skipped Count : 224</div></td><td><div>Total Count : 6912</div><div>Failed Count : 2</div><div>Skipped Count : 224</div></td><td><div>Total Count : 7022</div><div>Failed Count : 3</div><div>Skipped Count : 227</div></td></tr><tr><td>Result</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hive.hcatalog.mapreduce.TestHCatMultiOutputFormat.testOutputFormat</li></div><div><li>org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskCommunicator.testFinishableStateUpdateFailure</li></div><div><li>org.apache.hadoop.hive.ql.TestTxnCommandsForMmTable.testInsertOverwriteForPartitionedMmTable</li></div><div><li>org.apache.hadoop.hive.ql.TestTxnCommandsForOrcMmTable.testInsertOverwriteForPartitionedMmTable</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapRedPlan1</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapRedPlan2</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapRedPlan3</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapRedPlan4</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapRedPlan5</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapRedPlan6</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapPlan1</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapPlan2</li></div><div><li>org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.testStringLikeMultiByte</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hive.hcatalog.mapreduce.TestHCatMultiOutputFormat.testOutputFormat</li></div><div><li>org.apache.hadoop.hive.llap.daemon.impl.comparator.TestAMReporter.testMultipleAM</li></div><div><li>org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskCommunicator.testFinishableStateUpdateFailure</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapRedPlan1</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapRedPlan2</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapRedPlan3</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapRedPlan4</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapRedPlan5</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapRedPlan6</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapPlan1</li></div><div><li>org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapPlan2</li></div><div><li>org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.testStringLikeMultiByte</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskCommunicator.testFinishableStateUpdateFailure</li></div><div><li>org.apache.hadoop.hive.ql.TestTxnCommandsForMmTable.testInsertOverwriteForPartitionedMmTable</li></div><div><li>org.apache.hadoop.hive.ql.TestTxnCommandsForOrcMmTable.testInsertOverwriteForPartitionedMmTable</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskCommunicator.testFinishableStateUpdateFailure</li></div><div><li>org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.testStringLikeMultiByte</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.hive.ql.TestTxnCommandsForMmTable.testInsertOverwriteForPartitionedMmTable</li></div><div><li>org.apache.hadoop.hive.ql.TestTxnCommandsForOrcMmTable.testInsertOverwriteForPartitionedMmTable</li></div><div><li>org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager2.testWriteSetTracking3</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hive.hcatalog.mapreduce.TestHCatMultiOutputFormat.testOutputFormat(TestHCatMultiOutputFormat.java:301)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeM</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos$SignableVertexSpec$Builder.setUser(LlapDaemonProtocolProtos.java:4899)
	at org.apache.hadoop.hive.llap.tez.Converters.constructSignableVertexSpec(Converters.java:135)
	at org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.constructSubmitWorkRequest(LlapTaskCommunicator.java:806)
	at org.apac</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.hive.ql.TestTxnCommandsForMmTable.testInsertOverwriteForPartitionedMmTable(TestTxnCommandsForMmTable.java:359)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.hive.ql.TestTxnCommandsForMmTable.testInsertOverwriteForPartitionedMmTable(TestTxnCommandsForMmTable.java:359)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;1&gt; but was:&lt;0&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hive.hcatalog.mapreduce.TestHCatMultiOutputFormat.testOutputFormat(TestHCatMultiOutputFormat.java:301)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeM</li></div><div><li>test timed out after 5000 milliseconds</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos$SignableVertexSpec$Builder.setUser(LlapDaemonProtocolProtos.java:4899)
	at org.apache.hadoop.hive.llap.tez.Converters.constructSignableVertexSpec(Converters.java:135)
	at org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.constructSubmitWorkRequest(LlapTaskCommunicator.java:806)
	at org.apac</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;1&gt; but was:&lt;0&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>java.lang.NullPointerException
	at org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos$SignableVertexSpec$Builder.setUser(LlapDaemonProtocolProtos.java:4899)
	at org.apache.hadoop.hive.llap.tez.Converters.constructSignableVertexSpec(Converters.java:135)
	at org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.constructSubmitWorkRequest(LlapTaskCommunicator.java:806)
	at org.apac</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.hive.ql.TestTxnCommandsForMmTable.testInsertOverwriteForPartitionedMmTable(TestTxnCommandsForMmTable.java:359)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.hive.ql.TestTxnCommandsForMmTable.testInsertOverwriteForPartitionedMmTable(TestTxnCommandsForMmTable.java:359)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>java.lang.NullPointerException
	at org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos$SignableVertexSpec$Builder.setUser(LlapDaemonProtocolProtos.java:4899)
	at org.apache.hadoop.hive.llap.tez.Converters.constructSignableVertexSpec(Converters.java:135)
	at org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.constructSubmitWorkRequest(LlapTaskCommunicator.java:806)
	at org.apac</li></div><div><li>expected:&lt;1&gt; but was:&lt;0&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.hive.ql.TestTxnCommandsForMmTable.testInsertOverwriteForPartitionedMmTable(TestTxnCommandsForMmTable.java:359)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.hive.ql.TestTxnCommandsForMmTable.testInsertOverwriteForPartitionedMmTable(TestTxnCommandsForMmTable.java:359)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl</li></div><div><li>Exception msg didn't match expected:&lt;...conflict on default/[TAB_PART]/p=blah committed by...&gt; but was:&lt;...conflict on default/[tab_part]/p=blah committed by...&gt;</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.hive.ql.TestTxnCommandsForMmTable.testInsertOverwriteForPartitionedMmTable</div></li><li><div>org.apache.hadoop.hive.ql.TestTxnCommandsForOrcMmTable.testInsertOverwriteForPartitionedMmTable</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.hive.llap.daemon.impl.comparator.TestAMReporter.testMultipleAM</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.hive.ql.TestTxnCommandsForMmTable.testInsertOverwriteForPartitionedMmTable</div></li><li><div>org.apache.hadoop.hive.ql.TestTxnCommandsForOrcMmTable.testInsertOverwriteForPartitionedMmTable</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.testStringLikeMultiByte</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="kafka"><div style="font-weight:bold;" class="panel-heading">KAFKA<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Sonia)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/trunk</div><div><b>Last Revision: </b>885abbfcd40aab57acec278d976956f07be15090</div><div><b>Last Run: </b>26-04-2018 19:00 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th><th>PPC RHEL7.5</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 8918</div><div>Failed Count : 0</div><div>Skipped Count : 6</div></td><td><div>Total Count : 8918</div><div>Failed Count : 1</div><div>Skipped Count : 6</div></td><td><div>Total Count : 8918</div><div>Failed Count : 1</div><div>Skipped Count : 6</div></td><td><div>Total Count : 8918</div><div>Failed Count : 0</div><div>Skipped Count : 6</div></td><td><div>Total Count : 8918</div><div>Failed Count : 0</div><div>Skipped Count : 6</div></td></tr><tr><td>Result</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>kafka.server.LogDirFailureTest.testIOExceptionDuringCheckpoint</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>kafka.api.UserQuotaTest.testThrottledProducerConsumer</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>java.lang.AssertionError: Expected some messages</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>java.lang.AssertionError: Should have been throttled</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>kafka.server.LogDirFailureTest.testIOExceptionDuringCheckpoint</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>kafka.api.UserQuotaTest.testThrottledProducerConsumer</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="knox"><div style="font-weight:bold;" class="panel-heading">KNOX<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Yussuf)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>2420226cbde79a8b7649e02122baadbbe4acbf06</div><div><b>Last Run: </b>24-04-2018 01:04 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th><th>PPC RHEL7.5</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 985</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 984</div><div>Failed Count : 1</div><div>Skipped Count : 2</div></td><td><div>Total Count : 985</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 985</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 985</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td></tr><tr><td>Result</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.knox.gateway.GatewayLdapDynamicGroupFuncTest.org.apache.knox.gateway.GatewayLdapDynamicGroupFuncTest</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>Timed out 10000 waiting for URL http://localhost:44642/gateway/testdg-cluster/test-service-path/test-service-resource</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.knox.gateway.GatewayLdapDynamicGroupFuncTest.org.apache.knox.gateway.GatewayLdapDynamicGroupFuncTest</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="metron"><div style="font-weight:bold;" class="panel-heading">METRON<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Pravin)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>a41611b1ae2bb17fa9333ef6f965749652e95538</div><div><b>Last Run: </b>23-04-2018 01:39 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th><th>PPC RHEL7.5</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 1681</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1681</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1681</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1681</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1724</div><div>Failed Count : 1</div><div>Skipped Count : 2</div></td></tr><tr><td>Result</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.metron.stellar.common.CachingStellarProcessorTest.testCaching</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;6&gt; but was:&lt;4&gt;</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="oozie"><div style="font-weight:bold;" class="panel-heading">OOZIE<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Alisha)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>35451d50529634c9ce8730abe782bd4cf182b9a1</div><div><b>Last Run: </b>26-04-2018 21:58 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th><th>PPC RHEL7.5</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 2112</div><div>Failed Count : 30</div><div>Skipped Count : 2</div></td><td><div>Total Count : 2112</div><div>Failed Count : 5</div><div>Skipped Count : 2</div></td><td><div>Total Count : 2112</div><div>Failed Count : 1</div><div>Skipped Count : 2</div></td><td><div>Total Count : 2112</div><div>Failed Count : 1</div><div>Skipped Count : 2</div></td><td><div>Total Count : 2112</div><div>Failed Count : 3</div><div>Skipped Count : 2</div></td></tr><tr><td>Result</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"><div><li>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testCredentialsSkip</li></div><div><li>org.apache.oozie.jms.TestDefaultConnectionContext.testThreadLocalSession</li></div><div><li>org.apache.oozie.jms.TestHCatMessageHandler.testDropEventTypeMessage</li></div><div><li>org.apache.oozie.jms.TestHCatMessageHandler.testCacheUpdateByMessage</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testCoordinatorActionSelectors</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testCoordinatorActionSelectorsNegative</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectors</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorJobSuccessEvent</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobSuspendEvent</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectorsAnd</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorJobFailureEvent</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testConnectionDrop</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectorsNegative</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectorsOr</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobSuccessEvent</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorActionWaitingEvent</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobFailureEvent</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorActionStartEvent</li></div><div><li>org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobStartedEvent</li></div><div><li>org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLADurationMetEvent</li></div><div><li>org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAStartMetEvent</li></div><div><li>org.apache.oozie.jms.TestJMSSLAEventListener.testSLAJobSelectorsNegative</li></div><div><li>org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAEndMetEvent</li></div><div><li>org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAStartMissEvent</li></div><div><li>org.apache.oozie.jms.TestJMSSLAEventListener.testSLAJobSelectors</li></div><div><li>org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAEndMissEvent</li></div><div><li>org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLADurationMissEvent</li></div><div><li>org.apache.oozie.service.TestJMSAccessorService.testConnectionRetry</li></div><div><li>org.apache.oozie.service.TestPartitionDependencyManagerEhcache.testPartitionDependency</li></div><div><li>org.apache.oozie.tools.TestOozieDBCLI.testOozieDBCLI</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.oozie.TestLocalOozieClientCoord.testJobsOperations</li></div><div><li>org.apache.oozie.action.hadoop.TestJavaActionExecutor.testCredentialsSkip</li></div><div><li>org.apache.oozie.command.coord.TestCoordActionsKillXCommand.testActionKillCommandActionNumbers</li></div><div><li>org.apache.oozie.util.TestOozieRollingPolicy.testDeletingAuditOldFiles</li></div><div><li>org.apache.oozie.tools.TestOozieDBCLI.testOozieDBCLI</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.oozie.tools.TestOozieDBCLI.testOozieDBCLI</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.oozie.tools.TestOozieDBCLI.testOozieDBCLI</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.oozie.command.TestXCommand.testXCommandLifecycleLockingFailingToLock</li></div><div><li>org.apache.oozie.service.TestCoordMaterializeTriggerService.testCoordMaterializeTriggerService3</li></div><div><li>org.apache.oozie.tools.TestOozieDBCLI.testOozieDBCLI</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"><div><li>JA020: Could not load credentials of type [abc] with name [abcname]]; perhaps it was not defined in oozie-site.xml?</li></div><div><li>java.lang.NullPointerException
	at org.apache.oozie.jms.TestDefaultConnectionContext.testThreadLocalSession(TestDefaultConnectionContext.java:74)
</li></div><div><li>Could not create Transport. Reason: javax.management.InstanceAlreadyExistsException: org.apache.activemq:type=Broker,brokerName=localhost</li></div><div><li>Could not create Transport. Reason: javax.management.InstanceAlreadyExistsException: org.apache.activemq:type=Broker,brokerName=localhost</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testCoordinatorActionSelectors(TestJMSJobEventListener.java:544)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testCoordinatorActionSelectorsNegative(TestJMSJobEventListener.java:568)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectors(TestJMSJobEventListener.java:239)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorJobSuccessEvent(TestJMSJobEventListener.java:477)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobSuspendEvent(TestJMSJobEventListener.java:214)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectorsAnd(TestJMSJobEventListener.java:316)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorJobFailureEvent(TestJMSJobEventListener.java:517)
</li></div><div><li>org.apache.activemq:type=Broker,brokerName=localhost</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectorsNegative(TestJMSJobEventListener.java:262)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectorsOr(TestJMSJobEventListener.java:289)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobSuccessEvent(TestJMSJobEventListener.java:143)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorActionWaitingEvent(TestJMSJobEventListener.java:403)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobFailureEvent(TestJMSJobEventListener.java:180)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorActionStartEvent(TestJMSJobEventListener.java:439)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobStartedEvent(TestJMSJobEventListener.java:108)
</li></div><div><li>java.lang.NullPointerException
	at org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLADurationMetEvent(TestJMSSLAEventListener.java:382)
</li></div><div><li>java.lang.NullPointerException
	at org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAStartMetEvent(TestJMSSLAEventListener.java:292)
</li></div><div><li>java.lang.NullPointerException
	at org.apache.oozie.jms.TestJMSSLAEventListener.testSLAJobSelectorsNegative(TestJMSSLAEventListener.java:261)
</li></div><div><li>java.lang.NullPointerException
	at org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAEndMetEvent(TestJMSSLAEventListener.java:332)
</li></div><div><li>java.lang.NullPointerException
	at org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAStartMissEvent(TestJMSSLAEventListener.java:103)
</li></div><div><li>java.lang.NullPointerException
	at org.apache.oozie.jms.TestJMSSLAEventListener.testSLAJobSelectors(TestJMSSLAEventListener.java:231)
</li></div><div><li>java.lang.NullPointerException
	at org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAEndMissEvent(TestJMSSLAEventListener.java:143)
</li></div><div><li>java.lang.NullPointerException
	at org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLADurationMissEvent(TestJMSSLAEventListener.java:191)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.service.TestJMSAccessorService.testConnectionRetry(TestJMSAccessorService.java:183)
</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.service.TestPartitionDependencyManagerEhcache.testPartitionDependency(TestPartitionDependencyManagerEhcache.java:47)
</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;KILLED&gt; but was:&lt;FAILED&gt;</li></div><div><li>JA020: Could not load credentials of type [abc] with name [abcname]]; perhaps it was not defined in oozie-site.xml?</li></div><div><li>expected:&lt;RUNNING&gt; but was:&lt;RUNNINGWITHERROR&gt;</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.util.TestOozieRollingPolicy._testDeletingOldFiles(TestOozieRollingPolicy.java:147)
	at org.apache.oozie.util.TestOozieRollingPolicy.testDeletingAuditOldFiles(TestOozieRollingPolicy.java:55)
</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>junit.framework.AssertionFailedError
	at org.apache.oozie.command.TestXCommand.testXCommandLifecycleLockingFailingToLock(TestXCommand.java:186)
</li></div><div><li>expected:&lt;PREP&gt; but was:&lt;RUNNING&gt;</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.oozie.jms.TestDefaultConnectionContext.testThreadLocalSession</div></li><li><div>org.apache.oozie.jms.TestHCatMessageHandler.testDropEventTypeMessage</div></li><li><div>org.apache.oozie.jms.TestHCatMessageHandler.testCacheUpdateByMessage</div></li><li><div>org.apache.oozie.jms.TestJMSJobEventListener.testCoordinatorActionSelectors</div></li><li><div>org.apache.oozie.jms.TestJMSJobEventListener.testCoordinatorActionSelectorsNegative</div></li><li><div>org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectors</div></li><li><div>org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorJobSuccessEvent</div></li><li><div>org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobSuspendEvent</div></li><li><div>org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectorsAnd</div></li><li><div>org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorJobFailureEvent</div></li><li><div>org.apache.oozie.jms.TestJMSJobEventListener.testConnectionDrop</div></li><li><div>org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectorsNegative</div></li><li><div>org.apache.oozie.jms.TestJMSJobEventListener.testWorkflowJobSelectorsOr</div></li><li><div>org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobSuccessEvent</div></li><li><div>org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorActionWaitingEvent</div></li><li><div>org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobFailureEvent</div></li><li><div>org.apache.oozie.jms.TestJMSJobEventListener.testOnCoordinatorActionStartEvent</div></li><li><div>org.apache.oozie.jms.TestJMSJobEventListener.testOnWorkflowJobStartedEvent</div></li><li><div>org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLADurationMetEvent</div></li><li><div>org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAStartMetEvent</div></li><li><div>org.apache.oozie.jms.TestJMSSLAEventListener.testSLAJobSelectorsNegative</div></li><li><div>org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAEndMetEvent</div></li><li><div>org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAStartMissEvent</div></li><li><div>org.apache.oozie.jms.TestJMSSLAEventListener.testSLAJobSelectors</div></li><li><div>org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLAEndMissEvent</div></li><li><div>org.apache.oozie.jms.TestJMSSLAEventListener.testOnSLADurationMissEvent</div></li><li><div>org.apache.oozie.service.TestJMSAccessorService.testConnectionRetry</div></li><li><div>org.apache.oozie.service.TestPartitionDependencyManagerEhcache.testPartitionDependency</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.oozie.TestLocalOozieClientCoord.testJobsOperations</div></li><li><div>org.apache.oozie.command.coord.TestCoordActionsKillXCommand.testActionKillCommandActionNumbers</div></li><li><div>org.apache.oozie.util.TestOozieRollingPolicy.testDeletingAuditOldFiles</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="phoenix"><div style="font-weight:bold;" class="panel-heading">PHOENIX<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Valencia)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>4b84199681f241843855ce652e582e06bb99c1cf</div><div><b>Last Run: </b>26-04-2018 23:45 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th><th>PPC RHEL7.5</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 1694</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 1694</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 1694</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 1694</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 1694</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td></tr><tr><td>Result</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="pig"><div style="font-weight:bold;" class="panel-heading">PIG<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Yussuf)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/trunk</div><div><b>Last Revision: </b>84889e7852c98c67b40d12116e3bb6dd311a3363</div><div><b>Last Run: </b>19-04-2018 02:58 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th><th>PPC RHEL7.5</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 895</div><div>Failed Count : 20</div><div>Skipped Count : 0</div></td><td><div>Total Count : 895</div><div>Failed Count : 20</div><div>Skipped Count : 0</div></td><td><div>Total Count : 895</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 895</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 723</div><div>Failed Count : 6</div><div>Skipped Count : 0</div></td></tr><tr><td>Result</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"><div><li>org.apache.pig.test.TestBuiltin.testRANDOMWithJob</li></div><div><li>org.apache.pig.test.TestBuiltin.testSFPig</li></div><div><li>org.apache.pig.test.TestBuiltin.testUniqueID</li></div><div><li>org.apache.pig.test.TestLoad.testCommaSeparatedString3</li></div><div><li>org.apache.pig.test.TestLoad.testCommaSeparatedString5</li></div><div><li>org.apache.pig.test.TestLoad.testLoadRemoteAbsScheme</li></div><div><li>org.apache.pig.test.TestLocalRearrange.testMultiQueryJiraPig1194</li></div><div><li>org.apache.pig.test.TestPigServer.testRegisterRemoteScript</li></div><div><li>org.apache.pig.test.TestSchema.testEnabledDisambiguationPassesForDupeAliases</li></div><div><li>org.apache.pig.test.TestSchema.testSchemaSerialization</li></div><div><li>org.apache.pig.test.TestSchema.testDisabledDisambiguationContainsNoColonsForNestedSchema</li></div><div><li>org.apache.pig.test.TestSchema.testDisabledDisambiguationContainsNoColons</li></div><div><li>org.apache.pig.test.TestStore.testStore</li></div><div><li>org.apache.pig.test.TestStore.testStoreComplexDataWithNull</li></div><div><li>org.apache.pig.test.TestStore.testBinStorageGetSchema</li></div><div><li>org.apache.pig.test.TestStore.testStoreComplexData</li></div><div><li>org.apache.pig.test.TestStore.testSetStoreSchema</li></div><div><li>org.apache.pig.test.TestStore.testSuccessFileCreation1</li></div><div><li>org.apache.pig.test.TestStore.testCleanupOnFailureMultiStore</li></div><div><li>org.apache.pig.test.TestStore.testEmptyPartFileCreation</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.pig.test.TestBuiltin.testRANDOMWithJob</li></div><div><li>org.apache.pig.test.TestBuiltin.testSFPig</li></div><div><li>org.apache.pig.test.TestBuiltin.testUniqueID</li></div><div><li>org.apache.pig.test.TestLoad.testCommaSeparatedString3</li></div><div><li>org.apache.pig.test.TestLoad.testCommaSeparatedString5</li></div><div><li>org.apache.pig.test.TestLoad.testLoadRemoteAbsScheme</li></div><div><li>org.apache.pig.test.TestLocalRearrange.testMultiQueryJiraPig1194</li></div><div><li>org.apache.pig.test.TestPigServer.testRegisterRemoteScript</li></div><div><li>org.apache.pig.test.TestSchema.testEnabledDisambiguationPassesForDupeAliases</li></div><div><li>org.apache.pig.test.TestSchema.testSchemaSerialization</li></div><div><li>org.apache.pig.test.TestSchema.testDisabledDisambiguationContainsNoColonsForNestedSchema</li></div><div><li>org.apache.pig.test.TestSchema.testDisabledDisambiguationContainsNoColons</li></div><div><li>org.apache.pig.test.TestStore.testStore</li></div><div><li>org.apache.pig.test.TestStore.testStoreComplexDataWithNull</li></div><div><li>org.apache.pig.test.TestStore.testBinStorageGetSchema</li></div><div><li>org.apache.pig.test.TestStore.testStoreComplexData</li></div><div><li>org.apache.pig.test.TestStore.testSetStoreSchema</li></div><div><li>org.apache.pig.test.TestStore.testSuccessFileCreation1</li></div><div><li>org.apache.pig.test.TestStore.testCleanupOnFailureMultiStore</li></div><div><li>org.apache.pig.test.TestStore.testEmptyPartFileCreation</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.pig.test.TestBuiltin.testRANDOMWithJob</li></div><div><li>org.apache.pig.test.TestLoad.testCommaSeparatedString3</li></div><div><li>org.apache.pig.test.TestLocalRearrange.testMultiQueryJiraPig1194</li></div><div><li>org.apache.pig.test.TestPigServer.testRegisterRemoteScript</li></div><div><li>org.apache.pig.test.TestSchema.testEnabledDisambiguationPassesForDupeAliases</li></div><div><li>org.apache.pig.test.TestStore.testStore</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"><div><li>Unable to open iterator for alias B</li></div><div><li>Input path does not exist: hdfs://localhost:43628/user/jenkins/testSFPig-output.txt</li></div><div><li>Unable to open iterator for alias B</li></div><div><li>Unable to open iterator for alias a</li></div><div><li>Unable to open iterator for alias a</li></div><div><li>Unable to open iterator for alias a</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.pig.test.TestLocalRearrange.testMultiQueryJiraPig1194(TestLocalRearrange.java:231)
</li></div><div><li>Unable to open iterator for alias b</li></div><div><li>Unable to open iterator for alias C</li></div><div><li>Unable to open iterator for alias c</li></div><div><li>Unable to open iterator for alias F</li></div><div><li>Unable to open iterator for alias E</li></div><div><li>File /tmp/TestStore/TestStore-output-5838167414281430663.txt does not exist.</li></div><div><li>File /tmp/TestStore/TestStore-output-5396067415083439737.txt does not exist.</li></div><div><li>Checking binstorage getSchema output</li></div><div><li>File /tmp/TestStore/TestStore-output--7994633028529183552.txt does not exist.</li></div><div><li>Checking if file /tmp/TestStore/_commitJob_called does  exists in MAPREDUCE mode expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>Checking if _SUCCESS file exists in MAPREDUCE mode expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>Checking if file /tmp/TestStore/_setupTask_called1 does  exists in MAPREDUCE mode expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>File /tmp/TestStore/TestStore-output-8062439654982504915.txt_1 does not exist.</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Unable to open iterator for alias B</li></div><div><li>Input path does not exist: hdfs://localhost:46193/user/jenkins/testSFPig-output.txt</li></div><div><li>Unable to open iterator for alias B</li></div><div><li>Unable to open iterator for alias a</li></div><div><li>Unable to open iterator for alias a</li></div><div><li>Unable to open iterator for alias a</li></div><div><li>junit.framework.AssertionFailedError
	at org.apache.pig.test.TestLocalRearrange.testMultiQueryJiraPig1194(TestLocalRearrange.java:231)
</li></div><div><li>Unable to open iterator for alias b</li></div><div><li>Unable to open iterator for alias C</li></div><div><li>Unable to open iterator for alias c</li></div><div><li>Unable to open iterator for alias F</li></div><div><li>Unable to open iterator for alias E</li></div><div><li>File /tmp/TestStore/TestStore-output-7744004612277514124.txt does not exist.</li></div><div><li>File /tmp/TestStore/TestStore-output-8357066113947161454.txt does not exist.</li></div><div><li>Checking binstorage getSchema output</li></div><div><li>File /tmp/TestStore/TestStore-output--9010671093805249482.txt does not exist.</li></div><div><li>Checking if file /tmp/TestStore/_commitJob_called does  exists in MAPREDUCE mode expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>Checking if _SUCCESS file exists in MAPREDUCE mode expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>Checking if file /tmp/TestStore/_setupTask_called1 does  exists in MAPREDUCE mode expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>File /tmp/TestStore/TestStore-output--5232853012136264681.txt_1 does not exist.</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>Timeout occurred. Please note the time in the report does not reflect the time until the timeout.</li></div><div><li>Timeout occurred. Please note the time in the report does not reflect the time until the timeout.</li></div><div><li>Timeout occurred. Please note the time in the report does not reflect the time until the timeout.</li></div><div><li>Timeout occurred. Please note the time in the report does not reflect the time until the timeout.</li></div><div><li>Timeout occurred. Please note the time in the report does not reflect the time until the timeout.</li></div><div><li>Timeout occurred. Please note the time in the report does not reflect the time until the timeout.</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="ranger"><div style="font-weight:bold;" class="panel-heading">RANGER<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Sneha)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>343668b42afe7265c08064c7fb0bf40f7184ea1e</div><div><b>Last Run: </b>12-04-2018 01:58 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th><th>PPC RHEL7.5</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 1077</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1077</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1077</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1077</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1077</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td></tr><tr><td>Result</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="slider"><div style="font-weight:bold;" class="panel-heading">SLIDER<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Yussuf)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/develop</div><div><b>Last Revision: </b>4032999f35db4877b6b8ffc5e97a59837e22365b</div><div><b>Last Run: </b>26-04-2018 20:54 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th><th>PPC RHEL7.5</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 607</div><div>Failed Count : 24</div><div>Skipped Count : 13</div></td><td><div>Total Count : 607</div><div>Failed Count : 24</div><div>Skipped Count : 13</div></td><td><div>Total Count : 607</div><div>Failed Count : 0</div><div>Skipped Count : 13</div></td><td><div>Total Count : 607</div><div>Failed Count : 0</div><div>Skipped Count : 13</div></td><td><div>Total Count : 607</div><div>Failed Count : 0</div><div>Skipped Count : 13</div></td></tr><tr><td>Result</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"><div><li>org.apache.slider.agent.actions.TestActionExists.testExistsLiveCluster</li></div><div><li>org.apache.slider.agent.actions.TestActionList.testActionListSuite</li></div><div><li>org.apache.slider.agent.actions.TestActionStatus.testSuite</li></div><div><li>org.apache.slider.agent.freezethaw.TestFreezeCommands.testFreezeCommands</li></div><div><li>org.apache.slider.agent.freezethaw.TestFreezeThawFlexStandaloneAM.testFreezeThawFlexStandaloneAM</li></div><div><li>org.apache.slider.agent.rest.TestStandaloneREST.testStandaloneREST</li></div><div><li>org.apache.slider.agent.standalone.TestBuildStandaloneAM.testBuildCluster</li></div><div><li>org.apache.slider.agent.standalone.TestBuildStandaloneAM.testUpdateCluster</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneAMDestroy.testStandaloneAMDestroy</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneAMKill.testKillStandaloneAM</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneAMRestart.testStandaloneAMRestartWithDefaultRetryWindow</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneAMRestart.testStandaloneAMRestart</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneAMRestart.testStandaloneAMRestartWithRetryWindow</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneAgentAM.testStandaloneAgentAM</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneYarnRegistryAM.testStandaloneYarnRegistryAM</li></div><div><li>org.apache.slider.client.TestDiagnostics.testContainerDiagsNoAppContainer</li></div><div><li>org.apache.slider.client.TestDiagnostics.testContainerDiagsWithAppPackage</li></div><div><li>org.apache.slider.client.TestUpgradeCommandOptions.testAll</li></div><div><li>org.apache.slider.providers.agent.TestAddonPackage.testEchoApplicationAddPackage</li></div><div><li>org.apache.slider.providers.agent.TestAgentAAEcho.testAgentEcho</li></div><div><li>org.apache.slider.providers.agent.TestAgentAMManagementWS.testAgentAMManagementWS</li></div><div><li>org.apache.slider.providers.agent.TestAgentEcho.testAgentEcho</li></div><div><li>org.apache.slider.server.appmaster.TestDelayInContainerLaunch.testDelayInContainerLaunch</li></div><div><li>org.apache.slider.server.appmaster.web.rest.publisher.TestPublisherRestResources.testRestURIs</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.slider.agent.actions.TestActionExists.testExistsLiveCluster</li></div><div><li>org.apache.slider.agent.actions.TestActionList.testActionListSuite</li></div><div><li>org.apache.slider.agent.actions.TestActionStatus.testSuite</li></div><div><li>org.apache.slider.agent.freezethaw.TestFreezeCommands.testFreezeCommands</li></div><div><li>org.apache.slider.agent.freezethaw.TestFreezeThawFlexStandaloneAM.testFreezeThawFlexStandaloneAM</li></div><div><li>org.apache.slider.agent.rest.TestStandaloneREST.testStandaloneREST</li></div><div><li>org.apache.slider.agent.standalone.TestBuildStandaloneAM.testBuildCluster</li></div><div><li>org.apache.slider.agent.standalone.TestBuildStandaloneAM.testUpdateCluster</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneAMDestroy.testStandaloneAMDestroy</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneAMKill.testKillStandaloneAM</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneAMRestart.testStandaloneAMRestartWithDefaultRetryWindow</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneAMRestart.testStandaloneAMRestart</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneAMRestart.testStandaloneAMRestartWithRetryWindow</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneAgentAM.testStandaloneAgentAM</li></div><div><li>org.apache.slider.agent.standalone.TestStandaloneYarnRegistryAM.testStandaloneYarnRegistryAM</li></div><div><li>org.apache.slider.client.TestDiagnostics.testContainerDiagsNoAppContainer</li></div><div><li>org.apache.slider.client.TestDiagnostics.testContainerDiagsWithAppPackage</li></div><div><li>org.apache.slider.client.TestUpgradeCommandOptions.testAll</li></div><div><li>org.apache.slider.providers.agent.TestAddonPackage.testEchoApplicationAddPackage</li></div><div><li>org.apache.slider.providers.agent.TestAgentAAEcho.testAgentEcho</li></div><div><li>org.apache.slider.providers.agent.TestAgentAMManagementWS.testAgentAMManagementWS</li></div><div><li>org.apache.slider.providers.agent.TestAgentEcho.testAgentEcho</li></div><div><li>org.apache.slider.server.appmaster.TestDelayInContainerLaunch.testDelayInContainerLaunch</li></div><div><li>org.apache.slider.server.appmaster.web.rest.publisher.TestPublisherRestResources.testRestURIs</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"><div><li>Launch failed with exit code -1</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Application not running: application_1524777087802_0001 state=FAILED </li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Failed on local exception: java.io.FileNotFoundException: http://439bb4e54c1d:33341/cluster/app/application_1524776641074_0001; Host Details : local host is: "localhost"; destination host is: "http://439bb4e54c1d:33341/cluster/app/application_1524776641074_0001":33341; </li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>assert report.yarnApplicationState == YarnApplicationState.RUNNING
       |      |                                            |
       |      FAILED                                       RUNNING
       applicationId { id: 1 cluster_timestamp: 1524777032287 } user: "jenkins" queue: "default" name: "testkillstandaloneam" host: "N/A" rpc_port: -1 yarn_application_state: FAILED trackingUrl: "http://43</li></div><div><li>Cluster teststandaloneamrestartwithdefaultretrywindow not live after 30000 ms</li></div><div><li>Cluster teststandaloneamrestart not live after 30000 ms</li></div><div><li>Cluster teststandaloneamrestartwithretrywindow not live after 30000 ms</li></div><div><li>assert uri.port in 60000..60010
       |   |            |
       |   38811        [60000, 60001, 60002, 60003, 60004, 60005, 60006, 60007, 60008, 60009, 60010]
       http://439bb4e54c1d:38811/cluster/app/application_1524776889212_0001</li></div><div><li>Application not running: application_1524776912707_0001 state=FAILED </li></div><div><li>assert 0 == status
         |  |
         |  -1
         false</li></div><div><li>Launch failed with exit code -1</li></div><div><li>Upgrade command should have failed</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Launch failed with exit code -1</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Application not running: application_1524777422613_0001 state=FAILED </li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Failed on local exception: java.io.FileNotFoundException: http://22263857bb62:44531/cluster/app/application_1524777449700_0001; Host Details : local host is: "localhost"; destination host is: "http://22263857bb62:44531/cluster/app/application_1524777449700_0001":44531; </li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>assert report.yarnApplicationState == YarnApplicationState.RUNNING
       |      |                                            |
       |      FAILED                                       RUNNING
       applicationId { id: 1 cluster_timestamp: 1524777054735 } user: "jenkins" queue: "default" name: "testkillstandaloneam" host: "N/A" rpc_port: -1 yarn_application_state: FAILED trackingUrl: "http://22</li></div><div><li>Cluster teststandaloneamrestartwithdefaultretrywindow not live after 30000 ms</li></div><div><li>Cluster teststandaloneamrestart not live after 30000 ms</li></div><div><li>Cluster teststandaloneamrestartwithretrywindow not live after 30000 ms</li></div><div><li>assert uri.port in 60000..60010
       |   |            |
       |   39420        [60000, 60001, 60002, 60003, 60004, 60005, 60006, 60007, 60008, 60009, 60010]
       http://22263857bb62:39420/cluster/app/application_1524777027872_0001</li></div><div><li>Application not running: application_1524777109997_0001 state=FAILED </li></div><div><li>assert 0 == status
         |  |
         |  -1
         false</li></div><div><li>Launch failed with exit code -1</li></div><div><li>Upgrade command should have failed</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div><div><li>Launch failed with exit code 65</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="spark"><div style="font-weight:bold;" class="panel-heading">SPARK<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Parita)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>20ca208bcda6f22fe7d9fb54144de435b4237536</div><div><b>Last Run: </b>25-04-2018 18:59 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th><th>PPC RHEL7.5</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 14080</div><div>Failed Count : 1</div><div>Skipped Count : 671</div></td><td><div>Total Count : 15464</div><div>Failed Count : 2</div><div>Skipped Count : 677</div></td><td><div>Total Count : 11508</div><div>Failed Count : 1</div><div>Skipped Count : 635</div></td><td><div>Total Count : 15464</div><div>Failed Count : 1</div><div>Skipped Count : 677</div></td><td><div>Total Count : 12084</div><div>Failed Count : 0</div><div>Skipped Count : 83</div></td></tr><tr><td>Result</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"><div><li>org.apache.spark.sql.hive.HiveSparkSubmitSuite.SPARK-8020: set sql conf in spark conf</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.spark.sql.streaming.continuous.ContinuousStressSuite.restarts</li></div><div><li>org.apache.spark.sql.streaming.continuous.ContinuousSuite.repeatedly restart</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.spark.deploy.yarn.YarnShuffleAuthSuite.external shuffle service</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.spark.sql.DataFrameRangeSuite.Cancelling stage in a query with Range.</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"><div><li>Timeout of './bin/spark-submit' '--class' 'org.apache.spark.sql.hive.SparkSQLConfTest' '--name' 'SparkSQLConfTest' '--master' 'local-cluster[2,1,1024]' '--conf' 'spark.ui.enabled=false' '--conf' 'spark.master.rest.enabled=false' '--conf' 'spark.sql.hive.metastore.version=0.12' '--conf' 'spark.sql.hive.metastore.jars=maven' '--driver-java-options' '-Dderby.system.durability=test' 'file:/var/lib/jen</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>&amp;#010;Error while stopping stream: &amp;#010;query.exception() is not empty after clean stop: org.apache.spark.sql.streaming.StreamingQueryException: Writing job aborted.&amp;#010;=== Streaming Query ===&amp;#010;Identifier: [id = dd0ae58f-55be-4bdb-8b5f-78dc1df5c4c6, runId = 6c8bf877-1fde-446c-9dfc-b61e7f0badc2]&amp;#010;Current Committed Offsets: {org.apache.spark.sql.execution.streaming.continuous.RateStreamCo</li></div><div><li>&amp;#010;Error while stopping stream: &amp;#010;query.exception() is not empty after clean stop: org.apache.spark.sql.streaming.StreamingQueryException: Writing job aborted.&amp;#010;=== Streaming Query ===&amp;#010;Identifier: [id = 3be2cb4a-c1f6-473a-97ff-a302ec8e1307, runId = 82eefb18-21a3-4713-a797-2d20d54bead8]&amp;#010;Current Committed Offsets: {org.apache.spark.sql.execution.streaming.continuous.RateStreamCo</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>The code passed to eventually never returned normally. Attempted 130 times over 2.001807514616667 minutes. Last failure message: handle.getState().isFinal() was false.</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Expected exception org.apache.spark.SparkException to be thrown, but no exception was thrown</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.spark.sql.hive.HiveSparkSubmitSuite.SPARK-8020: set sql conf in spark conf</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.spark.sql.streaming.continuous.ContinuousStressSuite.restarts</div></li><li><div>org.apache.spark.sql.streaming.continuous.ContinuousSuite.repeatedly restart</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.spark.deploy.yarn.YarnShuffleAuthSuite.external shuffle service</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.spark.sql.DataFrameRangeSuite.Cancelling stage in a query with Range.</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="sqoop"><div style="font-weight:bold;" class="panel-heading">SQOOP<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Talat)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/trunk</div><div><b>Last Revision: </b>af7a594d987ece6c1990be950c48d94bbab8271f</div><div><b>Last Run: </b>13-04-2018 01:55 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th><th>PPC RHEL7.5</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 734</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 734</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 734</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 734</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 776</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td></tr><tr><td>Result</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="storm"><div style="font-weight:bold;" class="panel-heading">STORM<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Parita)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>8ffa920d3894634aa078f0fdf6b02d270262caf4</div><div><b>Last Run: </b>19-04-2018 01:18 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th><th>PPC RHEL7.5</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 1187</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 1181</div><div>Failed Count : 1</div><div>Skipped Count : 5</div></td><td><div>Total Count : 1187</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 1187</div><div>Failed Count : 2</div><div>Skipped Count : 5</div></td><td><div>Total Count : 1197</div><div>Failed Count : 1</div><div>Skipped Count : 5</div></td></tr><tr><td>Result</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.storm.TickTupleTest.testTickTupleWorksWithSystemBolt</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.storm.flux.multilang.MultilangEnvironmentTest.testInvokeNode</li></div><div><li>org.apache.storm.utils.JCQueueTest.testFirstMessageFirst</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.storm.utils.JCQueueTest.testFirstMessageFirst</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>took over 110000 ms of simulated time to get a message back...</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>Cannot run program "node": error=2, No such file or directory</li></div><div><li>Unable to send halt interrupt</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Unable to send halt interrupt</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.storm.TickTupleTest.testTickTupleWorksWithSystemBolt</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.storm.flux.multilang.MultilangEnvironmentTest.testInvokeNode</div></li><li><div>org.apache.storm.utils.JCQueueTest.testFirstMessageFirst</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="tez"><div style="font-weight:bold;" class="panel-heading">TEZ<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Valencia)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>2e66f3cb2ef082889551f6a0830c7014317d9680</div><div><b>Last Run: </b>26-04-2018 03:53 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th><th>PPC RHEL7.5</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 1791</div><div>Failed Count : 2</div><div>Skipped Count : 14</div></td><td><div>Total Count : 1791</div><div>Failed Count : 1</div><div>Skipped Count : 14</div></td><td><div>Total Count : 1791</div><div>Failed Count : 1</div><div>Skipped Count : 14</div></td><td><div>Total Count : 1788</div><div>Failed Count : 1</div><div>Skipped Count : 14</div></td><td><div>Total Count : 1695</div><div>Failed Count : 19</div><div>Skipped Count : 13</div></td></tr><tr><td>Result</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"><div><li>org.apache.tez.tests.TestExtServicesWithLocalMode.test1</li></div><div><li>org.apache.tez.history.TestHistoryParser.testParserWithSuccessfulJob</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.tez.tests.TestExtServicesWithLocalMode.test1</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.tez.tests.TestExtServicesWithLocalMode.test1</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.tez.tests.TestExtServicesWithLocalMode.test1</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.tez.dag.app.TestPreemption.testPreemptionWithSession</li></div><div><li>org.apache.tez.auxservices.TestShuffleHandlerJobs.testOrderedWordCount</li></div><div><li>org.apache.tez.dag.history.ats.acls.TestATSHistoryWithACLs.testDisableSessionLogging</li></div><div><li>org.apache.tez.dag.history.ats.acls.TestATSHistoryWithACLs.testSimpleAMACls</li></div><div><li>org.apache.tez.dag.history.ats.acls.TestATSHistoryWithACLs.testDAGACls</li></div><div><li>org.apache.tez.dag.history.logging.ats.TestATSHistoryWithMiniCluster.testDisabledACls</li></div><div><li>org.apache.tez.mapreduce.TestMRRJobs.testMRRSleepJob</li></div><div><li>org.apache.tez.mapreduce.TestMRRJobs.testMRRSleepJobWithCompression</li></div><div><li>org.apache.tez.mapreduce.TestMRRJobs.testFailingAttempt</li></div><div><li>org.apache.tez.mapreduce.TestMRRJobs.testFailingJob</li></div><div><li>org.apache.tez.mapreduce.TestMRRJobs.testRandomWriter</li></div><div><li>org.apache.tez.test.TestDAGRecovery.testBasicRecovery</li></div><div><li>org.apache.tez.test.TestDAGRecovery.testDelayedInit</li></div><div><li>org.apache.tez.test.TestDAGRecovery2.testSessionDisableMultiAttempts</li></div><div><li>org.apache.tez.test.TestDAGRecovery2.testFailingCommitter</li></div><div><li>org.apache.tez.test.TestExceptionPropagation.testExceptionPropagationNonSession</li></div><div><li>org.apache.tez.test.TestExceptionPropagation.testExceptionPropagationSession</li></div><div><li>org.apache.tez.analyzer.TestAnalyzer.testWithSimpleHistory</li></div><div><li>org.apache.tez.analyzer.TestAnalyzer.testWithATS</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"><div><li>org.apache.tez.dag.api.TezUncheckedException: java.lang.reflect.InvocationTargetException</li></div><div><li>java.lang.AssertionError: null
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.tez.history.TestHistoryParser.verifyJobSpecificInfo(TestHistoryParser.java:266)
	at org.apache.tez.history.TestHistoryParser.testParserWithSuccessfulJob(TestHistoryParser.java:212)
</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.tez.dag.api.TezUncheckedException: java.lang.reflect.InvocationTargetException</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.tez.dag.api.TezUncheckedException: java.lang.reflect.InvocationTargetException</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.tez.dag.api.TezUncheckedException: java.lang.reflect.InvocationTargetException</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>test timed out after 30000 milliseconds</li></div><div><li>test timed out after 300000 milliseconds</li></div><div><li>test timed out after 50000 milliseconds</li></div><div><li>test timed out after 50000 milliseconds</li></div><div><li>test timed out after 50000 milliseconds</li></div><div><li>test timed out after 50000 milliseconds</li></div><div><li>test timed out after 60000 milliseconds</li></div><div><li>test timed out after 60000 milliseconds</li></div><div><li>test timed out after 60000 milliseconds</li></div><div><li>test timed out after 60000 milliseconds</li></div><div><li>test timed out after 60000 milliseconds</li></div><div><li>test timed out after 120000 milliseconds</li></div><div><li>test timed out after 120000 milliseconds</li></div><div><li>test timed out after 120000 milliseconds</li></div><div><li>test timed out after 120000 milliseconds</li></div><div><li>test timed out after 120000 milliseconds</li></div><div><li>Application not running, applicationId=application_1524721748673_0001, yarnApplicationState=FAILED, finalApplicationStatus=FAILED, trackingUrl=N/A, diagnostics=[DAG completed with an ERROR state. Shutting down AM, Session stats:submittedDAGs=2, successfulDAGs=0, failedDAGs=3, killedDAGs=0]</li></div><div><li>test timed out after 300000 milliseconds</li></div><div><li>test timed out after 300000 milliseconds</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.tez.history.TestHistoryParser.testParserWithSuccessfulJob</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="zeppelin"><div style="font-weight:bold;" class="panel-heading">ZEPPELIN<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Sneha)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>9e18159d440cc771a13b81b56dcd62c72202bd0b</div><div><b>Last Run: </b>27-04-2018 07:00 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th><th>PPC RHEL7.5</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 807</div><div>Failed Count : 12</div><div>Skipped Count : 5</div></td><td><div>Total Count : 807</div><div>Failed Count : 16</div><div>Skipped Count : 5</div></td><td><div>Total Count : 807</div><div>Failed Count : 13</div><div>Skipped Count : 5</div></td><td><div>Total Count : 807</div><div>Failed Count : 11</div><div>Skipped Count : 5</div></td><td><div>Total Count : 807</div><div>Failed Count : 10</div><div>Skipped Count : 5</div></td></tr><tr><td>Result</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"><div><li>org.apache.zeppelin.pig.PigInterpreterTest.testIncludeJobStats</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTest.testBasics</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTezTest.testIncludeJobStats</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTezTest.testBasics</li></div><div><li>org.apache.zeppelin.pig.PigQueryInterpreterTest.testMaxResult</li></div><div><li>org.apache.zeppelin.pig.PigQueryInterpreterTest.testBasics</li></div><div><li>org.apache.zeppelin.python.IPythonInterpreterTest.testIPythonPlotting</li></div><div><li>org.apache.zeppelin.spark.DepInterpreterTest.testDefault</li></div><div><li>org.apache.zeppelin.spark.IPySparkInterpreterTest.testIPythonPlotting</li></div><div><li>org.apache.zeppelin.spark.IPySparkInterpreterTest.testPythonBasics</li></div><div><li>org.apache.zeppelin.spark.PySparkInterpreterTest.testPythonBasics</li></div><div><li>org.apache.zeppelin.rest.KnoxRestApiTest.testThatOtherUserCanAccessNoteIfPermissionNotSet</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.zeppelin.pig.PigInterpreterTest.testIncludeJobStats</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTest.testBasics</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTezTest.testIncludeJobStats</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTezTest.testBasics</li></div><div><li>org.apache.zeppelin.pig.PigQueryInterpreterTest.testMaxResult</li></div><div><li>org.apache.zeppelin.pig.PigQueryInterpreterTest.testBasics</li></div><div><li>org.apache.zeppelin.python.IPythonInterpreterTest.testIPythonPlotting</li></div><div><li>org.apache.zeppelin.spark.IPySparkInterpreterTest.testIPythonPlotting</li></div><div><li>org.apache.zeppelin.spark.IPySparkInterpreterTest.testPythonBasics</li></div><div><li>org.apache.zeppelin.spark.PySparkInterpreterTest.testPythonBasics</li></div><div><li>org.apache.zeppelin.rest.KnoxRestApiTest.testThatOtherUserCanAccessNoteIfPermissionNotSet</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClusterMode[0]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testLocalMode[0]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClusterMode[3]</li></div><div><li>org.apache.zeppelin.interpreter.SparkIntegrationTest.testLocalMode[3]</li></div><div><li>org.apache.zeppelin.notebook.NotebookTest.testAutoRestartInterpreterAfterSchedule</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.zeppelin.pig.PigInterpreterTest.testIncludeJobStats</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTest.testBasics</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTezTest.testIncludeJobStats</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTezTest.testBasics</li></div><div><li>org.apache.zeppelin.pig.PigQueryInterpreterTest.testMaxResult</li></div><div><li>org.apache.zeppelin.pig.PigQueryInterpreterTest.testBasics</li></div><div><li>org.apache.zeppelin.python.IPythonInterpreterTest.testIPythonPlotting</li></div><div><li>org.apache.zeppelin.spark.DepInterpreterTest.testDefault</li></div><div><li>org.apache.zeppelin.spark.IPySparkInterpreterTest.testIPythonPlotting</li></div><div><li>org.apache.zeppelin.spark.IPySparkInterpreterTest.testPythonBasics</li></div><div><li>org.apache.zeppelin.spark.PySparkInterpreterTest.testPythonBasics</li></div><div><li>org.apache.zeppelin.interpreter.InterpreterOutputChangeWatcherTest.test</li></div><div><li>org.apache.zeppelin.rest.KnoxRestApiTest.testThatOtherUserCanAccessNoteIfPermissionNotSet</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.zeppelin.pig.PigInterpreterTest.testIncludeJobStats</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTest.testBasics</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTezTest.testIncludeJobStats</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTezTest.testBasics</li></div><div><li>org.apache.zeppelin.pig.PigQueryInterpreterTest.testMaxResult</li></div><div><li>org.apache.zeppelin.pig.PigQueryInterpreterTest.testBasics</li></div><div><li>org.apache.zeppelin.python.IPythonInterpreterTest.testIPythonPlotting</li></div><div><li>org.apache.zeppelin.spark.IPySparkInterpreterTest.testIPythonPlotting</li></div><div><li>org.apache.zeppelin.spark.IPySparkInterpreterTest.testPythonBasics</li></div><div><li>org.apache.zeppelin.spark.PySparkInterpreterTest.testPythonBasics</li></div><div><li>org.apache.zeppelin.rest.KnoxRestApiTest.testThatOtherUserCanAccessNoteIfPermissionNotSet</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.zeppelin.pig.PigInterpreterTest.testIncludeJobStats</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTest.testBasics</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTezTest.testIncludeJobStats</li></div><div><li>org.apache.zeppelin.pig.PigInterpreterTezTest.testBasics</li></div><div><li>org.apache.zeppelin.pig.PigQueryInterpreterTest.testMaxResult</li></div><div><li>org.apache.zeppelin.pig.PigQueryInterpreterTest.testBasics</li></div><div><li>org.apache.zeppelin.python.IPythonInterpreterTest.testIPythonPlotting</li></div><div><li>org.apache.zeppelin.spark.DepInterpreterTest.testDefault</li></div><div><li>org.apache.zeppelin.spark.IPySparkInterpreterTest.testIPythonPlotting</li></div><div><li>org.apache.zeppelin.rest.KnoxRestApiTest.testThatOtherUserCanAccessNoteIfPermissionNotSet</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>org.apache.hadoop.yarn.api.records.LocalResource.setShouldBeUploadedToSharedCache(Z)V</li></div><div><li>org.apache.hadoop.yarn.api.records.LocalResource.setShouldBeUploadedToSharedCache(Z)V</li></div><div><li>expected:&lt;TABLE&gt; but was:&lt;TEXT&gt;</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>No Figure Text</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>No Figure Text</li></div><div><li>expected:&lt;[你好]
&gt; but was:&lt;[??]
&gt;</li></div><div><li>expected:&lt;[你好]
&gt; but was:&lt;[??]
&gt;</li></div><div><li>java.lang.NullPointerException: null
	at org.apache.zeppelin.rest.KnoxRestApiTest.testThatOtherUserCanAccessNoteIfPermissionNotSet(KnoxRestApiTest.java:80)
</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>org.apache.hadoop.yarn.api.records.LocalResource.setShouldBeUploadedToSharedCache(Z)V</li></div><div><li>org.apache.hadoop.yarn.api.records.LocalResource.setShouldBeUploadedToSharedCache(Z)V</li></div><div><li>expected:&lt;TABLE&gt; but was:&lt;TEXT&gt;</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>No Figure Text</li></div><div><li>No Figure Text</li></div><div><li>expected:&lt;[你好]
&gt; but was:&lt;[??]
&gt;</li></div><div><li>expected:&lt;[你好]
&gt; but was:&lt;[??]
&gt;</li></div><div><li>java.lang.NullPointerException: null
	at org.apache.zeppelin.rest.KnoxRestApiTest.testThatOtherUserCanAccessNoteIfPermissionNotSet(KnoxRestApiTest.java:80)
</li></div><div><li>org.apache.thrift.transport.TTransportException</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div><div><li>org.apache.thrift.transport.TTransportException</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div><div><li>org.apache.thrift.transport.TTransportException</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>org.apache.hadoop.yarn.api.records.LocalResource.setShouldBeUploadedToSharedCache(Z)V</li></div><div><li>org.apache.hadoop.yarn.api.records.LocalResource.setShouldBeUploadedToSharedCache(Z)V</li></div><div><li>expected:&lt;TABLE&gt; but was:&lt;TEXT&gt;</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>No Figure Text</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>No Figure Text</li></div><div><li>expected:&lt;[你好]
&gt; but was:&lt;[??]
&gt;</li></div><div><li>expected:&lt;[你好]
&gt; but was:&lt;[??]
&gt;</li></div><div><li>java.lang.AssertionError: null
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertNotNull(Assert.java:712)
	at org.junit.Assert.assertNotNull(Assert.java:722)
	at org.apache.zeppelin.interpreter.InterpreterOutputChangeWatcherTest.test(InterpreterOutputChangeWatcherTest.java:99)
</li></div><div><li>java.lang.NullPointerException: null
	at org.apache.zeppelin.rest.KnoxRestApiTest.testThatOtherUserCanAccessNoteIfPermissionNotSet(KnoxRestApiTest.java:80)
</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>org.apache.hadoop.yarn.api.records.LocalResource.setShouldBeUploadedToSharedCache(Z)V</li></div><div><li>org.apache.hadoop.yarn.api.records.LocalResource.setShouldBeUploadedToSharedCache(Z)V</li></div><div><li>expected:&lt;TABLE&gt; but was:&lt;TEXT&gt;</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>No Figure Text</li></div><div><li>No Figure Text</li></div><div><li>expected:&lt;[你好]
&gt; but was:&lt;[??]
&gt;</li></div><div><li>expected:&lt;[你好]
&gt; but was:&lt;[??]
&gt;</li></div><div><li>java.lang.NullPointerException: null
	at org.apache.zeppelin.rest.KnoxRestApiTest.testThatOtherUserCanAccessNoteIfPermissionNotSet(KnoxRestApiTest.java:80)
</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>org.apache.hadoop.yarn.api.records.LocalResource.setShouldBeUploadedToSharedCache(Z)V</li></div><div><li>org.apache.hadoop.yarn.api.records.LocalResource.setShouldBeUploadedToSharedCache(Z)V</li></div><div><li>expected:&lt;TABLE&gt; but was:&lt;TEXT&gt;</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>No Figure Text</li></div><div><li>java.lang.NullPointerException: null
	at org.sonatype.aether.impl.internal.DefaultRepositorySystem.resolveDependencies(DefaultRepositorySystem.java:352)
	at org.apache.zeppelin.spark.dep.SparkDependencyContext.fetchArtifactWithDep(SparkDependencyContext.java:171)
	at org.apache.zeppelin.spark.dep.SparkDependencyContext.fetch(SparkDependencyContext.java:121)
	at org.apache.zeppelin.spark.DepInterpr</li></div><div><li>No Figure Text</li></div><div><li>java.lang.NullPointerException: null
	at org.apache.zeppelin.rest.KnoxRestApiTest.testThatOtherUserCanAccessNoteIfPermissionNotSet(KnoxRestApiTest.java:80)
</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.zeppelin.spark.DepInterpreterTest.testDefault</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClusterMode[0]</div></li><li><div>org.apache.zeppelin.interpreter.SparkIntegrationTest.testLocalMode[0]</div></li><li><div>org.apache.zeppelin.interpreter.SparkIntegrationTest.testYarnClusterMode[3]</div></li><li><div>org.apache.zeppelin.interpreter.SparkIntegrationTest.testLocalMode[3]</div></li><li><div>org.apache.zeppelin.notebook.NotebookTest.testAutoRestartInterpreterAfterSchedule</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.zeppelin.spark.DepInterpreterTest.testDefault</div></li><li><div>org.apache.zeppelin.interpreter.InterpreterOutputChangeWatcherTest.test</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="zookeeper"><div style="font-weight:bold;" class="panel-heading">ZOOKEEPER<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Pravin)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>2c0168ad37b529447ac8cb1bf866d014f8a97981</div><div><b>Last Run: </b>27-04-2018 03:51 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC RHEL7</th><th>X86 RHEL7</th><th>PPC RHEL7.5</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 1138</div><div>Failed Count : 1</div><div>Skipped Count : 1</div></td><td><div>Total Count : 1137</div><div>Failed Count : 3</div><div>Skipped Count : 1</div></td><td><div>Total Count : 1135</div><div>Failed Count : 4</div><div>Skipped Count : 1</div></td><td><div>Total Count : 1135</div><div>Failed Count : 3</div><div>Skipped Count : 1</div></td><td><div>Total Count : 1127</div><div>Failed Count : 3</div><div>Skipped Count : 1</div></td></tr><tr><td>Result</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"><div><li>org.apache.zookeeper.server.quorum.QuorumPeerMainTest.testFailedTxnAsPartOfQuorumLoss</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.zookeeper.server.quorum.ReconfigRecoveryTest.testCurrentServersAreObserversInNextConfig</li></div><div><li>org.apache.zookeeper.server.quorum.StandaloneDisabledTest.startSingleServerTest</li></div><div><li>org.apache.zookeeper.test.WatchEventWhenAutoResetTest.testNodeDataChanged</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.zookeeper.server.quorum.Zab1_0Test.testNormalFollowerRunWithDiff</li></div><div><li>junit.framework.TestSuite.org.apache.zookeeper.server.quorum.auth.MiniKdcTest</li></div><div><li>junit.framework.TestSuite.org.apache.zookeeper.server.quorum.auth.QuorumKerberosAuthTest</li></div><div><li>junit.framework.TestSuite.org.apache.zookeeper.server.quorum.auth.QuorumKerberosHostBasedAuthTest</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>junit.framework.TestSuite.org.apache.zookeeper.server.quorum.auth.MiniKdcTest</li></div><div><li>junit.framework.TestSuite.org.apache.zookeeper.server.quorum.auth.QuorumKerberosAuthTest</li></div><div><li>junit.framework.TestSuite.org.apache.zookeeper.server.quorum.auth.QuorumKerberosHostBasedAuthTest</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.zookeeper.server.quorum.Zab1_0Test.testNormalObserverRun</li></div><div><li>org.apache.zookeeper.test.QuorumHammerTest.testHammerBasic</li></div><div><li>org.apache.zookeeper.test.WatchEventWhenAutoResetTest.testNodeDataChanged</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"><div><li>KeeperErrorCode = Session expired for /zk2</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>waiting for server 3 being up</li></div><div><li>Timeout occurred. Please note the time in the report does not reflect the time until the timeout.</li></div><div><li>expected:&lt;NodeDataChanged&gt; but was:&lt;NodeDeleted&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;4294967298&gt; but was:&lt;0&gt;</li></div><div><li>Unable to parse:includedir /etc/krb5.conf.d/</li></div><div><li>Unable to parse:includedir /etc/krb5.conf.d/</li></div><div><li>Unable to parse:includedir /etc/krb5.conf.d/</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Unable to parse:includedir /etc/krb5.conf.d/</li></div><div><li>Unable to parse:includedir /etc/krb5.conf.d/</li></div><div><li>Unable to parse:includedir /etc/krb5.conf.d/</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Timeout occurred. Please note the time in the report does not reflect the time until the timeout.</li></div><div><li>expected:&lt;1000&gt; but was:&lt;976&gt;</li></div><div><li>expected:&lt;NodeDataChanged&gt; but was:&lt;NodeDeleted&gt;</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.zookeeper.server.quorum.QuorumPeerMainTest.testFailedTxnAsPartOfQuorumLoss</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.zookeeper.server.quorum.ReconfigRecoveryTest.testCurrentServersAreObserversInNextConfig</div></li><li><div>org.apache.zookeeper.server.quorum.StandaloneDisabledTest.startSingleServerTest</div></li><li><div>org.apache.zookeeper.test.WatchEventWhenAutoResetTest.testNodeDataChanged</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.zookeeper.server.quorum.Zab1_0Test.testNormalFollowerRunWithDiff</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div id="ppcubuntu16" style="font-weight:bold;font-size:12;display:none" class="panel panel-info" name="summary"><div class="panel-heading"><div class="panel-title">PPC UBUNTU16 SUMMARY</div></div><div class="bs-callout bs-callout-info"><div><b>OS: </b>UBUNTU 16.04</div></div><table style="font-size:14" class="table table-striped"><tbody><tr><th>Package Name</th><th>Result</th><th></th><th></th></tr><tr><td><a href="#" id="anchor_accumulo" onclick="showme(this.id);">ACCUMULO</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_ambari" onclick="showme(this.id);">AMBARI</a></td><td><img src="resources/red.png" align="top" style="width: 16px; height: 16px;" title="FAILURE"></img>0</td></tr><tr><td><a href="#" id="anchor_atlas" onclick="showme(this.id);">ATLAS</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td></tr><tr><td><a href="#" id="anchor_falcon" onclick="showme(this.id);">FALCON</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_flume" onclick="showme(this.id);">FLUME</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_hadoop" onclick="showme(this.id);">HADOOP</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>21 (6)</td></tr><tr><td><a href="#" id="anchor_hbase" onclick="showme(this.id);">HBASE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td></tr><tr><td><a href="#" id="anchor_hive" onclick="showme(this.id);">HIVE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>13 (2)</td></tr><tr><td><a href="#" id="anchor_kafka" onclick="showme(this.id);">KAFKA</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_knox" onclick="showme(this.id);">KNOX</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_metron" onclick="showme(this.id);">METRON</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_oozie" onclick="showme(this.id);">OOZIE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>30 (28)</td></tr><tr><td><a href="#" id="anchor_phoenix" onclick="showme(this.id);">PHOENIX</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_pig" onclick="showme(this.id);">PIG</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>20</td></tr><tr><td><a href="#" id="anchor_ranger" onclick="showme(this.id);">RANGER</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_slider" onclick="showme(this.id);">SLIDER</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>24</td></tr><tr><td><a href="#" id="anchor_spark" onclick="showme(this.id);">SPARK</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td></tr><tr><td><a href="#" id="anchor_sqoop" onclick="showme(this.id);">SQOOP</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_storm" onclick="showme(this.id);">STORM</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_tez" onclick="showme(this.id);">TEZ</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (1)</td></tr><tr><td><a href="#" id="anchor_zeppelin" onclick="showme(this.id);">ZEPPELIN</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>12 (1)</td></tr><tr><td><a href="#" id="anchor_zookeeper" onclick="showme(this.id);">ZOOKEEPER</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td></tr></tbody></table></div><div id="x86ubuntu16" style="font-weight:bold;font-size:12;display:none" class="panel panel-info" name="summary"><div class="panel-heading"><div class="panel-title">X86 UBUNTU16 SUMMARY</div></div><div class="bs-callout bs-callout-info"><div><b>OS: </b>UBUNTU 16.04</div></div><table style="font-size:14" class="table table-striped"><tbody><tr><th>Package Name</th><th>Result</th><th></th><th></th></tr><tr><td><a href="#" id="anchor_accumulo" onclick="showme(this.id);">ACCUMULO</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (2)</td></tr><tr><td><a href="#" id="anchor_ambari" onclick="showme(this.id);">AMBARI</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_atlas" onclick="showme(this.id);">ATLAS</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_falcon" onclick="showme(this.id);">FALCON</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td></tr><tr><td><a href="#" id="anchor_flume" onclick="showme(this.id);">FLUME</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_hadoop" onclick="showme(this.id);">HADOOP</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>31 (16)</td></tr><tr><td><a href="#" id="anchor_hbase" onclick="showme(this.id);">HBASE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td></tr><tr><td><a href="#" id="anchor_hive" onclick="showme(this.id);">HIVE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>12 (1)</td></tr><tr><td><a href="#" id="anchor_kafka" onclick="showme(this.id);">KAFKA</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td></tr><tr><td><a href="#" id="anchor_knox" onclick="showme(this.id);">KNOX</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td></tr><tr><td><a href="#" id="anchor_metron" onclick="showme(this.id);">METRON</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_oozie" onclick="showme(this.id);">OOZIE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>5 (3)</td></tr><tr><td><a href="#" id="anchor_phoenix" onclick="showme(this.id);">PHOENIX</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_pig" onclick="showme(this.id);">PIG</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>20</td></tr><tr><td><a href="#" id="anchor_ranger" onclick="showme(this.id);">RANGER</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_slider" onclick="showme(this.id);">SLIDER</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>24</td></tr><tr><td><a href="#" id="anchor_spark" onclick="showme(this.id);">SPARK</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (2)</td></tr><tr><td><a href="#" id="anchor_sqoop" onclick="showme(this.id);">SQOOP</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_storm" onclick="showme(this.id);">STORM</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td></tr><tr><td><a href="#" id="anchor_tez" onclick="showme(this.id);">TEZ</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td></tr><tr><td><a href="#" id="anchor_zeppelin" onclick="showme(this.id);">ZEPPELIN</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>16 (5)</td></tr><tr><td><a href="#" id="anchor_zookeeper" onclick="showme(this.id);">ZOOKEEPER</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3 (3)</td></tr></tbody></table></div><div id="ppcrhel75" style="font-weight:bold;font-size:12;display:none" class="panel panel-info" name="summary"><div class="panel-heading"><div class="panel-title">PPC RHEL75 SUMMARY</div></div><div class="bs-callout bs-callout-info"><div><b>OS: </b>RHEL 7.5</div></div><table style="font-size:14" class="table table-striped"><tbody><tr><th>Package Name</th><th>Result</th><th></th><th></th></tr><tr><td><a href="#" id="anchor_accumulo" onclick="showme(this.id);">ACCUMULO</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td></tr><tr><td><a href="#" id="anchor_ambari" onclick="showme(this.id);">AMBARI</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>5</td></tr><tr><td><a href="#" id="anchor_atlas" onclick="showme(this.id);">ATLAS</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_falcon" onclick="showme(this.id);">FALCON</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_flume" onclick="showme(this.id);">FLUME</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td></tr><tr><td><a href="#" id="anchor_hadoop" onclick="showme(this.id);">HADOOP</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>639</td></tr><tr><td><a href="#" id="anchor_hbase" onclick="showme(this.id);">HBASE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td></tr><tr><td><a href="#" id="anchor_hive" onclick="showme(this.id);">HIVE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3</td></tr><tr><td><a href="#" id="anchor_kafka" onclick="showme(this.id);">KAFKA</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_knox" onclick="showme(this.id);">KNOX</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_metron" onclick="showme(this.id);">METRON</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td></tr><tr><td><a href="#" id="anchor_oozie" onclick="showme(this.id);">OOZIE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3</td></tr><tr><td><a href="#" id="anchor_phoenix" onclick="showme(this.id);">PHOENIX</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_pig" onclick="showme(this.id);">PIG</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>6</td></tr><tr><td><a href="#" id="anchor_ranger" onclick="showme(this.id);">RANGER</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_slider" onclick="showme(this.id);">SLIDER</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_spark" onclick="showme(this.id);">SPARK</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_sqoop" onclick="showme(this.id);">SQOOP</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_storm" onclick="showme(this.id);">STORM</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td></tr><tr><td><a href="#" id="anchor_tez" onclick="showme(this.id);">TEZ</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>19</td></tr><tr><td><a href="#" id="anchor_zeppelin" onclick="showme(this.id);">ZEPPELIN</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>10</td></tr><tr><td><a href="#" id="anchor_zookeeper" onclick="showme(this.id);">ZOOKEEPER</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3</td></tr></tbody></table></div><div id="ppcrhel7" style="font-weight:bold;font-size:12;display:none" class="panel panel-info" name="summary"><div class="panel-heading"><div class="panel-title">PPC RHEL7 SUMMARY</div></div><div class="bs-callout bs-callout-info"><div><b>OS: </b>RHEL 7.2</div></div><table style="font-size:14" class="table table-striped"><tbody><tr><th>Package Name</th><th>Result</th><th></th><th></th></tr><tr><td><a href="#" id="anchor_accumulo" onclick="showme(this.id);">ACCUMULO</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_ambari" onclick="showme(this.id);">AMBARI</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_atlas" onclick="showme(this.id);">ATLAS</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_falcon" onclick="showme(this.id);">FALCON</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_flume" onclick="showme(this.id);">FLUME</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td></tr><tr><td><a href="#" id="anchor_hadoop" onclick="showme(this.id);">HADOOP</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>18 (14)</td></tr><tr><td><a href="#" id="anchor_hbase" onclick="showme(this.id);">HBASE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td></tr><tr><td><a href="#" id="anchor_hive" onclick="showme(this.id);">HIVE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3 (2)</td></tr><tr><td><a href="#" id="anchor_kafka" onclick="showme(this.id);">KAFKA</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td></tr><tr><td><a href="#" id="anchor_knox" onclick="showme(this.id);">KNOX</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_metron" onclick="showme(this.id);">METRON</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_oozie" onclick="showme(this.id);">OOZIE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td></tr><tr><td><a href="#" id="anchor_phoenix" onclick="showme(this.id);">PHOENIX</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_pig" onclick="showme(this.id);">PIG</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_ranger" onclick="showme(this.id);">RANGER</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_slider" onclick="showme(this.id);">SLIDER</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_spark" onclick="showme(this.id);">SPARK</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td></tr><tr><td><a href="#" id="anchor_sqoop" onclick="showme(this.id);">SQOOP</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_storm" onclick="showme(this.id);">STORM</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_tez" onclick="showme(this.id);">TEZ</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td></tr><tr><td><a href="#" id="anchor_zeppelin" onclick="showme(this.id);">ZEPPELIN</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>13 (2)</td></tr><tr><td><a href="#" id="anchor_zookeeper" onclick="showme(this.id);">ZOOKEEPER</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>4 (1)</td></tr></tbody></table></div><div id="x86rhel7" style="font-weight:bold;font-size:12;display:none" class="panel panel-info" name="summary"><div class="panel-heading"><div class="panel-title">X86 RHEL7 SUMMARY</div></div><div class="bs-callout bs-callout-info"><div><b>OS: </b>RHEL 7.2</div></div><table style="font-size:14" class="table table-striped"><tbody><tr><th>Package Name</th><th>Result</th><th></th><th></th></tr><tr><td><a href="#" id="anchor_accumulo" onclick="showme(this.id);">ACCUMULO</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_ambari" onclick="showme(this.id);">AMBARI</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_atlas" onclick="showme(this.id);">ATLAS</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_falcon" onclick="showme(this.id);">FALCON</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_flume" onclick="showme(this.id);">FLUME</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td></tr><tr><td><a href="#" id="anchor_hadoop" onclick="showme(this.id);">HADOOP</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>20 (16)</td></tr><tr><td><a href="#" id="anchor_hbase" onclick="showme(this.id);">HBASE</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_hive" onclick="showme(this.id);">HIVE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (1)</td></tr><tr><td><a href="#" id="anchor_kafka" onclick="showme(this.id);">KAFKA</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_knox" onclick="showme(this.id);">KNOX</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_metron" onclick="showme(this.id);">METRON</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_oozie" onclick="showme(this.id);">OOZIE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td></tr><tr><td><a href="#" id="anchor_phoenix" onclick="showme(this.id);">PHOENIX</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_pig" onclick="showme(this.id);">PIG</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_ranger" onclick="showme(this.id);">RANGER</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_slider" onclick="showme(this.id);">SLIDER</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_spark" onclick="showme(this.id);">SPARK</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td></tr><tr><td><a href="#" id="anchor_sqoop" onclick="showme(this.id);">SQOOP</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_storm" onclick="showme(this.id);">STORM</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (2)</td></tr><tr><td><a href="#" id="anchor_tez" onclick="showme(this.id);">TEZ</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td></tr><tr><td><a href="#" id="anchor_zeppelin" onclick="showme(this.id);">ZEPPELIN</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>11</td></tr><tr><td><a href="#" id="anchor_zookeeper" onclick="showme(this.id);">ZOOKEEPER</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3</td></tr></tbody></table></div><div id="ppcx86" style="display:block;font-weight:bold" class="panel panel-info" name="summary"><div class="panel-heading"><div class="panel-title">FULL SUMMARY</div></div><table style="font-size:15" id="summarytable" class="table table-striped"><tbody><tr><th></th></tr><tr><th>Package Name</th><th>PPC UBUNTU16</th><th>x86 UBUNTU16</th><th>PPC RHEL7</th><th>x86 RHEL7</th><th>PPC RHEL7.5</th><th>Owner</th></tr><tr><td><a href="#" id="anchor_accumulo" onclick="showme(this.id);">ACCUMULO</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (2)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td><td>Sonia</td></tr><tr><td><a href="#" id="anchor_ambari" onclick="showme(this.id);">AMBARI</a></td><td><img src="resources/red.png" align="top" style="width: 16px; height: 16px;" title="FAILURE"></img>0</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>5</td><td>Valencia</td></tr><tr><td><a href="#" id="anchor_atlas" onclick="showme(this.id);">ATLAS</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td>Yussuf</td></tr><tr><td><a href="#" id="anchor_falcon" onclick="showme(this.id);">FALCON</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td>Sonia</td></tr><tr><td><a href="#" id="anchor_flume" onclick="showme(this.id);">FLUME</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td><td>Pravin</td></tr><tr><td><a href="#" id="anchor_hadoop" onclick="showme(this.id);">HADOOP</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>21 (6)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>31 (16)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>18 (14)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>20 (16)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>639</td><td>Parita</td></tr><tr><td><a href="#" id="anchor_hbase" onclick="showme(this.id);">HBASE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td><td>Valencia</td></tr><tr><td><a href="#" id="anchor_hive" onclick="showme(this.id);">HIVE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>13 (2)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>12 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3 (2)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3</td><td>Alisha/Pravin</td></tr><tr><td><a href="#" id="anchor_kafka" onclick="showme(this.id);">KAFKA</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td>Sonia</td></tr><tr><td><a href="#" id="anchor_knox" onclick="showme(this.id);">KNOX</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td>Yussuf</td></tr><tr><td><a href="#" id="anchor_metron" onclick="showme(this.id);">METRON</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td><td>Pravin</td></tr><tr><td><a href="#" id="anchor_oozie" onclick="showme(this.id);">OOZIE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>30 (28)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>5 (3)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3</td><td>Alisha</td></tr><tr><td><a href="#" id="anchor_phoenix" onclick="showme(this.id);">PHOENIX</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td>Valencia</td></tr><tr><td><a href="#" id="anchor_pig" onclick="showme(this.id);">PIG</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>20</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>20</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>6</td><td>Yussuf</td></tr><tr><td><a href="#" id="anchor_ranger" onclick="showme(this.id);">RANGER</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td>Sneha</td></tr><tr><td><a href="#" id="anchor_slider" onclick="showme(this.id);">SLIDER</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>24</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>24</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td>Yussuf</td></tr><tr><td><a href="#" id="anchor_spark" onclick="showme(this.id);">SPARK</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (2)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td>Parita</td></tr><tr><td><a href="#" id="anchor_sqoop" onclick="showme(this.id);">SQOOP</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td>Talat</td></tr><tr><td><a href="#" id="anchor_storm" onclick="showme(this.id);">STORM</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (2)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td><td>Parita</td></tr><tr><td><a href="#" id="anchor_tez" onclick="showme(this.id);">TEZ</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>19</td><td>Valencia</td></tr><tr><td><a href="#" id="anchor_zeppelin" onclick="showme(this.id);">ZEPPELIN</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>12 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>16 (5)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>13 (2)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>11</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>10</td><td>Sneha</td></tr><tr><td><a href="#" id="anchor_zookeeper" onclick="showme(this.id);">ZOOKEEPER</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3 (3)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>4 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3</td><td>Pravin</td></tr></tbody></table></div></div></body></html>