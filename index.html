<html><head><script src="resources/jquery.min.js"></script><link href="resources/bootstrap.min.css" rel="stylesheet"></link><link href="resources/bootstrap-theme.min.css" rel="stylesheet"></link><script src="resources/bootstrap.min.js"></script><script src="helper.js">function hideAll(){console.log("hideAll")}function showme(e){console.log("showme");var l,n=e.substring(7),o=document.getElementsByName("data");for(l=0;l&lt;o.length;l++)o[l].style.display="none";var t=document.getElementsByName("summary");for(l=0;l&lt;t.length;l++)t[l].style.display="none";document.getElementById(n).style.display="block"}</script><style>table, th, td { vertical-align:top; padding: 3px} table {table-layout:fixed} td {word-wrap:break-word} .bs-callout { padding: 5px; margin: 5px 0; border: 1px solid #eee; border-left-width: 5px; border-radius: 3px; font-weight:normal; }.bs-callout-info {border-left-color: #5bc0de;}</style></head><body><nav class="navbar navbar-light"><div style="background-color: #F0F8FF;" class="container-fluid"><ul class="nav nav-pills"><li role="presentation"><a style="font-weight:bold" href="#" id="anchor_ppcx86" onclick="showme(this.id);">FULL SUMMARY</a></li><li role="presentation"><a style="font-weight:bold" href="#" id="anchor_ubuntu16" onclick="showme(this.id);">UBUNTU16</a></li><li role="presentation"><a style="font-weight:bold" href="#" id="anchor_ubuntu18" onclick="showme(this.id);">UBUNTU18</a></li><li role="presentation"><a style="font-weight:bold" href="#" id="anchor_rhel72" onclick="showme(this.id);">RHEL72</a></li><li role="presentation"><a style="font-weight:bold" href="#" id="anchor_rhel75" onclick="showme(this.id);">RHEL75</a></li><li role="presentation"><a style="font-weight:bold" href="#" id="anchor_developers" onclick="showme(this.id);">DEVELOPERS</a></li><p style="float:right;color:grey;font-size:13;padding-top:5px" role="presentation">17-11-2018 10:30 UTC</p></ul><div style="float:right;color:grey;font-size:12">Notations:<img src="resources/red.png" style="width: 16px; height: 16px;">Build failed </img><img src="resources/blue.png" style="width: 16px; height: 16px;">Build success with no failure </img><img src="resources/yellow.png" style="width: 16px; height: 16px;">N (M) Build success with N test failures &amp; M unique failures </img></div></div></nav><div style="table-cell" class="col-sm-2 col-md-2 sidebar"><div class="list-group"><a href="#" class="list-group-item list-group-item-action active" onclick="showme(this.id);" id="anchor_ppcx86">Packages</a><a class="list-group-item list-group-item-action" href="#" id="anchor_accumulo" onclick="showme(this.id);" title="Owned by Prajyot">ACCUMULO</a><a class="list-group-item list-group-item-action" href="#" id="anchor_ambari" onclick="showme(this.id);" title="Owned by Prajyot">AMBARI</a><a class="list-group-item list-group-item-action" href="#" id="anchor_atlas" onclick="showme(this.id);" title="Owned by Yussuf">ATLAS</a><a class="list-group-item list-group-item-action" href="#" id="anchor_calcite" onclick="showme(this.id);" title="Owned by Pravin">CALCITE</a><a class="list-group-item list-group-item-action" href="#" id="anchor_datafu" onclick="showme(this.id);" title="Owned by N/A">DATAFU</a><a class="list-group-item list-group-item-action" href="#" id="anchor_druid" onclick="showme(this.id);" title="Owned by N/A">DRUID</a><a class="list-group-item list-group-item-action" href="#" id="anchor_falcon" onclick="showme(this.id);" title="Owned by Yussuf">FALCON</a><a class="list-group-item list-group-item-action" href="#" id="anchor_flume" onclick="showme(this.id);" title="Owned by Pravin">FLUME</a><a class="list-group-item list-group-item-action" href="#" id="anchor_hadoop" onclick="showme(this.id);" title="Owned by Pravin">HADOOP</a><a class="list-group-item list-group-item-action" href="#" id="anchor_hbase" onclick="showme(this.id);" title="Owned by Prajyot">HBASE</a><a class="list-group-item list-group-item-action" href="#" id="anchor_hive" onclick="showme(this.id);" title="Owned by Alisha">HIVE</a><a class="list-group-item list-group-item-action" href="#" id="anchor_kafka" onclick="showme(this.id);" title="Owned by Prajyot">KAFKA</a><a class="list-group-item list-group-item-action" href="#" id="anchor_knox" onclick="showme(this.id);" title="Owned by Yussuf">KNOX</a><a class="list-group-item list-group-item-action" href="#" id="anchor_metron" onclick="showme(this.id);" title="Owned by Pravin">METRON</a><a class="list-group-item list-group-item-action" href="#" id="anchor_oozie" onclick="showme(this.id);" title="Owned by Alisha">OOZIE</a><a class="list-group-item list-group-item-action" href="#" id="anchor_phoenix" onclick="showme(this.id);" title="Owned by Prajyot">PHOENIX</a><a class="list-group-item list-group-item-action" href="#" id="anchor_pig" onclick="showme(this.id);" title="Owned by Yussuf">PIG</a><a class="list-group-item list-group-item-action" href="#" id="anchor_ranger" onclick="showme(this.id);" title="Owned by Yussuf">RANGER</a><a class="list-group-item list-group-item-action" href="#" id="anchor_slider" onclick="showme(this.id);" title="Owned by Yussuf">SLIDER</a><a class="list-group-item list-group-item-action" href="#" id="anchor_spark" onclick="showme(this.id);" title="Owned by Prajyot">SPARK</a><a class="list-group-item list-group-item-action" href="#" id="anchor_sqoop" onclick="showme(this.id);" title="Owned by Yussuf">SQOOP</a><a class="list-group-item list-group-item-action" href="#" id="anchor_storm" onclick="showme(this.id);" title="Owned by Alisha">STORM</a><a class="list-group-item list-group-item-action" href="#" id="anchor_tez" onclick="showme(this.id);" title="Owned by Prajyot">TEZ</a><a class="list-group-item list-group-item-action" href="#" id="anchor_zeppelin" onclick="showme(this.id);" title="Owned by Alisha">ZEPPELIN</a><a class="list-group-item list-group-item-action" href="#" id="anchor_zookeeper" onclick="showme(this.id);" title="Owned by Pravin">ZOOKEEPER</a></div></div><div style="display: table-cell"><div id="developers" style="display:block;font-weight:bold;display:none;" class="panel panel-info" name="summary"><div class="panel-heading"><div class="panel-title">DEVELOPERS</div></div><div class="panel-body"><table style="font-size:15" id="summarytable" class="table table-striped"><tr><td style="width: 100px;font-weight:bold">ALISHA</td><td><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_storm">STORM </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_zeppelin">ZEPPELIN </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_oozie">OOZIE </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_hive">HIVE </button></td></tr><tr><td style="width: 100px;font-weight:bold">PRAVIN</td><td><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_metron">METRON </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_zookeeper">ZOOKEEPER </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_hadoop">HADOOP </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_calcite">CALCITE </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_flume">FLUME </button></td></tr><tr><td style="width: 100px;font-weight:bold">PRAJYOT</td><td><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_kafka">KAFKA </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_tez">TEZ </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_hbase">HBASE </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_accumulo">ACCUMULO </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_phoenix">PHOENIX </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_spark">SPARK </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_ambari">AMBARI </button></td></tr><tr><td style="width: 100px;font-weight:bold">YUSSUF</td><td><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_knox">KNOX </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_atlas">ATLAS </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_sqoop">SQOOP </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_slider">SLIDER </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_falcon">FALCON </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_pig">PIG </button><button type="button" class="btn btn-link btn-xs" onclick="showme(this.id);" id="anchor_ranger">RANGER </button></td></tr></table></div></div></div><div style="display: table-cell"><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="accumulo"><div style="font-weight:bold;" class="panel-heading">ACCUMULO<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Prajyot)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>bdb6f15a488cec871b6c6ebbe7820bc13c9153e5</div><div><b>Last Run: </b>13-11-2018 20:26 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC UBUNTU18</th><th>X86 UBUNTU18</th><th>PPC RHEL72</th><th>X86 RHEL72</th><th>PPC RHEL75</th><th>X86 RHEL75</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 1716</div><div>Failed Count : 0</div><div>Skipped Count : 6</div></td><td><div>Total Count : 1716</div><div>Failed Count : 0</div><div>Skipped Count : 6</div></td><td><div>Total Count : 1716</div><div>Failed Count : 0</div><div>Skipped Count : 6</div></td><td><div>Total Count : 1716</div><div>Failed Count : 0</div><div>Skipped Count : 6</div></td><td><div>Total Count : 1716</div><div>Failed Count : 0</div><div>Skipped Count : 6</div></td><td><div>Total Count : 1716</div><div>Failed Count : 0</div><div>Skipped Count : 6</div></td><td><div>Total Count : 1716</div><div>Failed Count : 0</div><div>Skipped Count : 6</div></td><td><div>Total Count : 1716</div><div>Failed Count : 0</div><div>Skipped Count : 6</div></td></tr><tr><td>Result</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="ambari"><div style="font-weight:bold;" class="panel-heading">AMBARI<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Prajyot)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> origin/trunk</div><div><b>Last Revision: </b>d7ce1ef3668d551061fa0539fda7befdb6cb1b43</div><div><b>Last Run: </b>31-10-2018 00:31 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC UBUNTU18</th><th>X86 UBUNTU18</th><th>PPC RHEL72</th><th>X86 RHEL72</th><th>PPC RHEL75</th><th>X86 RHEL75</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 0</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 5192</div><div>Failed Count : 1</div><div>Skipped Count : 72</div></td><td><div>Total Count : 0</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 5192</div><div>Failed Count : 1</div><div>Skipped Count : 72</div></td><td><div>Total Count : 5191</div><div>Failed Count : 1</div><div>Skipped Count : 72</div></td><td><div>Total Count : 5192</div><div>Failed Count : 0</div><div>Skipped Count : 72</div></td><td><div>Total Count : 5191</div><div>Failed Count : 1</div><div>Skipped Count : 72</div></td><td><div>Total Count : 5192</div><div>Failed Count : 2</div><div>Skipped Count : 72</div></td></tr><tr><td>Result</td><td>N/A</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td>N/A</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.ambari.server.security.encryption.CredentialStoreTest.testInMemoryCredentialStoreService_CredentialExpired</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.ambari.server.api.services.AmbariMetaInfoTest.testLatestVdf</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.ambari.server.controller.metrics.RestMetricsPropertyProviderTest.testRestMetricsPropertyProviderAsAdministrator</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.ambari.server.orm.dao.AlertDispatchDAOTest.testFindNoticeByUuid</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.ambari.server.controller.test.BufferedThreadPoolExecutorCompletionServiceTest.testScalingThreadPoolExecutor</li></div><div><li>org.apache.ambari.server.state.cluster.ClusterDeadlockTest.testDeadlockWhileRestartingComponents</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>java.lang.NullPointerException
	at org.apache.ambari.server.security.encryption.CredentialStoreTest.getExpiredCredentialTest(CredentialStoreTest.java:169)
	at org.apache.ambari.server.security.encryption.CredentialStoreTest.testInMemoryCredentialStoreService_CredentialExpired(CredentialStoreTest.java:90)
</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>java.lang.AssertionError
	at org.apache.ambari.server.api.services.AmbariMetaInfoTest.testLatestVdf(AmbariMetaInfoTest.java:1436)
</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;400&gt; but was:&lt;157&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>Host not found, hostname=c6401.ambari.apache.org</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;10&gt; but was:&lt;9&gt;</li></div><div><li>[Deadlocked Thread:
------------------
"Thread-22" Id=54 WAITING on java.util.concurrent.locks.ReentrantReadWriteLock$NonfairSync@dbd5c6 owned by "Thread-26" Id=58
 at sun.misc.Unsafe.park(Native Method)
 -  waiting on java.util.concurrent.locks.ReentrantReadWriteLock$NonfairSync@dbd5c6
 at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
 at java.util.concurrent.locks.AbstractQue</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.ambari.server.security.encryption.CredentialStoreTest.testInMemoryCredentialStoreService_CredentialExpired</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.ambari.server.api.services.AmbariMetaInfoTest.testLatestVdf</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.ambari.server.controller.metrics.RestMetricsPropertyProviderTest.testRestMetricsPropertyProviderAsAdministrator</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.ambari.server.orm.dao.AlertDispatchDAOTest.testFindNoticeByUuid</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.ambari.server.controller.test.BufferedThreadPoolExecutorCompletionServiceTest.testScalingThreadPoolExecutor</div></li><li><div>org.apache.ambari.server.state.cluster.ClusterDeadlockTest.testDeadlockWhileRestartingComponents</div></li></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="atlas"><div style="font-weight:bold;" class="panel-heading">ATLAS<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Yussuf)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>93bd535eb66562ea8aadf5219d09a7893fff724e</div><div><b>Last Run: </b>24-10-2018 15:22 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC UBUNTU18</th><th>X86 UBUNTU18</th><th>PPC RHEL72</th><th>X86 RHEL72</th><th>PPC RHEL75</th><th>X86 RHEL75</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 956</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 956</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 956</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 956</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 956</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 956</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 956</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 956</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td></tr><tr><td>Result</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="calcite"><div style="font-weight:bold;" class="panel-heading">CALCITE<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Pravin)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>6284d3c88d9c659e0a8ef3dd5cf4819133de7522</div><div><b>Last Run: </b>29-10-2018 01:44 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC UBUNTU18</th><th>X86 UBUNTU18</th><th>PPC RHEL72</th><th>X86 RHEL72</th><th>PPC RHEL75</th><th>X86 RHEL75</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 5884</div><div>Failed Count : 0</div><div>Skipped Count : 179</div></td><td><div>Total Count : 5895</div><div>Failed Count : 0</div><div>Skipped Count : 179</div></td><td><div>Total Count : 5884</div><div>Failed Count : 0</div><div>Skipped Count : 179</div></td><td><div>Total Count : 5895</div><div>Failed Count : 0</div><div>Skipped Count : 179</div></td><td><div>Total Count : 5884</div><div>Failed Count : 0</div><div>Skipped Count : 179</div></td><td><div>Total Count : 5895</div><div>Failed Count : 0</div><div>Skipped Count : 179</div></td><td><div>Total Count : 5884</div><div>Failed Count : 0</div><div>Skipped Count : 179</div></td><td><div>Total Count : 5895</div><div>Failed Count : 0</div><div>Skipped Count : 179</div></td></tr><tr><td>Result</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="datafu"><div style="font-weight:bold;" class="panel-heading">DATAFU<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(N/A)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>857cf164c30883d739c4895c9a9c758880526435</div><div><b>Last Run: </b>12-11-2018 01:11 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC UBUNTU18</th><th>X86 UBUNTU18</th><th>PPC RHEL72</th><th>X86 RHEL72</th><th>PPC RHEL75</th><th>X86 RHEL75</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 278</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 278</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 278</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 278</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 278</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 278</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 278</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 278</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td></tr><tr><td>Result</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="druid"><div style="font-weight:bold;" class="panel-heading">DRUID<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(N/A)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>c780aacc03e30b929ba14f70d7f278811fd8ba44</div><div><b>Last Run: </b>17-10-2018 06:25 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC UBUNTU18</th><th>X86 UBUNTU18</th><th>PPC RHEL72</th><th>X86 RHEL72</th><th>PPC RHEL75</th><th>X86 RHEL75</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 90903</div><div>Failed Count : 6</div><div>Skipped Count : 91</div></td><td><div>Total Count : 90903</div><div>Failed Count : 2</div><div>Skipped Count : 91</div></td><td><div>Total Count : 90903</div><div>Failed Count : 6</div><div>Skipped Count : 91</div></td><td><div>Total Count : 90903</div><div>Failed Count : 2</div><div>Skipped Count : 91</div></td><td><div>Total Count : 90903</div><div>Failed Count : 6</div><div>Skipped Count : 91</div></td><td><div>Total Count : 90903</div><div>Failed Count : 2</div><div>Skipped Count : 91</div></td><td><div>Total Count : 90903</div><div>Failed Count : 6</div><div>Skipped Count : 91</div></td><td><div>Total Count : 90903</div><div>Failed Count : 2</div><div>Skipped Count : 91</div></td></tr><tr><td>Result</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"><div><li>org.apache.druid.java.util.metrics.MonitorsTest.testDefaultFeed</li></div><div><li>org.apache.druid.java.util.metrics.MonitorsTest.testSetFeed</li></div><div><li>org.apache.druid.java.util.metrics.SigarLoadTest.testSigarLoad</li></div><div><li>org.apache.druid.java.util.metrics.SigarPidDiscovererTest.simpleTest</li></div><div><li>org.apache.druid.storage.s3.TestAWSCredentialsProvider.testWithFileSessionCredentials</li></div><div><li>org.apache.druid.storage.s3.TestAWSCredentialsProvider.testWithFixedAWSKeys</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.druid.storage.s3.TestAWSCredentialsProvider.testWithFileSessionCredentials</li></div><div><li>org.apache.druid.storage.s3.TestAWSCredentialsProvider.testWithFixedAWSKeys</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.druid.java.util.metrics.MonitorsTest.testDefaultFeed</li></div><div><li>org.apache.druid.java.util.metrics.MonitorsTest.testSetFeed</li></div><div><li>org.apache.druid.java.util.metrics.SigarLoadTest.testSigarLoad</li></div><div><li>org.apache.druid.java.util.metrics.SigarPidDiscovererTest.simpleTest</li></div><div><li>org.apache.druid.storage.s3.TestAWSCredentialsProvider.testWithFileSessionCredentials</li></div><div><li>org.apache.druid.storage.s3.TestAWSCredentialsProvider.testWithFixedAWSKeys</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.druid.storage.s3.TestAWSCredentialsProvider.testWithFileSessionCredentials</li></div><div><li>org.apache.druid.storage.s3.TestAWSCredentialsProvider.testWithFixedAWSKeys</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.druid.java.util.metrics.MonitorsTest.testDefaultFeed</li></div><div><li>org.apache.druid.java.util.metrics.MonitorsTest.testSetFeed</li></div><div><li>org.apache.druid.java.util.metrics.SigarLoadTest.testSigarLoad</li></div><div><li>org.apache.druid.java.util.metrics.SigarPidDiscovererTest.simpleTest</li></div><div><li>org.apache.druid.storage.s3.TestAWSCredentialsProvider.testWithFileSessionCredentials</li></div><div><li>org.apache.druid.storage.s3.TestAWSCredentialsProvider.testWithFixedAWSKeys</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.druid.storage.s3.TestAWSCredentialsProvider.testWithFileSessionCredentials</li></div><div><li>org.apache.druid.storage.s3.TestAWSCredentialsProvider.testWithFixedAWSKeys</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.druid.java.util.metrics.MonitorsTest.testDefaultFeed</li></div><div><li>org.apache.druid.java.util.metrics.MonitorsTest.testSetFeed</li></div><div><li>org.apache.druid.java.util.metrics.SigarLoadTest.testSigarLoad</li></div><div><li>org.apache.druid.java.util.metrics.SigarPidDiscovererTest.simpleTest</li></div><div><li>org.apache.druid.storage.s3.TestAWSCredentialsProvider.testWithFileSessionCredentials</li></div><div><li>org.apache.druid.storage.s3.TestAWSCredentialsProvider.testWithFixedAWSKeys</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.druid.storage.s3.TestAWSCredentialsProvider.testWithFileSessionCredentials</li></div><div><li>org.apache.druid.storage.s3.TestAWSCredentialsProvider.testWithFixedAWSKeys</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"><div><li>org.hyperic.sigar.Sigar.getPid()J</li></div><div><li>org.hyperic.sigar.Sigar.getPid()J</li></div><div><li>org.hyperic.sigar.Sigar.getPid()J</li></div><div><li>org.hyperic.sigar.Sigar.getPid()J</li></div><div><li>Unable to find a region via the region provider chain. Must provide an explicit region in the builder or setup environment to supply a region.</li></div><div><li>Unable to find a region via the region provider chain. Must provide an explicit region in the builder or setup environment to supply a region.</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Unable to find a region via the region provider chain. Must provide an explicit region in the builder or setup environment to supply a region.</li></div><div><li>Unable to find a region via the region provider chain. Must provide an explicit region in the builder or setup environment to supply a region.</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.hyperic.sigar.Sigar.getPid()J</li></div><div><li>org.hyperic.sigar.Sigar.getPid()J</li></div><div><li>org.hyperic.sigar.Sigar.getPid()J</li></div><div><li>org.hyperic.sigar.Sigar.getPid()J</li></div><div><li>Unable to find a region via the region provider chain. Must provide an explicit region in the builder or setup environment to supply a region.</li></div><div><li>Unable to find a region via the region provider chain. Must provide an explicit region in the builder or setup environment to supply a region.</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Unable to find a region via the region provider chain. Must provide an explicit region in the builder or setup environment to supply a region.</li></div><div><li>Unable to find a region via the region provider chain. Must provide an explicit region in the builder or setup environment to supply a region.</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.hyperic.sigar.Sigar.getPid()J</li></div><div><li>org.hyperic.sigar.Sigar.getPid()J</li></div><div><li>org.hyperic.sigar.Sigar.getPid()J</li></div><div><li>org.hyperic.sigar.Sigar.getPid()J</li></div><div><li>Unable to find a region via the region provider chain. Must provide an explicit region in the builder or setup environment to supply a region.</li></div><div><li>Unable to find a region via the region provider chain. Must provide an explicit region in the builder or setup environment to supply a region.</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Unable to find a region via the region provider chain. Must provide an explicit region in the builder or setup environment to supply a region.</li></div><div><li>Unable to find a region via the region provider chain. Must provide an explicit region in the builder or setup environment to supply a region.</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.hyperic.sigar.Sigar.getPid()J</li></div><div><li>org.hyperic.sigar.Sigar.getPid()J</li></div><div><li>org.hyperic.sigar.Sigar.getPid()J</li></div><div><li>org.hyperic.sigar.Sigar.getPid()J</li></div><div><li>Unable to find a region via the region provider chain. Must provide an explicit region in the builder or setup environment to supply a region.</li></div><div><li>Unable to find a region via the region provider chain. Must provide an explicit region in the builder or setup environment to supply a region.</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Unable to find a region via the region provider chain. Must provide an explicit region in the builder or setup environment to supply a region.</li></div><div><li>Unable to find a region via the region provider chain. Must provide an explicit region in the builder or setup environment to supply a region.</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.druid.java.util.metrics.MonitorsTest.testDefaultFeed</div></li><li><div>org.apache.druid.java.util.metrics.MonitorsTest.testSetFeed</div></li><li><div>org.apache.druid.java.util.metrics.SigarLoadTest.testSigarLoad</div></li><li><div>org.apache.druid.java.util.metrics.SigarPidDiscovererTest.simpleTest</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.druid.java.util.metrics.MonitorsTest.testDefaultFeed</div></li><li><div>org.apache.druid.java.util.metrics.MonitorsTest.testSetFeed</div></li><li><div>org.apache.druid.java.util.metrics.SigarLoadTest.testSigarLoad</div></li><li><div>org.apache.druid.java.util.metrics.SigarPidDiscovererTest.simpleTest</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.druid.java.util.metrics.MonitorsTest.testDefaultFeed</div></li><li><div>org.apache.druid.java.util.metrics.MonitorsTest.testSetFeed</div></li><li><div>org.apache.druid.java.util.metrics.SigarLoadTest.testSigarLoad</div></li><li><div>org.apache.druid.java.util.metrics.SigarPidDiscovererTest.simpleTest</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.druid.java.util.metrics.MonitorsTest.testDefaultFeed</div></li><li><div>org.apache.druid.java.util.metrics.MonitorsTest.testSetFeed</div></li><li><div>org.apache.druid.java.util.metrics.SigarLoadTest.testSigarLoad</div></li><li><div>org.apache.druid.java.util.metrics.SigarPidDiscovererTest.simpleTest</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="falcon"><div style="font-weight:bold;" class="panel-heading">FALCON<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Yussuf)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>470e5e9f5de9ba1b6149dec60e87d3a04270eda3</div><div><b>Last Run: </b>07-11-2018 03:27 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC UBUNTU18</th><th>X86 UBUNTU18</th><th>PPC RHEL72</th><th>X86 RHEL72</th><th>PPC RHEL75</th><th>X86 RHEL75</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 1000</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 1002</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 1002</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 1001</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 1000</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 997</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 1000</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td><td><div>Total Count : 994</div><div>Failed Count : 0</div><div>Skipped Count : 0</div></td></tr><tr><td>Result</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="flume"><div style="font-weight:bold;" class="panel-heading">FLUME<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Pravin)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/trunk</div><div><b>Last Revision: </b>fb16ffc076de9917be7585e5583344bf3fd1c62a</div><div><b>Last Run: </b>13-11-2018 01:48 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC UBUNTU18</th><th>X86 UBUNTU18</th><th>PPC RHEL72</th><th>X86 RHEL72</th><th>PPC RHEL75</th><th>X86 RHEL75</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 1312</div><div>Failed Count : 0</div><div>Skipped Count : 7</div></td><td><div>Total Count : 1312</div><div>Failed Count : 0</div><div>Skipped Count : 7</div></td><td><div>Total Count : 1279</div><div>Failed Count : 1</div><div>Skipped Count : 7</div></td><td><div>Total Count : 1312</div><div>Failed Count : 0</div><div>Skipped Count : 7</div></td><td><div>Total Count : 1311</div><div>Failed Count : 0</div><div>Skipped Count : 7</div></td><td><div>Total Count : 1312</div><div>Failed Count : 0</div><div>Skipped Count : 7</div></td><td><div>Total Count : 1311</div><div>Failed Count : 0</div><div>Skipped Count : 7</div></td><td><div>Total Count : 1312</div><div>Failed Count : 0</div><div>Skipped Count : 7</div></td></tr><tr><td>Result</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.flume.channel.file.TestLog.testMinimumRequiredSpaceTooSmallForPut</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>java.lang.AssertionError
	at org.apache.flume.channel.file.TestLog.doTestMinimumRequiredSpaceTooSmallForPut(TestLog.java:241)
	at org.apache.flume.channel.file.TestLog.testMinimumRequiredSpaceTooSmallForPut(TestLog.java:210)
</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.flume.channel.file.TestLog.testMinimumRequiredSpaceTooSmallForPut</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="hadoop"><div style="font-weight:bold;" class="panel-heading">HADOOP<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Pravin)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/trunk</div><div><b>Last Revision: </b>52cb766ad0bbea378a529fcff3cf372cf0a33cde</div><div><b>Last Run: </b>22-10-2018 13:31 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC UBUNTU18</th><th>X86 UBUNTU18</th><th>PPC RHEL72</th><th>X86 RHEL72</th><th>PPC RHEL75</th><th>X86 RHEL75</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 20268</div><div>Failed Count : 67</div><div>Skipped Count : 1186</div></td><td><div>Total Count : 19903</div><div>Failed Count : 61</div><div>Skipped Count : 1186</div></td><td><div>Total Count : 19374</div><div>Failed Count : 69</div><div>Skipped Count : 1140</div></td><td><div>Total Count : 19029</div><div>Failed Count : 71</div><div>Skipped Count : 1138</div></td><td><div>Total Count : 19926</div><div>Failed Count : 212</div><div>Skipped Count : 1185</div></td><td><div>Total Count : 19904</div><div>Failed Count : 63</div><div>Skipped Count : 1185</div></td><td><div>Total Count : 20273</div><div>Failed Count : 60</div><div>Skipped Count : 1186</div></td><td><div>Total Count : 19904</div><div>Failed Count : 68</div><div>Skipped Count : 1185</div></td></tr><tr><td>Result</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider.testTokenServiceCreationWithLegacyFormat</li></div><div><li>org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider.testTokenServiceCreationWithUriFormat</li></div><div><li>org.apache.hadoop.util.TestShell.testEnvVarsWithInheritance</li></div><div><li>org.apache.hadoop.hdfs.server.federation.router.TestRouterClientRejectOverload.testOverloadControl</li></div><div><li>org.apache.hadoop.hdfs.TestLeaseRecovery2.testCloseWhileRecoverLease</li></div><div><li>org.apache.hadoop.hdfs.TestReadStripedFileWithMissingBlocks.testReadFileWithMissingBlocks</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.TestNNHandlesBlockReportPerStorage.blockReport_08</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestLazyPersistReplicaRecovery.testDnRestartWithSavedReplicas</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestSpaceReservation.testReservedSpaceForLeaseRecovery</li></div><div><li>org.apache.hadoop.hdfs.server.namenode.TestPersistentStoragePolicySatisfier.testSPSxAttrWhenSpsCalledForDir</li></div><div><li>org.apache.hadoop.mapreduce.v2.hs.server.TestHSAdminServer.testRefreshSuperUserGroups[0]</li></div><div><li>org.apache.hadoop.mapreduce.v2.hs.server.TestHSAdminServer.testRefreshSuperUserGroups[1]</li></div><div><li>org.apache.hadoop.tools.TestDistCh.testDistCh</li></div><div><li>org.apache.hadoop.mapred.gridmix.TestDistCacheEmulation.org.apache.hadoop.mapred.gridmix.TestDistCacheEmulation</li></div><div><li>org.apache.hadoop.mapred.gridmix.TestGridmixSubmission.org.apache.hadoop.mapred.gridmix.TestGridmixSubmission</li></div><div><li>org.apache.hadoop.mapred.gridmix.TestLoadJob.org.apache.hadoop.mapred.gridmix.TestLoadJob</li></div><div><li>org.apache.hadoop.mapred.gridmix.TestSleepJob.org.apache.hadoop.mapred.gridmix.TestSleepJob</li></div><div><li>org.apache.hadoop.streaming.TestFileArgs.testCommandLine</li></div><div><li>org.apache.hadoop.streaming.TestMultipleArchiveFiles.testCommandLine</li></div><div><li>org.apache.hadoop.streaming.TestMultipleCachefiles.testMultipleCachefiles</li></div><div><li>org.apache.hadoop.streaming.TestStreamingBadRecords.testNoOp</li></div><div><li>org.apache.hadoop.streaming.TestSymLink.testSymLink</li></div><div><li>org.apache.hadoop.yarn.service.monitor.probe.TestDefaultProbe.testDefaultProbe[2]</li></div><div><li>org.apache.hadoop.yarn.util.TestProcfsBasedProcessTree.testProcessTree</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.testContainerUpgradeSuccessExplicitRollback</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestLogAggregationService.testFixedSizeThreadPool</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.TestApplicationMasterServiceFair.testUpdateTrackingUrl</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService.testRefreshAclWithDaemonUser</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService.testRMInitialsWithFileSystemBasedConfigurationProvider</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService.testAdminAclsWithFileSystemBasedConfigurationProvider</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.reservation.TestCapacityOverTimePolicy.testAllocation[Duration 90,000,000, height 0.25, numSubmission 1, periodic 86400000)]</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestContainerAllocation.testUserLimitAllocationMultipleContainers</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestQueueManagementDynamicEditPolicy.testEditSchedule</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.security.TestDelegationTokenRenewer.testRMRestartWithExpiredToken</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesMetricFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowsNotPresent</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRunsMetricsToRetrieve</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlows</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetAppsMetricsRange</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesInfoFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testForFlowRunsPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntityWithoutFlowInfo</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesMetricsTimeRange</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowApps</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRuns</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetAppWithoutFlowInfo</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRunAppsNotPresent</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRun</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntityDataToRetrieve</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesEventFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testUIDQueryWithAndWithoutFlowContextInfo</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowAppsFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesByUID</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowAppsNotPresent</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowsForPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testForFlowAppsPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesConfigFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesDataToRetrieve</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testUIDNotProperlyEscaped</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRunNotPresent</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRunApps</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesRelationFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetApp</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesWithoutFlowInfo</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGenericEntitiesForPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testForFlowRunAppsPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetAppNotPresent</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider.testTokenServiceCreationWithLegacyFormat</li></div><div><li>org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider.testTokenServiceCreationWithUriFormat</li></div><div><li>org.apache.hadoop.util.TestShell.testEnvVarsWithInheritance</li></div><div><li>org.apache.hadoop.hdfs.TestLeaseRecovery2.testCloseWhileRecoverLease</li></div><div><li>org.apache.hadoop.hdfs.TestLeaseRecovery2.testHardLeaseRecoveryWithRenameAfterNameNodeRestart</li></div><div><li>org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFSStriped.testRead</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestLazyPersistReplicaRecovery.testDnRestartWithSavedReplicas</li></div><div><li>org.apache.hadoop.hdfs.server.namenode.ha.TestHAAppend.testMultipleAppendsDuringCatchupTailing</li></div><div><li>org.apache.hadoop.mapreduce.v2.hs.server.TestHSAdminServer.testRefreshSuperUserGroups[0]</li></div><div><li>org.apache.hadoop.mapreduce.v2.hs.server.TestHSAdminServer.testRefreshSuperUserGroups[1]</li></div><div><li>org.apache.hadoop.tools.TestDistCh.testDistCh</li></div><div><li>org.apache.hadoop.mapred.gridmix.TestDistCacheEmulation.org.apache.hadoop.mapred.gridmix.TestDistCacheEmulation</li></div><div><li>org.apache.hadoop.mapred.gridmix.TestGridmixSubmission.org.apache.hadoop.mapred.gridmix.TestGridmixSubmission</li></div><div><li>org.apache.hadoop.mapred.gridmix.TestLoadJob.org.apache.hadoop.mapred.gridmix.TestLoadJob</li></div><div><li>org.apache.hadoop.mapred.gridmix.TestSleepJob.org.apache.hadoop.mapred.gridmix.TestSleepJob</li></div><div><li>org.apache.hadoop.streaming.TestFileArgs.testCommandLine</li></div><div><li>org.apache.hadoop.streaming.TestMultipleArchiveFiles.testCommandLine</li></div><div><li>org.apache.hadoop.streaming.TestMultipleCachefiles.testMultipleCachefiles</li></div><div><li>org.apache.hadoop.streaming.TestStreamingBadRecords.testNoOp</li></div><div><li>org.apache.hadoop.streaming.TestSymLink.testSymLink</li></div><div><li>org.apache.hadoop.yarn.service.monitor.probe.TestDefaultProbe.testDefaultProbe[2]</li></div><div><li>org.apache.hadoop.yarn.util.TestProcfsBasedProcessTree.testProcessTree</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.testLocalingResourceWhileContainerRunning</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService.testRefreshAclWithDaemonUser</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService.testRMInitialsWithFileSystemBasedConfigurationProvider</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService.testAdminAclsWithFileSystemBasedConfigurationProvider</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestContainerAllocation.testUserLimitAllocationMultipleContainers</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestQueueManagementDynamicEditPolicy.testEditSchedule</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesMetricFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowsNotPresent</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRunsMetricsToRetrieve</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlows</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetAppsMetricsRange</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesInfoFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testForFlowRunsPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntityWithoutFlowInfo</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesMetricsTimeRange</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowApps</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRuns</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetAppWithoutFlowInfo</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRunAppsNotPresent</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRun</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntityDataToRetrieve</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesEventFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testUIDQueryWithAndWithoutFlowContextInfo</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowAppsFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesByUID</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowAppsNotPresent</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowsForPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testForFlowAppsPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesConfigFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesDataToRetrieve</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testUIDNotProperlyEscaped</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRunNotPresent</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRunApps</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesRelationFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetApp</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesWithoutFlowInfo</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGenericEntitiesForPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testForFlowRunAppsPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetAppNotPresent</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.io.compress.TestCompressorDecompressor.testCompressorDecompressor</li></div><div><li>org.apache.hadoop.io.compress.TestCompressorDecompressor.testCompressorDecompressorWithExeedBufferLimit</li></div><div><li>org.apache.hadoop.io.compress.snappy.TestSnappyCompressorDecompressor.testSnappyCompressDecompressInMultiThreads</li></div><div><li>org.apache.hadoop.io.compress.snappy.TestSnappyCompressorDecompressor.testSnappyCompressDecompress</li></div><div><li>org.apache.hadoop.ipc.TestRPCWaitForProxy.testInterruptedWaitForProxy</li></div><div><li>org.apache.hadoop.util.TestShell.testEnvVarsWithInheritance</li></div><div><li>org.apache.hadoop.hdfs.TestLeaseRecovery2.testCloseWhileRecoverLease</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.TestDataNodeUUID.testUUIDRegeneration</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testAuthUrlConnectTimeout[timeoutSource=ConnectionFactory]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testConnectTimeout[timeoutSource=ConnectionFactory]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testRedirectConnectTimeout[timeoutSource=ConnectionFactory]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testTwoStepWriteConnectTimeout[timeoutSource=ConnectionFactory]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testAuthUrlConnectTimeout[timeoutSource=Configuration]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testConnectTimeout[timeoutSource=Configuration]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testRedirectConnectTimeout[timeoutSource=Configuration]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testTwoStepWriteConnectTimeout[timeoutSource=Configuration]</li></div><div><li>org.apache.hadoop.mapreduce.v2.hs.server.TestHSAdminServer.testRefreshSuperUserGroups[0]</li></div><div><li>org.apache.hadoop.mapreduce.v2.hs.server.TestHSAdminServer.testRefreshSuperUserGroups[1]</li></div><div><li>org.apache.hadoop.tools.TestDistCh.testDistCh</li></div><div><li>org.apache.hadoop.mapred.gridmix.TestDistCacheEmulation.org.apache.hadoop.mapred.gridmix.TestDistCacheEmulation</li></div><div><li>org.apache.hadoop.mapred.gridmix.TestGridmixSubmission.org.apache.hadoop.mapred.gridmix.TestGridmixSubmission</li></div><div><li>org.apache.hadoop.mapred.gridmix.TestLoadJob.org.apache.hadoop.mapred.gridmix.TestLoadJob</li></div><div><li>org.apache.hadoop.mapred.gridmix.TestSleepJob.org.apache.hadoop.mapred.gridmix.TestSleepJob</li></div><div><li>org.apache.hadoop.streaming.TestFileArgs.testCommandLine</li></div><div><li>org.apache.hadoop.streaming.TestMultipleArchiveFiles.testCommandLine</li></div><div><li>org.apache.hadoop.streaming.TestMultipleCachefiles.testMultipleCachefiles</li></div><div><li>org.apache.hadoop.streaming.TestStreamingBadRecords.testNoOp</li></div><div><li>org.apache.hadoop.streaming.TestSymLink.testSymLink</li></div><div><li>org.apache.hadoop.yarn.service.monitor.probe.TestDefaultProbe.testDefaultProbe[2]</li></div><div><li>org.apache.hadoop.yarn.util.TestProcfsBasedProcessTree.testProcessTree</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService.testRefreshAclWithDaemonUser</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService.testRMInitialsWithFileSystemBasedConfigurationProvider</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService.testAdminAclsWithFileSystemBasedConfigurationProvider</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestContainerAllocation.testUserLimitAllocationMultipleContainers</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestQueueManagementDynamicEditPolicy.testEditSchedule</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.security.TestRMDelegationTokens.testRMDTMasterKeyStateOnRollingMasterKey</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesMetricFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowsNotPresent</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRunsMetricsToRetrieve</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlows</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetAppsMetricsRange</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesInfoFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testForFlowRunsPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntityWithoutFlowInfo</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesMetricsTimeRange</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowApps</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRuns</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetAppWithoutFlowInfo</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRunAppsNotPresent</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRun</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntityDataToRetrieve</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesEventFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testUIDQueryWithAndWithoutFlowContextInfo</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowAppsFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesByUID</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowAppsNotPresent</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowsForPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testForFlowAppsPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesConfigFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesDataToRetrieve</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testUIDNotProperlyEscaped</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRunNotPresent</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRunApps</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesRelationFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetApp</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesWithoutFlowInfo</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGenericEntitiesForPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testForFlowRunAppsPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetAppNotPresent</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider.testTokenServiceCreationWithLegacyFormat</li></div><div><li>org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider.testTokenServiceCreationWithUriFormat</li></div><div><li>org.apache.hadoop.io.compress.TestCompressorDecompressor.testCompressorDecompressor</li></div><div><li>org.apache.hadoop.io.compress.TestCompressorDecompressor.testCompressorDecompressorWithExeedBufferLimit</li></div><div><li>org.apache.hadoop.io.compress.snappy.TestSnappyCompressorDecompressor.testSnappyCompressDecompressInMultiThreads</li></div><div><li>org.apache.hadoop.io.compress.snappy.TestSnappyCompressorDecompressor.testSnappyCompressDecompress</li></div><div><li>org.apache.hadoop.util.TestShell.testEnvVarsWithInheritance</li></div><div><li>org.apache.hadoop.fs.contract.router.web.TestRouterWebHDFSContractAppend.testRenameFileBeingAppended</li></div><div><li>org.apache.hadoop.hdfs.server.federation.router.TestRouterAllResolver.testSpaceAll</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailure.testDataNodeShutdownAfterNumFailedVolumeExceedsTolerated</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestLazyPersistReplicaRecovery.testDnRestartWithSavedReplicas</li></div><div><li>org.apache.hadoop.hdfs.server.namenode.TestDecommissioningStatus.testDecommissionStatus</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testAuthUrlConnectTimeout[timeoutSource=ConnectionFactory]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testConnectTimeout[timeoutSource=ConnectionFactory]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testRedirectConnectTimeout[timeoutSource=ConnectionFactory]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testTwoStepWriteConnectTimeout[timeoutSource=ConnectionFactory]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testAuthUrlConnectTimeout[timeoutSource=Configuration]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testConnectTimeout[timeoutSource=Configuration]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testRedirectConnectTimeout[timeoutSource=Configuration]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testTwoStepWriteConnectTimeout[timeoutSource=Configuration]</li></div><div><li>org.apache.hadoop.mapreduce.v2.hs.server.TestHSAdminServer.testRefreshSuperUserGroups[0]</li></div><div><li>org.apache.hadoop.mapreduce.v2.hs.server.TestHSAdminServer.testRefreshSuperUserGroups[1]</li></div><div><li>org.apache.hadoop.tools.TestDistCh.testDistCh</li></div><div><li>org.apache.hadoop.mapred.gridmix.TestDistCacheEmulation.org.apache.hadoop.mapred.gridmix.TestDistCacheEmulation</li></div><div><li>org.apache.hadoop.mapred.gridmix.TestGridmixSubmission.org.apache.hadoop.mapred.gridmix.TestGridmixSubmission</li></div><div><li>org.apache.hadoop.mapred.gridmix.TestLoadJob.org.apache.hadoop.mapred.gridmix.TestLoadJob</li></div><div><li>org.apache.hadoop.mapred.gridmix.TestSleepJob.org.apache.hadoop.mapred.gridmix.TestSleepJob</li></div><div><li>org.apache.hadoop.streaming.TestFileArgs.testCommandLine</li></div><div><li>org.apache.hadoop.streaming.TestMultipleArchiveFiles.testCommandLine</li></div><div><li>org.apache.hadoop.streaming.TestMultipleCachefiles.testMultipleCachefiles</li></div><div><li>org.apache.hadoop.streaming.TestStreamingBadRecords.testNoOp</li></div><div><li>org.apache.hadoop.streaming.TestSymLink.testSymLink</li></div><div><li>org.apache.hadoop.yarn.util.TestProcfsBasedProcessTree.testProcessTree</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService.testRefreshAclWithDaemonUser</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService.testRMInitialsWithFileSystemBasedConfigurationProvider</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService.testAdminAclsWithFileSystemBasedConfigurationProvider</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.rmapp.TestRMAppTransitions.testAppSuccessPath[1]</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestContainerAllocation.testUserLimitAllocationMultipleContainers</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesMetricFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowsNotPresent</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRunsMetricsToRetrieve</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlows</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetAppsMetricsRange</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesInfoFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testForFlowRunsPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntityWithoutFlowInfo</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesMetricsTimeRange</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowApps</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRuns</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetAppWithoutFlowInfo</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRunAppsNotPresent</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRun</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntityDataToRetrieve</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesEventFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testUIDQueryWithAndWithoutFlowContextInfo</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowAppsFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesByUID</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowAppsNotPresent</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowsForPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testForFlowAppsPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesConfigFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesDataToRetrieve</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testUIDNotProperlyEscaped</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRunNotPresent</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRunApps</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesRelationFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetApp</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesWithoutFlowInfo</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGenericEntitiesForPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testForFlowRunAppsPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetAppNotPresent</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider.testCreation</li></div><div><li>org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider.testTokenServiceCreationWithLegacyFormat</li></div><div><li>org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider.testTokenServiceCreationWithUriFormat</li></div><div><li>org.apache.hadoop.util.TestShell.testEnvVarsWithInheritance</li></div><div><li>org.apache.hadoop.hdfs.TestDFSShell.testCopyCommandsWithPreserveOption</li></div><div><li>org.apache.hadoop.hdfs.TestLeaseRecovery2.testHardLeaseRecoveryAfterNameNodeRestart2</li></div><div><li>org.apache.hadoop.hdfs.TestReadStripedFileWithMissingBlocks.testReadFileWithMissingBlocks</li></div><div><li>org.apache.hadoop.hdfs.server.balancer.TestBalancerRPCDelay.testBalancerRPCDelay</li></div><div><li>org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.TestInMemoryLevelDBAliasMapClient.writeRead</li></div><div><li>org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.TestInMemoryLevelDBAliasMapClient.iterateThreeBatches</li></div><div><li>org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.TestInMemoryLevelDBAliasMapClient.multipleReads</li></div><div><li>org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.TestInMemoryLevelDBAliasMapClient.iterateSingleBatch</li></div><div><li>org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.TestInMemoryLevelDBAliasMapClient.testServerBindHost</li></div><div><li>org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.TestInMemoryLevelDBAliasMapClient.testNonExistentBlock</li></div><div><li>org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.TestLevelDBFileRegionAliasMap.testReadBack</li></div><div><li>org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.TestLevelDBFileRegionAliasMap.testIterate</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting.testHotSwapOutFailedVolumeAndReporting</li></div><div><li>org.apache.hadoop.hdfs.server.namenode.TestNameNodeMXBean.testDecommissioningNodes</li></div><div><li>org.apache.hadoop.hdfs.tools.offlineImageViewer.TestOfflineImageViewer.testPBDelimitedWriter</li></div><div><li>org.apache.hadoop.mapreduce.v2.hs.TestHistoryServerLeveldbStateStoreService.testTokenStore</li></div><div><li>org.apache.hadoop.mapreduce.v2.hs.TestHistoryServerLeveldbStateStoreService.testCheckVersion</li></div><div><li>org.apache.hadoop.mapreduce.v2.hs.server.TestHSAdminServer.testRefreshSuperUserGroups[0]</li></div><div><li>org.apache.hadoop.mapreduce.v2.hs.server.TestHSAdminServer.testRefreshSuperUserGroups[1]</li></div><div><li>org.apache.hadoop.mapred.TestClusterMRNotification.testMR</li></div><div><li>org.apache.hadoop.mapred.TestShuffleHandler.testRecoveryFromOtherVersions</li></div><div><li>org.apache.hadoop.mapred.TestShuffleHandler.testRecovery</li></div><div><li>org.apache.hadoop.tools.TestDistCh.testDistCh</li></div><div><li>org.apache.hadoop.mapred.gridmix.TestDistCacheEmulation.org.apache.hadoop.mapred.gridmix.TestDistCacheEmulation</li></div><div><li>org.apache.hadoop.mapred.gridmix.TestGridmixSubmission.org.apache.hadoop.mapred.gridmix.TestGridmixSubmission</li></div><div><li>org.apache.hadoop.mapred.gridmix.TestLoadJob.org.apache.hadoop.mapred.gridmix.TestLoadJob</li></div><div><li>org.apache.hadoop.mapred.gridmix.TestSleepJob.org.apache.hadoop.mapred.gridmix.TestSleepJob</li></div><div><li>org.apache.hadoop.streaming.TestFileArgs.testCommandLine</li></div><div><li>org.apache.hadoop.streaming.TestMultipleArchiveFiles.testCommandLine</li></div><div><li>org.apache.hadoop.streaming.TestMultipleCachefiles.testMultipleCachefiles</li></div><div><li>org.apache.hadoop.streaming.TestStreamingBadRecords.testNoOp</li></div><div><li>org.apache.hadoop.streaming.TestSymLink.testSymLink</li></div><div><li>org.apache.hadoop.yarn.applications.distributedshell.TestDistributedShell.testDSShellWithDomainV1_5</li></div><div><li>org.apache.hadoop.yarn.applications.distributedshell.TestDistributedShell.testDSShellWithoutDomainV1_5</li></div><div><li>org.apache.hadoop.yarn.service.monitor.probe.TestDefaultProbe.testDefaultProbe[2]</li></div><div><li>org.apache.hadoop.yarn.util.TestProcfsBasedProcessTree.testProcessTree</li></div><div><li>org.apache.hadoop.yarn.server.applicationhistoryservice.TestApplicationHistoryServer.testLaunchWithArguments</li></div><div><li>org.apache.hadoop.yarn.server.applicationhistoryservice.TestApplicationHistoryServer.testLaunch</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testGetDomains</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testRelatingToNonExistingEntity</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testValidateConfig</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testGetEntitiesWithPrimaryFilters</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testRelatingToOldEntityWithoutDomainId</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testRootDirPermission</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testGetSingleEntity</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testDeleteEntitiesPrimaryFilters</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testDeleteEntities</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testCacheSizes</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testGetEntities</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testGetEntityTypes</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testFromTsWithDeletion</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testLevelDbRepair</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testGetDomain</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testGetEvents</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testGetEntitiesWithSecondaryFilters</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testCheckVersion</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testGetEntitiesWithFromId</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testGetEntitiesWithFromTs</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDB.testInsertForPreviousPeriodAfterRollPeriodRollsDB</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDB.testInsertAfterRollPeriodRollsDB</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testGetDomains</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testGetDomains</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testRelatingToNonExistingEntity</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testRelatingToNonExistingEntity</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testValidateConfig</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testValidateConfig</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testGetEntitiesWithPrimaryFilters</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testGetEntitiesWithPrimaryFilters</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testRelatingToOldEntityWithoutDomainId</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testRelatingToOldEntityWithoutDomainId</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testRootDirPermission</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testRootDirPermission</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testGetSingleEntity</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testGetSingleEntity</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testCacheSizes</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testCacheSizes</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testGetEntities</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testGetEntities</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testRelatingToEntityInSamePut</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testRelatingToEntityInSamePut</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testGetDomain</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testGetDomain</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testGetEvents</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testGetEvents</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testGetEntitiesWithSecondaryFilters</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testGetEntitiesWithSecondaryFilters</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testCheckVersion</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testCheckVersion</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testGetEntitiesWithFromId</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testGetEntitiesWithFromId</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testGetEntitiesWithFromTs</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testGetEntitiesWithFromTs</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestTimelineDataManager.testGetOldEntitiesWithOutDomainId</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestTimelineDataManager.testGetEntitiesAclEnabled</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestTimelineDataManager.testGetOldEntityWithOutDomainId</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestTimelineDataManager.testUpdatingOldEntityWithoutDomainId</li></div><div><li>org.apache.hadoop.yarn.server.timeline.recovery.TestLeveldbTimelineStateStore.testTokenStore</li></div><div><li>org.apache.hadoop.yarn.server.timeline.recovery.TestLeveldbTimelineStateStore.testCheckVersion</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerShutdown.testStateStoreRemovalOnDecommission</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerShutdown.testStateStoreRemovalOnDecommission</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.testContainerUpgradeSuccessExplicitRollback</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestLogAggregationService.testFixedSizeThreadPool</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService.testAMRMProxyStorage</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService.testEmptyRestartTimes</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService.testEmptyState</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService.testStateStoreNodeHealth</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService.testContainerStorage</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService.testFinishResourceLocalization</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService.testNMTokenStorage</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService.testCompactionCycle</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService.testLogDeleterStorage</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService.testDeletionTaskStorage</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService.testContainerTokenStorage</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService.testLocalTrackerStateIterator</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService.testStateStoreForResourceMapping</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService.testStartResourceLocalization</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService.testApplicationStorage</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService.testCheckVersion</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService.testRemoveLocalizedResource</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService.testIsNewlyCreated</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService.testUnexpectedKeyDoesntThrowException</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService.testRefreshAclWithDaemonUser</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService.testRMInitialsWithFileSystemBasedConfigurationProvider</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService.testAdminAclsWithFileSystemBasedConfigurationProvider</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.metrics.TestCombinedSystemMetricsPublisher.testTimelineServiceEventPublishingNoService</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.metrics.TestCombinedSystemMetricsPublisher.testTimelineServiceEventPublishingV2Enabled</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.recovery.TestLeveldbRMStateStore.testRemoveAttempt</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.recovery.TestLeveldbRMStateStore.testBadKeyIteration</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.recovery.TestLeveldbRMStateStore.testDeleteStore</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.recovery.TestLeveldbRMStateStore.testEpoch</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.recovery.TestLeveldbRMStateStore.testApps</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.recovery.TestLeveldbRMStateStore.testProxyCA</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.recovery.TestLeveldbRMStateStore.testClientTokens</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.recovery.TestLeveldbRMStateStore.testVersion</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.recovery.TestLeveldbRMStateStore.testAMTokens</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.recovery.TestLeveldbRMStateStore.testAppDeletion</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.recovery.TestLeveldbRMStateStore.testRemoveApplication</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.recovery.TestLeveldbRMStateStore.testReservation</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestContainerAllocation.testUserLimitAllocationMultipleContainers</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.TestLeveldbConfigurationStore.testVersioning</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.TestLeveldbConfigurationStore.testPersistConfiguration</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.TestLeveldbConfigurationStore.testMaxLogs</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.TestLeveldbConfigurationStore.testRestartReadsFromUpdatedStore</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.TestLeveldbConfigurationStore.testPersistUpdatedConfiguration</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.TestLeveldbConfigurationStore.testConfigurationUpdate</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.TestLeveldbConfigurationStore.testNullConfigurationUpdate</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.TestPlacementConstraintsUtil.testInterAppConstraintsByAppID</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.security.TestRMDelegationTokens.testRMDTMasterKeyStateOnRollingMasterKey</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestEntityGroupFSTimelineStore.testSummaryRead</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestEntityGroupFSTimelineStore.testAppLogsScanLogs</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestEntityGroupFSTimelineStore.testPluginRead</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestEntityGroupFSTimelineStore.testCleanLogs</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestEntityGroupFSTimelineStore.testGetEntityPluginRead</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestEntityGroupFSTimelineStore.testScanActiveLogsAndMoveToDonePluginRead</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestEntityGroupFSTimelineStore.testParseSummaryLogs</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestEntityGroupFSTimelineStore.testMoveToDone</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestLevelDBCacheTimelineStore.testGetDomains</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestLevelDBCacheTimelineStore.testGetDomains</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestLevelDBCacheTimelineStore.testGetEntitiesWithPrimaryFilters</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestLevelDBCacheTimelineStore.testGetEntitiesWithPrimaryFilters</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestLevelDBCacheTimelineStore.testGetSingleEntity</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestLevelDBCacheTimelineStore.testGetSingleEntity</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestLevelDBCacheTimelineStore.testGetEntities</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestLevelDBCacheTimelineStore.testGetEntities</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestLevelDBCacheTimelineStore.testGetDomain</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestLevelDBCacheTimelineStore.testGetDomain</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestLevelDBCacheTimelineStore.testGetEvents</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestLevelDBCacheTimelineStore.testGetEvents</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestLevelDBCacheTimelineStore.testGetEntitiesWithSecondaryFilters</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestLevelDBCacheTimelineStore.testGetEntitiesWithSecondaryFilters</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestLevelDBCacheTimelineStore.testGetEntitiesWithFromId</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestLevelDBCacheTimelineStore.testGetEntitiesWithFromId</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestLevelDBCacheTimelineStore.testGetEntitiesWithFromTs</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestLevelDBCacheTimelineStore.testGetEntitiesWithFromTs</li></div><div><li>org.apache.hadoop.yarn.server.timeline.TestOverrideTimelineStoreYarnClient.testLifecycleAndOverride</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesMetricFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowsNotPresent</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRunsMetricsToRetrieve</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlows</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetAppsMetricsRange</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesInfoFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testForFlowRunsPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntityWithoutFlowInfo</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesMetricsTimeRange</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowApps</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRuns</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetAppWithoutFlowInfo</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRunAppsNotPresent</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRun</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntityDataToRetrieve</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesEventFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testUIDQueryWithAndWithoutFlowContextInfo</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowAppsFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesByUID</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowAppsNotPresent</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowsForPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testForFlowAppsPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesConfigFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesDataToRetrieve</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testUIDNotProperlyEscaped</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRunNotPresent</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRunApps</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesRelationFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetApp</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesWithoutFlowInfo</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGenericEntitiesForPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testForFlowRunAppsPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetAppNotPresent</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider.testTokenServiceCreationWithLegacyFormat</li></div><div><li>org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider.testTokenServiceCreationWithUriFormat</li></div><div><li>org.apache.hadoop.util.TestShell.testEnvVarsWithInheritance</li></div><div><li>org.apache.hadoop.hdfs.TestDistributedFileSystem.testDFSClientPeerWriteTimeout</li></div><div><li>org.apache.hadoop.hdfs.TestLeaseRecovery2.testCloseWhileRecoverLease</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestLazyPersistReplicaRecovery.testDnRestartWithSavedReplicas</li></div><div><li>org.apache.hadoop.mapreduce.v2.hs.server.TestHSAdminServer.testRefreshSuperUserGroups[0]</li></div><div><li>org.apache.hadoop.mapreduce.v2.hs.server.TestHSAdminServer.testRefreshSuperUserGroups[1]</li></div><div><li>org.apache.hadoop.tools.TestDistCh.testDistCh</li></div><div><li>org.apache.hadoop.mapred.gridmix.TestDistCacheEmulation.org.apache.hadoop.mapred.gridmix.TestDistCacheEmulation</li></div><div><li>org.apache.hadoop.mapred.gridmix.TestGridmixSubmission.org.apache.hadoop.mapred.gridmix.TestGridmixSubmission</li></div><div><li>org.apache.hadoop.mapred.gridmix.TestLoadJob.org.apache.hadoop.mapred.gridmix.TestLoadJob</li></div><div><li>org.apache.hadoop.mapred.gridmix.TestSleepJob.org.apache.hadoop.mapred.gridmix.TestSleepJob</li></div><div><li>org.apache.hadoop.streaming.TestFileArgs.testCommandLine</li></div><div><li>org.apache.hadoop.streaming.TestMultipleArchiveFiles.testCommandLine</li></div><div><li>org.apache.hadoop.streaming.TestMultipleCachefiles.testMultipleCachefiles</li></div><div><li>org.apache.hadoop.streaming.TestStreamingBadRecords.testNoOp</li></div><div><li>org.apache.hadoop.streaming.TestSymLink.testSymLink</li></div><div><li>org.apache.hadoop.yarn.service.monitor.probe.TestDefaultProbe.testDefaultProbe[2]</li></div><div><li>org.apache.hadoop.yarn.client.api.impl.TestAMRMProxy.testAMRMProxyTokenRenewal</li></div><div><li>org.apache.hadoop.yarn.util.TestProcfsBasedProcessTree.testProcessTree</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.TestContainerLaunch.testKillProcessGroup</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.TestContainersMonitor.testContainerKillOnMemoryOverflow</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.TestApplicationMasterServiceFair.testUpdateTrackingUrl</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService.testRefreshAclWithDaemonUser</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService.testRMInitialsWithFileSystemBasedConfigurationProvider</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService.testAdminAclsWithFileSystemBasedConfigurationProvider</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestContainerAllocation.testUserLimitAllocationMultipleContainers</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestIncreaseAllocationExpirer.testContainerIncreaseAllocationExpiration</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestQueueManagementDynamicEditPolicy.testEditSchedule</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesMetricFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowsNotPresent</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRunsMetricsToRetrieve</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlows</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetAppsMetricsRange</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesInfoFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testForFlowRunsPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntityWithoutFlowInfo</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesMetricsTimeRange</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowApps</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRuns</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetAppWithoutFlowInfo</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRunAppsNotPresent</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRun</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntityDataToRetrieve</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesEventFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testUIDQueryWithAndWithoutFlowContextInfo</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowAppsFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesByUID</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowAppsNotPresent</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowsForPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testForFlowAppsPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesConfigFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesDataToRetrieve</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testUIDNotProperlyEscaped</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRunNotPresent</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRunApps</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesRelationFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetApp</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesWithoutFlowInfo</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGenericEntitiesForPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testForFlowRunAppsPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetAppNotPresent</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.util.TestShell.testEnvVarsWithInheritance</li></div><div><li>org.apache.hadoop.hdfs.server.blockmanagement.TestUnderReplicatedBlocks.testSetRepIncWithUnderReplicatedBlocks</li></div><div><li>org.apache.hadoop.hdfs.server.datanode.TestDataNodeErasureCodingMetrics.testReconstructionBytesPartialGroup2</li></div><div><li>org.apache.hadoop.mapreduce.v2.hs.server.TestHSAdminServer.testRefreshSuperUserGroups[0]</li></div><div><li>org.apache.hadoop.mapreduce.v2.hs.server.TestHSAdminServer.testRefreshSuperUserGroups[1]</li></div><div><li>org.apache.hadoop.mapred.TestMRTimelineEventHandling.testMapreduceJobTimelineServiceEnabled</li></div><div><li>org.apache.hadoop.mapreduce.v2.TestUberAM.testJobWithChangePriority</li></div><div><li>org.apache.hadoop.tools.TestDistCh.testDistCh</li></div><div><li>org.apache.hadoop.mapred.gridmix.TestDistCacheEmulation.org.apache.hadoop.mapred.gridmix.TestDistCacheEmulation</li></div><div><li>org.apache.hadoop.mapred.gridmix.TestGridmixSubmission.org.apache.hadoop.mapred.gridmix.TestGridmixSubmission</li></div><div><li>org.apache.hadoop.mapred.gridmix.TestLoadJob.org.apache.hadoop.mapred.gridmix.TestLoadJob</li></div><div><li>org.apache.hadoop.mapred.gridmix.TestSleepJob.org.apache.hadoop.mapred.gridmix.TestSleepJob</li></div><div><li>org.apache.hadoop.yarn.sls.TestSLSRunner.testSimulatorRunning[Testing with: SYNTH, org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler, (nodeFile null)]</li></div><div><li>org.apache.hadoop.streaming.TestFileArgs.testCommandLine</li></div><div><li>org.apache.hadoop.streaming.TestMultipleArchiveFiles.testCommandLine</li></div><div><li>org.apache.hadoop.streaming.TestMultipleCachefiles.testMultipleCachefiles</li></div><div><li>org.apache.hadoop.streaming.TestStreamingBadRecords.testNoOp</li></div><div><li>org.apache.hadoop.streaming.TestSymLink.testSymLink</li></div><div><li>org.apache.hadoop.yarn.service.monitor.probe.TestDefaultProbe.testDefaultProbe[2]</li></div><div><li>org.apache.hadoop.yarn.util.TestProcfsBasedProcessTree.testProcessTree</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.TestContainerLaunch.testKillProcessGroup</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.TestContainersMonitor.testContainerKillOnMemoryOverflow</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService.testRefreshAclWithDaemonUser</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService.testRMInitialsWithFileSystemBasedConfigurationProvider</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService.testAdminAclsWithFileSystemBasedConfigurationProvider</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestContainerAllocation.testUserLimitAllocationMultipleContainers</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestQueueManagementDynamicEditPolicy.testEditSchedule</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesMetricFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowsNotPresent</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRunsMetricsToRetrieve</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlows</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetAppsMetricsRange</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesInfoFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testForFlowRunsPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntityWithoutFlowInfo</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesMetricsTimeRange</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowApps</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRuns</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetAppWithoutFlowInfo</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRunAppsNotPresent</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRun</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntityDataToRetrieve</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesEventFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testUIDQueryWithAndWithoutFlowContextInfo</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowAppsFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesByUID</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowAppsNotPresent</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowsForPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testForFlowAppsPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesConfigFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesDataToRetrieve</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testUIDNotProperlyEscaped</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRunNotPresent</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRunApps</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesRelationFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetApp</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesWithoutFlowInfo</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGenericEntitiesForPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testForFlowRunAppsPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetAppNotPresent</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider.testTokenServiceCreationWithLegacyFormat</li></div><div><li>org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider.testTokenServiceCreationWithUriFormat</li></div><div><li>org.apache.hadoop.util.TestShell.testEnvVarsWithInheritance</li></div><div><li>org.apache.hadoop.hdfs.TestDistributedFileSystem.testDFSClientPeerWriteTimeout</li></div><div><li>org.apache.hadoop.hdfs.TestEncryptedTransfer.testEncryptedAppendRequiringBlockTransfer[1]</li></div><div><li>org.apache.hadoop.hdfs.server.blockmanagement.TestUnderReplicatedBlocks.testSetRepIncWithUnderReplicatedBlocks</li></div><div><li>org.apache.hadoop.hdfs.server.namenode.sps.TestBlockStorageMovementAttemptedItems.testNoBlockMovementAttemptFinishedReportAdded</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testAuthUrlConnectTimeout[timeoutSource=ConnectionFactory]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testConnectTimeout[timeoutSource=ConnectionFactory]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testRedirectConnectTimeout[timeoutSource=ConnectionFactory]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testTwoStepWriteConnectTimeout[timeoutSource=ConnectionFactory]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testAuthUrlConnectTimeout[timeoutSource=Configuration]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testConnectTimeout[timeoutSource=Configuration]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testRedirectConnectTimeout[timeoutSource=Configuration]</li></div><div><li>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testTwoStepWriteConnectTimeout[timeoutSource=Configuration]</li></div><div><li>org.apache.hadoop.mapreduce.v2.hs.server.TestHSAdminServer.testRefreshSuperUserGroups[0]</li></div><div><li>org.apache.hadoop.mapreduce.v2.hs.server.TestHSAdminServer.testRefreshSuperUserGroups[1]</li></div><div><li>org.apache.hadoop.tools.TestDistCh.testDistCh</li></div><div><li>org.apache.hadoop.mapred.gridmix.TestDistCacheEmulation.org.apache.hadoop.mapred.gridmix.TestDistCacheEmulation</li></div><div><li>org.apache.hadoop.mapred.gridmix.TestGridmixSubmission.org.apache.hadoop.mapred.gridmix.TestGridmixSubmission</li></div><div><li>org.apache.hadoop.mapred.gridmix.TestLoadJob.org.apache.hadoop.mapred.gridmix.TestLoadJob</li></div><div><li>org.apache.hadoop.mapred.gridmix.TestSleepJob.org.apache.hadoop.mapred.gridmix.TestSleepJob</li></div><div><li>org.apache.hadoop.streaming.TestFileArgs.testCommandLine</li></div><div><li>org.apache.hadoop.streaming.TestMultipleArchiveFiles.testCommandLine</li></div><div><li>org.apache.hadoop.streaming.TestMultipleCachefiles.testMultipleCachefiles</li></div><div><li>org.apache.hadoop.streaming.TestStreamingBadRecords.testNoOp</li></div><div><li>org.apache.hadoop.streaming.TestSymLink.testSymLink</li></div><div><li>org.apache.hadoop.yarn.util.TestProcfsBasedProcessTree.testProcessTree</li></div><div><li>org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.testLocalingResourceWhileContainerRunning</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService.testRefreshAclWithDaemonUser</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService.testRMInitialsWithFileSystemBasedConfigurationProvider</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService.testAdminAclsWithFileSystemBasedConfigurationProvider</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.rmapp.TestRMAppTransitions.testAppSuccessPath[1]</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacitySchedulerSchedulingRequestUpdate.testBasicPendingResourceUpdate</li></div><div><li>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestContainerAllocation.testUserLimitAllocationMultipleContainers</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesMetricFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowsNotPresent</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRunsMetricsToRetrieve</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlows</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetAppsMetricsRange</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesInfoFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testForFlowRunsPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntityWithoutFlowInfo</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesMetricsTimeRange</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowApps</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRuns</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetAppWithoutFlowInfo</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRunAppsNotPresent</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRun</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntityDataToRetrieve</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesEventFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testUIDQueryWithAndWithoutFlowContextInfo</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowAppsFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesByUID</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowAppsNotPresent</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowsForPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testForFlowAppsPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesConfigFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesDataToRetrieve</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testUIDNotProperlyEscaped</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRunNotPresent</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetFlowRunApps</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesRelationFilters</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetApp</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesWithoutFlowInfo</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGenericEntitiesForPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testForFlowRunAppsPagination</li></div><div><li>org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage.testGetAppNotPresent</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;host1[]:9600&gt; but was:&lt;host1[.persistent.co.in.]:9600&gt;</li></div><div><li>expected:&lt;host1[]:9600&gt; but was:&lt;host1[.persistent.co.in.]:9600&gt;</li></div><div><li>String index out of range: -1</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div><div><li> Expected to find 'whereas it is under recovery' but got unexpected exception: java.io.IOException: Unable to close file because the last blockBP-1173887286-172.17.0.2-1540242846856:blk_1073741826_1002 does not have enough number of replicas.
 at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:964)
 at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.jav</li></div><div><li>test timed out after 300000 milliseconds</li></div><div><li>Was waiting too long for a replica to become TEMPORARY</li></div><div><li>Timed out waiting for condition. Thread diagnostics:
Timestamp: 2018-10-22 05:08:28,666

"org.eclipse.jetty.server.session.HashSessionManager@3fc86c08Timer" daemon prio=5 tid=33 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.Abs</li></div><div><li>2</li></div><div><li>Timed out waiting for condition. Thread diagnostics:
Timestamp: 2018-10-22 04:33:36,770

"qtp1055415357-2122" daemon prio=5 tid=2122 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.mapreduce.v2.hs.server.TestHSAdminServer.testRefreshSuperUserGroups(TestHSAdminServer.java:208)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeM</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.mapreduce.v2.hs.server.TestHSAdminServer.testRefreshSuperUserGroups(TestHSAdminServer.java:208)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeM</li></div><div><li>org/bouncycastle/jce/provider/BouncyCastleProvider</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/jce/provider/BouncyCastleProvider</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>Expected success for Probe Status, time="Tue Oct 23 04:07:49 UTC 2018", outcome="failure", message="Failure in Default probe: IP presence with DNS checking and DNS server address 8.8.8.8", exception="java.io.IOException: comp-0: DNS checking is enabled, but lookup for example.com is not available yet"</li></div><div><li>Child process owned by init escaped process tree.</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.doRestartTests(TestContainerManager.java:482)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.testContainerUpgradeSuc</li></div><div><li>expected:&lt;3&gt; but was:&lt;2&gt;</li></div><div><li>expected:&lt;[hadoop.apache.org]&gt; but was:&lt;[N/A]&gt;</li></div><div><li>expected:&lt;jenkins[xyz,jenkins]&gt; but was:&lt;jenkins[,jenkinsxyz]&gt;</li></div><div><li>expected:&lt;[jenkins,world:anyone:rwcda]&gt; but was:&lt;[world:anyone:rwcda,jenkins]&gt;</li></div><div><li>expected:&lt;[jenkins,world:anyone:rwcda]&gt; but was:&lt;[world:anyone:rwcda,jenkins]&gt;</li></div><div><li>junit.framework.AssertionFailedError
	at junit.framework.Assert.fail(Assert.java:55)
	at junit.framework.Assert.fail(Assert.java:64)
	at junit.framework.TestCase.fail(TestCase.java:235)
	at org.apache.hadoop.yarn.server.resourcemanager.reservation.BaseSharingPolicyTest.runTest(BaseSharingPolicyTest.java:146)
	at org.apache.hadoop.yarn.server.resourcemanager.reservation.TestCapacityOverTimePolicy.t</li></div><div><li>expected:&lt;101&gt; but was:&lt;71&gt;</li></div><div><li>expected:&lt;0.5&gt; but was:&lt;0.0&gt;</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertNotNull(Assert.java:621)
	at org.junit.Assert.assertNotNull(Assert.java:631)
	at org.apache.hadoop.yarn.server.resourcemanager.security.TestDelegationTokenRenewer.testRMRestartWithExpiredToken(TestDelegationTokenRenewer.java:1067)
	at sun.reflect.NativeMetho</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Response from server should have been Bad Request</li></div><div><li>Response from server should have been Not Found</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Response from server should have been Not Found</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;host1[]:9600&gt; but was:&lt;host1[.persistent.co.in.]:9600&gt;</li></div><div><li>expected:&lt;host1[]:9600&gt; but was:&lt;host1[.persistent.co.in.]:9600&gt;</li></div><div><li>String index out of range: -1</li></div><div><li> Expected to find 'whereas it is under recovery' but got unexpected exception: java.io.IOException: Unable to close file because the last blockBP-471152024-172.17.0.2-1540234579442:blk_1073741826_1002 does not have enough number of replicas.
 at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:964)
 at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java</li></div><div><li>lease holder should now be the NN</li></div><div><li>Problem binding to [localhost:36160] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException</li></div><div><li>Timed out waiting for condition. Thread diagnostics:
Timestamp: 2018-10-22 03:32:07,363

"org.eclipse.jetty.server.session.HashSessionManager@31cfc07cTimer" daemon prio=5 tid=160 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.Ab</li></div><div><li>inode should complete in ~60000 ms.
Expected: is &lt;true&gt;
     but: was &lt;false&gt;</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.mapreduce.v2.hs.server.TestHSAdminServer.testRefreshSuperUserGroups(TestHSAdminServer.java:208)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeM</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.mapreduce.v2.hs.server.TestHSAdminServer.testRefreshSuperUserGroups(TestHSAdminServer.java:208)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeM</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>Expected success for Probe Status, time="Tue Oct 23 01:05:07 UTC 2018", outcome="failure", message="Failure in Default probe: IP presence with DNS checking and DNS server address 8.8.8.8", exception="java.io.IOException: comp-0: DNS checking is enabled, but lookup for example.com is not available yet"</li></div><div><li>Child process owned by init escaped process tree.</li></div><div><li>Timed out waiting for condition. Thread diagnostics:
Timestamp: 2018-10-22 07:51:48,157

"DeletionService #2"  prio=5 tid=789 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.</li></div><div><li>expected:&lt;jenkins[xyz,jenkins]&gt; but was:&lt;jenkins[,jenkinsxyz]&gt;</li></div><div><li>expected:&lt;[jenkins,world:anyone:rwcda]&gt; but was:&lt;[world:anyone:rwcda,jenkins]&gt;</li></div><div><li>expected:&lt;[jenkins,world:anyone:rwcda]&gt; but was:&lt;[world:anyone:rwcda,jenkins]&gt;</li></div><div><li>expected:&lt;101&gt; but was:&lt;71&gt;</li></div><div><li>expected:&lt;0.5&gt; but was:&lt;0.0&gt;</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Response from server should have been Bad Request</li></div><div><li>Response from server should have been Not Found</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Response from server should have been Not Found</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li> Expected to find 'testCompressorDecompressor error !!!' but got unexpected exception: java.lang.NullPointerException
 at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:187)
 at com.google.common.base.Joiner.toString(Joiner.java:532)
 at com.google.common.base.Joiner.appendTo(Joiner.java:124)
 at com.google.common.base.Joiner.appendTo(Joiner.java:181)
 at com.google.common.ba</li></div><div><li> Expected to find 'testCompressorDecompressorWithExeedBufferLimit error !!!' but got unexpected exception: java.lang.NullPointerException
 at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:187)
 at com.google.common.base.Joiner.toString(Joiner.java:532)
 at com.google.common.base.Joiner.appendTo(Joiner.java:124)
 at com.google.common.base.Joiner.appendTo(Joiner.java:181)
 at </li></div><div><li>Deferred</li></div><div><li>Could not decompress data. Input is invalid.</li></div><div><li>java.net.ConnectException: Your endpoint configuration is wrong; For more details see:  http://wiki.apache.org/hadoop/UnsetHostnameOrPort</li></div><div><li>String index out of range: -1</li></div><div><li> Expected to find 'whereas it is under recovery' but got unexpected exception: java.io.IOException: Unable to close file because the last blockBP-1488314123-172.17.0.2-1540231197760:blk_1073741826_1002 does not have enough number of replicas.
 at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:964)
 at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.jav</li></div><div><li>test timed out after 10000 milliseconds</li></div><div><li> Expected to find 'localhost:34863: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:34863: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li> Expected to find 'localhost:38197: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:38197: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li> Expected to find 'localhost:42791: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:42791: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li>expected timeout</li></div><div><li> Expected to find 'localhost:35989: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:35989: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li> Expected to find 'localhost:42653: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:42653: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li> Expected to find 'localhost:46802: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:46802: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li>expected timeout</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.mapreduce.v2.hs.server.TestHSAdminServer.testRefreshSuperUserGroups(TestHSAdminServer.java:208)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeM</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.mapreduce.v2.hs.server.TestHSAdminServer.testRefreshSuperUserGroups(TestHSAdminServer.java:208)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeM</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>Expected success for Probe Status, time="Mon Oct 22 20:43:05 CDT 2018", outcome="failure", message="Failure in Default probe: IP presence with DNS checking and DNS server address 8.8.8.8", exception="java.io.IOException: comp-0: DNS checking is enabled, but lookup for example.com is not available yet"</li></div><div><li>Child process owned by init escaped process tree.</li></div><div><li>expected:&lt;jenkins[xyz,jenkins]&gt; but was:&lt;jenkins[,jenkinsxyz]&gt;</li></div><div><li>expected:&lt;[jenkins,world:anyone:rwcda]&gt; but was:&lt;[world:anyone:rwcda,jenkins]&gt;</li></div><div><li>expected:&lt;[jenkins,world:anyone:rwcda]&gt; but was:&lt;[world:anyone:rwcda,jenkins]&gt;</li></div><div><li>expected:&lt;101&gt; but was:&lt;71&gt;</li></div><div><li>expected:&lt;0.5&gt; but was:&lt;0.0&gt;</li></div><div><li>expected:&lt;[org.apache.hadoop.security.token.delegation.DelegationKey@c445c2f1, org.apache.hadoop.security.token.delegation.DelegationKey@270bb094]&gt; but was:&lt;[org.apache.hadoop.security.token.delegation.DelegationKey@c445c2f1, org.apache.hadoop.security.token.delegation.DelegationKey@2c290ff4, org.apache.hadoop.security.token.delegation.DelegationKey@270bb094]&gt;</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Response from server should have been Bad Request</li></div><div><li>Response from server should have been Not Found</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Response from server should have been Not Found</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;host1[]:9600&gt; but was:&lt;host1[.persistent.co.in.]:9600&gt;</li></div><div><li>expected:&lt;host1[]:9600&gt; but was:&lt;host1[.persistent.co.in.]:9600&gt;</li></div><div><li> Expected to find 'testCompressorDecompressor error !!!' but got unexpected exception: java.lang.NullPointerException
 at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:187)
 at com.google.common.base.Joiner.toString(Joiner.java:532)
 at com.google.common.base.Joiner.appendTo(Joiner.java:124)
 at com.google.common.base.Joiner.appendTo(Joiner.java:181)
 at com.google.common.ba</li></div><div><li> Expected to find 'testCompressorDecompressorWithExeedBufferLimit error !!!' but got unexpected exception: java.lang.NullPointerException
 at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:187)
 at com.google.common.base.Joiner.toString(Joiner.java:532)
 at com.google.common.base.Joiner.appendTo(Joiner.java:124)
 at com.google.common.base.Joiner.appendTo(Joiner.java:181)
 at </li></div><div><li>Deferred</li></div><div><li>Could not decompress data. Input is invalid.</li></div><div><li>String index out of range: -1</li></div><div><li>Failed to append to non-existent file /test/test/target for client 127.0.0.1
 at org.apache.hadoop.hdfs.server.namenode.FSDirAppendOp.appendFile(FSDirAppendOp.java:104)
 at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.appendFile(FSNamesystem.java:2705)
 at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.append(NameNodeRpcServer.java:806)
 at org.apache.hadoop.hdfs.protocolPB.Client</li></div><div><li>Files not distributed: [10, 0]</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertFalse(Assert.java:64)
	at org.junit.Assert.assertFalse(Assert.java:74)
	at org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailure.testDataNodeShutdownAfterNumFailedVolumeExceedsTolerated(TestDataNodeVolumeFailure.java:311)
	at sun.reflect.NativeMet</li></div><div><li>Timed out waiting for condition. Thread diagnostics:
Timestamp: 2018-10-22 11:06:05,864

"IPC Server handler 0 on 35065" daemon prio=5 tid=52 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObj</li></div><div><li>Unexpected num under-replicated blocks expected:&lt;3&gt; but was:&lt;4&gt;</li></div><div><li> Expected to find 'localhost:37420: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:37420: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li> Expected to find 'localhost:34023: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:34023: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li> Expected to find 'localhost:38776: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:38776: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li>expected timeout</li></div><div><li> Expected to find 'localhost:45936: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:45936: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li> Expected to find 'localhost:35823: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:35823: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li> Expected to find 'localhost:41443: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:41443: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li>expected timeout</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.mapreduce.v2.hs.server.TestHSAdminServer.testRefreshSuperUserGroups(TestHSAdminServer.java:208)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeM</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.mapreduce.v2.hs.server.TestHSAdminServer.testRefreshSuperUserGroups(TestHSAdminServer.java:208)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeM</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>Child process owned by init escaped process tree.</li></div><div><li>expected:&lt;jenkins[xyz,jenkins]&gt; but was:&lt;jenkins[,jenkinsxyz]&gt;</li></div><div><li>expected:&lt;[jenkins,world:anyone:rwcda]&gt; but was:&lt;[world:anyone:rwcda,jenkins]&gt;</li></div><div><li>expected:&lt;[jenkins,world:anyone:rwcda]&gt; but was:&lt;[world:anyone:rwcda,jenkins]&gt;</li></div><div><li>expected:&lt;1&gt; but was:&lt;0&gt;</li></div><div><li>expected:&lt;101&gt; but was:&lt;71&gt;</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Response from server should have been Bad Request</li></div><div><li>Response from server should have been Not Found</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Response from server should have been Not Found</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>test timed out after 30000 milliseconds</li></div><div><li>expected:&lt;host1[]:9600&gt; but was:&lt;host1[.persistent.co.in.]:9600&gt;</li></div><div><li>expected:&lt;host1[]:9600&gt; but was:&lt;host1[.persistent.co.in.]:9600&gt;</li></div><div><li>String index out of range: -1</li></div><div><li>expected:&lt;1540241685122&gt; but was:&lt;1540241686275&gt;</li></div><div><li>lease holder should now be the NN</li></div><div><li>test timed out after 300000 milliseconds</li></div><div><li>test timed out after 100000 milliseconds</li></div><div><li>Could not load library. Reasons: [no leveldbjni64-1.8 in java.library.path, no leveldbjni-1.8 in java.library.path, no leveldbjni in java.library.path, /tmp/libleveldbjni-64-1-6846183329515870239.8: /tmp/libleveldbjni-64-1-6846183329515870239.8: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64-bit platform)]</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not load library. Reasons: [no leveldbjni64-1.8 in java.library.path, no leveldbjni-1.8 in java.library.path, no leveldbjni in java.library.path, /tmp/libleveldbjni-64-1-554128541044744978.8: /tmp/libleveldbjni-64-1-554128541044744978.8: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64-bit platform)]</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Actual async detected volume failures should be greater or equal than [Ljava.lang.String;@2f5db674</li></div><div><li>expected:&lt;...0,"lastBlockReport":[0},"127.0.0.1:42858":{"infoAddr":"127.0.0.1:34790","infoSecureAddr":"127.0.0.1:0","xferaddr":"127.0.0.1:42858","lastContact":0,"usedSpace":8192,"adminState":"In Service","nonDfsUsedSpace":342604595200,"capacity":1051753185280,"numBlocks":0,"version":"3.3.0-SNAPSHOT","used":8192,"remaining":709148581888,"blockScheduled":0,"blockPoolUsed":8192,"blockPoolUsedPercent"</li></div><div><li>Could not load library. Reasons: [no leveldbjni64-1.8 in java.library.path, no leveldbjni-1.8 in java.library.path, no leveldbjni in java.library.path, /tmp/libleveldbjni-64-1-2259625289895769694.8: /tmp/libleveldbjni-64-1-2259625289895769694.8: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64-bit platform)]</li></div><div><li>Could not load library. Reasons: [no leveldbjni64-1.8 in java.library.path, no leveldbjni-1.8 in java.library.path, no leveldbjni in java.library.path, /tmp/libleveldbjni-64-1-523006923745447619.8: /tmp/libleveldbjni-64-1-523006923745447619.8: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64-bit platform)]</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.mapreduce.v2.hs.server.TestHSAdminServer.testRefreshSuperUserGroups(TestHSAdminServer.java:208)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeM</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.mapreduce.v2.hs.server.TestHSAdminServer.testRefreshSuperUserGroups(TestHSAdminServer.java:208)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeM</li></div><div><li>Job didn't finish in 30 seconds</li></div><div><li>Could not load library. Reasons: [no leveldbjni64-1.8 in java.library.path, no leveldbjni-1.8 in java.library.path, no leveldbjni in java.library.path, /tmp/libleveldbjni-64-1-3087808647152313191.8: /tmp/libleveldbjni-64-1-3087808647152313191.8: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64-bit platform)]</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>org/bouncycastle/jce/provider/BouncyCastleProvider</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/jce/provider/BouncyCastleProvider</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/jce/provider/BouncyCastleProvider</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/jce/provider/BouncyCastleProvider</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/jce/provider/BouncyCastleProvider</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>Could not load library. Reasons: [no leveldbjni64-1.8 in java.library.path, no leveldbjni-1.8 in java.library.path, no leveldbjni in java.library.path, /tmp/libleveldbjni-64-1-6894306488027051862.8: /tmp/libleveldbjni-64-1-6894306488027051862.8: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64-bit platform)]</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Expected success for Probe Status, time="Tue Oct 23 02:51:44 UTC 2018", outcome="failure", message="Failure in Default probe: IP presence with DNS checking and DNS server address 8.8.8.8", exception="java.io.IOException: comp-0: DNS checking is enabled, but lookup for example.com is not available yet"</li></div><div><li>Child process owned by init escaped process tree.</li></div><div><li>expected:&lt;0&gt; but was:&lt;-1&gt;</li></div><div><li>expected:&lt;0&gt; but was:&lt;-1&gt;</li></div><div><li>Could not load library. Reasons: [no leveldbjni64-1.8 in java.library.path, no leveldbjni-1.8 in java.library.path, no leveldbjni in java.library.path, /tmp/libleveldbjni-64-1-6106966317519162310.8: /tmp/libleveldbjni-64-1-6106966317519162310.8: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64-bit platform)]</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not load library. Reasons: [no leveldbjni64-1.8 in java.library.path, no leveldbjni-1.8 in java.library.path, no leveldbjni in java.library.path, /tmp/libleveldbjni-64-1-8074716728000596817.8: /tmp/libleveldbjni-64-1-8074716728000596817.8: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64-bit platform)]</li></div><div><li>Could not initialize class org.apache.hadoop.yarn.server.timeline.TestRollingLevelDB$MyRollingLevelDB</li></div><div><li>Could not load library. Reasons: [no leveldbjni64-1.8 in java.library.path, no leveldbjni-1.8 in java.library.path, no leveldbjni in java.library.path, /tmp/libleveldbjni-64-1-2749967554164131014.8: /tmp/libleveldbjni-64-1-2749967554164131014.8: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64-bit platform)]</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore.serviceStop(RollingLevelDBTimelineStore.java:374)
	at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:220)
	at org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.tearDown(TestRollingLevelDBTimelineStore.java:74)
	at sun.reflect.NativeMethodAccessorImpl.invok</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore.serviceStop(RollingLevelDBTimelineStore.java:374)
	at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:220)
	at org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.tearDown(TestRollingLevelDBTimelineStore.java:74)
	at sun.reflect.NativeMethodAccessorImpl.invok</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore.serviceStop(RollingLevelDBTimelineStore.java:374)
	at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:220)
	at org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.tearDown(TestRollingLevelDBTimelineStore.java:74)
	at sun.reflect.NativeMethodAccessorImpl.invok</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore.serviceStop(RollingLevelDBTimelineStore.java:374)
	at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:220)
	at org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.tearDown(TestRollingLevelDBTimelineStore.java:74)
	at sun.reflect.NativeMethodAccessorImpl.invok</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore.serviceStop(RollingLevelDBTimelineStore.java:374)
	at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:220)
	at org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.tearDown(TestRollingLevelDBTimelineStore.java:74)
	at sun.reflect.NativeMethodAccessorImpl.invok</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore.serviceStop(RollingLevelDBTimelineStore.java:374)
	at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:220)
	at org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.tearDown(TestRollingLevelDBTimelineStore.java:74)
	at sun.reflect.NativeMethodAccessorImpl.invok</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore.serviceStop(RollingLevelDBTimelineStore.java:374)
	at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:220)
	at org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.tearDown(TestRollingLevelDBTimelineStore.java:74)
	at sun.reflect.NativeMethodAccessorImpl.invok</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore.serviceStop(RollingLevelDBTimelineStore.java:374)
	at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:220)
	at org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.tearDown(TestRollingLevelDBTimelineStore.java:74)
	at sun.reflect.NativeMethodAccessorImpl.invok</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore.serviceStop(RollingLevelDBTimelineStore.java:374)
	at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:220)
	at org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.tearDown(TestRollingLevelDBTimelineStore.java:74)
	at sun.reflect.NativeMethodAccessorImpl.invok</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore.serviceStop(RollingLevelDBTimelineStore.java:374)
	at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:220)
	at org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.tearDown(TestRollingLevelDBTimelineStore.java:74)
	at sun.reflect.NativeMethodAccessorImpl.invok</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore.serviceStop(RollingLevelDBTimelineStore.java:374)
	at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:220)
	at org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.tearDown(TestRollingLevelDBTimelineStore.java:74)
	at sun.reflect.NativeMethodAccessorImpl.invok</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore.serviceStop(RollingLevelDBTimelineStore.java:374)
	at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:220)
	at org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.tearDown(TestRollingLevelDBTimelineStore.java:74)
	at sun.reflect.NativeMethodAccessorImpl.invok</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore.serviceStop(RollingLevelDBTimelineStore.java:374)
	at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:220)
	at org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.tearDown(TestRollingLevelDBTimelineStore.java:74)
	at sun.reflect.NativeMethodAccessorImpl.invok</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore.serviceStop(RollingLevelDBTimelineStore.java:374)
	at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:220)
	at org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.tearDown(TestRollingLevelDBTimelineStore.java:74)
	at sun.reflect.NativeMethodAccessorImpl.invok</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore.serviceStop(RollingLevelDBTimelineStore.java:374)
	at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:220)
	at org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.tearDown(TestRollingLevelDBTimelineStore.java:74)
	at sun.reflect.NativeMethodAccessorImpl.invok</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore.serviceStop(RollingLevelDBTimelineStore.java:374)
	at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:220)
	at org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.tearDown(TestRollingLevelDBTimelineStore.java:74)
	at sun.reflect.NativeMethodAccessorImpl.invok</li></div><div><li>Could not load library. Reasons: [no leveldbjni64-1.8 in java.library.path, no leveldbjni-1.8 in java.library.path, no leveldbjni in java.library.path, /tmp/libleveldbjni-64-1-2064119338140698141.8: /tmp/libleveldbjni-64-1-2064119338140698141.8: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64-bit platform)]</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not load library. Reasons: [no leveldbjni64-1.8 in java.library.path, no leveldbjni-1.8 in java.library.path, no leveldbjni in java.library.path, /tmp/libleveldbjni-64-1-3323024006714592726.8: /tmp/libleveldbjni-64-1-3323024006714592726.8: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64-bit platform)]</li></div><div><li>Could not initialize class org.apache.hadoop.yarn.server.timeline.recovery.LeveldbTimelineStateStore</li></div><div><li>Could not load library. Reasons: [no leveldbjni64-1.8 in java.library.path, no leveldbjni-1.8 in java.library.path, no leveldbjni in java.library.path, /tmp/libleveldbjni-64-1-7138150054086281878.8: /tmp/libleveldbjni-64-1-7138150054086281878.8: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64-bit platform)]</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceStop(NodeManager.java:529)
	at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:220)
	at org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerShutdown.tearDown(TestNodeManagerShutdown.java:107)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeM</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.doRestartTests(TestContainerManager.java:482)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.testContainerUpgradeSuc</li></div><div><li>expected:&lt;3&gt; but was:&lt;2&gt;</li></div><div><li>Could not load library. Reasons: [no leveldbjni64-1.8 in java.library.path, no leveldbjni-1.8 in java.library.path, no leveldbjni in java.library.path, /tmp/libleveldbjni-64-1-6774930248829059649.8: /tmp/libleveldbjni-64-1-6774930248829059649.8: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64-bit platform)]</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>expected:&lt;jenkins[xyz,jenkins]&gt; but was:&lt;jenkins[,jenkinsxyz]&gt;</li></div><div><li>expected:&lt;[jenkins,world:anyone:rwcda]&gt; but was:&lt;[world:anyone:rwcda,jenkins]&gt;</li></div><div><li>expected:&lt;[jenkins,world:anyone:rwcda]&gt; but was:&lt;[world:anyone:rwcda,jenkins]&gt;</li></div><div><li>Could not load library. Reasons: [no leveldbjni64-1.8 in java.library.path, no leveldbjni-1.8 in java.library.path, no leveldbjni in java.library.path, /tmp/libleveldbjni-64-1-5831130919690654925.8: /tmp/libleveldbjni-64-1-5831130919690654925.8: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64-bit platform)]</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not load library. Reasons: [no leveldbjni64-1.8 in java.library.path, no leveldbjni-1.8 in java.library.path, no leveldbjni in java.library.path, /tmp/libleveldbjni-64-1-8064991655020391824.8: /tmp/libleveldbjni-64-1-8064991655020391824.8: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64-bit platform)]</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>expected:&lt;101&gt; but was:&lt;71&gt;</li></div><div><li>Could not load library. Reasons: [no leveldbjni64-1.8 in java.library.path, no leveldbjni-1.8 in java.library.path, no leveldbjni in java.library.path, /tmp/libleveldbjni-64-1-3057714886751738676.8: /tmp/libleveldbjni-64-1-3057714886751738676.8: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64-bit platform)]</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertFalse(Assert.java:64)
	at org.junit.Assert.assertFalse(Assert.java:74)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.TestPlacementConstraintsUtil.testInterAppConstraintsByAppID(TestPlacementConstraintsUtil.java:965)
	at sun.reflect.N</li></div><div><li>expected:&lt;[org.apache.hadoop.security.token.delegation.DelegationKey@f4d60782, org.apache.hadoop.security.token.delegation.DelegationKey@69898573]&gt; but was:&lt;[org.apache.hadoop.security.token.delegation.DelegationKey@4f72a6d0, org.apache.hadoop.security.token.delegation.DelegationKey@f4d60782, org.apache.hadoop.security.token.delegation.DelegationKey@69898573]&gt;</li></div><div><li>Could not load library. Reasons: [no leveldbjni64-1.8 in java.library.path, no leveldbjni-1.8 in java.library.path, no leveldbjni in java.library.path, /tmp/libleveldbjni-64-1-6599754015035025982.8: /tmp/libleveldbjni-64-1-6599754015035025982.8: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64-bit platform)]</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not load library. Reasons: [no leveldbjni64-1.8 in java.library.path, no leveldbjni-1.8 in java.library.path, no leveldbjni in java.library.path, /tmp/libleveldbjni-64-1-5931867130731789331.8: /tmp/libleveldbjni-64-1-5931867130731789331.8: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64-bit platform)]</li></div><div><li>java.io.IOException: Couldn't delete data file for leveldb timeline store /var/lib/jenkins/workspace/hadoop/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timeline-pluginstorage/target/tmp/yarn/timeline/app1-timeline-cache.ldb</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>java.io.IOException: Couldn't delete data file for leveldb timeline store /var/lib/jenkins/workspace/hadoop/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timeline-pluginstorage/target/tmp/yarn/timeline/app1-timeline-cache.ldb</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>java.io.IOException: Couldn't delete data file for leveldb timeline store /var/lib/jenkins/workspace/hadoop/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timeline-pluginstorage/target/tmp/yarn/timeline/app1-timeline-cache.ldb</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>java.io.IOException: Couldn't delete data file for leveldb timeline store /var/lib/jenkins/workspace/hadoop/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timeline-pluginstorage/target/tmp/yarn/timeline/app1-timeline-cache.ldb</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>java.io.IOException: Couldn't delete data file for leveldb timeline store /var/lib/jenkins/workspace/hadoop/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timeline-pluginstorage/target/tmp/yarn/timeline/app1-timeline-cache.ldb</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>java.io.IOException: Couldn't delete data file for leveldb timeline store /var/lib/jenkins/workspace/hadoop/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timeline-pluginstorage/target/tmp/yarn/timeline/app1-timeline-cache.ldb</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>java.io.IOException: Couldn't delete data file for leveldb timeline store /var/lib/jenkins/workspace/hadoop/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timeline-pluginstorage/target/tmp/yarn/timeline/app1-timeline-cache.ldb</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>java.io.IOException: Couldn't delete data file for leveldb timeline store /var/lib/jenkins/workspace/hadoop/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timeline-pluginstorage/target/tmp/yarn/timeline/app1-timeline-cache.ldb</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>java.io.IOException: Couldn't delete data file for leveldb timeline store /var/lib/jenkins/workspace/hadoop/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timeline-pluginstorage/target/tmp/yarn/timeline/app1-timeline-cache.ldb</li></div><div><li>Could not load library. Reasons: [no leveldbjni64-1.8 in java.library.path, no leveldbjni-1.8 in java.library.path, no leveldbjni in java.library.path, /tmp/libleveldbjni-64-1-6757987075476754166.8: /tmp/libleveldbjni-64-1-6757987075476754166.8: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64-bit platform)]</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Response from server should have been Bad Request</li></div><div><li>Response from server should have been Not Found</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Response from server should have been Not Found</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;host1[]:9600&gt; but was:&lt;host1[.persistent.co.in.]:9600&gt;</li></div><div><li>expected:&lt;host1[]:9600&gt; but was:&lt;host1[.persistent.co.in.]:9600&gt;</li></div><div><li>String index out of range: -1</li></div><div><li>write timedout too late in 1228 ms.</li></div><div><li> Expected to find 'whereas it is under recovery' but got unexpected exception: java.io.IOException: Unable to close file because the last blockBP-1207188834-172.17.0.2-1540231270220:blk_1073741826_1002 does not have enough number of replicas.
 at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:964)
 at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.jav</li></div><div><li>Timed out waiting for condition. Thread diagnostics:
Timestamp: 2018-10-22 03:56:15,799

"IPC Server handler 1 on 43121" daemon prio=5 tid=53 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObj</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.mapreduce.v2.hs.server.TestHSAdminServer.testRefreshSuperUserGroups(TestHSAdminServer.java:208)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeM</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.mapreduce.v2.hs.server.TestHSAdminServer.testRefreshSuperUserGroups(TestHSAdminServer.java:208)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeM</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>Expected success for Probe Status, time="Tue Oct 23 01:08:32 UTC 2018", outcome="failure", message="Failure in Default probe: IP presence with DNS checking and DNS server address 8.8.8.8", exception="java.io.IOException: comp-0: DNS checking is enabled, but lookup for example.com is not available yet"</li></div><div><li>Application attempt appattempt_1540245924560_0001_000001 doesn't exist in ApplicationMasterService cache.
 at org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.allocate(ApplicationMasterService.java:398)
 at org.apache.hadoop.yarn.server.nodemanager.amrmproxy.DefaultRequestInterceptor$3.allocate(DefaultRequestInterceptor.java:224)
 at org.apache.hadoop.yarn.server.nodemanager.</li></div><div><li>ProcessTree shouldn't be alive</li></div><div><li>Process is still alive!</li></div><div><li>Process is still alive!</li></div><div><li>expected:&lt;[hadoop.apache.org]&gt; but was:&lt;[N/A]&gt;</li></div><div><li>expected:&lt;jenkins[xyz,jenkins]&gt; but was:&lt;jenkins[,jenkinsxyz]&gt;</li></div><div><li>expected:&lt;[jenkins,world:anyone:rwcda]&gt; but was:&lt;[world:anyone:rwcda,jenkins]&gt;</li></div><div><li>expected:&lt;[jenkins,world:anyone:rwcda]&gt; but was:&lt;[world:anyone:rwcda,jenkins]&gt;</li></div><div><li>expected:&lt;101&gt; but was:&lt;71&gt;</li></div><div><li>expected:&lt;2048&gt; but was:&lt;4096&gt;</li></div><div><li>expected:&lt;0.5&gt; but was:&lt;0.0&gt;</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Response from server should have been Bad Request</li></div><div><li>Response from server should have been Not Found</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Response from server should have been Not Found</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>String index out of range: -1</li></div><div><li>test timed out after 120000 milliseconds</li></div><div><li>Wrongly computed block reconstruction work</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.mapreduce.v2.hs.server.TestHSAdminServer.testRefreshSuperUserGroups(TestHSAdminServer.java:208)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeM</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.mapreduce.v2.hs.server.TestHSAdminServer.testRefreshSuperUserGroups(TestHSAdminServer.java:208)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeM</li></div><div><li>Job didn't finish in 30 seconds</li></div><div><li>expected:&lt;DEFAULT&gt; but was:&lt;HIGH&gt;</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/jce/provider/BouncyCastleProvider</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>TestSLSRunner catched exception from child thread (TaskRunner.TaskDefinition): [java.lang.reflect.UndeclaredThrowableException]</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>Expected success for Probe Status, time="Tue Oct 23 02:08:04 UTC 2018", outcome="failure", message="Failure in Default probe: IP presence with DNS checking and DNS server address 8.8.8.8", exception="java.io.IOException: comp-0: DNS checking is enabled, but lookup for example.com is not available yet"</li></div><div><li>ProcessTree shouldn't be alive</li></div><div><li>Process is still alive!</li></div><div><li>Process is still alive!</li></div><div><li>expected:&lt;jenkins[xyz,jenkins]&gt; but was:&lt;jenkins[,jenkinsxyz]&gt;</li></div><div><li>expected:&lt;[jenkins,world:anyone:rwcda]&gt; but was:&lt;[world:anyone:rwcda,jenkins]&gt;</li></div><div><li>expected:&lt;[jenkins,world:anyone:rwcda]&gt; but was:&lt;[world:anyone:rwcda,jenkins]&gt;</li></div><div><li>expected:&lt;101&gt; but was:&lt;71&gt;</li></div><div><li>expected:&lt;0.5&gt; but was:&lt;0.0&gt;</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Response from server should have been Bad Request</li></div><div><li>Response from server should have been Not Found</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Response from server should have been Not Found</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;host1[]:9600&gt; but was:&lt;host1[.persistent.co.in.]:9600&gt;</li></div><div><li>expected:&lt;host1[]:9600&gt; but was:&lt;host1[.persistent.co.in.]:9600&gt;</li></div><div><li>String index out of range: -1</li></div><div><li>write timedout too late in 1319 ms.</li></div><div><li>expected:&lt;3&gt; but was:&lt;2&gt;</li></div><div><li>test timed out after 120000 milliseconds</li></div><div><li>Item doesn't exist in the attempted list expected:&lt;1&gt; but was:&lt;0&gt;</li></div><div><li> Expected to find 'localhost:38366: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:38366: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li> Expected to find 'localhost:41502: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:41502: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li> Expected to find 'localhost:44340: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:44340: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li>expected timeout</li></div><div><li> Expected to find 'localhost:36677: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:36677: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li> Expected to find 'localhost:37084: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:37084: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li> Expected to find 'localhost:35003: connect timed out' but got unexpected exception: java.net.SocketTimeoutException: localhost:35003: Read timed out
 at java.net.SocketInputStream.socketRead0(Native Method)
 at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
 at java.net.SocketInputStream.read(SocketInputStream.java:171)
 at java.net.SocketInputStream.read(SocketInputStream.java</li></div><div><li>expected timeout</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.mapreduce.v2.hs.server.TestHSAdminServer.testRefreshSuperUserGroups(TestHSAdminServer.java:208)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeM</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.mapreduce.v2.hs.server.TestHSAdminServer.testRefreshSuperUserGroups(TestHSAdminServer.java:208)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeM</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>org/bouncycastle/operator/OperatorCreationException</li></div><div><li>Child process owned by init escaped process tree.</li></div><div><li>Timed out waiting for condition. Thread diagnostics:
Timestamp: 2018-10-23 07:41:20,688

"Finalizer" daemon prio=8 tid=3 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143)
        at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:164)
        at java.la</li></div><div><li>expected:&lt;jenkins[xyz,jenkins]&gt; but was:&lt;jenkins[,jenkinsxyz]&gt;</li></div><div><li>expected:&lt;[jenkins,world:anyone:rwcda]&gt; but was:&lt;[world:anyone:rwcda,jenkins]&gt;</li></div><div><li>expected:&lt;[jenkins,world:anyone:rwcda]&gt; but was:&lt;[world:anyone:rwcda,jenkins]&gt;</li></div><div><li>expected:&lt;1&gt; but was:&lt;0&gt;</li></div><div><li>expected:&lt;0&gt; but was:&lt;8192&gt;</li></div><div><li>expected:&lt;101&gt; but was:&lt;71&gt;</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Response from server should have been Bad Request</li></div><div><li>Response from server should have been Not Found</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Incorrect response from timeline reader. Status=500</li></div><div><li>Response from server should have been Not Found</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.hdfs.server.federation.router.TestRouterClientRejectOverload.testOverloadControl</div></li><li><div>org.apache.hadoop.hdfs.TestReadStripedFileWithMissingBlocks.testReadFileWithMissingBlocks</div></li><li><div>org.apache.hadoop.hdfs.server.datanode.TestNNHandlesBlockReportPerStorage.blockReport_08</div></li><li><div>org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestSpaceReservation.testReservedSpaceForLeaseRecovery</div></li><li><div>org.apache.hadoop.hdfs.server.namenode.TestPersistentStoragePolicySatisfier.testSPSxAttrWhenSpsCalledForDir</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.testContainerUpgradeSuccessExplicitRollback</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestLogAggregationService.testFixedSizeThreadPool</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.TestApplicationMasterServiceFair.testUpdateTrackingUrl</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.reservation.TestCapacityOverTimePolicy.testAllocation[Duration 90,000,000, height 0.25, numSubmission 1, periodic 86400000)]</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.security.TestDelegationTokenRenewer.testRMRestartWithExpiredToken</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.hdfs.TestLeaseRecovery2.testHardLeaseRecoveryWithRenameAfterNameNodeRestart</div></li><li><div>org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFSStriped.testRead</div></li><li><div>org.apache.hadoop.hdfs.server.namenode.ha.TestHAAppend.testMultipleAppendsDuringCatchupTailing</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.testLocalingResourceWhileContainerRunning</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.ipc.TestRPCWaitForProxy.testInterruptedWaitForProxy</div></li><li><div>org.apache.hadoop.hdfs.TestLeaseRecovery2.testCloseWhileRecoverLease</div></li><li><div>org.apache.hadoop.hdfs.server.datanode.TestDataNodeUUID.testUUIDRegeneration</div></li><li><div>org.apache.hadoop.yarn.service.monitor.probe.TestDefaultProbe.testDefaultProbe[2]</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestQueueManagementDynamicEditPolicy.testEditSchedule</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.security.TestRMDelegationTokens.testRMDTMasterKeyStateOnRollingMasterKey</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider.testTokenServiceCreationWithLegacyFormat</div></li><li><div>org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider.testTokenServiceCreationWithUriFormat</div></li><li><div>org.apache.hadoop.fs.contract.router.web.TestRouterWebHDFSContractAppend.testRenameFileBeingAppended</div></li><li><div>org.apache.hadoop.hdfs.server.federation.router.TestRouterAllResolver.testSpaceAll</div></li><li><div>org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailure.testDataNodeShutdownAfterNumFailedVolumeExceedsTolerated</div></li><li><div>org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestLazyPersistReplicaRecovery.testDnRestartWithSavedReplicas</div></li><li><div>org.apache.hadoop.hdfs.server.namenode.TestDecommissioningStatus.testDecommissionStatus</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.rmapp.TestRMAppTransitions.testAppSuccessPath[1]</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider.testCreation</div></li><li><div>org.apache.hadoop.hdfs.TestDFSShell.testCopyCommandsWithPreserveOption</div></li><li><div>org.apache.hadoop.hdfs.TestLeaseRecovery2.testHardLeaseRecoveryAfterNameNodeRestart2</div></li><li><div>org.apache.hadoop.hdfs.TestReadStripedFileWithMissingBlocks.testReadFileWithMissingBlocks</div></li><li><div>org.apache.hadoop.hdfs.server.balancer.TestBalancerRPCDelay.testBalancerRPCDelay</div></li><li><div>org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.TestInMemoryLevelDBAliasMapClient.writeRead</div></li><li><div>org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.TestInMemoryLevelDBAliasMapClient.iterateThreeBatches</div></li><li><div>org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.TestInMemoryLevelDBAliasMapClient.multipleReads</div></li><li><div>org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.TestInMemoryLevelDBAliasMapClient.iterateSingleBatch</div></li><li><div>org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.TestInMemoryLevelDBAliasMapClient.testServerBindHost</div></li><li><div>org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.TestInMemoryLevelDBAliasMapClient.testNonExistentBlock</div></li><li><div>org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.TestLevelDBFileRegionAliasMap.testReadBack</div></li><li><div>org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.TestLevelDBFileRegionAliasMap.testIterate</div></li><li><div>org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting.testHotSwapOutFailedVolumeAndReporting</div></li><li><div>org.apache.hadoop.hdfs.server.namenode.TestNameNodeMXBean.testDecommissioningNodes</div></li><li><div>org.apache.hadoop.hdfs.tools.offlineImageViewer.TestOfflineImageViewer.testPBDelimitedWriter</div></li><li><div>org.apache.hadoop.mapreduce.v2.hs.TestHistoryServerLeveldbStateStoreService.testTokenStore</div></li><li><div>org.apache.hadoop.mapreduce.v2.hs.TestHistoryServerLeveldbStateStoreService.testCheckVersion</div></li><li><div>org.apache.hadoop.mapred.TestClusterMRNotification.testMR</div></li><li><div>org.apache.hadoop.mapred.TestShuffleHandler.testRecoveryFromOtherVersions</div></li><li><div>org.apache.hadoop.mapred.TestShuffleHandler.testRecovery</div></li><li><div>org.apache.hadoop.yarn.applications.distributedshell.TestDistributedShell.testDSShellWithDomainV1_5</div></li><li><div>org.apache.hadoop.yarn.applications.distributedshell.TestDistributedShell.testDSShellWithoutDomainV1_5</div></li><li><div>org.apache.hadoop.yarn.server.applicationhistoryservice.TestApplicationHistoryServer.testLaunchWithArguments</div></li><li><div>org.apache.hadoop.yarn.server.applicationhistoryservice.TestApplicationHistoryServer.testLaunch</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testGetDomains</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testRelatingToNonExistingEntity</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testValidateConfig</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testGetEntitiesWithPrimaryFilters</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testRelatingToOldEntityWithoutDomainId</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testRootDirPermission</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testGetSingleEntity</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testDeleteEntitiesPrimaryFilters</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testDeleteEntities</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testCacheSizes</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testGetEntities</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testGetEntityTypes</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testFromTsWithDeletion</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testLevelDbRepair</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testGetDomain</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testGetEvents</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testGetEntitiesWithSecondaryFilters</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testCheckVersion</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testGetEntitiesWithFromId</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore.testGetEntitiesWithFromTs</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDB.testInsertForPreviousPeriodAfterRollPeriodRollsDB</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDB.testInsertAfterRollPeriodRollsDB</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testGetDomains</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testGetDomains</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testRelatingToNonExistingEntity</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testRelatingToNonExistingEntity</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testValidateConfig</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testValidateConfig</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testGetEntitiesWithPrimaryFilters</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testGetEntitiesWithPrimaryFilters</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testRelatingToOldEntityWithoutDomainId</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testRelatingToOldEntityWithoutDomainId</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testRootDirPermission</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testRootDirPermission</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testGetSingleEntity</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testGetSingleEntity</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testCacheSizes</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testCacheSizes</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testGetEntities</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testGetEntities</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testRelatingToEntityInSamePut</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testRelatingToEntityInSamePut</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testGetDomain</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testGetDomain</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testGetEvents</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testGetEvents</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testGetEntitiesWithSecondaryFilters</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testGetEntitiesWithSecondaryFilters</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testCheckVersion</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testCheckVersion</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testGetEntitiesWithFromId</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testGetEntitiesWithFromId</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testGetEntitiesWithFromTs</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore.testGetEntitiesWithFromTs</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestTimelineDataManager.testGetOldEntitiesWithOutDomainId</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestTimelineDataManager.testGetEntitiesAclEnabled</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestTimelineDataManager.testGetOldEntityWithOutDomainId</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestTimelineDataManager.testUpdatingOldEntityWithoutDomainId</div></li><li><div>org.apache.hadoop.yarn.server.timeline.recovery.TestLeveldbTimelineStateStore.testTokenStore</div></li><li><div>org.apache.hadoop.yarn.server.timeline.recovery.TestLeveldbTimelineStateStore.testCheckVersion</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerShutdown.testStateStoreRemovalOnDecommission</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerShutdown.testStateStoreRemovalOnDecommission</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.testContainerUpgradeSuccessExplicitRollback</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestLogAggregationService.testFixedSizeThreadPool</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService.testAMRMProxyStorage</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService.testEmptyRestartTimes</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService.testEmptyState</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService.testStateStoreNodeHealth</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService.testContainerStorage</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService.testFinishResourceLocalization</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService.testNMTokenStorage</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService.testCompactionCycle</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService.testLogDeleterStorage</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService.testDeletionTaskStorage</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService.testContainerTokenStorage</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService.testLocalTrackerStateIterator</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService.testStateStoreForResourceMapping</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService.testStartResourceLocalization</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService.testApplicationStorage</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService.testCheckVersion</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService.testRemoveLocalizedResource</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService.testIsNewlyCreated</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService.testUnexpectedKeyDoesntThrowException</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.metrics.TestCombinedSystemMetricsPublisher.testTimelineServiceEventPublishingNoService</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.metrics.TestCombinedSystemMetricsPublisher.testTimelineServiceEventPublishingV2Enabled</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.recovery.TestLeveldbRMStateStore.testRemoveAttempt</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.recovery.TestLeveldbRMStateStore.testBadKeyIteration</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.recovery.TestLeveldbRMStateStore.testDeleteStore</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.recovery.TestLeveldbRMStateStore.testEpoch</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.recovery.TestLeveldbRMStateStore.testApps</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.recovery.TestLeveldbRMStateStore.testProxyCA</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.recovery.TestLeveldbRMStateStore.testClientTokens</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.recovery.TestLeveldbRMStateStore.testVersion</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.recovery.TestLeveldbRMStateStore.testAMTokens</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.recovery.TestLeveldbRMStateStore.testAppDeletion</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.recovery.TestLeveldbRMStateStore.testRemoveApplication</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.recovery.TestLeveldbRMStateStore.testReservation</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.TestLeveldbConfigurationStore.testVersioning</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.TestLeveldbConfigurationStore.testPersistConfiguration</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.TestLeveldbConfigurationStore.testMaxLogs</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.TestLeveldbConfigurationStore.testRestartReadsFromUpdatedStore</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.TestLeveldbConfigurationStore.testPersistUpdatedConfiguration</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.TestLeveldbConfigurationStore.testConfigurationUpdate</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.TestLeveldbConfigurationStore.testNullConfigurationUpdate</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.TestPlacementConstraintsUtil.testInterAppConstraintsByAppID</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.security.TestRMDelegationTokens.testRMDTMasterKeyStateOnRollingMasterKey</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestEntityGroupFSTimelineStore.testSummaryRead</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestEntityGroupFSTimelineStore.testAppLogsScanLogs</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestEntityGroupFSTimelineStore.testPluginRead</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestEntityGroupFSTimelineStore.testCleanLogs</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestEntityGroupFSTimelineStore.testGetEntityPluginRead</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestEntityGroupFSTimelineStore.testScanActiveLogsAndMoveToDonePluginRead</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestEntityGroupFSTimelineStore.testParseSummaryLogs</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestEntityGroupFSTimelineStore.testMoveToDone</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestLevelDBCacheTimelineStore.testGetDomains</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestLevelDBCacheTimelineStore.testGetDomains</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestLevelDBCacheTimelineStore.testGetEntitiesWithPrimaryFilters</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestLevelDBCacheTimelineStore.testGetEntitiesWithPrimaryFilters</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestLevelDBCacheTimelineStore.testGetSingleEntity</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestLevelDBCacheTimelineStore.testGetSingleEntity</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestLevelDBCacheTimelineStore.testGetEntities</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestLevelDBCacheTimelineStore.testGetEntities</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestLevelDBCacheTimelineStore.testGetDomain</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestLevelDBCacheTimelineStore.testGetDomain</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestLevelDBCacheTimelineStore.testGetEvents</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestLevelDBCacheTimelineStore.testGetEvents</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestLevelDBCacheTimelineStore.testGetEntitiesWithSecondaryFilters</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestLevelDBCacheTimelineStore.testGetEntitiesWithSecondaryFilters</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestLevelDBCacheTimelineStore.testGetEntitiesWithFromId</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestLevelDBCacheTimelineStore.testGetEntitiesWithFromId</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestLevelDBCacheTimelineStore.testGetEntitiesWithFromTs</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestLevelDBCacheTimelineStore.testGetEntitiesWithFromTs</div></li><li><div>org.apache.hadoop.yarn.server.timeline.TestOverrideTimelineStoreYarnClient.testLifecycleAndOverride</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.hdfs.TestDistributedFileSystem.testDFSClientPeerWriteTimeout</div></li><li><div>org.apache.hadoop.hdfs.TestLeaseRecovery2.testCloseWhileRecoverLease</div></li><li><div>org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestLazyPersistReplicaRecovery.testDnRestartWithSavedReplicas</div></li><li><div>org.apache.hadoop.yarn.client.api.impl.TestAMRMProxy.testAMRMProxyTokenRenewal</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.TestContainerLaunch.testKillProcessGroup</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.TestContainersMonitor.testContainerKillOnMemoryOverflow</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.TestApplicationMasterServiceFair.testUpdateTrackingUrl</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestIncreaseAllocationExpirer.testContainerIncreaseAllocationExpiration</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestQueueManagementDynamicEditPolicy.testEditSchedule</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.hdfs.server.datanode.TestDataNodeErasureCodingMetrics.testReconstructionBytesPartialGroup2</div></li><li><div>org.apache.hadoop.mapred.TestMRTimelineEventHandling.testMapreduceJobTimelineServiceEnabled</div></li><li><div>org.apache.hadoop.mapreduce.v2.TestUberAM.testJobWithChangePriority</div></li><li><div>org.apache.hadoop.yarn.sls.TestSLSRunner.testSimulatorRunning[Testing with: SYNTH, org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler, (nodeFile null)]</div></li><li><div>org.apache.hadoop.yarn.service.monitor.probe.TestDefaultProbe.testDefaultProbe[2]</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.TestContainerLaunch.testKillProcessGroup</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.TestContainersMonitor.testContainerKillOnMemoryOverflow</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestQueueManagementDynamicEditPolicy.testEditSchedule</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider.testTokenServiceCreationWithLegacyFormat</div></li><li><div>org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider.testTokenServiceCreationWithUriFormat</div></li><li><div>org.apache.hadoop.hdfs.TestDistributedFileSystem.testDFSClientPeerWriteTimeout</div></li><li><div>org.apache.hadoop.hdfs.TestEncryptedTransfer.testEncryptedAppendRequiringBlockTransfer[1]</div></li><li><div>org.apache.hadoop.hdfs.server.namenode.sps.TestBlockStorageMovementAttemptedItems.testNoBlockMovementAttemptFinishedReportAdded</div></li><li><div>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testAuthUrlConnectTimeout[timeoutSource=ConnectionFactory]</div></li><li><div>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testConnectTimeout[timeoutSource=ConnectionFactory]</div></li><li><div>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testRedirectConnectTimeout[timeoutSource=ConnectionFactory]</div></li><li><div>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testTwoStepWriteConnectTimeout[timeoutSource=ConnectionFactory]</div></li><li><div>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testAuthUrlConnectTimeout[timeoutSource=Configuration]</div></li><li><div>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testConnectTimeout[timeoutSource=Configuration]</div></li><li><div>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testRedirectConnectTimeout[timeoutSource=Configuration]</div></li><li><div>org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts.testTwoStepWriteConnectTimeout[timeoutSource=Configuration]</div></li><li><div>org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.testLocalingResourceWhileContainerRunning</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.rmapp.TestRMAppTransitions.testAppSuccessPath[1]</div></li><li><div>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacitySchedulerSchedulingRequestUpdate.testBasicPendingResourceUpdate</div></li></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="hbase"><div style="font-weight:bold;" class="panel-heading">HBASE<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Prajyot)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>130057f13774f6b213cdb06952c805a29d59396e</div><div><b>Last Run: </b>15-11-2018 14:31 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC UBUNTU18</th><th>X86 UBUNTU18</th><th>PPC RHEL72</th><th>X86 RHEL72</th><th>PPC RHEL75</th><th>X86 RHEL75</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 4738</div><div>Failed Count : 8</div><div>Skipped Count : 41</div></td><td><div>Total Count : 4749</div><div>Failed Count : 6</div><div>Skipped Count : 41</div></td><td><div>Total Count : 4694</div><div>Failed Count : 69</div><div>Skipped Count : 41</div></td><td><div>Total Count : 4749</div><div>Failed Count : 6</div><div>Skipped Count : 41</div></td><td><div>Total Count : 4737</div><div>Failed Count : 1</div><div>Skipped Count : 41</div></td><td><div>Total Count : 4750</div><div>Failed Count : 3</div><div>Skipped Count : 41</div></td><td><div>Total Count : 4737</div><div>Failed Count : 0</div><div>Skipped Count : 41</div></td><td><div>Total Count : 4749</div><div>Failed Count : 1</div><div>Skipped Count : 41</div></td></tr><tr><td>Result</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.testExportWithTargetName</li></div><div><li>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.testExportFileSystemState</li></div><div><li>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.testConsecutiveExports</li></div><div><li>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.testEmptyExportFileSystemState</li></div><div><li>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.testExportFileSystemStateWithSkipTmp</li></div><div><li>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.testExportRetry</li></div><div><li>org.apache.hadoop.hbase.master.cleaner.TestSnapshotFromMaster.testSnapshotHFileArchiving</li></div><div><li>org.apache.hadoop.hbase.regionserver.TestRegionServerAbortTimeout.org.apache.hadoop.hbase.regionserver.TestRegionServerAbortTimeout</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.testExportWithTargetName</li></div><div><li>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.testExportFileSystemState</li></div><div><li>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.testConsecutiveExports</li></div><div><li>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.testEmptyExportFileSystemState</li></div><div><li>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.testExportFileSystemStateWithSkipTmp</li></div><div><li>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.testExportRetry</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.hbase.mapreduce.TestImportExport.org.apache.hadoop.hbase.mapreduce.TestImportExport</li></div><div><li>org.apache.hadoop.hbase.mapreduce.TestImportExport.org.apache.hadoop.hbase.mapreduce.TestImportExport</li></div><div><li>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.testExportWithTargetName</li></div><div><li>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.testExportFileSystemState</li></div><div><li>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.testConsecutiveExports</li></div><div><li>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.testEmptyExportFileSystemState</li></div><div><li>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory</li></div><div><li>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory</li></div><div><li>org.apache.hadoop.hbase.rest.TestGetAndPutResource.testIncrementXML</li></div><div><li>org.apache.hadoop.hbase.rest.TestGetAndPutResource.org.apache.hadoop.hbase.rest.TestGetAndPutResource</li></div><div><li>org.apache.hadoop.hbase.rest.TestGetAndPutResource.org.apache.hadoop.hbase.rest.TestGetAndPutResource</li></div><div><li>org.apache.hadoop.hbase.rsgroup.TestRSGroupsAdmin1.org.apache.hadoop.hbase.rsgroup.TestRSGroupsAdmin1</li></div><div><li>org.apache.hadoop.hbase.rsgroup.TestRSGroupsAdmin1.org.apache.hadoop.hbase.rsgroup.TestRSGroupsAdmin1</li></div><div><li>org.apache.hadoop.hbase.rsgroup.TestRSGroupsAdmin1.testMultiTableMove</li></div><div><li>org.apache.hadoop.hbase.rsgroup.TestRSGroupsBasics.org.apache.hadoop.hbase.rsgroup.TestRSGroupsBasics</li></div><div><li>org.apache.hadoop.hbase.rsgroup.TestRSGroupsBasics.org.apache.hadoop.hbase.rsgroup.TestRSGroupsBasics</li></div><div><li>org.apache.hadoop.hbase.TestClientOperationTimeout.testPutTimeout</li></div><div><li>org.apache.hadoop.hbase.TestClientOperationTimeout.testGetTimeout</li></div><div><li>org.apache.hadoop.hbase.TestClientOperationTimeout.testScanTimeout</li></div><div><li>org.apache.hadoop.hbase.client.TestAsyncDecommissionAdminApi.testAsyncDecommissionRegionServers[0]</li></div><div><li>org.apache.hadoop.hbase.client.TestAsyncDecommissionAdminApi.testAsyncDecommissionRegionServers[1]</li></div><div><li>org.apache.hadoop.hbase.client.TestReplicaWithCluster.org.apache.hadoop.hbase.client.TestReplicaWithCluster</li></div><div><li>org.apache.hadoop.hbase.client.TestReplicaWithCluster.org.apache.hadoop.hbase.client.TestReplicaWithCluster</li></div><div><li>org.apache.hadoop.hbase.client.TestSeparateClientZKCluster.testMetaMoveDuringClientZkClusterRestart</li></div><div><li>org.apache.hadoop.hbase.client.TestTableFavoredNodes.testMergeTable</li></div><div><li>org.apache.hadoop.hbase.client.replication.TestReplicationAdmin.org.apache.hadoop.hbase.client.replication.TestReplicationAdmin</li></div><div><li>org.apache.hadoop.hbase.client.replication.TestReplicationAdmin.org.apache.hadoop.hbase.client.replication.TestReplicationAdmin</li></div><div><li>org.apache.hadoop.hbase.client.replication.TestReplicationAdminWithClusters.org.apache.hadoop.hbase.client.replication.TestReplicationAdminWithClusters</li></div><div><li>org.apache.hadoop.hbase.client.replication.TestReplicationAdminWithClusters.org.apache.hadoop.hbase.client.replication.TestReplicationAdminWithClusters</li></div><div><li>org.apache.hadoop.hbase.master.TestAssignmentManagerMetrics.testRITAssignmentManagerMetrics</li></div><div><li>org.apache.hadoop.hbase.master.TestMasterMetricsWrapper.testOfflineRegion</li></div><div><li>org.apache.hadoop.hbase.master.TestMasterRepairMode.testNewCluster</li></div><div><li>org.apache.hadoop.hbase.master.assignment.TestReportRegionStateTransitionRetry.testRetryOnClose</li></div><div><li>org.apache.hadoop.hbase.master.assignment.TestSplitTableRegionProcedure.org.apache.hadoop.hbase.master.assignment.TestSplitTableRegionProcedure</li></div><div><li>org.apache.hadoop.hbase.master.assignment.TestSplitTableRegionProcedure.org.apache.hadoop.hbase.master.assignment.TestSplitTableRegionProcedure</li></div><div><li>org.apache.hadoop.hbase.master.procedure.TestRestoreSnapshotProcedure.org.apache.hadoop.hbase.master.procedure.TestRestoreSnapshotProcedure</li></div><div><li>org.apache.hadoop.hbase.master.procedure.TestRestoreSnapshotProcedure.org.apache.hadoop.hbase.master.procedure.TestRestoreSnapshotProcedure</li></div><div><li>org.apache.hadoop.hbase.master.procedure.TestTruncateTableProcedure.org.apache.hadoop.hbase.master.procedure.TestTruncateTableProcedure</li></div><div><li>org.apache.hadoop.hbase.master.procedure.TestTruncateTableProcedure.org.apache.hadoop.hbase.master.procedure.TestTruncateTableProcedure</li></div><div><li>org.apache.hadoop.hbase.namespace.TestNamespaceAuditor.org.apache.hadoop.hbase.namespace.TestNamespaceAuditor</li></div><div><li>org.apache.hadoop.hbase.namespace.TestNamespaceAuditor.org.apache.hadoop.hbase.namespace.TestNamespaceAuditor</li></div><div><li>org.apache.hadoop.hbase.namespace.TestNamespaceAuditor.testStatePreserve</li></div><div><li>org.apache.hadoop.hbase.quotas.TestQuotaTableUtil.org.apache.hadoop.hbase.quotas.TestQuotaTableUtil</li></div><div><li>org.apache.hadoop.hbase.regionserver.TestCompactSplitThread.testFlushWithTableCompactionDisabled</li></div><div><li>org.apache.hadoop.hbase.regionserver.TestCompactionFileNotFound.testSplitAfterRefresh</li></div><div><li>org.apache.hadoop.hbase.regionserver.TestRegionReplicasWithModifyTable.org.apache.hadoop.hbase.regionserver.TestRegionReplicasWithModifyTable</li></div><div><li>org.apache.hadoop.hbase.regionserver.TestRegionReplicasWithModifyTable.org.apache.hadoop.hbase.regionserver.TestRegionReplicasWithModifyTable</li></div><div><li>org.apache.hadoop.hbase.regionserver.compactions.TestFIFOCompactionPolicy.testPurgeExpiredFiles</li></div><div><li>org.apache.hadoop.hbase.regionserver.throttle.TestFlushWithThroughputController.testFlushControlForStripedStore</li></div><div><li>org.apache.hadoop.hbase.regionserver.throttle.TestFlushWithThroughputController.testFlushControl</li></div><div><li>org.apache.hadoop.hbase.replication.TestAddToSerialReplicationPeer.org.apache.hadoop.hbase.replication.TestAddToSerialReplicationPeer</li></div><div><li>org.apache.hadoop.hbase.replication.TestAddToSerialReplicationPeer.org.apache.hadoop.hbase.replication.TestAddToSerialReplicationPeer</li></div><div><li>org.apache.hadoop.hbase.replication.TestNamespaceReplication.org.apache.hadoop.hbase.replication.TestNamespaceReplication</li></div><div><li>org.apache.hadoop.hbase.replication.TestNamespaceReplication.org.apache.hadoop.hbase.replication.TestNamespaceReplication</li></div><div><li>org.apache.hadoop.hbase.replication.TestNamespaceReplication.testNamespaceReplication[1: serialPeer=false]</li></div><div><li>org.apache.hadoop.hbase.replication.TestSerialReplication.org.apache.hadoop.hbase.replication.TestSerialReplication</li></div><div><li>org.apache.hadoop.hbase.replication.TestSerialReplication.org.apache.hadoop.hbase.replication.TestSerialReplication</li></div><div><li>org.apache.hadoop.hbase.replication.TestSerialReplication.testRegionMove</li></div><div><li>org.apache.hadoop.hbase.replication.multiwal.TestReplicationEndpointWithMultipleAsyncWAL.testMetricsSourceBaseSourcePassthrough</li></div><div><li>org.apache.hadoop.hbase.replication.multiwal.TestReplicationEndpointWithMultipleAsyncWAL.org.apache.hadoop.hbase.replication.multiwal.TestReplicationEndpointWithMultipleAsyncWAL</li></div><div><li>org.apache.hadoop.hbase.replication.multiwal.TestReplicationEndpointWithMultipleAsyncWAL.org.apache.hadoop.hbase.replication.multiwal.TestReplicationEndpointWithMultipleAsyncWAL</li></div><div><li>org.apache.hadoop.hbase.replication.multiwal.TestReplicationEndpointWithMultipleAsyncWAL.testWALEntryFilterUpdateValidation</li></div><div><li>org.apache.hadoop.hbase.replication.multiwal.TestReplicationEndpointWithMultipleWAL.testWALEntryFilterFromReplicationEndpoint</li></div><div><li>org.apache.hadoop.hbase.replication.regionserver.TestDrainReplicationQueuesForStandBy.org.apache.hadoop.hbase.replication.regionserver.TestDrainReplicationQueuesForStandBy</li></div><div><li>org.apache.hadoop.hbase.replication.regionserver.TestDrainReplicationQueuesForStandBy.org.apache.hadoop.hbase.replication.regionserver.TestDrainReplicationQueuesForStandBy</li></div><div><li>org.apache.hadoop.hbase.replication.regionserver.TestDrainReplicationQueuesForStandBy.test</li></div><div><li>org.apache.hadoop.hbase.tool.TestCanaryTool.testBasicCanaryWorks</li></div><div><li>org.apache.hadoop.hbase.util.TestRegionMover.org.apache.hadoop.hbase.util.TestRegionMover</li></div><div><li>org.apache.hadoop.hbase.util.TestRegionMover.org.apache.hadoop.hbase.util.TestRegionMover</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.testExportWithTargetName</li></div><div><li>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.testExportFileSystemState</li></div><div><li>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.testConsecutiveExports</li></div><div><li>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.testEmptyExportFileSystemState</li></div><div><li>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.testExportFileSystemStateWithSkipTmp</li></div><div><li>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.testExportRetry</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.hbase.client.TestAsyncRegionLocatorTimeout.test</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.hbase.master.locking.TestLockManager.org.apache.hadoop.hbase.master.locking.TestLockManager</li></div><div><li>org.apache.hadoop.hbase.master.locking.TestLockManager.org.apache.hadoop.hbase.master.locking.TestLockManager</li></div><div><li>org.apache.hadoop.hbase.replication.regionserver.TestReplicator.testReplicatorBatching</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.hbase.master.cleaner.TestSnapshotFromMaster.testSnapshotHFileArchiving</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div><div><li>Archived hfiles [] and table hfiles [89483f2140de4e7b9621d3c6c3e2e0f2] is missing snapshot file:dcd5b27feeeb48bab66afccb1412ec42</li></div><div><li>java.util.concurrent.TimeoutException: The procedure 21 is still running</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>test timed out after 780 seconds</li></div><div><li>Appears to be stuck in thread RS-EventLoopGroup-3-2</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div><div><li>test timed out after 780 seconds</li></div><div><li>Appears to be stuck in thread RS-EventLoopGroup-5-3</li></div><div><li>Interrupt while waiting on Operation: DISABLE, Table Name: default:TestRowResource, procId: 99</li></div><div><li>test timed out after 780 seconds</li></div><div><li>Appears to be stuck in thread DataXceiver for client DFSClient_NONMAPREDUCE_-735014251_13 at /127.0.0.1:48172 [Receiving block BP-904114154-172.17.0.2-1542339195387:blk_1073741829_1005]</li></div><div><li>test timed out after 780 seconds</li></div><div><li>Appears to be stuck in thread DataXceiver for client DFSClient_NONMAPREDUCE_-1243418594_11 at /127.0.0.1:55396 [Receiving block BP-1617833546-172.17.0.2-1542334579000:blk_1073741829_1005]</li></div><div><li>Interrupt while waiting on Operation: DISABLE, Table Name: default:GrouptestMultiTableMoveA, procId: 106</li></div><div><li>test timed out after 780 seconds</li></div><div><li>Appears to be stuck in thread SyncThread:0</li></div><div><li>callTimeout=500, callDuration=617: Call to 4dc6a27b65b8/172.17.0.2:33372 failed on local exception: org.apache.hadoop.hbase.ipc.CallTimeoutException: Call id=2, waitTime=477, rpcTimeout=468 </li></div><div><li>callTimeout=500, callDuration=610: Call to 4dc6a27b65b8/172.17.0.2:33372 failed on local exception: org.apache.hadoop.hbase.ipc.CallTimeoutException: Call id=4, waitTime=505, rpcTimeout=496 </li></div><div><li>callTimeout=500, callDuration=609: Call to 4dc6a27b65b8/172.17.0.2:33372 failed on local exception: org.apache.hadoop.hbase.ipc.CallTimeoutException: Call id=6, waitTime=505, rpcTimeout=497 </li></div><div><li>Could not find region testAsyncDecommissionRegionServers,,1542316191361.91beec234ad95808ea07f9e5ff4fbf6b. on server 4dc6a27b65b8,40684,1542316146558</li></div><div><li>java.lang.AssertionError
	at org.apache.hadoop.hbase.client.TestAsyncDecommissionAdminApi.testAsyncDecommissionRegionServers(TestAsyncDecommissionAdminApi.java:50)
</li></div><div><li>test timed out after 780 seconds</li></div><div><li>Appears to be stuck in thread Socket Reader #1 for port 34868</li></div><div><li>Failed after attempts=3, exceptions:
Thu Nov 15 14:55:07 CST 2018, RpcRetryingCaller{globalStartTime=1542315304225, pause=100, maxAttempts=3}, org.apache.hadoop.hbase.client.RetriesExhaustedException: Failed after attempts=3, exceptions:
Thu Nov 15 14:55:06 CST 2018, RpcRetryingCaller{globalStartTime=1542315306976, pause=100, maxAttempts=3}, java.io.IOException: Call to 4dc6a27b65b8/172.17.0.2:422</li></div><div><li>The procedure 16 is still running</li></div><div><li>test timed out after 780 seconds</li></div><div><li>Appears to be stuck in thread DataXceiver for client DFSClient_NONMAPREDUCE_1631531274_13 at /127.0.0.1:54334 [Receiving block BP-469436082-172.17.0.2-1542311313523:blk_1073741829_1005]</li></div><div><li>test timed out after 780 seconds</li></div><div><li>Appears to be stuck in thread Socket Reader #1 for port 45495</li></div><div><li>java.util.concurrent.TimeoutException: The procedure 9 is still running</li></div><div><li>test timed out after 30000 milliseconds</li></div><div><li>Shutting down</li></div><div><li>Waiting timed out after [10,000] msec</li></div><div><li>test timed out after 780 seconds</li></div><div><li>Appears to be stuck in thread Socket Reader #1 for port 37924</li></div><div><li>test timed out after 780 seconds</li></div><div><li>Appears to be stuck in thread Socket Reader #1 for port 33942</li></div><div><li>test timed out after 780 seconds</li></div><div><li>Appears to be stuck in thread Socket Reader #1 for port 35501</li></div><div><li>test timed out after 780 seconds</li></div><div><li>Appears to be stuck in thread DataXceiver for client DFSClient_NONMAPREDUCE_-420643917_13 at /127.0.0.1:40212 [Receiving block BP-609283487-172.17.0.2-1542328878295:blk_1073741829_1005]</li></div><div><li>java.io.InterruptedIOException
	at org.apache.hadoop.hbase.namespace.TestNamespaceAuditor.testStatePreserve(TestNamespaceAuditor.java:523)
Caused by: java.lang.InterruptedException
	at org.apache.hadoop.hbase.namespace.TestNamespaceAuditor.testStatePreserve(TestNamespaceAuditor.java:523)
</li></div><div><li>Waiting timed out after [30,000] msec TableState in META: Not ENABLED state, but tableName=hbase:quota, state=ENABLING, </li></div><div><li>java.lang.AssertionError
	at org.apache.hadoop.hbase.regionserver.TestCompactSplitThread.testFlushWithTableCompactionDisabled(TestCompactSplitThread.java:171)
</li></div><div><li>Waiting timed out after [20,000] msec</li></div><div><li>test timed out after 780 seconds</li></div><div><li>Appears to be stuck in thread DataXceiver for client DFSClient_NONMAPREDUCE_-1238583949_13 at /127.0.0.1:44078 [Receiving block BP-1419162616-172.17.0.2-1542298527707:blk_1073741829_1005]</li></div><div><li>Waiting timed out after [30,000] msec The store file count 2 is still greater than 1</li></div><div><li>java.lang.AssertionError
	at org.apache.hadoop.hbase.regionserver.throttle.TestFlushWithThroughputController.testFlushWithThroughputLimit(TestFlushWithThroughputController.java:154)
	at org.apache.hadoop.hbase.regionserver.throttle.TestFlushWithThroughputController.testFlushControlForStripedStore(TestFlushWithThroughputController.java:227)
</li></div><div><li>java.lang.AssertionError
	at org.apache.hadoop.hbase.regionserver.throttle.TestFlushWithThroughputController.testFlushWithThroughputLimit(TestFlushWithThroughputController.java:154)
	at org.apache.hadoop.hbase.regionserver.throttle.TestFlushWithThroughputController.testFlushControl(TestFlushWithThroughputController.java:160)
</li></div><div><li>test timed out after 780 seconds</li></div><div><li>Appears to be stuck in thread RS-EventLoopGroup-5-4</li></div><div><li>test timed out after 780 seconds</li></div><div><li>Appears to be stuck in thread Socket Reader #1 for port 32965</li></div><div><li>Interrupt while waiting on Operation: REMOVE_REPLICATION_PEER, peerId: 2</li></div><div><li>test timed out after 780 seconds</li></div><div><li>Appears to be stuck in thread RS-EventLoopGroup-5-4</li></div><div><li>Interrupt while waiting on Operation: REMOVE_REPLICATION_PEER, peerId: 1</li></div><div><li>Waiting timed out after [3,000] msec Still waiting for log roll on regionservers: [4dc6a27b65b8,37463,1542321811433, 4dc6a27b65b8,39807,1542321811543]</li></div><div><li>test timed out after 780 seconds</li></div><div><li>Appears to be stuck in thread Socket Reader #1 for port 42063</li></div><div><li>Interrupt while waiting on Operation: ADD_REPLICATION_PEER, peerId: 2</li></div><div><li>Waiting timed out after [3,000] msec Still waiting for log roll on regionservers: [4dc6a27b65b8,45495,1542321810718]</li></div><div><li>test timed out after 780 seconds</li></div><div><li>Appears to be stuck in thread Socket Reader #1 for port 38730</li></div><div><li>Interrupt while waiting on Operation: TRANSIT_REPLICATION_PEER_SYNCHRONOUS_REPLICATION_STATE, peerId: 1</li></div><div><li>expected:&lt;0&gt; but was:&lt;3&gt;</li></div><div><li>test timed out after 780 seconds</li></div><div><li>Appears to be stuck in thread DataXceiver for client DFSClient_NONMAPREDUCE_2002915814_13 at /127.0.0.1:45028 [Receiving block BP-1696531934-172.17.0.2-1542318063603:blk_1073741829_1005]</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div><div><li>expected:&lt;0&gt; but was:&lt;1&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>java.lang.AssertionError
	at org.apache.hadoop.hbase.client.TestAsyncRegionLocatorTimeout.test(TestAsyncRegionLocatorTimeout.java:123)
</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>test timed out after 780 seconds</li></div><div><li>Appears to be stuck in thread RS-EventLoopGroup-1-2</li></div><div><li>Waiting timed out after [60,000] msec We waited too long for expected replication of 10 entries</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>Archived hfiles [] and table hfiles [37a13e7b715042d7bdfba8628c1e9a35] is missing snapshot file:f50cc128ba784c15b438ed1121afa738</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.hbase.master.cleaner.TestSnapshotFromMaster.testSnapshotHFileArchiving</div></li><li><div>org.apache.hadoop.hbase.regionserver.TestRegionServerAbortTimeout.org.apache.hadoop.hbase.regionserver.TestRegionServerAbortTimeout</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.hbase.mapreduce.TestImportExport.org.apache.hadoop.hbase.mapreduce.TestImportExport</div></li><li><div>org.apache.hadoop.hbase.mapreduce.TestImportExport.org.apache.hadoop.hbase.mapreduce.TestImportExport</div></li><li><div>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory</div></li><li><div>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory</div></li><li><div>org.apache.hadoop.hbase.rest.TestGetAndPutResource.testIncrementXML</div></li><li><div>org.apache.hadoop.hbase.rest.TestGetAndPutResource.org.apache.hadoop.hbase.rest.TestGetAndPutResource</div></li><li><div>org.apache.hadoop.hbase.rest.TestGetAndPutResource.org.apache.hadoop.hbase.rest.TestGetAndPutResource</div></li><li><div>org.apache.hadoop.hbase.rsgroup.TestRSGroupsAdmin1.org.apache.hadoop.hbase.rsgroup.TestRSGroupsAdmin1</div></li><li><div>org.apache.hadoop.hbase.rsgroup.TestRSGroupsAdmin1.org.apache.hadoop.hbase.rsgroup.TestRSGroupsAdmin1</div></li><li><div>org.apache.hadoop.hbase.rsgroup.TestRSGroupsAdmin1.testMultiTableMove</div></li><li><div>org.apache.hadoop.hbase.rsgroup.TestRSGroupsBasics.org.apache.hadoop.hbase.rsgroup.TestRSGroupsBasics</div></li><li><div>org.apache.hadoop.hbase.rsgroup.TestRSGroupsBasics.org.apache.hadoop.hbase.rsgroup.TestRSGroupsBasics</div></li><li><div>org.apache.hadoop.hbase.TestClientOperationTimeout.testPutTimeout</div></li><li><div>org.apache.hadoop.hbase.TestClientOperationTimeout.testGetTimeout</div></li><li><div>org.apache.hadoop.hbase.TestClientOperationTimeout.testScanTimeout</div></li><li><div>org.apache.hadoop.hbase.client.TestAsyncDecommissionAdminApi.testAsyncDecommissionRegionServers[0]</div></li><li><div>org.apache.hadoop.hbase.client.TestAsyncDecommissionAdminApi.testAsyncDecommissionRegionServers[1]</div></li><li><div>org.apache.hadoop.hbase.client.TestReplicaWithCluster.org.apache.hadoop.hbase.client.TestReplicaWithCluster</div></li><li><div>org.apache.hadoop.hbase.client.TestReplicaWithCluster.org.apache.hadoop.hbase.client.TestReplicaWithCluster</div></li><li><div>org.apache.hadoop.hbase.client.TestSeparateClientZKCluster.testMetaMoveDuringClientZkClusterRestart</div></li><li><div>org.apache.hadoop.hbase.client.TestTableFavoredNodes.testMergeTable</div></li><li><div>org.apache.hadoop.hbase.client.replication.TestReplicationAdmin.org.apache.hadoop.hbase.client.replication.TestReplicationAdmin</div></li><li><div>org.apache.hadoop.hbase.client.replication.TestReplicationAdmin.org.apache.hadoop.hbase.client.replication.TestReplicationAdmin</div></li><li><div>org.apache.hadoop.hbase.client.replication.TestReplicationAdminWithClusters.org.apache.hadoop.hbase.client.replication.TestReplicationAdminWithClusters</div></li><li><div>org.apache.hadoop.hbase.client.replication.TestReplicationAdminWithClusters.org.apache.hadoop.hbase.client.replication.TestReplicationAdminWithClusters</div></li><li><div>org.apache.hadoop.hbase.master.TestAssignmentManagerMetrics.testRITAssignmentManagerMetrics</div></li><li><div>org.apache.hadoop.hbase.master.TestMasterMetricsWrapper.testOfflineRegion</div></li><li><div>org.apache.hadoop.hbase.master.TestMasterRepairMode.testNewCluster</div></li><li><div>org.apache.hadoop.hbase.master.assignment.TestReportRegionStateTransitionRetry.testRetryOnClose</div></li><li><div>org.apache.hadoop.hbase.master.assignment.TestSplitTableRegionProcedure.org.apache.hadoop.hbase.master.assignment.TestSplitTableRegionProcedure</div></li><li><div>org.apache.hadoop.hbase.master.assignment.TestSplitTableRegionProcedure.org.apache.hadoop.hbase.master.assignment.TestSplitTableRegionProcedure</div></li><li><div>org.apache.hadoop.hbase.master.procedure.TestRestoreSnapshotProcedure.org.apache.hadoop.hbase.master.procedure.TestRestoreSnapshotProcedure</div></li><li><div>org.apache.hadoop.hbase.master.procedure.TestRestoreSnapshotProcedure.org.apache.hadoop.hbase.master.procedure.TestRestoreSnapshotProcedure</div></li><li><div>org.apache.hadoop.hbase.master.procedure.TestTruncateTableProcedure.org.apache.hadoop.hbase.master.procedure.TestTruncateTableProcedure</div></li><li><div>org.apache.hadoop.hbase.master.procedure.TestTruncateTableProcedure.org.apache.hadoop.hbase.master.procedure.TestTruncateTableProcedure</div></li><li><div>org.apache.hadoop.hbase.namespace.TestNamespaceAuditor.org.apache.hadoop.hbase.namespace.TestNamespaceAuditor</div></li><li><div>org.apache.hadoop.hbase.namespace.TestNamespaceAuditor.org.apache.hadoop.hbase.namespace.TestNamespaceAuditor</div></li><li><div>org.apache.hadoop.hbase.namespace.TestNamespaceAuditor.testStatePreserve</div></li><li><div>org.apache.hadoop.hbase.quotas.TestQuotaTableUtil.org.apache.hadoop.hbase.quotas.TestQuotaTableUtil</div></li><li><div>org.apache.hadoop.hbase.regionserver.TestCompactSplitThread.testFlushWithTableCompactionDisabled</div></li><li><div>org.apache.hadoop.hbase.regionserver.TestCompactionFileNotFound.testSplitAfterRefresh</div></li><li><div>org.apache.hadoop.hbase.regionserver.TestRegionReplicasWithModifyTable.org.apache.hadoop.hbase.regionserver.TestRegionReplicasWithModifyTable</div></li><li><div>org.apache.hadoop.hbase.regionserver.TestRegionReplicasWithModifyTable.org.apache.hadoop.hbase.regionserver.TestRegionReplicasWithModifyTable</div></li><li><div>org.apache.hadoop.hbase.regionserver.compactions.TestFIFOCompactionPolicy.testPurgeExpiredFiles</div></li><li><div>org.apache.hadoop.hbase.regionserver.throttle.TestFlushWithThroughputController.testFlushControlForStripedStore</div></li><li><div>org.apache.hadoop.hbase.regionserver.throttle.TestFlushWithThroughputController.testFlushControl</div></li><li><div>org.apache.hadoop.hbase.replication.TestAddToSerialReplicationPeer.org.apache.hadoop.hbase.replication.TestAddToSerialReplicationPeer</div></li><li><div>org.apache.hadoop.hbase.replication.TestAddToSerialReplicationPeer.org.apache.hadoop.hbase.replication.TestAddToSerialReplicationPeer</div></li><li><div>org.apache.hadoop.hbase.replication.TestNamespaceReplication.org.apache.hadoop.hbase.replication.TestNamespaceReplication</div></li><li><div>org.apache.hadoop.hbase.replication.TestNamespaceReplication.org.apache.hadoop.hbase.replication.TestNamespaceReplication</div></li><li><div>org.apache.hadoop.hbase.replication.TestNamespaceReplication.testNamespaceReplication[1: serialPeer=false]</div></li><li><div>org.apache.hadoop.hbase.replication.TestSerialReplication.org.apache.hadoop.hbase.replication.TestSerialReplication</div></li><li><div>org.apache.hadoop.hbase.replication.TestSerialReplication.org.apache.hadoop.hbase.replication.TestSerialReplication</div></li><li><div>org.apache.hadoop.hbase.replication.TestSerialReplication.testRegionMove</div></li><li><div>org.apache.hadoop.hbase.replication.multiwal.TestReplicationEndpointWithMultipleAsyncWAL.testMetricsSourceBaseSourcePassthrough</div></li><li><div>org.apache.hadoop.hbase.replication.multiwal.TestReplicationEndpointWithMultipleAsyncWAL.org.apache.hadoop.hbase.replication.multiwal.TestReplicationEndpointWithMultipleAsyncWAL</div></li><li><div>org.apache.hadoop.hbase.replication.multiwal.TestReplicationEndpointWithMultipleAsyncWAL.org.apache.hadoop.hbase.replication.multiwal.TestReplicationEndpointWithMultipleAsyncWAL</div></li><li><div>org.apache.hadoop.hbase.replication.multiwal.TestReplicationEndpointWithMultipleAsyncWAL.testWALEntryFilterUpdateValidation</div></li><li><div>org.apache.hadoop.hbase.replication.multiwal.TestReplicationEndpointWithMultipleWAL.testWALEntryFilterFromReplicationEndpoint</div></li><li><div>org.apache.hadoop.hbase.replication.regionserver.TestDrainReplicationQueuesForStandBy.org.apache.hadoop.hbase.replication.regionserver.TestDrainReplicationQueuesForStandBy</div></li><li><div>org.apache.hadoop.hbase.replication.regionserver.TestDrainReplicationQueuesForStandBy.org.apache.hadoop.hbase.replication.regionserver.TestDrainReplicationQueuesForStandBy</div></li><li><div>org.apache.hadoop.hbase.replication.regionserver.TestDrainReplicationQueuesForStandBy.test</div></li><li><div>org.apache.hadoop.hbase.tool.TestCanaryTool.testBasicCanaryWorks</div></li><li><div>org.apache.hadoop.hbase.util.TestRegionMover.org.apache.hadoop.hbase.util.TestRegionMover</div></li><li><div>org.apache.hadoop.hbase.util.TestRegionMover.org.apache.hadoop.hbase.util.TestRegionMover</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.testExportFileSystemStateWithSkipTmp</div></li><li><div>org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.testExportRetry</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.hbase.client.TestAsyncRegionLocatorTimeout.test</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.hbase.master.locking.TestLockManager.org.apache.hadoop.hbase.master.locking.TestLockManager</div></li><li><div>org.apache.hadoop.hbase.master.locking.TestLockManager.org.apache.hadoop.hbase.master.locking.TestLockManager</div></li><li><div>org.apache.hadoop.hbase.replication.regionserver.TestReplicator.testReplicatorBatching</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.hbase.master.cleaner.TestSnapshotFromMaster.testSnapshotHFileArchiving</div></li></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="hive"><div style="font-weight:bold;" class="panel-heading">HIVE<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Alisha)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>99d25f02421a84bf0f96660f9248fd6518dc7c8a</div><div><b>Last Run: </b>13-11-2018 15:00 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC UBUNTU18</th><th>X86 UBUNTU18</th><th>PPC RHEL72</th><th>X86 RHEL72</th><th>PPC RHEL75</th><th>X86 RHEL75</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 7675</div><div>Failed Count : 3</div><div>Skipped Count : 246</div></td><td><div>Total Count : 7649</div><div>Failed Count : 3</div><div>Skipped Count : 246</div></td><td><div>Total Count : 7651</div><div>Failed Count : 15</div><div>Skipped Count : 246</div></td><td><div>Total Count : 7678</div><div>Failed Count : 3</div><div>Skipped Count : 246</div></td><td><div>Total Count : 7678</div><div>Failed Count : 3</div><div>Skipped Count : 246</div></td><td><div>Total Count : 7678</div><div>Failed Count : 3</div><div>Skipped Count : 246</div></td><td><div>Total Count : 7678</div><div>Failed Count : 8</div><div>Skipped Count : 246</div></td><td><div>Total Count : 7678</div><div>Failed Count : 1</div><div>Skipped Count : 246</div></td></tr><tr><td>Result</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hive.hcatalog.pig.TestAvroHCatStorer.testWriteDecimalX</li></div><div><li>org.apache.hive.hcatalog.pig.TestAvroHCatStorer.testWriteDecimal</li></div><div><li>org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskCommunicator.testFinishableStateUpdateFailure</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskCommunicator.testFinishableStateUpdateFailure</li></div><div><li>org.apache.hadoop.hive.ql.TestTxnCommands.testMergeOnTezEdges</li></div><div><li>org.apache.hadoop.hive.ql.TestTxnCommands.testMergeOnTezEdges</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hive.hcatalog.cli.TestPermsGrp.testCustomPerms</li></div><div><li>org.apache.hive.hcatalog.mapreduce.TestHCatMultiOutputFormat.org.apache.hive.hcatalog.mapreduce.TestHCatMultiOutputFormat</li></div><div><li>org.apache.hive.hcatalog.mapreduce.TestHCatPartitionPublish.org.apache.hive.hcatalog.mapreduce.TestHCatPartitionPublish</li></div><div><li>org.apache.hive.hcatalog.pig.TestAvroHCatStorer.testWriteDecimalX</li></div><div><li>org.apache.hive.hcatalog.pig.TestAvroHCatStorer.testWriteDecimal</li></div><div><li>org.apache.hive.hcatalog.api.TestHCatClient.org.apache.hive.hcatalog.api.TestHCatClient</li></div><div><li>org.apache.hive.hcatalog.api.repl.commands.TestCommands.org.apache.hive.hcatalog.api.repl.commands.TestCommands</li></div><div><li>org.apache.hadoop.hive.llap.daemon.impl.TestContainerRunnerImpl.testRegisterDag</li></div><div><li>org.apache.hadoop.hive.llap.daemon.impl.TestLlapDaemonProtocolServerImpl.testSimpleCall</li></div><div><li>org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskCommunicator.testFinishableStateUpdateFailure</li></div><div><li>org.apache.hadoop.hive.ql.metadata.TestHiveRemote.testPartition</li></div><div><li>org.apache.hadoop.hive.ql.metadata.TestHiveRemote.testAutoPurgeTablesAndPartitions</li></div><div><li>org.apache.hadoop.hive.ql.metadata.TestHiveRemote.testThriftTable</li></div><div><li>org.apache.hadoop.hive.ql.metadata.TestHiveRemote.testMetaStoreApiTiming</li></div><div><li>org.apache.hadoop.hive.ql.metadata.TestHiveRemote.testDropPartitionsWithPurge</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hive.hcatalog.pig.TestAvroHCatStorer.testWriteDecimalX</li></div><div><li>org.apache.hive.hcatalog.pig.TestAvroHCatStorer.testWriteDecimal</li></div><div><li>org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskCommunicator.testFinishableStateUpdateFailure</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hive.hcatalog.pig.TestAvroHCatStorer.testWriteDecimalX</li></div><div><li>org.apache.hive.hcatalog.pig.TestAvroHCatStorer.testWriteDecimal</li></div><div><li>org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskCommunicator.testFinishableStateUpdateFailure</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hive.hcatalog.pig.TestAvroHCatStorer.testWriteDecimalX</li></div><div><li>org.apache.hive.hcatalog.pig.TestAvroHCatStorer.testWriteDecimal</li></div><div><li>org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskCommunicator.testFinishableStateUpdateFailure</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hive.hcatalog.pig.TestAvroHCatStorer.testWriteDecimalX</li></div><div><li>org.apache.hive.hcatalog.pig.TestAvroHCatStorer.testWriteDecimal</li></div><div><li>org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskCommunicator.testFinishableStateUpdateFailure</li></div><div><li>org.apache.hadoop.hive.metastore.TestObjectStore.testDirectSQLDropPartitionsCacheCrossSession</li></div><div><li>org.apache.hadoop.hive.metastore.TestObjectStore.testDirectSQLDropParitionsCleanup</li></div><div><li>org.apache.hadoop.hive.metastore.TestObjectStore.testPartitionOps</li></div><div><li>org.apache.hadoop.hive.metastore.TestObjectStore.testQueryCloseOnError</li></div><div><li>org.apache.hadoop.hive.metastore.TestObjectStore.testMaxEventResponse</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskCommunicator.testFinishableStateUpdateFailure</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"><div><li>Failed to execute "create table junitTypeTest1(f1 decimal(2)) stored as AVRO TBLPROPERTIES ('transactional'='false')". Driver returned 1 Error: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. java.lang.RuntimeException: MetaException(message:java.lang.NoSuchMethodError org.codehaus.jackson.JsonNode.asInt()I)</li></div><div><li>Failed to execute "create table junitTypeTest1(f1 decimal) stored as AVRO TBLPROPERTIES ('transactional'='false')". Driver returned 1 Error: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. java.lang.RuntimeException: MetaException(message:java.lang.NoSuchMethodError org.codehaus.jackson.JsonNode.asInt()I)</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos$SignableVertexSpec$Builder.setUser(LlapDaemonProtocolProtos.java:4899)
	at org.apache.hadoop.hive.llap.tez.Converters.constructSignableVertexSpec(Converters.java:135)
	at org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.constructSubmitWorkRequest(LlapTaskCommunicator.java:845)
	at org.apac</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>java.lang.NullPointerException
	at org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos$SignableVertexSpec$Builder.setUser(LlapDaemonProtocolProtos.java:4899)
	at org.apache.hadoop.hive.llap.tez.Converters.constructSignableVertexSpec(Converters.java:135)
	at org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.constructSubmitWorkRequest(LlapTaskCommunicator.java:845)
	at org.apac</li></div><div><li>drop table if exists acidTbl failed: (responseCode = 10280, errorMessage = FAILED: LockException [Error 10280]: Error communicating with the metastore, hiveErrorCode = 10280, SQLState = 42000, exception = Error communicating with the metastore)</li></div><div><li>drop table if exists acidTbl failed: (responseCode = 10280, errorMessage = FAILED: LockException [Error 10280]: Error communicating with the metastore, hiveErrorCode = 10280, SQLState = 42000, exception = Error communicating with the metastore)</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Connection refused (Connection refused)</li></div><div><li>InvalidObjectException(message:default)</li></div><div><li>Connection refused (Connection refused)</li></div><div><li>Failed to execute "create table junitTypeTest1(f1 decimal(2)) stored as AVRO TBLPROPERTIES ('transactional'='false')". Driver returned 1 Error: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. java.lang.RuntimeException: MetaException(message:java.lang.NoSuchMethodError org.codehaus.jackson.JsonNode.asInt()I)</li></div><div><li>Failed to execute "create table junitTypeTest1(f1 decimal) stored as AVRO TBLPROPERTIES ('transactional'='false')". Driver returned 1 Error: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. java.lang.RuntimeException: MetaException(message:java.lang.NoSuchMethodError org.codehaus.jackson.JsonNode.asInt()I)</li></div><div><li>Connection refused (Connection refused)</li></div><div><li>Connection refused (Connection refused)</li></div><div><li>test timed out after 10000 milliseconds</li></div><div><li>test timed out after 10000 milliseconds</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos$SignableVertexSpec$Builder.setUser(LlapDaemonProtocolProtos.java:4899)
	at org.apache.hadoop.hive.llap.tez.Converters.constructSignableVertexSpec(Converters.java:135)
	at org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.constructSubmitWorkRequest(LlapTaskCommunicator.java:845)
	at org.apac</li></div><div><li>Connection refused (Connection refused)</li></div><div><li>Unable to drop and create table default.table_for_testAutoPurgeTablesAndPartitions because org.apache.hadoop.hive.ql.metadata.HiveException: InvalidObjectException(message:default)
 at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1013)
 at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1018)
 at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:934)
 at </li></div><div><li>Unable to create table: table_for_test_thrifttable</li></div><div><li>Connection refused (Connection refused)</li></div><div><li>Unable to drop and create table default.table_for_testDropPartitionsWithPurge because org.apache.hadoop.hive.ql.metadata.HiveException: InvalidObjectException(message:default)
 at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1013)
 at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:1018)
 at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:934)
 at org.a</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Failed to execute "create table junitTypeTest1(f1 decimal(2)) stored as AVRO TBLPROPERTIES ('transactional'='false')". Driver returned 1 Error: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. java.lang.RuntimeException: MetaException(message:java.lang.NoSuchMethodError org.codehaus.jackson.JsonNode.asInt()I)</li></div><div><li>Failed to execute "create table junitTypeTest1(f1 decimal) stored as AVRO TBLPROPERTIES ('transactional'='false')". Driver returned 1 Error: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. java.lang.RuntimeException: MetaException(message:java.lang.NoSuchMethodError org.codehaus.jackson.JsonNode.asInt()I)</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos$SignableVertexSpec$Builder.setUser(LlapDaemonProtocolProtos.java:4899)
	at org.apache.hadoop.hive.llap.tez.Converters.constructSignableVertexSpec(Converters.java:135)
	at org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.constructSubmitWorkRequest(LlapTaskCommunicator.java:845)
	at org.apac</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Failed to execute "create table junitTypeTest1(f1 decimal(2)) stored as AVRO TBLPROPERTIES ('transactional'='false')". Driver returned 1 Error: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. java.lang.RuntimeException: MetaException(message:java.lang.NoSuchMethodError org.codehaus.jackson.JsonNode.asInt()I)</li></div><div><li>Failed to execute "create table junitTypeTest1(f1 decimal) stored as AVRO TBLPROPERTIES ('transactional'='false')". Driver returned 1 Error: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. java.lang.RuntimeException: MetaException(message:java.lang.NoSuchMethodError org.codehaus.jackson.JsonNode.asInt()I)</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos$SignableVertexSpec$Builder.setUser(LlapDaemonProtocolProtos.java:4899)
	at org.apache.hadoop.hive.llap.tez.Converters.constructSignableVertexSpec(Converters.java:135)
	at org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.constructSubmitWorkRequest(LlapTaskCommunicator.java:845)
	at org.apac</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Failed to execute "create table junitTypeTest1(f1 decimal(2)) stored as AVRO TBLPROPERTIES ('transactional'='false')". Driver returned 1 Error: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. java.lang.RuntimeException: MetaException(message:java.lang.NoSuchMethodError org.codehaus.jackson.JsonNode.asInt()I)</li></div><div><li>Failed to execute "create table junitTypeTest1(f1 decimal) stored as AVRO TBLPROPERTIES ('transactional'='false')". Driver returned 1 Error: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. java.lang.RuntimeException: MetaException(message:java.lang.NoSuchMethodError org.codehaus.jackson.JsonNode.asInt()I)</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos$SignableVertexSpec$Builder.setUser(LlapDaemonProtocolProtos.java:4899)
	at org.apache.hadoop.hive.llap.tez.Converters.constructSignableVertexSpec(Converters.java:135)
	at org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.constructSubmitWorkRequest(LlapTaskCommunicator.java:845)
	at org.apac</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Failed to execute "create table junitTypeTest1(f1 decimal(2)) stored as AVRO TBLPROPERTIES ('transactional'='false')". Driver returned 1 Error: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. java.lang.RuntimeException: MetaException(message:java.lang.NoSuchMethodError org.codehaus.jackson.JsonNode.asInt()I)</li></div><div><li>Failed to execute "create table junitTypeTest1(f1 decimal) stored as AVRO TBLPROPERTIES ('transactional'='false')". Driver returned 1 Error: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. java.lang.RuntimeException: MetaException(message:java.lang.NoSuchMethodError org.codehaus.jackson.JsonNode.asInt()I)</li></div><div><li>java.lang.NullPointerException
	at org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos$SignableVertexSpec$Builder.setUser(LlapDaemonProtocolProtos.java:4899)
	at org.apache.hadoop.hive.llap.tez.Converters.constructSignableVertexSpec(Converters.java:135)
	at org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.constructSubmitWorkRequest(LlapTaskCommunicator.java:845)
	at org.apac</li></div><div><li>GC overhead limit exceeded</li></div><div><li>GC overhead limit exceeded</li></div><div><li>Timeout when executing method: getPartition; 356191ms exceeds 100000ms</li></div><div><li>Timeout when executing method: getPartition; 356248ms exceeds 100000ms</li></div><div><li>Timeout when executing method: getPartition; 356290ms exceeds 100000ms</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>java.lang.NullPointerException
	at org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos$SignableVertexSpec$Builder.setUser(LlapDaemonProtocolProtos.java:4899)
	at org.apache.hadoop.hive.llap.tez.Converters.constructSignableVertexSpec(Converters.java:135)
	at org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.constructSubmitWorkRequest(LlapTaskCommunicator.java:845)
	at org.apac</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hive.hcatalog.pig.TestAvroHCatStorer.testWriteDecimalX</div></li><li><div>org.apache.hive.hcatalog.pig.TestAvroHCatStorer.testWriteDecimal</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hadoop.hive.ql.TestTxnCommands.testMergeOnTezEdges</div></li><li><div>org.apache.hadoop.hive.ql.TestTxnCommands.testMergeOnTezEdges</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hive.hcatalog.cli.TestPermsGrp.testCustomPerms</div></li><li><div>org.apache.hive.hcatalog.mapreduce.TestHCatMultiOutputFormat.org.apache.hive.hcatalog.mapreduce.TestHCatMultiOutputFormat</div></li><li><div>org.apache.hive.hcatalog.mapreduce.TestHCatPartitionPublish.org.apache.hive.hcatalog.mapreduce.TestHCatPartitionPublish</div></li><li><div>org.apache.hive.hcatalog.api.TestHCatClient.org.apache.hive.hcatalog.api.TestHCatClient</div></li><li><div>org.apache.hive.hcatalog.api.repl.commands.TestCommands.org.apache.hive.hcatalog.api.repl.commands.TestCommands</div></li><li><div>org.apache.hadoop.hive.llap.daemon.impl.TestContainerRunnerImpl.testRegisterDag</div></li><li><div>org.apache.hadoop.hive.llap.daemon.impl.TestLlapDaemonProtocolServerImpl.testSimpleCall</div></li><li><div>org.apache.hadoop.hive.ql.metadata.TestHiveRemote.testPartition</div></li><li><div>org.apache.hadoop.hive.ql.metadata.TestHiveRemote.testAutoPurgeTablesAndPartitions</div></li><li><div>org.apache.hadoop.hive.ql.metadata.TestHiveRemote.testThriftTable</div></li><li><div>org.apache.hadoop.hive.ql.metadata.TestHiveRemote.testMetaStoreApiTiming</div></li><li><div>org.apache.hadoop.hive.ql.metadata.TestHiveRemote.testDropPartitionsWithPurge</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.hive.hcatalog.pig.TestAvroHCatStorer.testWriteDecimalX</div></li><li><div>org.apache.hive.hcatalog.pig.TestAvroHCatStorer.testWriteDecimal</div></li><li><div>org.apache.hadoop.hive.metastore.TestObjectStore.testDirectSQLDropPartitionsCacheCrossSession</div></li><li><div>org.apache.hadoop.hive.metastore.TestObjectStore.testDirectSQLDropParitionsCleanup</div></li><li><div>org.apache.hadoop.hive.metastore.TestObjectStore.testPartitionOps</div></li><li><div>org.apache.hadoop.hive.metastore.TestObjectStore.testQueryCloseOnError</div></li><li><div>org.apache.hadoop.hive.metastore.TestObjectStore.testMaxEventResponse</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="kafka"><div style="font-weight:bold;" class="panel-heading">KAFKA<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Prajyot)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/trunk</div><div><b>Last Revision: </b>14d3ead19d250f2f3117af473ff6244c663ef8ca</div><div><b>Last Run: </b>15-11-2018 19:00 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC UBUNTU18</th><th>X86 UBUNTU18</th><th>PPC RHEL72</th><th>X86 RHEL72</th><th>PPC RHEL75</th><th>X86 RHEL75</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 10226</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 10226</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 10226</div><div>Failed Count : 1</div><div>Skipped Count : 5</div></td><td><div>Total Count : 10226</div><div>Failed Count : 1</div><div>Skipped Count : 5</div></td><td><div>Total Count : 10226</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 10226</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 10226</div><div>Failed Count : 1</div><div>Skipped Count : 5</div></td><td><div>Total Count : 10226</div><div>Failed Count : 1</div><div>Skipped Count : 5</div></td></tr><tr><td>Result</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>kafka.admin.DeleteTopicTest.testAddPartitionDuringDeleteTopic</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.kafka.common.network.SslSelectorTest.testCloseConnectionInClosingState</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>kafka.api.PlaintextConsumerTest.testLowMaxFetchSizeForRequestAndPartition</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>kafka.server.DynamicBrokerReconfigurationTest.testKeyStoreDescribeUsingAdminClient</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>kafka.admin.AdminOperationException: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /brokers/topics/test</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>java.lang.AssertionError: Channel not expired expected null, but was:&lt;org.apache.kafka.common.network.KafkaChannel@4f id=0&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>java.lang.AssertionError: Timed out before consuming expected 2700 records. The number consumed was 2610.</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>java.lang.SecurityException: zookeeper.set.acl is true, but the verification of the JAAS login file failed.</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>kafka.admin.DeleteTopicTest.testAddPartitionDuringDeleteTopic</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.kafka.common.network.SslSelectorTest.testCloseConnectionInClosingState</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>kafka.api.PlaintextConsumerTest.testLowMaxFetchSizeForRequestAndPartition</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>kafka.server.DynamicBrokerReconfigurationTest.testKeyStoreDescribeUsingAdminClient</div></li></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="knox"><div style="font-weight:bold;" class="panel-heading">KNOX<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Yussuf)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>fc04f413971e81a6f07ae02dca647035c2cc45a3</div><div><b>Last Run: </b>13-11-2018 01:04 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC UBUNTU18</th><th>X86 UBUNTU18</th><th>PPC RHEL72</th><th>X86 RHEL72</th><th>PPC RHEL75</th><th>X86 RHEL75</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 1095</div><div>Failed Count : 1</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1097</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1095</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1097</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1095</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1097</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1095</div><div>Failed Count : 1</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1097</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td></tr><tr><td>Result</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"><div><li>org.apache.knox.gateway.topology.monitor.RemoteConfigurationMonitorTest.testZooKeeperConfigMonitorSASLCreateNodes</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.knox.gateway.topology.monitor.RemoteConfigurationMonitorTest.testZooKeeperConfigMonitorSASLNodesExistWithUnacceptableACL</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"><div><li>java.lang.AssertionError
	at org.apache.knox.gateway.topology.monitor.RemoteConfigurationMonitorTest.configureAndStartZKCluster(RemoteConfigurationMonitorTest.java:188)
	at org.apache.knox.gateway.topology.monitor.RemoteConfigurationMonitorTest.setupTest(RemoteConfigurationMonitorTest.java:111)
</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>java.lang.AssertionError
	at org.apache.knox.gateway.topology.monitor.RemoteConfigurationMonitorTest.configureAndStartZKCluster(RemoteConfigurationMonitorTest.java:188)
	at org.apache.knox.gateway.topology.monitor.RemoteConfigurationMonitorTest.setupTest(RemoteConfigurationMonitorTest.java:111)
</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.knox.gateway.topology.monitor.RemoteConfigurationMonitorTest.testZooKeeperConfigMonitorSASLCreateNodes</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.knox.gateway.topology.monitor.RemoteConfigurationMonitorTest.testZooKeeperConfigMonitorSASLNodesExistWithUnacceptableACL</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="metron"><div style="font-weight:bold;" class="panel-heading">METRON<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Pravin)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>42068d95192e7346bf0179399193a6afe01d9925</div><div><b>Last Run: </b>11-11-2018 12:39 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC UBUNTU18</th><th>X86 UBUNTU18</th><th>PPC RHEL72</th><th>X86 RHEL72</th><th>PPC RHEL75</th><th>X86 RHEL75</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 2030</div><div>Failed Count : 13</div><div>Skipped Count : 2</div></td><td><div>Total Count : 2030</div><div>Failed Count : 13</div><div>Skipped Count : 2</div></td><td><div>Total Count : 2030</div><div>Failed Count : 13</div><div>Skipped Count : 2</div></td><td><div>Total Count : 2030</div><div>Failed Count : 13</div><div>Skipped Count : 2</div></td><td><div>Total Count : 2030</div><div>Failed Count : 13</div><div>Skipped Count : 2</div></td><td><div>Total Count : 2030</div><div>Failed Count : 13</div><div>Skipped Count : 2</div></td><td><div>Total Count : 2030</div><div>Failed Count : 13</div><div>Skipped Count : 2</div></td><td><div>Total Count : 2030</div><div>Failed Count : 13</div><div>Skipped Count : 2</div></td></tr><tr><td>Result</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testBytesMatch[0]</li></div><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testStringMatch[0]</li></div><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testBytesMatch[1]</li></div><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testStringMatch[1]</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathStringReturned</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathFormatVariable</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathFormatConstant</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testSingleFileIfNoStreamClosed</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testHandleAttemptsRotateIfStreamClosed</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathMultipleFunctions</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFile</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteMultipleFiles</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFileWithNull</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testBytesMatch[0]</li></div><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testStringMatch[0]</li></div><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testBytesMatch[1]</li></div><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testStringMatch[1]</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathStringReturned</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathFormatVariable</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathFormatConstant</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testSingleFileIfNoStreamClosed</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testHandleAttemptsRotateIfStreamClosed</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathMultipleFunctions</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFile</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteMultipleFiles</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFileWithNull</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testBytesMatch[0]</li></div><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testStringMatch[0]</li></div><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testBytesMatch[1]</li></div><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testStringMatch[1]</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathStringReturned</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathFormatVariable</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathFormatConstant</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testSingleFileIfNoStreamClosed</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testHandleAttemptsRotateIfStreamClosed</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathMultipleFunctions</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFile</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteMultipleFiles</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFileWithNull</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testBytesMatch[0]</li></div><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testStringMatch[0]</li></div><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testBytesMatch[1]</li></div><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testStringMatch[1]</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathStringReturned</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathFormatVariable</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathFormatConstant</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testSingleFileIfNoStreamClosed</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testHandleAttemptsRotateIfStreamClosed</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathMultipleFunctions</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFile</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteMultipleFiles</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFileWithNull</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testBytesMatch[0]</li></div><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testStringMatch[0]</li></div><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testBytesMatch[1]</li></div><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testStringMatch[1]</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathStringReturned</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathFormatVariable</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathFormatConstant</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testSingleFileIfNoStreamClosed</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testHandleAttemptsRotateIfStreamClosed</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathMultipleFunctions</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFile</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteMultipleFiles</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFileWithNull</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testBytesMatch[0]</li></div><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testStringMatch[0]</li></div><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testBytesMatch[1]</li></div><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testStringMatch[1]</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathStringReturned</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathFormatVariable</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathFormatConstant</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testSingleFileIfNoStreamClosed</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testHandleAttemptsRotateIfStreamClosed</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathMultipleFunctions</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFile</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteMultipleFiles</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFileWithNull</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testBytesMatch[0]</li></div><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testStringMatch[0]</li></div><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testBytesMatch[1]</li></div><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testStringMatch[1]</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathStringReturned</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathFormatVariable</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathFormatConstant</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testSingleFileIfNoStreamClosed</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testHandleAttemptsRotateIfStreamClosed</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathMultipleFunctions</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFile</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteMultipleFiles</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFileWithNull</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testBytesMatch[0]</li></div><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testStringMatch[0]</li></div><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testBytesMatch[1]</li></div><div><li>org.apache.metron.pcap.pattern.ByteArrayMatchingUtilTest.testStringMatch[1]</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathStringReturned</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathFormatVariable</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathFormatConstant</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testSingleFileIfNoStreamClosed</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testHandleAttemptsRotateIfStreamClosed</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testGetHdfsPathMultipleFunctions</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFile</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteMultipleFiles</li></div><div><li>org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFileWithNull</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"><div><li>Unable to parse BYTEARRAY_MATCHER(pattern, data): Unable to parse: BYTEARRAY_MATCHER(pattern, data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing,pattern=missing</li></div><div><li>Unable to parse BYTEARRAY_MATCHER(pattern, data): Unable to parse: BYTEARRAY_MATCHER(pattern, data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing,pattern=missing</li></div><div><li>Unable to parse BYTEARRAY_MATCHER('2f56abd814bc56420489ca38e7faf8cec3d4', data): Unable to parse: BYTEARRAY_MATCHER('2f56abd814bc56420489ca38e7faf8cec3d4', data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing</li></div><div><li>Unable to parse BYTEARRAY_MATCHER('`metron`', data): Unable to parse: BYTEARRAY_MATCHER('`metron`', data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing</li></div><div><li>Unable to parse: TO_UPPER(FORMAT(MAP_GET('key', {'key': 'AbC%s'}), test.key)) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables test.key=test.value</li></div><div><li>Unable to parse: FORMAT('%s/%s/%s', test.key, test.key.2, test.key.3) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables test.key.2=test.value.2,test.key=test.value,test.key.3=test.value.3</li></div><div><li>Unable to parse: FORMAT('/test/folder/') due to: org/apache/http/conn/HttpClientConnectionManager</li></div><div><li>java.lang.NullPointerException
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testSingleFileIfNoStreamClosed(HdfsWriterTest.java:447)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method</li></div><div><li>expected:&lt;1&gt; but was:&lt;0&gt;</li></div><div><li>Unable to parse: FORMAT('%s', test.key) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables test.key=test.value</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFile(HdfsWriterTest.java:313)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:6</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteMultipleFiles(HdfsWriterTest.java:356)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.jav</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFileWithNull(HdfsWriterTest.java:408)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImp</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Unable to parse BYTEARRAY_MATCHER(pattern, data): Unable to parse: BYTEARRAY_MATCHER(pattern, data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing,pattern=missing</li></div><div><li>Unable to parse BYTEARRAY_MATCHER(pattern, data): Unable to parse: BYTEARRAY_MATCHER(pattern, data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing,pattern=missing</li></div><div><li>Unable to parse BYTEARRAY_MATCHER('2f56abd814bc56420489ca38e7faf8cec3d4', data): Unable to parse: BYTEARRAY_MATCHER('2f56abd814bc56420489ca38e7faf8cec3d4', data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing</li></div><div><li>Unable to parse BYTEARRAY_MATCHER('`metron`', data): Unable to parse: BYTEARRAY_MATCHER('`metron`', data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing</li></div><div><li>Unable to parse: TO_UPPER(FORMAT(MAP_GET('key', {'key': 'AbC%s'}), test.key)) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables test.key=test.value</li></div><div><li>Unable to parse: FORMAT('%s/%s/%s', test.key, test.key.2, test.key.3) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables test.key.2=test.value.2,test.key=test.value,test.key.3=test.value.3</li></div><div><li>Unable to parse: FORMAT('/test/folder/') due to: org/apache/http/conn/HttpClientConnectionManager</li></div><div><li>java.lang.NullPointerException
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testSingleFileIfNoStreamClosed(HdfsWriterTest.java:447)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method</li></div><div><li>expected:&lt;1&gt; but was:&lt;0&gt;</li></div><div><li>Unable to parse: FORMAT('%s', test.key) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables test.key=test.value</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFile(HdfsWriterTest.java:313)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:6</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteMultipleFiles(HdfsWriterTest.java:356)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.jav</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFileWithNull(HdfsWriterTest.java:408)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImp</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Unable to parse BYTEARRAY_MATCHER(pattern, data): Unable to parse: BYTEARRAY_MATCHER(pattern, data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing,pattern=missing</li></div><div><li>Unable to parse BYTEARRAY_MATCHER(pattern, data): Unable to parse: BYTEARRAY_MATCHER(pattern, data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing,pattern=missing</li></div><div><li>Unable to parse BYTEARRAY_MATCHER('2f56abd814bc56420489ca38e7faf8cec3d4', data): Unable to parse: BYTEARRAY_MATCHER('2f56abd814bc56420489ca38e7faf8cec3d4', data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing</li></div><div><li>Unable to parse BYTEARRAY_MATCHER('`metron`', data): Unable to parse: BYTEARRAY_MATCHER('`metron`', data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing</li></div><div><li>Unable to parse: TO_UPPER(FORMAT(MAP_GET('key', {'key': 'AbC%s'}), test.key)) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables test.key=test.value</li></div><div><li>Unable to parse: FORMAT('%s/%s/%s', test.key, test.key.2, test.key.3) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables test.key.2=test.value.2,test.key=test.value,test.key.3=test.value.3</li></div><div><li>Unable to parse: FORMAT('/test/folder/') due to: org/apache/http/conn/HttpClientConnectionManager</li></div><div><li>java.lang.NullPointerException
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testSingleFileIfNoStreamClosed(HdfsWriterTest.java:447)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method</li></div><div><li>expected:&lt;1&gt; but was:&lt;0&gt;</li></div><div><li>Unable to parse: FORMAT('%s', test.key) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables test.key=test.value</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFile(HdfsWriterTest.java:313)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:6</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteMultipleFiles(HdfsWriterTest.java:356)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.jav</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFileWithNull(HdfsWriterTest.java:408)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImp</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Unable to parse BYTEARRAY_MATCHER(pattern, data): Unable to parse: BYTEARRAY_MATCHER(pattern, data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing,pattern=missing</li></div><div><li>Unable to parse BYTEARRAY_MATCHER(pattern, data): Unable to parse: BYTEARRAY_MATCHER(pattern, data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing,pattern=missing</li></div><div><li>Unable to parse BYTEARRAY_MATCHER('2f56abd814bc56420489ca38e7faf8cec3d4', data): Unable to parse: BYTEARRAY_MATCHER('2f56abd814bc56420489ca38e7faf8cec3d4', data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing</li></div><div><li>Unable to parse BYTEARRAY_MATCHER('`metron`', data): Unable to parse: BYTEARRAY_MATCHER('`metron`', data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing</li></div><div><li>Unable to parse: TO_UPPER(FORMAT(MAP_GET('key', {'key': 'AbC%s'}), test.key)) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables test.key=test.value</li></div><div><li>Unable to parse: FORMAT('%s/%s/%s', test.key, test.key.2, test.key.3) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables test.key.2=test.value.2,test.key=test.value,test.key.3=test.value.3</li></div><div><li>Unable to parse: FORMAT('/test/folder/') due to: org/apache/http/conn/HttpClientConnectionManager</li></div><div><li>java.lang.NullPointerException
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testSingleFileIfNoStreamClosed(HdfsWriterTest.java:447)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method</li></div><div><li>expected:&lt;1&gt; but was:&lt;0&gt;</li></div><div><li>Unable to parse: FORMAT('%s', test.key) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables test.key=test.value</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFile(HdfsWriterTest.java:313)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:6</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteMultipleFiles(HdfsWriterTest.java:356)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.jav</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFileWithNull(HdfsWriterTest.java:408)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImp</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Unable to parse BYTEARRAY_MATCHER(pattern, data): Unable to parse: BYTEARRAY_MATCHER(pattern, data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing,pattern=missing</li></div><div><li>Unable to parse BYTEARRAY_MATCHER(pattern, data): Unable to parse: BYTEARRAY_MATCHER(pattern, data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing,pattern=missing</li></div><div><li>Unable to parse BYTEARRAY_MATCHER('2f56abd814bc56420489ca38e7faf8cec3d4', data): Unable to parse: BYTEARRAY_MATCHER('2f56abd814bc56420489ca38e7faf8cec3d4', data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing</li></div><div><li>Unable to parse BYTEARRAY_MATCHER('`metron`', data): Unable to parse: BYTEARRAY_MATCHER('`metron`', data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing</li></div><div><li>Unable to parse: TO_UPPER(FORMAT(MAP_GET('key', {'key': 'AbC%s'}), test.key)) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables test.key=test.value</li></div><div><li>Unable to parse: FORMAT('%s/%s/%s', test.key, test.key.2, test.key.3) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables test.key.2=test.value.2,test.key=test.value,test.key.3=test.value.3</li></div><div><li>Unable to parse: FORMAT('/test/folder/') due to: org/apache/http/conn/HttpClientConnectionManager</li></div><div><li>java.lang.NullPointerException
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testSingleFileIfNoStreamClosed(HdfsWriterTest.java:447)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method</li></div><div><li>expected:&lt;1&gt; but was:&lt;0&gt;</li></div><div><li>Unable to parse: FORMAT('%s', test.key) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables test.key=test.value</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFile(HdfsWriterTest.java:313)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:6</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteMultipleFiles(HdfsWriterTest.java:356)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.jav</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFileWithNull(HdfsWriterTest.java:408)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImp</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Unable to parse BYTEARRAY_MATCHER(pattern, data): Unable to parse: BYTEARRAY_MATCHER(pattern, data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing,pattern=missing</li></div><div><li>Unable to parse BYTEARRAY_MATCHER(pattern, data): Unable to parse: BYTEARRAY_MATCHER(pattern, data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing,pattern=missing</li></div><div><li>Unable to parse BYTEARRAY_MATCHER('2f56abd814bc56420489ca38e7faf8cec3d4', data): Unable to parse: BYTEARRAY_MATCHER('2f56abd814bc56420489ca38e7faf8cec3d4', data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing</li></div><div><li>Unable to parse BYTEARRAY_MATCHER('`metron`', data): Unable to parse: BYTEARRAY_MATCHER('`metron`', data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing</li></div><div><li>Unable to parse: TO_UPPER(FORMAT(MAP_GET('key', {'key': 'AbC%s'}), test.key)) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables test.key=test.value</li></div><div><li>Unable to parse: FORMAT('%s/%s/%s', test.key, test.key.2, test.key.3) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables test.key.2=test.value.2,test.key=test.value,test.key.3=test.value.3</li></div><div><li>Unable to parse: FORMAT('/test/folder/') due to: org/apache/http/conn/HttpClientConnectionManager</li></div><div><li>java.lang.NullPointerException
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testSingleFileIfNoStreamClosed(HdfsWriterTest.java:447)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method</li></div><div><li>expected:&lt;1&gt; but was:&lt;0&gt;</li></div><div><li>Unable to parse: FORMAT('%s', test.key) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables test.key=test.value</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFile(HdfsWriterTest.java:313)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:6</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteMultipleFiles(HdfsWriterTest.java:356)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.jav</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFileWithNull(HdfsWriterTest.java:408)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImp</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Unable to parse BYTEARRAY_MATCHER(pattern, data): Unable to parse: BYTEARRAY_MATCHER(pattern, data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing,pattern=missing</li></div><div><li>Unable to parse BYTEARRAY_MATCHER(pattern, data): Unable to parse: BYTEARRAY_MATCHER(pattern, data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing,pattern=missing</li></div><div><li>Unable to parse BYTEARRAY_MATCHER('2f56abd814bc56420489ca38e7faf8cec3d4', data): Unable to parse: BYTEARRAY_MATCHER('2f56abd814bc56420489ca38e7faf8cec3d4', data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing</li></div><div><li>Unable to parse BYTEARRAY_MATCHER('`metron`', data): Unable to parse: BYTEARRAY_MATCHER('`metron`', data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing</li></div><div><li>Unable to parse: TO_UPPER(FORMAT(MAP_GET('key', {'key': 'AbC%s'}), test.key)) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables test.key=test.value</li></div><div><li>Unable to parse: FORMAT('%s/%s/%s', test.key, test.key.2, test.key.3) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables test.key.2=test.value.2,test.key=test.value,test.key.3=test.value.3</li></div><div><li>Unable to parse: FORMAT('/test/folder/') due to: org/apache/http/conn/HttpClientConnectionManager</li></div><div><li>java.lang.NullPointerException
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testSingleFileIfNoStreamClosed(HdfsWriterTest.java:447)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method</li></div><div><li>expected:&lt;1&gt; but was:&lt;0&gt;</li></div><div><li>Unable to parse: FORMAT('%s', test.key) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables test.key=test.value</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFile(HdfsWriterTest.java:313)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:6</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteMultipleFiles(HdfsWriterTest.java:356)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.jav</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFileWithNull(HdfsWriterTest.java:408)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImp</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Unable to parse BYTEARRAY_MATCHER(pattern, data): Unable to parse: BYTEARRAY_MATCHER(pattern, data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing,pattern=missing</li></div><div><li>Unable to parse BYTEARRAY_MATCHER(pattern, data): Unable to parse: BYTEARRAY_MATCHER(pattern, data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing,pattern=missing</li></div><div><li>Unable to parse BYTEARRAY_MATCHER('2f56abd814bc56420489ca38e7faf8cec3d4', data): Unable to parse: BYTEARRAY_MATCHER('2f56abd814bc56420489ca38e7faf8cec3d4', data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing</li></div><div><li>Unable to parse BYTEARRAY_MATCHER('`metron`', data): Unable to parse: BYTEARRAY_MATCHER('`metron`', data) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables data=missing</li></div><div><li>Unable to parse: TO_UPPER(FORMAT(MAP_GET('key', {'key': 'AbC%s'}), test.key)) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables test.key=test.value</li></div><div><li>Unable to parse: FORMAT('%s/%s/%s', test.key, test.key.2, test.key.3) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables test.key.2=test.value.2,test.key=test.value,test.key.3=test.value.3</li></div><div><li>Unable to parse: FORMAT('/test/folder/') due to: org/apache/http/conn/HttpClientConnectionManager</li></div><div><li>java.lang.NullPointerException
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testSingleFileIfNoStreamClosed(HdfsWriterTest.java:447)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method</li></div><div><li>expected:&lt;1&gt; but was:&lt;0&gt;</li></div><div><li>Unable to parse: FORMAT('%s', test.key) due to: org/apache/http/conn/HttpClientConnectionManager with relevant variables test.key=test.value</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFile(HdfsWriterTest.java:313)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:6</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteMultipleFiles(HdfsWriterTest.java:356)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.jav</li></div><div><li>java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.metron.writer.hdfs.HdfsWriterTest.testWriteSingleFileWithNull(HdfsWriterTest.java:408)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImp</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="oozie"><div style="font-weight:bold;" class="panel-heading">OOZIE<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Alisha)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>34289c4c7bb3f3ff4301c5074b5bbc4dad0b35d6</div><div><b>Last Run: </b>15-11-2018 21:58 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC UBUNTU18</th><th>X86 UBUNTU18</th><th>PPC RHEL72</th><th>X86 RHEL72</th><th>PPC RHEL75</th><th>X86 RHEL75</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 3070</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 3070</div><div>Failed Count : 1</div><div>Skipped Count : 2</div></td><td><div>Total Count : 3070</div><div>Failed Count : 2</div><div>Skipped Count : 2</div></td><td><div>Total Count : 3070</div><div>Failed Count : 2</div><div>Skipped Count : 2</div></td><td><div>Total Count : 3070</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 3070</div><div>Failed Count : 1</div><div>Skipped Count : 2</div></td><td><div>Total Count : 3070</div><div>Failed Count : 1</div><div>Skipped Count : 2</div></td><td><div>Total Count : 3070</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td></tr><tr><td>Result</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.oozie.command.coord.TestCoordActionInputCheckXCommandNonUTC.testActionInputCheckLatestActionCreationTimeWithPushDependency</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.oozie.command.coord.TestCoordActionInputCheckXCommand.testCoordActionInputCheckXCommandUniqueness</li></div><div><li>org.apache.oozie.util.TestMetricsInstrumentation.testJMXInstrumentation</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.oozie.command.coord.TestCoordActionInputCheckXCommand.testNone</li></div><div><li>org.apache.oozie.command.wf.TestWorkflowActionRetryInfoXCommand.testRetryConsoleUrlForked</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.oozie.command.coord.TestCoordActionsKillXCommand.testActionKillCommandActionNumbers</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.oozie.service.TestCallableQueueService.testQueueSizeWithDelayedElements</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;[!!${coord:latestRange(-3,0)}]&gt; but was:&lt;[]&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>junit.framework.AssertionFailedError
	at junit.framework.Assert.fail(Assert.java:55)
	at junit.framework.Assert.assertTrue(Assert.java:22)
	at junit.framework.Assert.assertTrue(Assert.java:31)
	at junit.framework.TestCase.assertTrue(TestCase.java:201)
	at org.apache.oozie.command.coord.TestCoordActionInputCheckXCommand.testCoordActionInputCheckXCommandUniqueness(TestCoordActionInputCheckXCommand.j</li></div><div><li>Could not find own virtual machine</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;SKIPPED&gt; but was:&lt;WAITING&gt;</li></div><div><li>expected:&lt;2&gt; but was:&lt;1&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;RUNNING&gt; but was:&lt;RUNNINGWITHERROR&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Queue size after execution expected:&lt;0&gt; but was:&lt;1&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.oozie.command.coord.TestCoordActionInputCheckXCommandNonUTC.testActionInputCheckLatestActionCreationTimeWithPushDependency</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.oozie.command.coord.TestCoordActionInputCheckXCommand.testCoordActionInputCheckXCommandUniqueness</div></li><li><div>org.apache.oozie.util.TestMetricsInstrumentation.testJMXInstrumentation</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.oozie.command.coord.TestCoordActionInputCheckXCommand.testNone</div></li><li><div>org.apache.oozie.command.wf.TestWorkflowActionRetryInfoXCommand.testRetryConsoleUrlForked</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.oozie.command.coord.TestCoordActionsKillXCommand.testActionKillCommandActionNumbers</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.oozie.service.TestCallableQueueService.testQueueSizeWithDelayedElements</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="phoenix"><div style="font-weight:bold;" class="panel-heading">PHOENIX<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Prajyot)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>cd31ed5e8cef8ad8b6d20f7417974c47002f4297</div><div><b>Last Run: </b>15-11-2018 23:45 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC UBUNTU18</th><th>X86 UBUNTU18</th><th>PPC RHEL72</th><th>X86 RHEL72</th><th>PPC RHEL75</th><th>X86 RHEL75</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 1719</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 1719</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 1719</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 1719</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 1719</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 1719</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 1719</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 1719</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td></tr><tr><td>Result</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="pig"><div style="font-weight:bold;" class="panel-heading">PIG<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Yussuf)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/trunk</div><div><b>Last Revision: </b>c25a84233900f499757bd0cbde79c7b77b5b1caf</div><div><b>Last Run: </b>15-11-2018 02:58 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC UBUNTU18</th><th>X86 UBUNTU18</th><th>PPC RHEL72</th><th>X86 RHEL72</th><th>PPC RHEL75</th><th>X86 RHEL75</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 895</div><div>Failed Count : 9</div><div>Skipped Count : 0</div></td><td><div>Total Count : 895</div><div>Failed Count : 9</div><div>Skipped Count : 0</div></td><td><div>Total Count : 895</div><div>Failed Count : 9</div><div>Skipped Count : 0</div></td><td><div>Total Count : 895</div><div>Failed Count : 9</div><div>Skipped Count : 0</div></td><td><div>Total Count : 895</div><div>Failed Count : 9</div><div>Skipped Count : 0</div></td><td><div>Total Count : 895</div><div>Failed Count : 9</div><div>Skipped Count : 0</div></td><td><div>Total Count : 895</div><div>Failed Count : 9</div><div>Skipped Count : 0</div></td><td><div>Total Count : 895</div><div>Failed Count : 10</div><div>Skipped Count : 0</div></td></tr><tr><td>Result</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileDeclareCombo</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testSameParamInMultipleFiles</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileComboDuplicate</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileCombo</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testMultipleParamsinSingleLine</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileDeclareDefaultComboDuplicates</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testFileParamsFromMultipleFiles</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testMultipleDeclareScope</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileDeclareComboDuplicates</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileDeclareCombo</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testSameParamInMultipleFiles</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileComboDuplicate</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileCombo</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testMultipleParamsinSingleLine</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileDeclareDefaultComboDuplicates</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testFileParamsFromMultipleFiles</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testMultipleDeclareScope</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileDeclareComboDuplicates</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileDeclareCombo</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testSameParamInMultipleFiles</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileComboDuplicate</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileCombo</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testMultipleParamsinSingleLine</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileDeclareDefaultComboDuplicates</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testFileParamsFromMultipleFiles</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testMultipleDeclareScope</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileDeclareComboDuplicates</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileDeclareCombo</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testSameParamInMultipleFiles</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileComboDuplicate</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileCombo</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testMultipleParamsinSingleLine</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileDeclareDefaultComboDuplicates</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testFileParamsFromMultipleFiles</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testMultipleDeclareScope</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileDeclareComboDuplicates</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileDeclareCombo</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testSameParamInMultipleFiles</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileComboDuplicate</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileCombo</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testMultipleParamsinSingleLine</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileDeclareDefaultComboDuplicates</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testFileParamsFromMultipleFiles</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testMultipleDeclareScope</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileDeclareComboDuplicates</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileDeclareCombo</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testSameParamInMultipleFiles</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileComboDuplicate</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileCombo</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testMultipleParamsinSingleLine</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileDeclareDefaultComboDuplicates</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testFileParamsFromMultipleFiles</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testMultipleDeclareScope</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileDeclareComboDuplicates</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileDeclareCombo</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testSameParamInMultipleFiles</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileComboDuplicate</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileCombo</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testMultipleParamsinSingleLine</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileDeclareDefaultComboDuplicates</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testFileParamsFromMultipleFiles</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testMultipleDeclareScope</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileDeclareComboDuplicates</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.pig.test.TestBuiltin.testRANDOMWithJob</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileDeclareCombo</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testSameParamInMultipleFiles</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileComboDuplicate</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileCombo</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testMultipleParamsinSingleLine</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileDeclareDefaultComboDuplicates</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testFileParamsFromMultipleFiles</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testMultipleDeclareScope</li></div><div><li>org.apache.pig.test.TestParamSubPreproc.testCmdlineFileDeclareComboDuplicates</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 27 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 27 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 27 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 27 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 27 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 27 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 27 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Values should be different. Actual: 0.2919803076379234</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 27 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div><div><li>Command line parameter substitution failed. Expected : store inactiveAccounts into '/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct'; , but got : store inactiveAccounts into \'/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct\'; in line num : 26 expected:&lt;...activeAccounts into ['/user/kaleidoscope/pow_stats/20080228/acct/InactiveAcct]';&gt; but was:&lt;...activeAccounts into [\'/user/kale</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.pig.test.TestBuiltin.testRANDOMWithJob</div></li></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="ranger"><div style="font-weight:bold;" class="panel-heading">RANGER<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Yussuf)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>a7d29df1f735a102c59079b4dbe7dde0129554f9</div><div><b>Last Run: </b>15-11-2018 01:58 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC UBUNTU18</th><th>X86 UBUNTU18</th><th>PPC RHEL72</th><th>X86 RHEL72</th><th>PPC RHEL75</th><th>X86 RHEL75</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 1315</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1315</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1293</div><div>Failed Count : 1</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1315</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1315</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1315</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1315</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td><td><div>Total Count : 1315</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td></tr><tr><td>Result</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.ranger.services.hive.HIVERangerAuthorizerTest.org.apache.ranger.services.hive.HIVERangerAuthorizerTest</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>Could not open client transport with JDBC Uri: jdbc:hive2://localhost:34397: java.net.ConnectException: Connection refused (Connection refused)</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.ranger.services.hive.HIVERangerAuthorizerTest.org.apache.ranger.services.hive.HIVERangerAuthorizerTest</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="slider"><div style="font-weight:bold;" class="panel-heading">SLIDER<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Yussuf)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/develop</div><div><b>Last Revision: </b>1d4f519d763210f46e327338be72efa99e65cb5d</div><div><b>Last Run: </b>15-11-2018 20:54 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC UBUNTU18</th><th>X86 UBUNTU18</th><th>PPC RHEL72</th><th>X86 RHEL72</th><th>PPC RHEL75</th><th>X86 RHEL75</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 607</div><div>Failed Count : 0</div><div>Skipped Count : 13</div></td><td><div>Total Count : 607</div><div>Failed Count : 0</div><div>Skipped Count : 13</div></td><td><div>Total Count : 607</div><div>Failed Count : 1</div><div>Skipped Count : 13</div></td><td><div>Total Count : 607</div><div>Failed Count : 1</div><div>Skipped Count : 13</div></td><td><div>Total Count : 607</div><div>Failed Count : 0</div><div>Skipped Count : 13</div></td><td><div>Total Count : 607</div><div>Failed Count : 0</div><div>Skipped Count : 13</div></td><td><div>Total Count : 607</div><div>Failed Count : 0</div><div>Skipped Count : 13</div></td><td><div>Total Count : 607</div><div>Failed Count : 0</div><div>Skipped Count : 13</div></td></tr><tr><td>Result</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.slider.agent.standalone.TestStandaloneAMKill.testKillStandaloneAM</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.slider.providers.agent.TestAgentAAEcho.testAgentEcho</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>assert 0 == killAM(SIGTERM)
         |  |      |
         |  123    -15
         false</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>assert cd.getRoleOptInt(rolename, RoleKeys.ROLE_PENDING_AA_INSTANCES, -1) == 0
       |  |             |                  |                              |
       |  49            echo               role.pending.aa.instances      false
       {
         "version" : "1.0",
         "name" : "testagentaaecho",
         "type" : "agent",
         "state" : 3,
         "createTime" : 1542317992237,
   </li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.slider.agent.standalone.TestStandaloneAMKill.testKillStandaloneAM</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.slider.providers.agent.TestAgentAAEcho.testAgentEcho</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="spark"><div style="font-weight:bold;" class="panel-heading">SPARK<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Prajyot)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>4035c98a0c03cf61d1fb9a9916df513ab1081a9b</div><div><b>Last Run: </b>14-11-2018 18:59 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC UBUNTU18</th><th>X86 UBUNTU18</th><th>PPC RHEL72</th><th>X86 RHEL72</th><th>PPC RHEL75</th><th>X86 RHEL75</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 12639</div><div>Failed Count : 3</div><div>Skipped Count : 632</div></td><td><div>Total Count : 2798</div><div>Failed Count : 862</div><div>Skipped Count : 20</div></td><td><div>Total Count : 14920</div><div>Failed Count : 54</div><div>Skipped Count : 645</div></td><td><div>Total Count : 2814</div><div>Failed Count : 885</div><div>Skipped Count : 20</div></td><td><div>Total Count : 14920</div><div>Failed Count : 46</div><div>Skipped Count : 645</div></td><td><div>Total Count : 17045</div><div>Failed Count : 0</div><div>Skipped Count : 653</div></td><td><div>Total Count : 15254</div><div>Failed Count : 0</div><div>Skipped Count : 646</div></td><td><div>Total Count : 17045</div><div>Failed Count : 1</div><div>Skipped Count : 653</div></td></tr><tr><td>Result</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"><div><li>org.apache.spark.sql.execution.ui.SQLAppStatusListenerSuite.driver side SQL metrics</li></div><div><li>org.apache.spark.sql.streaming.StreamingQueryListenerSuite.single listener, check trigger events are generated correctly</li></div><div><li>org.apache.spark.sql.hive.HiveSparkSubmitSuite.SPARK-8020: set sql conf in spark conf</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.spark.network.ChunkFetchIntegrationSuite.fetchFileChunk</li></div><div><li>org.apache.spark.JavaJdbcRDDSuite.testJavaJdbcRDD</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.leftOuterJoin</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.leftOuterJoin</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.foldReduce</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.foldReduce</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.mapsFromPairsToPairs</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.mapsFromPairsToPairs</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.flatMap</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.flatMap</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.foreach</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.foreach</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.map</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.map</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.zip</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.zip</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.keyBy</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.keyBy</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.groupBy</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.mapPartitions</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.foldByKey</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.mapOnPairRDD</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.sequenceFile</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.sequenceFile</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.collectPartitions</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.collectPartitions</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.reduceByKey</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.reduceByKey</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.foreachWithAnonymousClass</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.foreachWithAnonymousClass</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.collectAsMapWithIntArrayValues</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.collectAsMapWithIntArrayValues</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.zipPartitions</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.zipPartitions</li></div><div><li>test.org.apache.spark.JavaAPISuite.groupByOnPairRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.groupByOnPairRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.binaryFilesCaching</li></div><div><li>test.org.apache.spark.JavaAPISuite.binaryFilesCaching</li></div><div><li>test.org.apache.spark.JavaAPISuite.sparkContextUnion</li></div><div><li>test.org.apache.spark.JavaAPISuite.sparkContextUnion</li></div><div><li>test.org.apache.spark.JavaAPISuite.checkpointAndComputation</li></div><div><li>test.org.apache.spark.JavaAPISuite.checkpointAndComputation</li></div><div><li>test.org.apache.spark.JavaAPISuite.leftOuterJoin</li></div><div><li>test.org.apache.spark.JavaAPISuite.leftOuterJoin</li></div><div><li>test.org.apache.spark.JavaAPISuite.keyByOnPairRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.keyByOnPairRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.getNumPartitions</li></div><div><li>test.org.apache.spark.JavaAPISuite.wholeTextFiles</li></div><div><li>test.org.apache.spark.JavaAPISuite.wholeTextFiles</li></div><div><li>test.org.apache.spark.JavaAPISuite.binaryFiles</li></div><div><li>test.org.apache.spark.JavaAPISuite.binaryFiles</li></div><div><li>test.org.apache.spark.JavaAPISuite.foldReduce</li></div><div><li>test.org.apache.spark.JavaAPISuite.foldReduce</li></div><div><li>test.org.apache.spark.JavaAPISuite.writeWithNewAPIHadoopFile</li></div><div><li>test.org.apache.spark.JavaAPISuite.writeWithNewAPIHadoopFile</li></div><div><li>test.org.apache.spark.JavaAPISuite.hadoopFile</li></div><div><li>test.org.apache.spark.JavaAPISuite.hadoopFile</li></div><div><li>test.org.apache.spark.JavaAPISuite.lookup</li></div><div><li>test.org.apache.spark.JavaAPISuite.lookup</li></div><div><li>test.org.apache.spark.JavaAPISuite.countAsync</li></div><div><li>test.org.apache.spark.JavaAPISuite.countAsync</li></div><div><li>test.org.apache.spark.JavaAPISuite.textFiles</li></div><div><li>test.org.apache.spark.JavaAPISuite.textFiles</li></div><div><li>test.org.apache.spark.JavaAPISuite.binaryRecords</li></div><div><li>test.org.apache.spark.JavaAPISuite.binaryRecords</li></div><div><li>test.org.apache.spark.JavaAPISuite.toLocalIterator</li></div><div><li>test.org.apache.spark.JavaAPISuite.toLocalIterator</li></div><div><li>test.org.apache.spark.JavaAPISuite.repartitionAndSortWithinPartitions</li></div><div><li>test.org.apache.spark.JavaAPISuite.repartitionAndSortWithinPartitions</li></div><div><li>test.org.apache.spark.JavaAPISuite.reduce</li></div><div><li>test.org.apache.spark.JavaAPISuite.reduce</li></div><div><li>test.org.apache.spark.JavaAPISuite.sample</li></div><div><li>test.org.apache.spark.JavaAPISuite.sample</li></div><div><li>test.org.apache.spark.JavaAPISuite.sortBy</li></div><div><li>test.org.apache.spark.JavaAPISuite.sortBy</li></div><div><li>test.org.apache.spark.JavaAPISuite.mapsFromPairsToPairs</li></div><div><li>test.org.apache.spark.JavaAPISuite.mapsFromPairsToPairs</li></div><div><li>test.org.apache.spark.JavaAPISuite.flatMap</li></div><div><li>test.org.apache.spark.JavaAPISuite.flatMap</li></div><div><li>test.org.apache.spark.JavaAPISuite.cogroup3</li></div><div><li>test.org.apache.spark.JavaAPISuite.cogroup3</li></div><div><li>test.org.apache.spark.JavaAPISuite.cogroup4</li></div><div><li>test.org.apache.spark.JavaAPISuite.cogroup4</li></div><div><li>test.org.apache.spark.JavaAPISuite.randomSplit</li></div><div><li>test.org.apache.spark.JavaAPISuite.randomSplit</li></div><div><li>test.org.apache.spark.JavaAPISuite.persist</li></div><div><li>test.org.apache.spark.JavaAPISuite.foreach</li></div><div><li>test.org.apache.spark.JavaAPISuite.foreach</li></div><div><li>test.org.apache.spark.JavaAPISuite.hadoopFileCompressed</li></div><div><li>test.org.apache.spark.JavaAPISuite.hadoopFileCompressed</li></div><div><li>test.org.apache.spark.JavaAPISuite.textFilesCompressed</li></div><div><li>test.org.apache.spark.JavaAPISuite.textFilesCompressed</li></div><div><li>test.org.apache.spark.JavaAPISuite.testAsyncActionCancellation</li></div><div><li>test.org.apache.spark.JavaAPISuite.testAsyncActionCancellation</li></div><div><li>test.org.apache.spark.JavaAPISuite.checkpointAndRestore</li></div><div><li>test.org.apache.spark.JavaAPISuite.sortByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.aggregateByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.aggregateByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.map</li></div><div><li>test.org.apache.spark.JavaAPISuite.max</li></div><div><li>test.org.apache.spark.JavaAPISuite.max</li></div><div><li>test.org.apache.spark.JavaAPISuite.min</li></div><div><li>test.org.apache.spark.JavaAPISuite.min</li></div><div><li>test.org.apache.spark.JavaAPISuite.top</li></div><div><li>test.org.apache.spark.JavaAPISuite.top</li></div><div><li>test.org.apache.spark.JavaAPISuite.zip</li></div><div><li>test.org.apache.spark.JavaAPISuite.zip</li></div><div><li>test.org.apache.spark.JavaAPISuite.fold</li></div><div><li>test.org.apache.spark.JavaAPISuite.fold</li></div><div><li>test.org.apache.spark.JavaAPISuite.glom</li></div><div><li>test.org.apache.spark.JavaAPISuite.glom</li></div><div><li>test.org.apache.spark.JavaAPISuite.take</li></div><div><li>test.org.apache.spark.JavaAPISuite.take</li></div><div><li>test.org.apache.spark.JavaAPISuite.javaDoubleRDDHistoGram</li></div><div><li>test.org.apache.spark.JavaAPISuite.javaDoubleRDDHistoGram</li></div><div><li>test.org.apache.spark.JavaAPISuite.collectUnderlyingScalaRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.collectUnderlyingScalaRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.keyBy</li></div><div><li>test.org.apache.spark.JavaAPISuite.keyBy</li></div><div><li>test.org.apache.spark.JavaAPISuite.mapPartitionsWithIndex</li></div><div><li>test.org.apache.spark.JavaAPISuite.mapPartitionsWithIndex</li></div><div><li>test.org.apache.spark.JavaAPISuite.sampleByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.sampleByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.intersection</li></div><div><li>test.org.apache.spark.JavaAPISuite.intersection</li></div><div><li>test.org.apache.spark.JavaAPISuite.aggregate</li></div><div><li>test.org.apache.spark.JavaAPISuite.aggregate</li></div><div><li>test.org.apache.spark.JavaAPISuite.cartesian</li></div><div><li>test.org.apache.spark.JavaAPISuite.cartesian</li></div><div><li>test.org.apache.spark.JavaAPISuite.countApproxDistinctByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.countApproxDistinctByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.readWithNewAPIHadoopFile</li></div><div><li>test.org.apache.spark.JavaAPISuite.readWithNewAPIHadoopFile</li></div><div><li>test.org.apache.spark.JavaAPISuite.testRegisterKryoClasses</li></div><div><li>test.org.apache.spark.JavaAPISuite.testRegisterKryoClasses</li></div><div><li>test.org.apache.spark.JavaAPISuite.groupBy</li></div><div><li>test.org.apache.spark.JavaAPISuite.groupBy</li></div><div><li>test.org.apache.spark.JavaAPISuite.sampleByKeyExact</li></div><div><li>test.org.apache.spark.JavaAPISuite.mapPartitions</li></div><div><li>test.org.apache.spark.JavaAPISuite.takeOrdered</li></div><div><li>test.org.apache.spark.JavaAPISuite.takeOrdered</li></div><div><li>test.org.apache.spark.JavaAPISuite.foldByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.foldByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.objectFilesOfInts</li></div><div><li>test.org.apache.spark.JavaAPISuite.objectFilesOfInts</li></div><div><li>test.org.apache.spark.JavaAPISuite.treeAggregate</li></div><div><li>test.org.apache.spark.JavaAPISuite.treeAggregate</li></div><div><li>test.org.apache.spark.JavaAPISuite.testGetPersistentRDDs</li></div><div><li>test.org.apache.spark.JavaAPISuite.testGetPersistentRDDs</li></div><div><li>test.org.apache.spark.JavaAPISuite.approximateResults</li></div><div><li>test.org.apache.spark.JavaAPISuite.approximateResults</li></div><div><li>test.org.apache.spark.JavaAPISuite.treeReduce</li></div><div><li>test.org.apache.spark.JavaAPISuite.treeReduce</li></div><div><li>test.org.apache.spark.JavaAPISuite.collectAsMapAndSerialize</li></div><div><li>test.org.apache.spark.JavaAPISuite.collectAsMapAndSerialize</li></div><div><li>test.org.apache.spark.JavaAPISuite.countApproxDistinct</li></div><div><li>test.org.apache.spark.JavaAPISuite.countApproxDistinct</li></div><div><li>test.org.apache.spark.JavaAPISuite.javaDoubleRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.javaDoubleRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.mapOnPairRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.mapOnPairRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.testAsyncActionErrorWrapping</li></div><div><li>test.org.apache.spark.JavaAPISuite.testAsyncActionErrorWrapping</li></div><div><li>test.org.apache.spark.JavaAPISuite.naturalMax</li></div><div><li>test.org.apache.spark.JavaAPISuite.naturalMax</li></div><div><li>test.org.apache.spark.JavaAPISuite.naturalMin</li></div><div><li>test.org.apache.spark.JavaAPISuite.naturalMin</li></div><div><li>test.org.apache.spark.JavaAPISuite.sequenceFile</li></div><div><li>test.org.apache.spark.JavaAPISuite.sequenceFile</li></div><div><li>test.org.apache.spark.JavaAPISuite.collectPartitions</li></div><div><li>test.org.apache.spark.JavaAPISuite.collectPartitions</li></div><div><li>test.org.apache.spark.JavaAPISuite.cogroup</li></div><div><li>test.org.apache.spark.JavaAPISuite.cogroup</li></div><div><li>test.org.apache.spark.JavaAPISuite.reduceByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.reduceByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.repartition</li></div><div><li>test.org.apache.spark.JavaAPISuite.repartition</li></div><div><li>test.org.apache.spark.JavaAPISuite.iterator</li></div><div><li>test.org.apache.spark.JavaAPISuite.iterator</li></div><div><li>test.org.apache.spark.JavaAPISuite.emptyRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.emptyRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.zipWithIndex</li></div><div><li>test.org.apache.spark.JavaAPISuite.zipWithIndex</li></div><div><li>test.org.apache.spark.JavaAPISuite.foreachPartition</li></div><div><li>test.org.apache.spark.JavaAPISuite.foreachPartition</li></div><div><li>test.org.apache.spark.JavaAPISuite.combineByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.combineByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.takeAsync</li></div><div><li>test.org.apache.spark.JavaAPISuite.takeAsync</li></div><div><li>test.org.apache.spark.JavaAPISuite.collectAsMapWithIntArrayValues</li></div><div><li>test.org.apache.spark.JavaAPISuite.collectAsMapWithIntArrayValues</li></div><div><li>test.org.apache.spark.JavaAPISuite.objectFilesOfComplexTypes</li></div><div><li>test.org.apache.spark.JavaAPISuite.objectFilesOfComplexTypes</li></div><div><li>test.org.apache.spark.JavaAPISuite.zipWithUniqueId</li></div><div><li>test.org.apache.spark.JavaAPISuite.zipWithUniqueId</li></div><div><li>test.org.apache.spark.JavaAPISuite.collectAsync</li></div><div><li>test.org.apache.spark.JavaAPISuite.collectAsync</li></div><div><li>test.org.apache.spark.JavaAPISuite.foreachAsync</li></div><div><li>test.org.apache.spark.JavaAPISuite.foreachAsync</li></div><div><li>test.org.apache.spark.JavaAPISuite.zipPartitions</li></div><div><li>test.org.apache.spark.JavaAPISuite.zipPartitions</li></div><div><li>test.org.apache.spark.JavaAPISuite.reduceOnJavaDoubleRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.reduceOnJavaDoubleRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.isEmpty</li></div><div><li>test.org.apache.spark.JavaAPISuite.isEmpty</li></div><div><li>test.org.apache.spark.JavaSparkContextSuite.javaSparkContext</li></div><div><li>test.org.apache.spark.JavaSparkContextSuite.scalaSparkContext</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single BooleanType with seed 7308411183748884782</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single ByteType with seed 4446789926156407554</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single ShortType with seed -7338248970079421220</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single IntegerType with seed 1888812313568949446</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single LongType with seed 7853185275645657472</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single FloatType with seed 7319090749879310724</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single DoubleType with seed 6958769278960946522</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single DecimalType(8,0) with seed 5618052675792332838</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single DecimalType(8,4) with seed 7947691028072466321</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single DecimalType(16,0) with seed -7612334778196347769</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single DecimalType(16,11) with seed -3121050904531359657</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single DecimalType(38,0) with seed 5099428991776716343</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single DecimalType(38,38) with seed -2751330306731763895</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single StringType with seed 7066602185030133631</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single BinaryType with seed -8343309833225958373</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.flat schema struct&lt;col_0:string,col_1:decimal(38,38),col_2:binary,col_3:boolean,col_4:binary&gt; with seed 7389130032968028322</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.flat schema struct&lt;col_0:double,col_1:smallint,col_2:double,col_3:boolean,col_4:decimal(16,0)&gt; with seed 8224834773586957684</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.flat schema struct&lt;col_0:int,col_1:boolean,col_2:int,col_3:binary,col_4:decimal(8,4)&gt; with seed 3547253314093384550</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.flat schema struct&lt;col_0:boolean,col_1:bigint,col_2:decimal(8,4),col_3:double,col_4:double&gt; with seed -4492483418035148266</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.flat schema struct&lt;col_0:decimal(8,0),col_1:smallint,col_2:decimal(16,0),col_3:decimal(38,0),col_4:decimal(16,0)&gt; with seed -7421572993677096238</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.nested schema struct&lt;col_0:array&lt;string&gt;,col_1:struct&lt;col_0:array&lt;int&gt;,col_1:array&lt;boolean&gt;,col_2:decimal(38,38)&gt;,col_2:array&lt;smallint&gt;,col_3:array&lt;decimal(8,0)&gt;,col_4:struct&lt;col_0:decimal(16,11)&gt;,col_5:struct&lt;col_0:array&lt;decimal(8,4)&gt;&gt;,col_6:array&lt;smallint&gt;,col_7:array&lt;decimal(16,0)&gt;&gt; with seed -3844835813531326399</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.nested schema struct&lt;col_0:boolean,col_1:array&lt;float&gt;,col_2:smallint,col_3:decimal(38,0),col_4:decimal(16,0),col_5:struct&lt;col_0:smallint&gt;,col_6:array&lt;binary&gt;,col_7:array&lt;decimal(8,0)&gt;,col_8:array&lt;smallint&gt;,col_9:float&gt; with seed -8130169798711733448</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.nested schema struct&lt;col_0:bigint,col_1:struct&lt;col_0:array&lt;bigint&gt;,col_1:struct&lt;col_0:array&lt;binary&gt;&gt;,col_2:struct&lt;col_0:array&lt;decimal(16,0)&gt;&gt;&gt;,col_2:tinyint,col_3:struct&lt;col_0:array&lt;decimal(16,0)&gt;&gt;,col_4:array&lt;decimal(8,0)&gt;,col_5:struct&lt;col_0:array&lt;tinyint&gt;,col_1:struct&lt;col_0:struct&lt;col_0:array&lt;decimal(38,38)&gt;&gt;&gt;&gt;,col_6:array&lt;decimal(38,0)&gt;&gt; with seed 206193538942822233</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.nested schema struct&lt;col_0:struct&lt;col_0:float,col_1:array&lt;decimal(16,11)&gt;&gt;,col_1:array&lt;decimal(8,4)&gt;,col_2:struct&lt;col_0:struct&lt;col_0:struct&lt;col_0:int,col_1:array&lt;smallint&gt;&gt;,col_1:array&lt;decimal(38,38)&gt;,col_2:struct&lt;col_0:struct&lt;col_0:boolean&gt;&gt;&gt;,col_1:struct&lt;col_0:array&lt;bigint&gt;&gt;&gt;,col_3:struct&lt;col_0:struct&lt;col_0:struct&lt;col_0:struct&lt;col_0:struct&lt;col_0:array&lt;decimal(8,0)&gt;&gt;&gt;&gt;&gt;&gt;,col_4:struct&lt;col_0:array&lt;boolean&gt;&gt;&gt; with seed 7821504959510597237</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.nested schema struct&lt;col_0:array&lt;tinyint&gt;,col_1:boolean,col_2:array&lt;smallint&gt;,col_3:tinyint,col_4:array&lt;decimal(8,0)&gt;,col_5:struct&lt;col_0:decimal(16,11)&gt;,col_6:array&lt;float&gt;,col_7:array&lt;decimal(16,0)&gt;,col_8:array&lt;double&gt;,col_9:array&lt;string&gt;&gt; with seed -1627446941656039482</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.read int as string</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.read string as int</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.read float as double</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.read double as float</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.Handle unsupported input of record type</li></div><div><li>org.apache.spark.sql.avro.AvroFunctionsSuite.roundtrip in to_avro and from_avro - int and string</li></div><div><li>org.apache.spark.sql.avro.AvroFunctionsSuite.roundtrip in to_avro and from_avro - struct</li></div><div><li>org.apache.spark.sql.avro.AvroFunctionsSuite.handle invalid input in from_avro</li></div><div><li>org.apache.spark.sql.avro.AvroFunctionsSuite.roundtrip in to_avro and from_avro - array with null</li></div><div><li>org.apache.spark.streaming.kafka010.JavaDirectKafkaStreamSuite.testKafkaStream</li></div><div><li>org.apache.spark.streaming.kafka010.JavaKafkaRDDSuite.testKafkaRDD</li></div><div><li>org.apache.spark.graphx.EdgeRDDSuite.cache, getStorageLevel</li></div><div><li>org.apache.spark.graphx.EdgeRDDSuite.checkpointing</li></div><div><li>org.apache.spark.graphx.GraphLoaderSuite.GraphLoader.edgeListFile</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.joinVertices</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.collectNeighborIds</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.removeSelfEdges</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.filter</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.convertToCanonicalEdges</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.collectEdgesCycleDirectionOut</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.collectEdgesCycleDirectionIn</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.collectEdgesCycleDirectionEither</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.collectEdgesChainDirectionOut</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.collectEdgesChainDirectionIn</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.collectEdgesChainDirectionEither</li></div><div><li>org.apache.spark.graphx.GraphSuite.Graph.fromEdgeTuples</li></div><div><li>org.apache.spark.graphx.GraphSuite.Graph.fromEdges</li></div><div><li>org.apache.spark.graphx.GraphSuite.Graph.apply</li></div><div><li>org.apache.spark.graphx.GraphSuite.triplets</li></div><div><li>org.apache.spark.graphx.GraphSuite.partitionBy</li></div><div><li>org.apache.spark.graphx.GraphSuite.mapVertices</li></div><div><li>org.apache.spark.graphx.GraphSuite.mapVertices changing type with same erased type</li></div><div><li>org.apache.spark.graphx.GraphSuite.mapEdges</li></div><div><li>org.apache.spark.graphx.GraphSuite.mapTriplets</li></div><div><li>org.apache.spark.graphx.GraphSuite.reverse</li></div><div><li>org.apache.spark.graphx.GraphSuite.reverse with join elimination</li></div><div><li>org.apache.spark.graphx.GraphSuite.subgraph</li></div><div><li>org.apache.spark.graphx.GraphSuite.mask</li></div><div><li>org.apache.spark.graphx.GraphSuite.groupEdges</li></div><div><li>org.apache.spark.graphx.GraphSuite.aggregateMessages</li></div><div><li>org.apache.spark.graphx.GraphSuite.outerJoinVertices</li></div><div><li>org.apache.spark.graphx.GraphSuite.more edge partitions than vertex partitions</li></div><div><li>org.apache.spark.graphx.GraphSuite.checkpoint</li></div><div><li>org.apache.spark.graphx.GraphSuite.cache, getStorageLevel</li></div><div><li>org.apache.spark.graphx.GraphSuite.non-default number of edge partitions</li></div><div><li>org.apache.spark.graphx.GraphSuite.unpersist graph RDD</li></div><div><li>org.apache.spark.graphx.GraphSuite.SPARK-14219: pickRandomVertex</li></div><div><li>org.apache.spark.graphx.PregelSuite.1 iteration</li></div><div><li>org.apache.spark.graphx.PregelSuite.chain propagation</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.filter</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.mapValues</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.minus</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.minus with RDD[(VertexId, VD)]</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.minus with non-equal number of partitions</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.diff</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.diff with RDD[(VertexId, VD)]</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.diff vertices with non-equal number of partitions</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.leftJoin</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.leftJoin vertices with non-equal number of partitions</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.innerJoin</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.innerJoin vertices with the non-equal number of partitions</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.aggregateUsingIndex</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.mergeFunc</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.cache, getStorageLevel</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.checkpoint</li></div><div><li>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Grid Connected Components</li></div><div><li>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Reverse Grid Connected Components</li></div><div><li>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Chain Connected Components</li></div><div><li>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Reverse Chain Connected Components</li></div><div><li>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Connected Components on a Toy Connected Graph</li></div><div><li>org.apache.spark.graphx.lib.LabelPropagationSuite.Label Propagation</li></div><div><li>org.apache.spark.graphx.lib.PageRankSuite.Star PageRank</li></div><div><li>org.apache.spark.graphx.lib.PageRankSuite.Star PersonalPageRank</li></div><div><li>org.apache.spark.graphx.lib.PageRankSuite.Grid PageRank</li></div><div><li>org.apache.spark.graphx.lib.PageRankSuite.Chain PageRank</li></div><div><li>org.apache.spark.graphx.lib.PageRankSuite.Chain PersonalizedPageRank</li></div><div><li>org.apache.spark.graphx.lib.PageRankSuite.Loop with source PageRank</li></div><div><li>org.apache.spark.graphx.lib.PageRankSuite.Loop with sink PageRank</li></div><div><li>org.apache.spark.graphx.lib.SVDPlusPlusSuite.Test SVD++ with mean square error on training set</li></div><div><li>org.apache.spark.graphx.lib.ShortestPathsSuite.Shortest Path Computations</li></div><div><li>org.apache.spark.graphx.lib.StronglyConnectedComponentsSuite.Island Strongly Connected Components</li></div><div><li>org.apache.spark.graphx.lib.StronglyConnectedComponentsSuite.Cycle Strongly Connected Components</li></div><div><li>org.apache.spark.graphx.lib.StronglyConnectedComponentsSuite.2 Cycle Strongly Connected Components</li></div><div><li>org.apache.spark.graphx.lib.TriangleCountSuite.Count a single triangle</li></div><div><li>org.apache.spark.graphx.lib.TriangleCountSuite.Count two triangles</li></div><div><li>org.apache.spark.graphx.lib.TriangleCountSuite.Count two triangles with bi-directed edges</li></div><div><li>org.apache.spark.graphx.lib.TriangleCountSuite.Count a single triangle with duplicate edges</li></div><div><li>org.apache.spark.graphx.util.GraphGeneratorsSuite.GraphGenerators.logNormalGraph</li></div><div><li>org.apache.spark.graphx.util.GraphGeneratorsSuite.SPARK-5064 GraphGenerators.rmatGraph numEdges upper bound</li></div><div><li>org.apache.spark.graphx.util.PeriodicGraphCheckpointerSuite.Persisting</li></div><div><li>org.apache.spark.graphx.util.PeriodicGraphCheckpointerSuite.Checkpointing</li></div><div><li>org.apache.spark.ml.JavaPipelineSuite.pipeline</li></div><div><li>org.apache.spark.ml.JavaPipelineSuite.pipeline</li></div><div><li>org.apache.spark.ml.classification.JavaDecisionTreeClassifierSuite.runDT</li></div><div><li>org.apache.spark.ml.classification.JavaDecisionTreeClassifierSuite.runDT</li></div><div><li>org.apache.spark.ml.classification.JavaGBTClassifierSuite.runDT</li></div><div><li>org.apache.spark.ml.classification.JavaGBTClassifierSuite.runDT</li></div><div><li>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionWithSetters</li></div><div><li>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionWithSetters</li></div><div><li>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionTrainingSummary</li></div><div><li>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionTrainingSummary</li></div><div><li>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionPredictorClassifierMethods</li></div><div><li>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionPredictorClassifierMethods</li></div><div><li>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionDefaultParams</li></div><div><li>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionDefaultParams</li></div><div><li>org.apache.spark.ml.classification.JavaMultilayerPerceptronClassifierSuite.testMLPC</li></div><div><li>org.apache.spark.ml.classification.JavaMultilayerPerceptronClassifierSuite.testMLPC</li></div><div><li>org.apache.spark.ml.classification.JavaNaiveBayesSuite.testNaiveBayes</li></div><div><li>org.apache.spark.ml.classification.JavaNaiveBayesSuite.naiveBayesDefaultParams</li></div><div><li>org.apache.spark.ml.classification.JavaNaiveBayesSuite.naiveBayesDefaultParams</li></div><div><li>org.apache.spark.ml.classification.JavaOneVsRestSuite.oneVsRestDefaultParams</li></div><div><li>org.apache.spark.ml.classification.JavaOneVsRestSuite.oneVsRestDefaultParams</li></div><div><li>org.apache.spark.ml.classification.JavaRandomForestClassifierSuite.runDT</li></div><div><li>org.apache.spark.ml.classification.JavaRandomForestClassifierSuite.runDT</li></div><div><li>org.apache.spark.ml.clustering.JavaKMeansSuite.fitAndTransform</li></div><div><li>org.apache.spark.ml.feature.JavaBucketizerSuite.bucketizerTest</li></div><div><li>org.apache.spark.ml.feature.JavaBucketizerSuite.bucketizerTest</li></div><div><li>org.apache.spark.ml.feature.JavaBucketizerSuite.bucketizerMultipleColumnsTest</li></div><div><li>org.apache.spark.ml.feature.JavaBucketizerSuite.bucketizerMultipleColumnsTest</li></div><div><li>org.apache.spark.ml.feature.JavaDCTSuite.javaCompatibilityTest</li></div><div><li>org.apache.spark.ml.feature.JavaHashingTFSuite.hashingTF</li></div><div><li>org.apache.spark.ml.feature.JavaHashingTFSuite.hashingTF</li></div><div><li>org.apache.spark.ml.feature.JavaNormalizerSuite.normalizer</li></div><div><li>org.apache.spark.ml.feature.JavaNormalizerSuite.normalizer</li></div><div><li>org.apache.spark.ml.feature.JavaPCASuite.testPCA</li></div><div><li>org.apache.spark.ml.feature.JavaPCASuite.testPCA</li></div><div><li>org.apache.spark.ml.feature.JavaPolynomialExpansionSuite.polynomialExpansionTest</li></div><div><li>org.apache.spark.ml.feature.JavaPolynomialExpansionSuite.polynomialExpansionTest</li></div><div><li>org.apache.spark.ml.feature.JavaStandardScalerSuite.standardScaler</li></div><div><li>org.apache.spark.ml.feature.JavaStandardScalerSuite.standardScaler</li></div><div><li>org.apache.spark.ml.feature.JavaStopWordsRemoverSuite.javaCompatibilityTest</li></div><div><li>org.apache.spark.ml.feature.JavaStopWordsRemoverSuite.javaCompatibilityTest</li></div><div><li>org.apache.spark.ml.feature.JavaStringIndexerSuite.testStringIndexer</li></div><div><li>org.apache.spark.ml.feature.JavaStringIndexerSuite.testStringIndexer</li></div><div><li>org.apache.spark.ml.feature.JavaTokenizerSuite.regexTokenizer</li></div><div><li>org.apache.spark.ml.feature.JavaTokenizerSuite.regexTokenizer</li></div><div><li>org.apache.spark.ml.feature.JavaVectorAssemblerSuite.testVectorAssembler</li></div><div><li>org.apache.spark.ml.feature.JavaVectorAssemblerSuite.testVectorAssembler</li></div><div><li>org.apache.spark.ml.feature.JavaVectorIndexerSuite.vectorIndexerAPI</li></div><div><li>org.apache.spark.ml.feature.JavaVectorIndexerSuite.vectorIndexerAPI</li></div><div><li>org.apache.spark.ml.feature.JavaVectorSlicerSuite.vectorSlice</li></div><div><li>org.apache.spark.ml.feature.JavaVectorSlicerSuite.vectorSlice</li></div><div><li>org.apache.spark.ml.feature.JavaWord2VecSuite.testJavaWord2Vec</li></div><div><li>org.apache.spark.ml.regression.JavaDecisionTreeRegressorSuite.runDT</li></div><div><li>org.apache.spark.ml.regression.JavaDecisionTreeRegressorSuite.runDT</li></div><div><li>org.apache.spark.ml.regression.JavaGBTRegressorSuite.runDT</li></div><div><li>org.apache.spark.ml.regression.JavaGBTRegressorSuite.runDT</li></div><div><li>org.apache.spark.ml.regression.JavaLinearRegressionSuite.linearRegressionDefaultParams</li></div><div><li>org.apache.spark.ml.regression.JavaLinearRegressionSuite.linearRegressionDefaultParams</li></div><div><li>org.apache.spark.ml.regression.JavaLinearRegressionSuite.linearRegressionWithSetters</li></div><div><li>org.apache.spark.ml.regression.JavaLinearRegressionSuite.linearRegressionWithSetters</li></div><div><li>org.apache.spark.ml.regression.JavaRandomForestRegressorSuite.runDT</li></div><div><li>org.apache.spark.ml.regression.JavaRandomForestRegressorSuite.runDT</li></div><div><li>org.apache.spark.ml.source.libsvm.JavaLibSVMRelationSuite.verifyLibSVMDF</li></div><div><li>org.apache.spark.ml.stat.JavaKolmogorovSmirnovTestSuite.testKSTestNamedDistribution</li></div><div><li>org.apache.spark.ml.stat.JavaKolmogorovSmirnovTestSuite.testKSTestNamedDistribution</li></div><div><li>org.apache.spark.ml.stat.JavaKolmogorovSmirnovTestSuite.testKSTestCDF</li></div><div><li>org.apache.spark.ml.stat.JavaKolmogorovSmirnovTestSuite.testKSTestCDF</li></div><div><li>org.apache.spark.ml.stat.JavaSummarizerSuite.testSummarizer</li></div><div><li>org.apache.spark.ml.stat.JavaSummarizerSuite.testSummarizer</li></div><div><li>org.apache.spark.ml.tuning.JavaCrossValidatorSuite.crossValidationWithLogisticRegression</li></div><div><li>org.apache.spark.ml.util.JavaDefaultReadWriteSuite.testDefaultReadWrite</li></div><div><li>org.apache.spark.mllib.classification.JavaLogisticRegressionSuite.runLRUsingConstructor</li></div><div><li>org.apache.spark.mllib.classification.JavaLogisticRegressionSuite.runLRUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.classification.JavaLogisticRegressionSuite.runLRUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.testPredictJavaRDD</li></div><div><li>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.testPredictJavaRDD</li></div><div><li>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.runUsingConstructor</li></div><div><li>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.runUsingConstructor</li></div><div><li>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.runUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.runUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.testModelTypeSetters</li></div><div><li>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.testModelTypeSetters</li></div><div><li>org.apache.spark.mllib.classification.JavaSVMSuite.runSVMUsingConstructor</li></div><div><li>org.apache.spark.mllib.classification.JavaSVMSuite.runSVMUsingConstructor</li></div><div><li>org.apache.spark.mllib.classification.JavaSVMSuite.runSVMUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.classification.JavaSVMSuite.runSVMUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.classification.JavaStreamingLogisticRegressionSuite.javaAPI</li></div><div><li>org.apache.spark.mllib.classification.JavaStreamingLogisticRegressionSuite.javaAPI</li></div><div><li>org.apache.spark.mllib.clustering.JavaBisectingKMeansSuite.twoDimensionalData</li></div><div><li>org.apache.spark.mllib.clustering.JavaBisectingKMeansSuite.twoDimensionalData</li></div><div><li>org.apache.spark.mllib.clustering.JavaGaussianMixtureSuite.runGaussianMixture</li></div><div><li>org.apache.spark.mllib.clustering.JavaGaussianMixtureSuite.runGaussianMixture</li></div><div><li>org.apache.spark.mllib.clustering.JavaKMeansSuite.testPredictJavaRDD</li></div><div><li>org.apache.spark.mllib.clustering.JavaKMeansSuite.runKMeansUsingConstructor</li></div><div><li>org.apache.spark.mllib.clustering.JavaKMeansSuite.runKMeansUsingConstructor</li></div><div><li>org.apache.spark.mllib.clustering.JavaKMeansSuite.runKMeansUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.clustering.JavaKMeansSuite.runKMeansUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.clustering.JavaLDASuite.onlineOptimizerCompatibility</li></div><div><li>org.apache.spark.mllib.clustering.JavaLDASuite.onlineOptimizerCompatibility</li></div><div><li>org.apache.spark.mllib.clustering.JavaLDASuite.distributedLDAModel</li></div><div><li>org.apache.spark.mllib.clustering.JavaLDASuite.distributedLDAModel</li></div><div><li>org.apache.spark.mllib.clustering.JavaLDASuite.localLDAModel</li></div><div><li>org.apache.spark.mllib.clustering.JavaLDASuite.localLDAModel</li></div><div><li>org.apache.spark.mllib.clustering.JavaLDASuite.localLdaMethods</li></div><div><li>org.apache.spark.mllib.clustering.JavaLDASuite.localLdaMethods</li></div><div><li>org.apache.spark.mllib.clustering.JavaStreamingKMeansSuite.javaAPI</li></div><div><li>org.apache.spark.mllib.evaluation.JavaRankingMetricsSuite.rankingMetrics</li></div><div><li>org.apache.spark.mllib.evaluation.JavaRankingMetricsSuite.rankingMetrics</li></div><div><li>org.apache.spark.mllib.feature.JavaTfIdfSuite.tfIdfMinimumDocumentFrequency</li></div><div><li>org.apache.spark.mllib.feature.JavaTfIdfSuite.tfIdfMinimumDocumentFrequency</li></div><div><li>org.apache.spark.mllib.feature.JavaTfIdfSuite.tfIdf</li></div><div><li>org.apache.spark.mllib.feature.JavaTfIdfSuite.tfIdf</li></div><div><li>org.apache.spark.mllib.feature.JavaWord2VecSuite.word2Vec</li></div><div><li>org.apache.spark.mllib.feature.JavaWord2VecSuite.word2Vec</li></div><div><li>org.apache.spark.mllib.fpm.JavaAssociationRulesSuite.runAssociationRules</li></div><div><li>org.apache.spark.mllib.fpm.JavaFPGrowthSuite.runFPGrowthSaveLoad</li></div><div><li>org.apache.spark.mllib.fpm.JavaFPGrowthSuite.runFPGrowthSaveLoad</li></div><div><li>org.apache.spark.mllib.fpm.JavaFPGrowthSuite.runFPGrowth</li></div><div><li>org.apache.spark.mllib.fpm.JavaFPGrowthSuite.runFPGrowth</li></div><div><li>org.apache.spark.mllib.fpm.JavaPrefixSpanSuite.runPrefixSpan</li></div><div><li>org.apache.spark.mllib.fpm.JavaPrefixSpanSuite.runPrefixSpan</li></div><div><li>org.apache.spark.mllib.fpm.JavaPrefixSpanSuite.runPrefixSpanSaveLoad</li></div><div><li>org.apache.spark.mllib.fpm.JavaPrefixSpanSuite.runPrefixSpanSaveLoad</li></div><div><li>org.apache.spark.mllib.linalg.distributed.JavaRowMatrixSuite.rowMatrixQRDecomposition</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testNormalVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testNormalVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testArbitrary</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testArbitrary</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testLogNormalVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testLogNormalVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testExponentialVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testExponentialVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testUniformRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testUniformRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testRandomVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testRandomVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testGammaRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testGammaRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testUniformVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testUniformVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testPoissonRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testPoissonRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testNormalRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testNormalRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testPoissonVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testPoissonVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testGammaVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testGammaVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testExponentialRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testExponentialRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testLNormalRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testLNormalRDD</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runALSUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runALSUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSUsingConstructor</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSUsingConstructor</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runRecommend</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runRecommend</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSWithNegativeWeight</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSWithNegativeWeight</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runALSUsingConstructor</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runALSUsingConstructor</li></div><div><li>org.apache.spark.mllib.regression.JavaIsotonicRegressionSuite.testIsotonicRegressionJavaRDD</li></div><div><li>org.apache.spark.mllib.regression.JavaIsotonicRegressionSuite.testIsotonicRegressionJavaRDD</li></div><div><li>org.apache.spark.mllib.regression.JavaIsotonicRegressionSuite.testIsotonicRegressionPredictionsJavaRDD</li></div><div><li>org.apache.spark.mllib.regression.JavaIsotonicRegressionSuite.testIsotonicRegressionPredictionsJavaRDD</li></div><div><li>org.apache.spark.mllib.regression.JavaLassoSuite.runLassoUsingConstructor</li></div><div><li>org.apache.spark.mllib.regression.JavaLassoSuite.runLassoUsingConstructor</li></div><div><li>org.apache.spark.mllib.regression.JavaLassoSuite.runLassoUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.regression.JavaLassoSuite.runLassoUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.testPredictJavaRDD</li></div><div><li>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.testPredictJavaRDD</li></div><div><li>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.runLinearRegressionUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.runLinearRegressionUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.runLinearRegressionUsingConstructor</li></div><div><li>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.runLinearRegressionUsingConstructor</li></div><div><li>org.apache.spark.mllib.regression.JavaRidgeRegressionSuite.runRidgeRegressionUsingConstructor</li></div><div><li>org.apache.spark.mllib.regression.JavaRidgeRegressionSuite.runRidgeRegressionUsingConstructor</li></div><div><li>org.apache.spark.mllib.regression.JavaRidgeRegressionSuite.runRidgeRegressionUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.regression.JavaRidgeRegressionSuite.runRidgeRegressionUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.regression.JavaStreamingLinearRegressionSuite.javaAPI</li></div><div><li>org.apache.spark.mllib.regression.JavaStreamingLinearRegressionSuite.javaAPI</li></div><div><li>org.apache.spark.mllib.stat.JavaStatisticsSuite.testCorr</li></div><div><li>org.apache.spark.mllib.stat.JavaStatisticsSuite.testCorr</li></div><div><li>org.apache.spark.mllib.stat.JavaStatisticsSuite.chiSqTest</li></div><div><li>org.apache.spark.mllib.stat.JavaStatisticsSuite.chiSqTest</li></div><div><li>org.apache.spark.mllib.stat.JavaStatisticsSuite.streamingTest</li></div><div><li>org.apache.spark.mllib.stat.JavaStatisticsSuite.streamingTest</li></div><div><li>org.apache.spark.mllib.stat.JavaStatisticsSuite.kolmogorovSmirnovTest</li></div><div><li>org.apache.spark.mllib.stat.JavaStatisticsSuite.kolmogorovSmirnovTest</li></div><div><li>org.apache.spark.mllib.tree.JavaDecisionTreeSuite.runDTUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.tree.JavaDecisionTreeSuite.runDTUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.tree.JavaDecisionTreeSuite.runDTUsingConstructor</li></div><div><li>org.apache.spark.mllib.tree.JavaDecisionTreeSuite.runDTUsingConstructor</li></div><div><li>org.apache.spark.mllib.util.JavaMLUtilsSuite.testConvertMatrixColumnsToAndFromML</li></div><div><li>org.apache.spark.mllib.util.JavaMLUtilsSuite.testConvertMatrixColumnsToAndFromML</li></div><div><li>org.apache.spark.mllib.util.JavaMLUtilsSuite.testConvertVectorColumnsToAndFromML</li></div><div><li>org.apache.spark.mllib.util.JavaMLUtilsSuite.testConvertVectorColumnsToAndFromML</li></div><div><li>org.apache.spark.repl.ReplSuite.SPARK-15236: use Hive catalog</li></div><div><li>org.apache.spark.repl.ReplSuite.SPARK-15236: use in-memory catalog</li></div><div><li>org.apache.spark.repl.ReplSuite.broadcast vars</li></div><div><li>org.apache.spark.repl.ReplSuite.line wrapper only initialized once when used as encoder outer scope</li></div><div><li>org.apache.spark.repl.ReplSuite.define case class and create Dataset together with paste mode</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.simple foreach with accumulator</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.external vars</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.external classes</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.external functions</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.external functions that access vars</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.broadcast vars</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.interacting with files</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.local-cluster mode</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.SPARK-2576 importing implicits</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.Datasets and encoders</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.SPARK-2632 importing a method from non serializable class and not using it.</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.collecting objects of class defined in repl</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.collecting objects of class defined in repl - shuffling</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.replicating blocks of object with class defined in repl</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.newProductSeqEncoder with REPL defined class</li></div><div><li>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationCount</li></div><div><li>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationCount</li></div><div><li>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationSumDouble</li></div><div><li>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationSumDouble</li></div><div><li>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationSumLong</li></div><div><li>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationSumLong</li></div><div><li>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationAverage</li></div><div><li>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationAverage</li></div><div><li>test.org.apache.spark.sql.JavaApplySchemaSuite.applySchema</li></div><div><li>test.org.apache.spark.sql.JavaApplySchemaSuite.applySchema</li></div><div><li>test.org.apache.spark.sql.JavaApplySchemaSuite.dataFrameRDDOperations</li></div><div><li>test.org.apache.spark.sql.JavaApplySchemaSuite.dataFrameRDDOperations</li></div><div><li>test.org.apache.spark.sql.JavaApplySchemaSuite.applySchemaToJSON</li></div><div><li>test.org.apache.spark.sql.JavaApplySchemaSuite.applySchemaToJSON</li></div><div><li>test.org.apache.spark.sql.JavaBeanDeserializationSuite.testBeanWithArrayFieldDeserialization</li></div><div><li>test.org.apache.spark.sql.JavaBeanDeserializationSuite.testBeanWithArrayFieldDeserialization</li></div><div><li>test.org.apache.spark.sql.JavaBeanDeserializationSuite.testBeanWithMapFieldsDeserialization</li></div><div><li>test.org.apache.spark.sql.JavaBeanDeserializationSuite.testBeanWithMapFieldsDeserialization</li></div><div><li>test.org.apache.spark.sql.JavaColumnExpressionSuite.isInCollectionCheckExceptionMessage</li></div><div><li>test.org.apache.spark.sql.JavaColumnExpressionSuite.isInCollectionCheckExceptionMessage</li></div><div><li>test.org.apache.spark.sql.JavaColumnExpressionSuite.isInCollectionWorksCorrectlyOnJava</li></div><div><li>test.org.apache.spark.sql.JavaColumnExpressionSuite.isInCollectionWorksCorrectlyOnJava</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testFormatAPI</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testTextAPI</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testJsonAPI</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testLoadAPI</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testOptionsAPI</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testSaveModeAPI</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testCsvAPI</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testParquetAPI</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testTextFileAPI</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCollectAndTake</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCollectAndTake</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testJsonRDDToDataFrame</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testJsonRDDToDataFrame</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testVarargMethods</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testVarargMethods</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testBeanWithoutGetter</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testBeanWithoutGetter</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateStructTypeFromList</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateStructTypeFromList</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testSampleBy</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testSampleBy</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCrosstab</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCrosstab</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testUDF</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testUDF</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFromFromList</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFromFromList</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCircularReferenceBean</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCircularReferenceBean</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testFrequentItems</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testFrequentItems</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testSampleByColumn</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testSampleByColumn</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testExecution</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testExecution</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testTextLoad</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testTextLoad</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.pivot</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.pivot</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testGenericLoad</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testGenericLoad</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCountMinSketch</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCountMinSketch</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.pivotColumnValues</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.pivotColumnValues</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFrameFromJavaBeans</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFrameFromJavaBeans</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCorrelation</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCorrelation</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testBloomFilter</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testBloomFilter</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCovariance</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCovariance</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFrameFromLocalJavaBeans</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFrameFromLocalJavaBeans</li></div><div><li>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationCount</li></div><div><li>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationCount</li></div><div><li>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationSumDouble</li></div><div><li>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationSumDouble</li></div><div><li>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationSumLong</li></div><div><li>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationSumLong</li></div><div><li>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationAnonClass</li></div><div><li>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationAnonClass</li></div><div><li>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationAverage</li></div><div><li>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationAverage</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testRuntimeNullabilityCheck</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean1</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean1</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean2</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean2</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean3</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean3</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testSerializeNull</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testSerializeNull</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testRandomSplit</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testRandomSplit</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testTypedFilterPreservingSchema</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testTypedFilterPreservingSchema</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJoin</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJoin</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testTake</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testTake</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testToLocalIterator</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testToLocalIterator</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testSpecificLists</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testSpecificLists</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testForeach</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testForeach</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJavaEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJavaEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testPrimitiveEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testPrimitiveEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testEmptyBean</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testEmptyBean</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testCommonOperation</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testNullInTopLevelBean</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testGroupBy</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testSetOperation</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testSetOperation</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testBeanWithEnum</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testBeanWithEnum</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testKryoEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testKryoEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.test</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.test</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJavaBeanEncoder2</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJavaBeanEncoder2</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testCollect</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testCollect</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testKryoEncoderErrorMessageForPrivateClass</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testKryoEncoderErrorMessageForPrivateClass</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJavaBeanEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJavaBeanEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testTupleEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testTupleEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testNestedTupleEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testNestedTupleEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testTupleEncoderSchema</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testTupleEncoderSchema</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testReduce</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testReduce</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testSelect</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testSelect</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJavaEncoderErrorMessageForPrivateClass</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJavaEncoderErrorMessageForPrivateClass</li></div><div><li>test.org.apache.spark.sql.JavaSaveLoadSuite.saveAndLoadWithSchema</li></div><div><li>test.org.apache.spark.sql.JavaSaveLoadSuite.saveAndLoadWithSchema</li></div><div><li>test.org.apache.spark.sql.JavaSaveLoadSuite.saveAndLoad</li></div><div><li>test.org.apache.spark.sql.JavaSaveLoadSuite.saveAndLoad</li></div><div><li>test.org.apache.spark.sql.JavaUDAFSuite.udf1Test</li></div><div><li>test.org.apache.spark.sql.JavaUDAFSuite.udf1Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf1Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf1Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf2Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf2Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf3Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf3Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf4Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf4Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf5Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf5Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf6Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf6Test</li></div><div><li>test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.testForeachBatchAPI</li></div><div><li>test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.testForeachBatchAPI</li></div><div><li>test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.testForeachAPI</li></div><div><li>test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.testForeachAPI</li></div><div><li>org.apache.spark.sql.hive.HiveMetastoreLazyInitializationSuite.lazily initialize Hive client</li></div><div><li>org.apache.spark.sql.hive.JavaDataFrameSuite.testUDAF</li></div><div><li>org.apache.spark.sql.hive.JavaDataFrameSuite.saveTableAndQueryIt</li></div><div><li>org.apache.spark.sql.hive.JavaMetastoreDataSourcesSuite.saveExternalTableAndQueryIt</li></div><div><li>org.apache.spark.sql.hive.JavaMetastoreDataSourcesSuite.saveTableAndQueryIt</li></div><div><li>org.apache.spark.sql.hive.JavaMetastoreDataSourcesSuite.saveExternalTableWithSchemaAndQueryIt</li></div><div><li>org.apache.spark.streaming.JavaMapWithStateSuite.testBasicFunction</li></div><div><li>org.apache.spark.streaming.JavaMapWithStateSuite.testBasicFunction</li></div><div><li>org.apache.spark.streaming.JavaReceiverAPISuite.testReceiver</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testStreamingContextTransform</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testStreamingContextTransform</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testFlatMapValues</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testFlatMapValues</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testMapPartitions</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testMapPartitions</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairFilter</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairFilter</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testCombineByKey</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testCombineByKey</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testMap</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testMap</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairToNormalRDDTransform</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairToNormalRDDTransform</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairReduceByKey</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairReduceByKey</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairMap</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairMap</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testFlatMap</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testFlatMap</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testReduceByKeyAndWindowWithInverse</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testReduceByKeyAndWindowWithInverse</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testReduceByWindow</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testReduceByWindow</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairFlatMap</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairFlatMap</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairToPairFlatMapWithChangingTypes</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairToPairFlatMapWithChangingTypes</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairMapPartitions</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairMapPartitions</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testVariousTransform</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testVariousTransform</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testTransformWith</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testTransformWith</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testVariousTransformWith</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testVariousTransformWith</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testReduceByKeyAndWindow</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testReduceByKeyAndWindow</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairTransform</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairTransform</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testFilter</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testFilter</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairMap2</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairMap2</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testMapValues</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testMapValues</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testReduce</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testReduce</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testUpdateStateByKey</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testTransform</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testTransform</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testStreamingContextTransform</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testFlatMapValues</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testReduceByWindowWithInverse</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testMapPartitions</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairFilter</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testRepartitionFewerPartitions</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testRepartitionFewerPartitions</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCombineByKey</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testContextGetOrCreate</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testContextGetOrCreate</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testWindowWithSlideDuration</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testWindowWithSlideDuration</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testQueueStream</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testQueueStream</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCountByValue</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCountByValue</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testMap</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testMap</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairToNormalRDDTransform</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairToNormalRDDTransform</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairReduceByKey</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairReduceByKey</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCount</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCount</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCheckpointMasterRecovery</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCheckpointMasterRecovery</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairMap</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairMap</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testUnion</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testUnion</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testFlatMap</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testFlatMap</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testReduceByKeyAndWindowWithInverse</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testGlom</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testGlom</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testJoin</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testJoin</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairFlatMap</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairFlatMap</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairToPairFlatMapWithChangingTypes</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairToPairFlatMapWithChangingTypes</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairMapPartitions</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairMapPartitions</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testRepartitionMorePartitions</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testRepartitionMorePartitions</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testReduceByWindowWithoutInverse</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testReduceByWindowWithoutInverse</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testLeftOuterJoin</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testLeftOuterJoin</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testVariousTransform</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testVariousTransform</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testTransformWith</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testTransformWith</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testVariousTransformWith</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testVariousTransformWith</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testTextFileStream</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairGroupByKey</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCoGroup</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testInitialization</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testInitialization</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testSocketString</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testSocketString</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testGroupByKeyAndWindow</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testGroupByKeyAndWindow</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testReduceByKeyAndWindow</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testReduceByKeyAndWindow</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testForeachRDD</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testForeachRDD</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testFileStream</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testFileStream</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairTransform</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairTransform</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testFilter</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testFilter</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairMap2</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testMapValues</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testReduce</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testReduce</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testUpdateStateByKey</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testTransform</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testTransform</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testWindow</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testWindow</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCountByValueAndWindow</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testSocketTextStream</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testSocketTextStream</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testUpdateStateByKeyWithInitial</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testUpdateStateByKeyWithInitial</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testContextState</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testContextState</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexDescendingWithStart</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexWithStart</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndexDescendingWithStart</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexDescending</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexWithStart</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexWithLast</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexWithSkip</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexWithMax</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexDescending</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndexDescendingWithLast</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexDescending</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexDescendingWithLast</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndex</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndexWithLast</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexWithStart</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexDescendingWithStart</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexWithLast</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexWithSkip</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndexDescending</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.testRefWithIntNaturalKey</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexDescending</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexDescendingWithStart</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexWithMax</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndex</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexWithLast</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexWithSkip</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexWithMax</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexDescendingWithLast</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexDescendingWithLast</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexDescendingWithStart</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndex</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexWithLast</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexWithSkip</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexWithStart</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndex</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexDescendingWithLast</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndexWithStart</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndex</li></div><div><li>org.apache.spark.util.kvstore.LevelDBSuite.testMultipleTypesWriteReadDelete</li></div><div><li>org.apache.spark.util.kvstore.LevelDBSuite.testObjectWriteReadDelete</li></div><div><li>org.apache.spark.util.kvstore.LevelDBSuite.testSkip</li></div><div><li>org.apache.spark.util.kvstore.LevelDBSuite.testMultipleObjectWriteReadDelete</li></div><div><li>org.apache.spark.util.kvstore.LevelDBSuite.testReopenAndVersionCheckDb</li></div><div><li>org.apache.spark.util.kvstore.LevelDBSuite.testMetadata</li></div><div><li>org.apache.spark.util.kvstore.LevelDBSuite.testUpdate</li></div><div><li>org.apache.spark.util.kvstore.LevelDBSuite.testNegativeIndexValues</li></div><div><li>org.apache.spark.sql.kafka010.KafkaDontFailOnDataLossSuite.failOnDataLoss=false should not return duplicated records: v1</li></div><div><li>org.apache.spark.sql.kafka010.KafkaMicroBatchV1SourceSuite.subscribing topic by pattern from earliest offsets (failOnDataLoss: false)</li></div><div><li>org.apache.spark.sql.kafka010.KafkaSourceStressForDontFailOnDataLossSuite.stress test for failOnDataLoss=false</li></div><div><li>org.apache.spark.streaming.kafka010.DirectKafkaStreamSuite.offset recovery from kafka</li></div><div><li>org.apache.spark.sql.streaming.EventTimeWatermarkSuite.delay in months and years handled correctly</li></div><div><li>org.apache.spark.sql.streaming.StreamingOuterJoinSuite.windowed right outer join</li></div><div><li>org.apache.spark.sql.hive.HiveSparkSubmitSuite.SPARK-8020: set sql conf in spark conf</li></div><div><li>org.apache.spark.sql.hive.HiveSparkSubmitSuite.SPARK-9757 Persist Parquet relation with decimal column</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.spark.network.ChunkFetchIntegrationSuite.fetchBothChunks</li></div><div><li>org.apache.spark.network.sasl.SparkSaslSuite.testFileRegionEncryption</li></div><div><li>org.apache.spark.ExecutorPluginSuite.testAddMultiplePlugins</li></div><div><li>org.apache.spark.ExecutorPluginSuite.testPluginClassDoesNotExist</li></div><div><li>org.apache.spark.ExecutorPluginSuite.testAddPlugin</li></div><div><li>org.apache.spark.ExecutorPluginSuite.testPluginShutdownWithException</li></div><div><li>org.apache.spark.JavaJdbcRDDSuite.testJavaJdbcRDD</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.leftOuterJoin</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.foldReduce</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.mapsFromPairsToPairs</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.mapsFromPairsToPairs</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.flatMap</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.flatMap</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.foreach</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.foreach</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.map</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.map</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.zip</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.zip</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.keyBy</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.keyBy</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.groupBy</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.groupBy</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.mapPartitions</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.mapPartitions</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.foldByKey</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.foldByKey</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.mapOnPairRDD</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.mapOnPairRDD</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.sequenceFile</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.sequenceFile</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.collectPartitions</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.collectPartitions</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.reduceByKey</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.reduceByKey</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.foreachWithAnonymousClass</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.foreachWithAnonymousClass</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.collectAsMapWithIntArrayValues</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.collectAsMapWithIntArrayValues</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.zipPartitions</li></div><div><li>test.org.apache.spark.Java8RDDAPISuite.zipPartitions</li></div><div><li>test.org.apache.spark.JavaAPISuite.groupByOnPairRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.groupByOnPairRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.binaryFilesCaching</li></div><div><li>test.org.apache.spark.JavaAPISuite.binaryFilesCaching</li></div><div><li>test.org.apache.spark.JavaAPISuite.sparkContextUnion</li></div><div><li>test.org.apache.spark.JavaAPISuite.sparkContextUnion</li></div><div><li>test.org.apache.spark.JavaAPISuite.checkpointAndComputation</li></div><div><li>test.org.apache.spark.JavaAPISuite.checkpointAndComputation</li></div><div><li>test.org.apache.spark.JavaAPISuite.leftOuterJoin</li></div><div><li>test.org.apache.spark.JavaAPISuite.leftOuterJoin</li></div><div><li>test.org.apache.spark.JavaAPISuite.keyByOnPairRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.keyByOnPairRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.getNumPartitions</li></div><div><li>test.org.apache.spark.JavaAPISuite.getNumPartitions</li></div><div><li>test.org.apache.spark.JavaAPISuite.wholeTextFiles</li></div><div><li>test.org.apache.spark.JavaAPISuite.wholeTextFiles</li></div><div><li>test.org.apache.spark.JavaAPISuite.binaryFiles</li></div><div><li>test.org.apache.spark.JavaAPISuite.binaryFiles</li></div><div><li>test.org.apache.spark.JavaAPISuite.foldReduce</li></div><div><li>test.org.apache.spark.JavaAPISuite.foldReduce</li></div><div><li>test.org.apache.spark.JavaAPISuite.writeWithNewAPIHadoopFile</li></div><div><li>test.org.apache.spark.JavaAPISuite.writeWithNewAPIHadoopFile</li></div><div><li>test.org.apache.spark.JavaAPISuite.hadoopFile</li></div><div><li>test.org.apache.spark.JavaAPISuite.hadoopFile</li></div><div><li>test.org.apache.spark.JavaAPISuite.lookup</li></div><div><li>test.org.apache.spark.JavaAPISuite.lookup</li></div><div><li>test.org.apache.spark.JavaAPISuite.countAsync</li></div><div><li>test.org.apache.spark.JavaAPISuite.countAsync</li></div><div><li>test.org.apache.spark.JavaAPISuite.textFiles</li></div><div><li>test.org.apache.spark.JavaAPISuite.textFiles</li></div><div><li>test.org.apache.spark.JavaAPISuite.binaryRecords</li></div><div><li>test.org.apache.spark.JavaAPISuite.binaryRecords</li></div><div><li>test.org.apache.spark.JavaAPISuite.toLocalIterator</li></div><div><li>test.org.apache.spark.JavaAPISuite.toLocalIterator</li></div><div><li>test.org.apache.spark.JavaAPISuite.repartitionAndSortWithinPartitions</li></div><div><li>test.org.apache.spark.JavaAPISuite.repartitionAndSortWithinPartitions</li></div><div><li>test.org.apache.spark.JavaAPISuite.reduce</li></div><div><li>test.org.apache.spark.JavaAPISuite.reduce</li></div><div><li>test.org.apache.spark.JavaAPISuite.sample</li></div><div><li>test.org.apache.spark.JavaAPISuite.sample</li></div><div><li>test.org.apache.spark.JavaAPISuite.sortBy</li></div><div><li>test.org.apache.spark.JavaAPISuite.sortBy</li></div><div><li>test.org.apache.spark.JavaAPISuite.mapsFromPairsToPairs</li></div><div><li>test.org.apache.spark.JavaAPISuite.mapsFromPairsToPairs</li></div><div><li>test.org.apache.spark.JavaAPISuite.flatMap</li></div><div><li>test.org.apache.spark.JavaAPISuite.flatMap</li></div><div><li>test.org.apache.spark.JavaAPISuite.cogroup3</li></div><div><li>test.org.apache.spark.JavaAPISuite.cogroup3</li></div><div><li>test.org.apache.spark.JavaAPISuite.cogroup4</li></div><div><li>test.org.apache.spark.JavaAPISuite.cogroup4</li></div><div><li>test.org.apache.spark.JavaAPISuite.randomSplit</li></div><div><li>test.org.apache.spark.JavaAPISuite.randomSplit</li></div><div><li>test.org.apache.spark.JavaAPISuite.persist</li></div><div><li>test.org.apache.spark.JavaAPISuite.persist</li></div><div><li>test.org.apache.spark.JavaAPISuite.foreach</li></div><div><li>test.org.apache.spark.JavaAPISuite.foreach</li></div><div><li>test.org.apache.spark.JavaAPISuite.hadoopFileCompressed</li></div><div><li>test.org.apache.spark.JavaAPISuite.hadoopFileCompressed</li></div><div><li>test.org.apache.spark.JavaAPISuite.textFilesCompressed</li></div><div><li>test.org.apache.spark.JavaAPISuite.textFilesCompressed</li></div><div><li>test.org.apache.spark.JavaAPISuite.testAsyncActionCancellation</li></div><div><li>test.org.apache.spark.JavaAPISuite.testAsyncActionCancellation</li></div><div><li>test.org.apache.spark.JavaAPISuite.checkpointAndRestore</li></div><div><li>test.org.apache.spark.JavaAPISuite.checkpointAndRestore</li></div><div><li>test.org.apache.spark.JavaAPISuite.sortByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.sortByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.aggregateByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.aggregateByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.map</li></div><div><li>test.org.apache.spark.JavaAPISuite.map</li></div><div><li>test.org.apache.spark.JavaAPISuite.max</li></div><div><li>test.org.apache.spark.JavaAPISuite.max</li></div><div><li>test.org.apache.spark.JavaAPISuite.min</li></div><div><li>test.org.apache.spark.JavaAPISuite.min</li></div><div><li>test.org.apache.spark.JavaAPISuite.top</li></div><div><li>test.org.apache.spark.JavaAPISuite.top</li></div><div><li>test.org.apache.spark.JavaAPISuite.zip</li></div><div><li>test.org.apache.spark.JavaAPISuite.fold</li></div><div><li>test.org.apache.spark.JavaAPISuite.glom</li></div><div><li>test.org.apache.spark.JavaAPISuite.take</li></div><div><li>test.org.apache.spark.JavaAPISuite.javaDoubleRDDHistoGram</li></div><div><li>test.org.apache.spark.JavaAPISuite.javaDoubleRDDHistoGram</li></div><div><li>test.org.apache.spark.JavaAPISuite.collectUnderlyingScalaRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.collectUnderlyingScalaRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.keyBy</li></div><div><li>test.org.apache.spark.JavaAPISuite.keyBy</li></div><div><li>test.org.apache.spark.JavaAPISuite.mapPartitionsWithIndex</li></div><div><li>test.org.apache.spark.JavaAPISuite.mapPartitionsWithIndex</li></div><div><li>test.org.apache.spark.JavaAPISuite.sampleByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.sampleByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.intersection</li></div><div><li>test.org.apache.spark.JavaAPISuite.intersection</li></div><div><li>test.org.apache.spark.JavaAPISuite.aggregate</li></div><div><li>test.org.apache.spark.JavaAPISuite.aggregate</li></div><div><li>test.org.apache.spark.JavaAPISuite.cartesian</li></div><div><li>test.org.apache.spark.JavaAPISuite.cartesian</li></div><div><li>test.org.apache.spark.JavaAPISuite.countApproxDistinctByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.countApproxDistinctByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.readWithNewAPIHadoopFile</li></div><div><li>test.org.apache.spark.JavaAPISuite.readWithNewAPIHadoopFile</li></div><div><li>test.org.apache.spark.JavaAPISuite.testRegisterKryoClasses</li></div><div><li>test.org.apache.spark.JavaAPISuite.testRegisterKryoClasses</li></div><div><li>test.org.apache.spark.JavaAPISuite.groupBy</li></div><div><li>test.org.apache.spark.JavaAPISuite.groupBy</li></div><div><li>test.org.apache.spark.JavaAPISuite.sampleByKeyExact</li></div><div><li>test.org.apache.spark.JavaAPISuite.sampleByKeyExact</li></div><div><li>test.org.apache.spark.JavaAPISuite.mapPartitions</li></div><div><li>test.org.apache.spark.JavaAPISuite.mapPartitions</li></div><div><li>test.org.apache.spark.JavaAPISuite.takeOrdered</li></div><div><li>test.org.apache.spark.JavaAPISuite.takeOrdered</li></div><div><li>test.org.apache.spark.JavaAPISuite.foldByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.foldByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.objectFilesOfInts</li></div><div><li>test.org.apache.spark.JavaAPISuite.objectFilesOfInts</li></div><div><li>test.org.apache.spark.JavaAPISuite.treeAggregate</li></div><div><li>test.org.apache.spark.JavaAPISuite.treeAggregate</li></div><div><li>test.org.apache.spark.JavaAPISuite.testGetPersistentRDDs</li></div><div><li>test.org.apache.spark.JavaAPISuite.testGetPersistentRDDs</li></div><div><li>test.org.apache.spark.JavaAPISuite.approximateResults</li></div><div><li>test.org.apache.spark.JavaAPISuite.approximateResults</li></div><div><li>test.org.apache.spark.JavaAPISuite.treeReduce</li></div><div><li>test.org.apache.spark.JavaAPISuite.treeReduce</li></div><div><li>test.org.apache.spark.JavaAPISuite.collectAsMapAndSerialize</li></div><div><li>test.org.apache.spark.JavaAPISuite.collectAsMapAndSerialize</li></div><div><li>test.org.apache.spark.JavaAPISuite.countApproxDistinct</li></div><div><li>test.org.apache.spark.JavaAPISuite.countApproxDistinct</li></div><div><li>test.org.apache.spark.JavaAPISuite.javaDoubleRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.javaDoubleRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.mapOnPairRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.mapOnPairRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.testAsyncActionErrorWrapping</li></div><div><li>test.org.apache.spark.JavaAPISuite.testAsyncActionErrorWrapping</li></div><div><li>test.org.apache.spark.JavaAPISuite.naturalMax</li></div><div><li>test.org.apache.spark.JavaAPISuite.naturalMax</li></div><div><li>test.org.apache.spark.JavaAPISuite.naturalMin</li></div><div><li>test.org.apache.spark.JavaAPISuite.naturalMin</li></div><div><li>test.org.apache.spark.JavaAPISuite.sequenceFile</li></div><div><li>test.org.apache.spark.JavaAPISuite.collectPartitions</li></div><div><li>test.org.apache.spark.JavaAPISuite.cogroup</li></div><div><li>test.org.apache.spark.JavaAPISuite.cogroup</li></div><div><li>test.org.apache.spark.JavaAPISuite.reduceByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.reduceByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.repartition</li></div><div><li>test.org.apache.spark.JavaAPISuite.repartition</li></div><div><li>test.org.apache.spark.JavaAPISuite.iterator</li></div><div><li>test.org.apache.spark.JavaAPISuite.iterator</li></div><div><li>test.org.apache.spark.JavaAPISuite.emptyRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.emptyRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.zipWithIndex</li></div><div><li>test.org.apache.spark.JavaAPISuite.zipWithIndex</li></div><div><li>test.org.apache.spark.JavaAPISuite.foreachPartition</li></div><div><li>test.org.apache.spark.JavaAPISuite.foreachPartition</li></div><div><li>test.org.apache.spark.JavaAPISuite.combineByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.combineByKey</li></div><div><li>test.org.apache.spark.JavaAPISuite.takeAsync</li></div><div><li>test.org.apache.spark.JavaAPISuite.takeAsync</li></div><div><li>test.org.apache.spark.JavaAPISuite.collectAsMapWithIntArrayValues</li></div><div><li>test.org.apache.spark.JavaAPISuite.collectAsMapWithIntArrayValues</li></div><div><li>test.org.apache.spark.JavaAPISuite.objectFilesOfComplexTypes</li></div><div><li>test.org.apache.spark.JavaAPISuite.objectFilesOfComplexTypes</li></div><div><li>test.org.apache.spark.JavaAPISuite.zipWithUniqueId</li></div><div><li>test.org.apache.spark.JavaAPISuite.zipWithUniqueId</li></div><div><li>test.org.apache.spark.JavaAPISuite.collectAsync</li></div><div><li>test.org.apache.spark.JavaAPISuite.collectAsync</li></div><div><li>test.org.apache.spark.JavaAPISuite.foreachAsync</li></div><div><li>test.org.apache.spark.JavaAPISuite.foreachAsync</li></div><div><li>test.org.apache.spark.JavaAPISuite.zipPartitions</li></div><div><li>test.org.apache.spark.JavaAPISuite.zipPartitions</li></div><div><li>test.org.apache.spark.JavaAPISuite.reduceOnJavaDoubleRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.reduceOnJavaDoubleRDD</li></div><div><li>test.org.apache.spark.JavaAPISuite.isEmpty</li></div><div><li>test.org.apache.spark.JavaAPISuite.isEmpty</li></div><div><li>test.org.apache.spark.JavaSparkContextSuite.javaSparkContext</li></div><div><li>test.org.apache.spark.JavaSparkContextSuite.scalaSparkContext</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single BooleanType with seed 841981079902811434</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single ByteType with seed 2243310682325962444</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single ShortType with seed 9028587868712380514</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single IntegerType with seed 3259778497745115189</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single LongType with seed 3951291623248444947</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single FloatType with seed 48979942793015499</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single DoubleType with seed -9114732779414028070</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single DecimalType(8,0) with seed 9065409153204310824</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single DecimalType(8,4) with seed 756481727812831179</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single DecimalType(16,0) with seed 5406300250631918627</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single DecimalType(16,11) with seed -678323241171125044</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single DecimalType(38,0) with seed -8422934417194024128</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single DecimalType(38,38) with seed -1905569850793528289</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single StringType with seed 3309456092726877802</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single BinaryType with seed 1174879911608330560</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.flat schema struct&lt;col_0:string,col_1:int,col_2:bigint,col_3:tinyint,col_4:bigint&gt; with seed 2038646069863484639</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.flat schema struct&lt;col_0:float,col_1:boolean,col_2:binary,col_3:decimal(38,38),col_4:binary&gt; with seed -7053316220995128080</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.flat schema struct&lt;col_0:decimal(16,11),col_1:boolean,col_2:decimal(8,4),col_3:string,col_4:float&gt; with seed -3311834729306860181</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.flat schema struct&lt;col_0:binary,col_1:float,col_2:decimal(16,11),col_3:tinyint,col_4:bigint&gt; with seed 2965881151385800849</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.flat schema struct&lt;col_0:string,col_1:float,col_2:decimal(16,0),col_3:boolean,col_4:tinyint&gt; with seed -5118190953296608296</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.nested schema struct&lt;col_0:array&lt;tinyint&gt;,col_1:smallint,col_2:smallint,col_3:string,col_4:struct&lt;col_0:array&lt;bigint&gt;,col_1:struct&lt;col_0:array&lt;decimal(8,0)&gt;&gt;,col_2:decimal(16,0)&gt;,col_5:tinyint,col_6:struct&lt;col_0:struct&lt;col_0:decimal(8,0)&gt;&gt;,col_7:struct&lt;col_0:struct&lt;col_0:struct&lt;col_0:boolean&gt;&gt;&gt;&gt; with seed 3345718503295646388</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.nested schema struct&lt;col_0:struct&lt;col_0:array&lt;decimal(38,38)&gt;,col_1:struct&lt;col_0:array&lt;string&gt;&gt;,col_2:struct&lt;col_0:array&lt;decimal(38,0)&gt;&gt;,col_3:decimal(8,0),col_4:array&lt;tinyint&gt;,col_5:array&lt;tinyint&gt;,col_6:decimal(16,0)&gt;,col_1:struct&lt;col_0:struct&lt;col_0:struct&lt;col_0:struct&lt;col_0:string&gt;&gt;&gt;&gt;,col_2:double,col_3:array&lt;binary&gt;&gt; with seed -8107408962085047900</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.nested schema struct&lt;col_0:struct&lt;col_0:array&lt;boolean&gt;,col_1:struct&lt;col_0:struct&lt;col_0:int&gt;&gt;,col_2:struct&lt;col_0:array&lt;float&gt;,col_1:struct&lt;col_0:string&gt;&gt;,col_3:decimal(8,0),col_4:struct&lt;col_0:struct&lt;col_0:array&lt;decimal(16,11)&gt;&gt;&gt;,col_5:decimal(16,0)&gt;,col_1:decimal(8,0),col_2:bigint,col_3:smallint&gt; with seed 8354719024368488688</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.nested schema struct&lt;col_0:array&lt;decimal(16,11)&gt;,col_1:float,col_2:array&lt;bigint&gt;,col_3:struct&lt;col_0:array&lt;binary&gt;&gt;,col_4:struct&lt;col_0:decimal(8,0)&gt;,col_5:array&lt;tinyint&gt;,col_6:decimal(16,0),col_7:array&lt;decimal(38,0)&gt;,col_8:struct&lt;col_0:struct&lt;col_0:array&lt;decimal(16,0)&gt;&gt;&gt;,col_9:struct&lt;col_0:array&lt;float&gt;&gt;&gt; with seed 7164111805930087924</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.nested schema struct&lt;col_0:array&lt;int&gt;,col_1:decimal(38,38),col_2:array&lt;decimal(38,0)&gt;,col_3:struct&lt;col_0:struct&lt;col_0:struct&lt;col_0:struct&lt;col_0:decimal(16,11)&gt;&gt;&gt;,col_1:struct&lt;col_0:struct&lt;col_0:struct&lt;col_0:struct&lt;col_0:struct&lt;col_0:array&lt;decimal(16,0)&gt;&gt;&gt;&gt;&gt;&gt;&gt;,col_4:int,col_5:array&lt;decimal(16,11)&gt;,col_6:array&lt;tinyint&gt;,col_7:string,col_8:array&lt;smallint&gt;&gt; with seed -3483466748189527307</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.read int as string</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.read string as int</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.read float as double</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.read double as float</li></div><div><li>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.Handle unsupported input of record type</li></div><div><li>org.apache.spark.sql.avro.AvroFunctionsSuite.roundtrip in to_avro and from_avro - int and string</li></div><div><li>org.apache.spark.sql.avro.AvroFunctionsSuite.roundtrip in to_avro and from_avro - struct</li></div><div><li>org.apache.spark.sql.avro.AvroFunctionsSuite.handle invalid input in from_avro</li></div><div><li>org.apache.spark.sql.avro.AvroFunctionsSuite.roundtrip in to_avro and from_avro - array with null</li></div><div><li>org.apache.spark.streaming.kafka010.JavaDirectKafkaStreamSuite.testKafkaStream</li></div><div><li>org.apache.spark.streaming.kafka010.JavaKafkaRDDSuite.testKafkaRDD</li></div><div><li>org.apache.spark.graphx.EdgeRDDSuite.cache, getStorageLevel</li></div><div><li>org.apache.spark.graphx.EdgeRDDSuite.checkpointing</li></div><div><li>org.apache.spark.graphx.GraphLoaderSuite.GraphLoader.edgeListFile</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.joinVertices</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.collectNeighborIds</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.removeSelfEdges</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.filter</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.convertToCanonicalEdges</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.collectEdgesCycleDirectionOut</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.collectEdgesCycleDirectionIn</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.collectEdgesCycleDirectionEither</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.collectEdgesChainDirectionOut</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.collectEdgesChainDirectionIn</li></div><div><li>org.apache.spark.graphx.GraphOpsSuite.collectEdgesChainDirectionEither</li></div><div><li>org.apache.spark.graphx.GraphSuite.Graph.fromEdgeTuples</li></div><div><li>org.apache.spark.graphx.GraphSuite.Graph.fromEdges</li></div><div><li>org.apache.spark.graphx.GraphSuite.Graph.apply</li></div><div><li>org.apache.spark.graphx.GraphSuite.triplets</li></div><div><li>org.apache.spark.graphx.GraphSuite.partitionBy</li></div><div><li>org.apache.spark.graphx.GraphSuite.mapVertices</li></div><div><li>org.apache.spark.graphx.GraphSuite.mapVertices changing type with same erased type</li></div><div><li>org.apache.spark.graphx.GraphSuite.mapEdges</li></div><div><li>org.apache.spark.graphx.GraphSuite.mapTriplets</li></div><div><li>org.apache.spark.graphx.GraphSuite.reverse</li></div><div><li>org.apache.spark.graphx.GraphSuite.reverse with join elimination</li></div><div><li>org.apache.spark.graphx.GraphSuite.subgraph</li></div><div><li>org.apache.spark.graphx.GraphSuite.mask</li></div><div><li>org.apache.spark.graphx.GraphSuite.groupEdges</li></div><div><li>org.apache.spark.graphx.GraphSuite.aggregateMessages</li></div><div><li>org.apache.spark.graphx.GraphSuite.outerJoinVertices</li></div><div><li>org.apache.spark.graphx.GraphSuite.more edge partitions than vertex partitions</li></div><div><li>org.apache.spark.graphx.GraphSuite.checkpoint</li></div><div><li>org.apache.spark.graphx.GraphSuite.cache, getStorageLevel</li></div><div><li>org.apache.spark.graphx.GraphSuite.non-default number of edge partitions</li></div><div><li>org.apache.spark.graphx.GraphSuite.unpersist graph RDD</li></div><div><li>org.apache.spark.graphx.GraphSuite.SPARK-14219: pickRandomVertex</li></div><div><li>org.apache.spark.graphx.PregelSuite.1 iteration</li></div><div><li>org.apache.spark.graphx.PregelSuite.chain propagation</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.filter</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.mapValues</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.minus</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.minus with RDD[(VertexId, VD)]</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.minus with non-equal number of partitions</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.diff</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.diff with RDD[(VertexId, VD)]</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.diff vertices with non-equal number of partitions</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.leftJoin</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.leftJoin vertices with non-equal number of partitions</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.innerJoin</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.innerJoin vertices with the non-equal number of partitions</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.aggregateUsingIndex</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.mergeFunc</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.cache, getStorageLevel</li></div><div><li>org.apache.spark.graphx.VertexRDDSuite.checkpoint</li></div><div><li>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Grid Connected Components</li></div><div><li>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Reverse Grid Connected Components</li></div><div><li>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Chain Connected Components</li></div><div><li>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Reverse Chain Connected Components</li></div><div><li>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Connected Components on a Toy Connected Graph</li></div><div><li>org.apache.spark.graphx.lib.LabelPropagationSuite.Label Propagation</li></div><div><li>org.apache.spark.graphx.lib.PageRankSuite.Star PageRank</li></div><div><li>org.apache.spark.graphx.lib.PageRankSuite.Star PersonalPageRank</li></div><div><li>org.apache.spark.graphx.lib.PageRankSuite.Grid PageRank</li></div><div><li>org.apache.spark.graphx.lib.PageRankSuite.Chain PageRank</li></div><div><li>org.apache.spark.graphx.lib.PageRankSuite.Chain PersonalizedPageRank</li></div><div><li>org.apache.spark.graphx.lib.PageRankSuite.Loop with source PageRank</li></div><div><li>org.apache.spark.graphx.lib.PageRankSuite.Loop with sink PageRank</li></div><div><li>org.apache.spark.graphx.lib.SVDPlusPlusSuite.Test SVD++ with mean square error on training set</li></div><div><li>org.apache.spark.graphx.lib.ShortestPathsSuite.Shortest Path Computations</li></div><div><li>org.apache.spark.graphx.lib.StronglyConnectedComponentsSuite.Island Strongly Connected Components</li></div><div><li>org.apache.spark.graphx.lib.StronglyConnectedComponentsSuite.Cycle Strongly Connected Components</li></div><div><li>org.apache.spark.graphx.lib.StronglyConnectedComponentsSuite.2 Cycle Strongly Connected Components</li></div><div><li>org.apache.spark.graphx.lib.TriangleCountSuite.Count a single triangle</li></div><div><li>org.apache.spark.graphx.lib.TriangleCountSuite.Count two triangles</li></div><div><li>org.apache.spark.graphx.lib.TriangleCountSuite.Count two triangles with bi-directed edges</li></div><div><li>org.apache.spark.graphx.lib.TriangleCountSuite.Count a single triangle with duplicate edges</li></div><div><li>org.apache.spark.graphx.util.GraphGeneratorsSuite.GraphGenerators.logNormalGraph</li></div><div><li>org.apache.spark.graphx.util.GraphGeneratorsSuite.SPARK-5064 GraphGenerators.rmatGraph numEdges upper bound</li></div><div><li>org.apache.spark.graphx.util.PeriodicGraphCheckpointerSuite.Persisting</li></div><div><li>org.apache.spark.graphx.util.PeriodicGraphCheckpointerSuite.Checkpointing</li></div><div><li>org.apache.spark.ml.JavaPipelineSuite.pipeline</li></div><div><li>org.apache.spark.ml.JavaPipelineSuite.pipeline</li></div><div><li>org.apache.spark.ml.classification.JavaDecisionTreeClassifierSuite.runDT</li></div><div><li>org.apache.spark.ml.classification.JavaDecisionTreeClassifierSuite.runDT</li></div><div><li>org.apache.spark.ml.classification.JavaGBTClassifierSuite.runDT</li></div><div><li>org.apache.spark.ml.classification.JavaGBTClassifierSuite.runDT</li></div><div><li>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionWithSetters</li></div><div><li>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionWithSetters</li></div><div><li>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionTrainingSummary</li></div><div><li>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionTrainingSummary</li></div><div><li>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionPredictorClassifierMethods</li></div><div><li>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionPredictorClassifierMethods</li></div><div><li>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionDefaultParams</li></div><div><li>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionDefaultParams</li></div><div><li>org.apache.spark.ml.classification.JavaMultilayerPerceptronClassifierSuite.testMLPC</li></div><div><li>org.apache.spark.ml.classification.JavaMultilayerPerceptronClassifierSuite.testMLPC</li></div><div><li>org.apache.spark.ml.classification.JavaNaiveBayesSuite.testNaiveBayes</li></div><div><li>org.apache.spark.ml.classification.JavaNaiveBayesSuite.testNaiveBayes</li></div><div><li>org.apache.spark.ml.classification.JavaNaiveBayesSuite.naiveBayesDefaultParams</li></div><div><li>org.apache.spark.ml.classification.JavaNaiveBayesSuite.naiveBayesDefaultParams</li></div><div><li>org.apache.spark.ml.classification.JavaOneVsRestSuite.oneVsRestDefaultParams</li></div><div><li>org.apache.spark.ml.classification.JavaOneVsRestSuite.oneVsRestDefaultParams</li></div><div><li>org.apache.spark.ml.classification.JavaRandomForestClassifierSuite.runDT</li></div><div><li>org.apache.spark.ml.classification.JavaRandomForestClassifierSuite.runDT</li></div><div><li>org.apache.spark.ml.clustering.JavaKMeansSuite.fitAndTransform</li></div><div><li>org.apache.spark.ml.clustering.JavaKMeansSuite.fitAndTransform</li></div><div><li>org.apache.spark.ml.feature.JavaBucketizerSuite.bucketizerTest</li></div><div><li>org.apache.spark.ml.feature.JavaBucketizerSuite.bucketizerTest</li></div><div><li>org.apache.spark.ml.feature.JavaBucketizerSuite.bucketizerMultipleColumnsTest</li></div><div><li>org.apache.spark.ml.feature.JavaBucketizerSuite.bucketizerMultipleColumnsTest</li></div><div><li>org.apache.spark.ml.feature.JavaDCTSuite.javaCompatibilityTest</li></div><div><li>org.apache.spark.ml.feature.JavaDCTSuite.javaCompatibilityTest</li></div><div><li>org.apache.spark.ml.feature.JavaHashingTFSuite.hashingTF</li></div><div><li>org.apache.spark.ml.feature.JavaHashingTFSuite.hashingTF</li></div><div><li>org.apache.spark.ml.feature.JavaNormalizerSuite.normalizer</li></div><div><li>org.apache.spark.ml.feature.JavaNormalizerSuite.normalizer</li></div><div><li>org.apache.spark.ml.feature.JavaPCASuite.testPCA</li></div><div><li>org.apache.spark.ml.feature.JavaPCASuite.testPCA</li></div><div><li>org.apache.spark.ml.feature.JavaPolynomialExpansionSuite.polynomialExpansionTest</li></div><div><li>org.apache.spark.ml.feature.JavaPolynomialExpansionSuite.polynomialExpansionTest</li></div><div><li>org.apache.spark.ml.feature.JavaStandardScalerSuite.standardScaler</li></div><div><li>org.apache.spark.ml.feature.JavaStandardScalerSuite.standardScaler</li></div><div><li>org.apache.spark.ml.feature.JavaStopWordsRemoverSuite.javaCompatibilityTest</li></div><div><li>org.apache.spark.ml.feature.JavaStopWordsRemoverSuite.javaCompatibilityTest</li></div><div><li>org.apache.spark.ml.feature.JavaStringIndexerSuite.testStringIndexer</li></div><div><li>org.apache.spark.ml.feature.JavaStringIndexerSuite.testStringIndexer</li></div><div><li>org.apache.spark.ml.feature.JavaTokenizerSuite.regexTokenizer</li></div><div><li>org.apache.spark.ml.feature.JavaTokenizerSuite.regexTokenizer</li></div><div><li>org.apache.spark.ml.feature.JavaVectorAssemblerSuite.testVectorAssembler</li></div><div><li>org.apache.spark.ml.feature.JavaVectorAssemblerSuite.testVectorAssembler</li></div><div><li>org.apache.spark.ml.feature.JavaVectorIndexerSuite.vectorIndexerAPI</li></div><div><li>org.apache.spark.ml.feature.JavaVectorIndexerSuite.vectorIndexerAPI</li></div><div><li>org.apache.spark.ml.feature.JavaVectorSlicerSuite.vectorSlice</li></div><div><li>org.apache.spark.ml.feature.JavaVectorSlicerSuite.vectorSlice</li></div><div><li>org.apache.spark.ml.feature.JavaWord2VecSuite.testJavaWord2Vec</li></div><div><li>org.apache.spark.ml.feature.JavaWord2VecSuite.testJavaWord2Vec</li></div><div><li>org.apache.spark.ml.regression.JavaDecisionTreeRegressorSuite.runDT</li></div><div><li>org.apache.spark.ml.regression.JavaDecisionTreeRegressorSuite.runDT</li></div><div><li>org.apache.spark.ml.regression.JavaGBTRegressorSuite.runDT</li></div><div><li>org.apache.spark.ml.regression.JavaGBTRegressorSuite.runDT</li></div><div><li>org.apache.spark.ml.regression.JavaLinearRegressionSuite.linearRegressionDefaultParams</li></div><div><li>org.apache.spark.ml.regression.JavaLinearRegressionSuite.linearRegressionDefaultParams</li></div><div><li>org.apache.spark.ml.regression.JavaLinearRegressionSuite.linearRegressionWithSetters</li></div><div><li>org.apache.spark.ml.regression.JavaLinearRegressionSuite.linearRegressionWithSetters</li></div><div><li>org.apache.spark.ml.regression.JavaRandomForestRegressorSuite.runDT</li></div><div><li>org.apache.spark.ml.source.libsvm.JavaLibSVMRelationSuite.verifyLibSVMDF</li></div><div><li>org.apache.spark.ml.source.libsvm.JavaLibSVMRelationSuite.verifyLibSVMDF</li></div><div><li>org.apache.spark.ml.stat.JavaKolmogorovSmirnovTestSuite.testKSTestNamedDistribution</li></div><div><li>org.apache.spark.ml.stat.JavaKolmogorovSmirnovTestSuite.testKSTestNamedDistribution</li></div><div><li>org.apache.spark.ml.stat.JavaKolmogorovSmirnovTestSuite.testKSTestCDF</li></div><div><li>org.apache.spark.ml.stat.JavaKolmogorovSmirnovTestSuite.testKSTestCDF</li></div><div><li>org.apache.spark.ml.stat.JavaSummarizerSuite.testSummarizer</li></div><div><li>org.apache.spark.ml.tuning.JavaCrossValidatorSuite.crossValidationWithLogisticRegression</li></div><div><li>org.apache.spark.ml.tuning.JavaCrossValidatorSuite.crossValidationWithLogisticRegression</li></div><div><li>org.apache.spark.ml.util.JavaDefaultReadWriteSuite.testDefaultReadWrite</li></div><div><li>org.apache.spark.ml.util.JavaDefaultReadWriteSuite.testDefaultReadWrite</li></div><div><li>org.apache.spark.mllib.classification.JavaLogisticRegressionSuite.runLRUsingConstructor</li></div><div><li>org.apache.spark.mllib.classification.JavaLogisticRegressionSuite.runLRUsingConstructor</li></div><div><li>org.apache.spark.mllib.classification.JavaLogisticRegressionSuite.runLRUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.classification.JavaLogisticRegressionSuite.runLRUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.testPredictJavaRDD</li></div><div><li>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.testPredictJavaRDD</li></div><div><li>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.runUsingConstructor</li></div><div><li>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.runUsingConstructor</li></div><div><li>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.runUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.runUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.testModelTypeSetters</li></div><div><li>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.testModelTypeSetters</li></div><div><li>org.apache.spark.mllib.classification.JavaSVMSuite.runSVMUsingConstructor</li></div><div><li>org.apache.spark.mllib.classification.JavaSVMSuite.runSVMUsingConstructor</li></div><div><li>org.apache.spark.mllib.classification.JavaSVMSuite.runSVMUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.classification.JavaSVMSuite.runSVMUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.classification.JavaStreamingLogisticRegressionSuite.javaAPI</li></div><div><li>org.apache.spark.mllib.classification.JavaStreamingLogisticRegressionSuite.javaAPI</li></div><div><li>org.apache.spark.mllib.clustering.JavaBisectingKMeansSuite.twoDimensionalData</li></div><div><li>org.apache.spark.mllib.clustering.JavaBisectingKMeansSuite.twoDimensionalData</li></div><div><li>org.apache.spark.mllib.clustering.JavaGaussianMixtureSuite.runGaussianMixture</li></div><div><li>org.apache.spark.mllib.clustering.JavaKMeansSuite.testPredictJavaRDD</li></div><div><li>org.apache.spark.mllib.clustering.JavaKMeansSuite.testPredictJavaRDD</li></div><div><li>org.apache.spark.mllib.clustering.JavaKMeansSuite.runKMeansUsingConstructor</li></div><div><li>org.apache.spark.mllib.clustering.JavaKMeansSuite.runKMeansUsingConstructor</li></div><div><li>org.apache.spark.mllib.clustering.JavaKMeansSuite.runKMeansUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.clustering.JavaLDASuite.onlineOptimizerCompatibility</li></div><div><li>org.apache.spark.mllib.clustering.JavaLDASuite.onlineOptimizerCompatibility</li></div><div><li>org.apache.spark.mllib.clustering.JavaLDASuite.distributedLDAModel</li></div><div><li>org.apache.spark.mllib.clustering.JavaLDASuite.distributedLDAModel</li></div><div><li>org.apache.spark.mllib.clustering.JavaLDASuite.localLDAModel</li></div><div><li>org.apache.spark.mllib.clustering.JavaLDASuite.localLDAModel</li></div><div><li>org.apache.spark.mllib.clustering.JavaLDASuite.localLdaMethods</li></div><div><li>org.apache.spark.mllib.clustering.JavaLDASuite.localLdaMethods</li></div><div><li>org.apache.spark.mllib.clustering.JavaStreamingKMeansSuite.javaAPI</li></div><div><li>org.apache.spark.mllib.clustering.JavaStreamingKMeansSuite.javaAPI</li></div><div><li>org.apache.spark.mllib.evaluation.JavaRankingMetricsSuite.rankingMetrics</li></div><div><li>org.apache.spark.mllib.evaluation.JavaRankingMetricsSuite.rankingMetrics</li></div><div><li>org.apache.spark.mllib.feature.JavaTfIdfSuite.tfIdfMinimumDocumentFrequency</li></div><div><li>org.apache.spark.mllib.feature.JavaTfIdfSuite.tfIdf</li></div><div><li>org.apache.spark.mllib.feature.JavaWord2VecSuite.word2Vec</li></div><div><li>org.apache.spark.mllib.feature.JavaWord2VecSuite.word2Vec</li></div><div><li>org.apache.spark.mllib.fpm.JavaFPGrowthSuite.runFPGrowthSaveLoad</li></div><div><li>org.apache.spark.mllib.fpm.JavaFPGrowthSuite.runFPGrowthSaveLoad</li></div><div><li>org.apache.spark.mllib.fpm.JavaFPGrowthSuite.runFPGrowth</li></div><div><li>org.apache.spark.mllib.fpm.JavaFPGrowthSuite.runFPGrowth</li></div><div><li>org.apache.spark.mllib.fpm.JavaPrefixSpanSuite.runPrefixSpan</li></div><div><li>org.apache.spark.mllib.fpm.JavaPrefixSpanSuite.runPrefixSpanSaveLoad</li></div><div><li>org.apache.spark.mllib.fpm.JavaPrefixSpanSuite.runPrefixSpanSaveLoad</li></div><div><li>org.apache.spark.mllib.linalg.distributed.JavaRowMatrixSuite.rowMatrixQRDecomposition</li></div><div><li>org.apache.spark.mllib.linalg.distributed.JavaRowMatrixSuite.rowMatrixQRDecomposition</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testNormalVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testNormalVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testArbitrary</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testArbitrary</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testLogNormalVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testLogNormalVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testExponentialVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testExponentialVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testUniformRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testUniformRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testRandomVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testRandomVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testGammaRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testGammaRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testUniformVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testUniformVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testPoissonRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testPoissonRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testNormalRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testNormalRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testPoissonVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testPoissonVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testGammaVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testGammaVectorRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testExponentialRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testExponentialRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testLNormalRDD</li></div><div><li>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testLNormalRDD</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runALSUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runALSUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSUsingConstructor</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSUsingConstructor</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runRecommend</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runRecommend</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSWithNegativeWeight</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSWithNegativeWeight</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runALSUsingConstructor</li></div><div><li>org.apache.spark.mllib.recommendation.JavaALSSuite.runALSUsingConstructor</li></div><div><li>org.apache.spark.mllib.regression.JavaIsotonicRegressionSuite.testIsotonicRegressionJavaRDD</li></div><div><li>org.apache.spark.mllib.regression.JavaIsotonicRegressionSuite.testIsotonicRegressionJavaRDD</li></div><div><li>org.apache.spark.mllib.regression.JavaIsotonicRegressionSuite.testIsotonicRegressionPredictionsJavaRDD</li></div><div><li>org.apache.spark.mllib.regression.JavaIsotonicRegressionSuite.testIsotonicRegressionPredictionsJavaRDD</li></div><div><li>org.apache.spark.mllib.regression.JavaLassoSuite.runLassoUsingConstructor</li></div><div><li>org.apache.spark.mllib.regression.JavaLassoSuite.runLassoUsingConstructor</li></div><div><li>org.apache.spark.mllib.regression.JavaLassoSuite.runLassoUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.regression.JavaLassoSuite.runLassoUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.testPredictJavaRDD</li></div><div><li>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.testPredictJavaRDD</li></div><div><li>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.runLinearRegressionUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.runLinearRegressionUsingConstructor</li></div><div><li>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.runLinearRegressionUsingConstructor</li></div><div><li>org.apache.spark.mllib.regression.JavaRidgeRegressionSuite.runRidgeRegressionUsingConstructor</li></div><div><li>org.apache.spark.mllib.regression.JavaRidgeRegressionSuite.runRidgeRegressionUsingConstructor</li></div><div><li>org.apache.spark.mllib.regression.JavaRidgeRegressionSuite.runRidgeRegressionUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.regression.JavaRidgeRegressionSuite.runRidgeRegressionUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.regression.JavaStreamingLinearRegressionSuite.javaAPI</li></div><div><li>org.apache.spark.mllib.regression.JavaStreamingLinearRegressionSuite.javaAPI</li></div><div><li>org.apache.spark.mllib.stat.JavaStatisticsSuite.testCorr</li></div><div><li>org.apache.spark.mllib.stat.JavaStatisticsSuite.testCorr</li></div><div><li>org.apache.spark.mllib.stat.JavaStatisticsSuite.chiSqTest</li></div><div><li>org.apache.spark.mllib.stat.JavaStatisticsSuite.chiSqTest</li></div><div><li>org.apache.spark.mllib.stat.JavaStatisticsSuite.streamingTest</li></div><div><li>org.apache.spark.mllib.stat.JavaStatisticsSuite.streamingTest</li></div><div><li>org.apache.spark.mllib.stat.JavaStatisticsSuite.kolmogorovSmirnovTest</li></div><div><li>org.apache.spark.mllib.stat.JavaStatisticsSuite.kolmogorovSmirnovTest</li></div><div><li>org.apache.spark.mllib.tree.JavaDecisionTreeSuite.runDTUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.tree.JavaDecisionTreeSuite.runDTUsingStaticMethods</li></div><div><li>org.apache.spark.mllib.tree.JavaDecisionTreeSuite.runDTUsingConstructor</li></div><div><li>org.apache.spark.mllib.tree.JavaDecisionTreeSuite.runDTUsingConstructor</li></div><div><li>org.apache.spark.mllib.util.JavaMLUtilsSuite.testConvertMatrixColumnsToAndFromML</li></div><div><li>org.apache.spark.mllib.util.JavaMLUtilsSuite.testConvertMatrixColumnsToAndFromML</li></div><div><li>org.apache.spark.mllib.util.JavaMLUtilsSuite.testConvertVectorColumnsToAndFromML</li></div><div><li>org.apache.spark.mllib.util.JavaMLUtilsSuite.testConvertVectorColumnsToAndFromML</li></div><div><li>org.apache.spark.repl.ReplSuite.SPARK-15236: use Hive catalog</li></div><div><li>org.apache.spark.repl.ReplSuite.SPARK-15236: use in-memory catalog</li></div><div><li>org.apache.spark.repl.ReplSuite.broadcast vars</li></div><div><li>org.apache.spark.repl.ReplSuite.line wrapper only initialized once when used as encoder outer scope</li></div><div><li>org.apache.spark.repl.ReplSuite.define case class and create Dataset together with paste mode</li></div><div><li>org.apache.spark.repl.ReplSuite.:replay should work correctly</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.simple foreach with accumulator</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.external vars</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.external classes</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.external functions</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.external functions that access vars</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.broadcast vars</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.interacting with files</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.local-cluster mode</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.SPARK-2576 importing implicits</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.Datasets and encoders</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.SPARK-2632 importing a method from non serializable class and not using it.</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.collecting objects of class defined in repl</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.collecting objects of class defined in repl - shuffling</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.replicating blocks of object with class defined in repl</li></div><div><li>org.apache.spark.repl.SingletonReplSuite.newProductSeqEncoder with REPL defined class</li></div><div><li>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationCount</li></div><div><li>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationCount</li></div><div><li>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationSumDouble</li></div><div><li>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationSumDouble</li></div><div><li>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationSumLong</li></div><div><li>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationSumLong</li></div><div><li>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationAverage</li></div><div><li>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationAverage</li></div><div><li>test.org.apache.spark.sql.JavaApplySchemaSuite.applySchema</li></div><div><li>test.org.apache.spark.sql.JavaApplySchemaSuite.dataFrameRDDOperations</li></div><div><li>test.org.apache.spark.sql.JavaApplySchemaSuite.applySchemaToJSON</li></div><div><li>test.org.apache.spark.sql.JavaBeanDeserializationSuite.testBeanWithArrayFieldDeserialization</li></div><div><li>test.org.apache.spark.sql.JavaBeanDeserializationSuite.testBeanWithArrayFieldDeserialization</li></div><div><li>test.org.apache.spark.sql.JavaBeanDeserializationSuite.testBeanWithMapFieldsDeserialization</li></div><div><li>test.org.apache.spark.sql.JavaBeanDeserializationSuite.testBeanWithMapFieldsDeserialization</li></div><div><li>test.org.apache.spark.sql.JavaColumnExpressionSuite.isInCollectionCheckExceptionMessage</li></div><div><li>test.org.apache.spark.sql.JavaColumnExpressionSuite.isInCollectionCheckExceptionMessage</li></div><div><li>test.org.apache.spark.sql.JavaColumnExpressionSuite.isInCollectionWorksCorrectlyOnJava</li></div><div><li>test.org.apache.spark.sql.JavaColumnExpressionSuite.isInCollectionWorksCorrectlyOnJava</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testFormatAPI</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testTextAPI</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testJsonAPI</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testLoadAPI</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testOptionsAPI</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testSaveModeAPI</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testCsvAPI</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testParquetAPI</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testTextFileAPI</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCollectAndTake</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCollectAndTake</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testJsonRDDToDataFrame</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testJsonRDDToDataFrame</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testVarargMethods</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testVarargMethods</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testBeanWithoutGetter</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testBeanWithoutGetter</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateStructTypeFromList</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateStructTypeFromList</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testSampleBy</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testSampleBy</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCrosstab</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCrosstab</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testUDF</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testUDF</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFromFromList</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFromFromList</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCircularReferenceBean</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCircularReferenceBean</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testFrequentItems</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testFrequentItems</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testSampleByColumn</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testSampleByColumn</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testExecution</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testExecution</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testTextLoad</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testTextLoad</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.pivot</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.pivot</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testGenericLoad</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testGenericLoad</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCountMinSketch</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCountMinSketch</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.pivotColumnValues</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.pivotColumnValues</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFrameFromJavaBeans</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFrameFromJavaBeans</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCorrelation</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCorrelation</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testBloomFilter</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testBloomFilter</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCovariance</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCovariance</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFrameFromLocalJavaBeans</li></div><div><li>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFrameFromLocalJavaBeans</li></div><div><li>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationCount</li></div><div><li>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationCount</li></div><div><li>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationSumDouble</li></div><div><li>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationSumDouble</li></div><div><li>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationSumLong</li></div><div><li>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationSumLong</li></div><div><li>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationAnonClass</li></div><div><li>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationAnonClass</li></div><div><li>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationAverage</li></div><div><li>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationAverage</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testRuntimeNullabilityCheck</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testRuntimeNullabilityCheck</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean1</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean1</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean2</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean2</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean3</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean3</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testSerializeNull</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testSerializeNull</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testRandomSplit</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testRandomSplit</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testTypedFilterPreservingSchema</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testTypedFilterPreservingSchema</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJoin</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJoin</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testTake</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testTake</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testToLocalIterator</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testToLocalIterator</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testSpecificLists</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testSpecificLists</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testForeach</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testForeach</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJavaEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJavaEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testPrimitiveEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testPrimitiveEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testEmptyBean</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testEmptyBean</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testCommonOperation</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testCommonOperation</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testNullInTopLevelBean</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testNullInTopLevelBean</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testGroupBy</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testGroupBy</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testSetOperation</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testSetOperation</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testBeanWithEnum</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testBeanWithEnum</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testKryoEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testKryoEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.test</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.test</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJavaBeanEncoder2</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJavaBeanEncoder2</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testCollect</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testCollect</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testKryoEncoderErrorMessageForPrivateClass</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testKryoEncoderErrorMessageForPrivateClass</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJavaBeanEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJavaBeanEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testTupleEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testTupleEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testNestedTupleEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testNestedTupleEncoder</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testTupleEncoderSchema</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testTupleEncoderSchema</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testReduce</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testReduce</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testSelect</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testSelect</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJavaEncoderErrorMessageForPrivateClass</li></div><div><li>test.org.apache.spark.sql.JavaDatasetSuite.testJavaEncoderErrorMessageForPrivateClass</li></div><div><li>test.org.apache.spark.sql.JavaSaveLoadSuite.saveAndLoadWithSchema</li></div><div><li>test.org.apache.spark.sql.JavaSaveLoadSuite.saveAndLoadWithSchema</li></div><div><li>test.org.apache.spark.sql.JavaSaveLoadSuite.saveAndLoad</li></div><div><li>test.org.apache.spark.sql.JavaSaveLoadSuite.saveAndLoad</li></div><div><li>test.org.apache.spark.sql.JavaUDAFSuite.udf1Test</li></div><div><li>test.org.apache.spark.sql.JavaUDAFSuite.udf1Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf1Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf1Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf2Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf2Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf3Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf3Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf4Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf4Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf5Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf5Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf6Test</li></div><div><li>test.org.apache.spark.sql.JavaUDFSuite.udf6Test</li></div><div><li>test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.testForeachBatchAPI</li></div><div><li>test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.testForeachBatchAPI</li></div><div><li>test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.testForeachAPI</li></div><div><li>test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.testForeachAPI</li></div><div><li>org.apache.spark.sql.hive.HiveMetastoreLazyInitializationSuite.lazily initialize Hive client</li></div><div><li>org.apache.spark.sql.hive.JavaDataFrameSuite.testUDAF</li></div><div><li>org.apache.spark.sql.hive.JavaDataFrameSuite.saveTableAndQueryIt</li></div><div><li>org.apache.spark.sql.hive.JavaMetastoreDataSourcesSuite.saveExternalTableAndQueryIt</li></div><div><li>org.apache.spark.sql.hive.JavaMetastoreDataSourcesSuite.saveTableAndQueryIt</li></div><div><li>org.apache.spark.sql.hive.JavaMetastoreDataSourcesSuite.saveExternalTableWithSchemaAndQueryIt</li></div><div><li>org.apache.spark.streaming.JavaMapWithStateSuite.testBasicFunction</li></div><div><li>org.apache.spark.streaming.JavaReceiverAPISuite.testReceiver</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testStreamingContextTransform</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testStreamingContextTransform</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testFlatMapValues</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testMapPartitions</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testMapPartitions</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairFilter</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairFilter</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testCombineByKey</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testCombineByKey</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testMap</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testMap</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairToNormalRDDTransform</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairToNormalRDDTransform</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairReduceByKey</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairReduceByKey</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairMap</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairMap</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testFlatMap</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testFlatMap</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testReduceByKeyAndWindowWithInverse</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testReduceByKeyAndWindowWithInverse</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testReduceByWindow</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testReduceByWindow</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairFlatMap</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairFlatMap</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairToPairFlatMapWithChangingTypes</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairToPairFlatMapWithChangingTypes</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairMapPartitions</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairMapPartitions</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testVariousTransform</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testVariousTransform</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testTransformWith</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testTransformWith</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testVariousTransformWith</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testVariousTransformWith</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testReduceByKeyAndWindow</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testReduceByKeyAndWindow</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairTransform</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairTransform</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testFilter</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testFilter</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairMap2</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testPairMap2</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testMapValues</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testMapValues</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testReduce</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testReduce</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testUpdateStateByKey</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testUpdateStateByKey</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testTransform</li></div><div><li>test.org.apache.spark.streaming.Java8APISuite.testTransform</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testStreamingContextTransform</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testFlatMapValues</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testReduceByWindowWithInverse</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testReduceByWindowWithInverse</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testMapPartitions</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testMapPartitions</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairFilter</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairFilter</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testRepartitionFewerPartitions</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testRepartitionFewerPartitions</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCombineByKey</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCombineByKey</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testContextGetOrCreate</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testContextGetOrCreate</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testWindowWithSlideDuration</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testWindowWithSlideDuration</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testQueueStream</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testQueueStream</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCountByValue</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCountByValue</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testMap</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testMap</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairToNormalRDDTransform</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairToNormalRDDTransform</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairReduceByKey</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairReduceByKey</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCount</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCount</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCheckpointMasterRecovery</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCheckpointMasterRecovery</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairMap</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairMap</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testUnion</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testUnion</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testFlatMap</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testFlatMap</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testReduceByKeyAndWindowWithInverse</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testReduceByKeyAndWindowWithInverse</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testGlom</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testGlom</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testJoin</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testJoin</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairFlatMap</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairFlatMap</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairToPairFlatMapWithChangingTypes</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairToPairFlatMapWithChangingTypes</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairMapPartitions</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairMapPartitions</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testRepartitionMorePartitions</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testRepartitionMorePartitions</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testReduceByWindowWithoutInverse</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testReduceByWindowWithoutInverse</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testLeftOuterJoin</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testVariousTransform</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testVariousTransform</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testTransformWith</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testTransformWith</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testVariousTransformWith</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testVariousTransformWith</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testTextFileStream</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testTextFileStream</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairGroupByKey</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairGroupByKey</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCoGroup</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCoGroup</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testInitialization</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testInitialization</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testSocketString</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testSocketString</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testGroupByKeyAndWindow</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testGroupByKeyAndWindow</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testReduceByKeyAndWindow</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testReduceByKeyAndWindow</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testForeachRDD</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testFileStream</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testFileStream</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairTransform</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairTransform</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testFilter</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testFilter</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairMap2</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testPairMap2</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testMapValues</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testMapValues</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testReduce</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testReduce</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testUpdateStateByKey</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testUpdateStateByKey</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testTransform</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testTransform</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testWindow</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testWindow</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCountByValueAndWindow</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testCountByValueAndWindow</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testRawSocketStream</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testRawSocketStream</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testSocketTextStream</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testSocketTextStream</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testUpdateStateByKeyWithInitial</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testUpdateStateByKeyWithInitial</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testContextState</li></div><div><li>test.org.apache.spark.streaming.JavaAPISuite.testContextState</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexDescendingWithStart</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexWithStart</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndexDescendingWithStart</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexDescending</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexWithStart</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexWithLast</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexWithSkip</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexWithMax</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexDescending</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndexDescendingWithLast</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexDescending</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexDescendingWithLast</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndex</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndexWithLast</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexWithStart</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexDescendingWithStart</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexWithLast</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexWithSkip</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndexDescending</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.testRefWithIntNaturalKey</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexDescending</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexDescendingWithStart</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexWithMax</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndex</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexWithLast</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexWithSkip</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexWithMax</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexDescendingWithLast</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexDescendingWithLast</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexDescendingWithStart</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndex</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexWithLast</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexWithSkip</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexWithStart</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndex</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexDescendingWithLast</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndexWithStart</li></div><div><li>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndex</li></div><div><li>org.apache.spark.util.kvstore.LevelDBSuite.testMultipleTypesWriteReadDelete</li></div><div><li>org.apache.spark.util.kvstore.LevelDBSuite.testObjectWriteReadDelete</li></div><div><li>org.apache.spark.util.kvstore.LevelDBSuite.testSkip</li></div><div><li>org.apache.spark.util.kvstore.LevelDBSuite.testMultipleObjectWriteReadDelete</li></div><div><li>org.apache.spark.util.kvstore.LevelDBSuite.testReopenAndVersionCheckDb</li></div><div><li>org.apache.spark.util.kvstore.LevelDBSuite.testMetadata</li></div><div><li>org.apache.spark.util.kvstore.LevelDBSuite.testUpdate</li></div><div><li>org.apache.spark.util.kvstore.LevelDBSuite.testNegativeIndexValues</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.spark.sql.kafka010.KafkaContinuousSourceStressForDontFailOnDataLossSuite.stress test for failOnDataLoss=false</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"><div><li>java.util.NoSuchElementException was thrown.</li></div><div><li>&amp;#010;Assert on query failed: : Queue() was empty&amp;#010;org.scalatest.Assertions$class.newAssertionFailedException(Assertions.scala:528)&amp;#010; org.scalatest.FunSuite.newAssertionFailedException(FunSuite.scala:1560)&amp;#010; org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:501)&amp;#010; org.apache.spark.sql.streaming.StreamingQueryListenerSuite$$anonfun$2$$anonfun$apply$mcV$sp$8.appl</li></div><div><li>Timeout of './bin/spark-submit' '--class' 'org.apache.spark.sql.hive.SparkSQLConfTest' '--name' 'SparkSQLConfTest' '--master' 'local-cluster[2,1,1024]' '--conf' 'spark.ui.enabled=false' '--conf' 'spark.master.rest.enabled=false' '--conf' 'spark.sql.hive.metastore.version=0.12' '--conf' 'spark.sql.hive.metastore.jars=maven' '--driver-java-options' '-Dderby.system.durability=test' 'file:/var/lib/jen</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Timeout getting response from the server</li></div><div><li>SparkContext has been shutdown</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:91)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:46)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:91)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:46)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:91)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:46)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:91)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:46)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:91)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:46)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:91)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:46)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:91)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:46)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:91)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:46)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMe</li></div><div><li>SparkContext has been shutdown</li></div><div><li>SparkContext has been shutdown</li></div><div><li>SparkContext has been shutdown</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAcce</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.N</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.N</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.N</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.N</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.N</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.N</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.N</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.N</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.N</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.N</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.N</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.N</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.N</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.N</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.N</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.N</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.N</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.N</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.N</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>SparkContext has been shutdown</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>SparkContext has been shutdown</li></div><div><li>SparkContext has been shutdown</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>SparkContext has been shutdown</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>SparkContext has been shutdown</li></div><div><li>SparkContext has been shutdown</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
sun.reflect.D</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:91)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:46)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:91)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:46)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>Exception evaluating from_avro(to_avro(false), "boolean")</li></div><div><li>Exception evaluating from_avro(to_avro(98), "int")</li></div><div><li>Exception evaluating from_avro(to_avro(-32768), "int")</li></div><div><li>Exception evaluating from_avro(to_avro(1648040869), "int")</li></div><div><li>Exception evaluating from_avro(to_avro(9223372036854775807), "long")</li></div><div><li>Exception evaluating from_avro(to_avro(-5.794263E26), "float")</li></div><div><li>Exception evaluating from_avro(to_avro(-1.7976931348623157E308), "double")</li></div><div><li>Exception evaluating from_avro(to_avro(18816363), {"type":"fixed","name":"fixed","namespace":"topLevelRecord","size":4,"logicalType":"decimal","precision":8,"scale":0})</li></div><div><li>Exception evaluating from_avro(to_avro(2644.0081), {"type":"fixed","name":"fixed","namespace":"topLevelRecord","size":4,"logicalType":"decimal","precision":8,"scale":4})</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>Exception evaluating from_avro(to_avro(-60815.91905640969), {"type":"fixed","name":"fixed","namespace":"topLevelRecord","size":7,"logicalType":"decimal","precision":16,"scale":11})</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>Exception evaluating from_avro(to_avro(-2.12106415467856523E-21), {"type":"fixed","name":"fixed","namespace":"topLevelRecord","size":16,"logicalType":"decimal","precision":38,"scale":38})</li></div><div><li>Exception evaluating from_avro(to_avro(潧ή뮣ꘀ惕䀊乍웳삃ᱍሁᬯ鲶绯廇꣠龱ꖭ勰鈆坸飓睚♁怉ꆵ늪ꑽ㻸㜂௮ꋓ黧㊎刡犦⛸贾霏̜夥䪔᎕䶆ꊶ셬纗贕蕀┹聺q졍ꬍẑᔵᴨ⢮ഡ丗ꢡ룶䮺烩犢녌搽⨞ꠜ钧뇨ぉ脅왪쀙ޭ厑禤咨ᏽᇪ綯銎騹鹥∀㫖輑∩紛眲펨譖䮃芢髊ӕ억잵鍑섉짞ꆻ䦥鹈૜ꏆ垷췢聺償昍ᦥ绪깽혿ᓥ΄鄤鯼콒㬚Ꮳ巋셩꾑捆㟍ᔨ═邐蓥爈洤뢎螐푓낁뮗尥睃鸪濇鷭圦盹഍䘑㖳Χ㤲鞊萩ဆ騭퉑Ⴍ鸞ʐ즔ꓰ턍㦎㭀冸辄훛တꑐ֤㰥땜邷ð菿鼑澅䟼斕젒곮삆맊촖䱤ꅝᦻ腍㲉煇ڏ藥鐂쬯녏裵葔ᝉ찗⭾᫊ꁔ刬Ỏ饣꥓蟣㹐ꃗ奃ຸ싧ଢ薓璒㋤遷冗劘✕蓕黊쾺࿟瑞齶蹙碬숙Ϝ쓎ꓺ䍴᫽됌擔쵠ټ䩉Ṩ滀᠜ሑ჏낻爘鈔ヘ驉쁽䦞軭࠾凼혉鑹䰌㵸❾▄莞т쬪ꂎ汣यⅻ়擣ꦓ⑱께쓡塜钦䖬䂑㧐拠您縆끍ꓥ챵⩼쌁뙖೫瞔ᓹؐ鞩⛼䏣䣪ꈮ궲㽩ꧻᒋ嵪⇉櫺霬賩侏⫣ਟ坺硹鐭ꢷ⾶Ј⳱붶ӽ槷찷帠掓僔랈㫴胪뀨펾萠쥵권紲쨢綤ⳅⱴ㬮ц恲点訤緯滛纕῔ҧ병䄘Ⲓ啦恃蒶洶폲ߍ췫⦱⛸躷 콺쿕</li></div><div><li>Exception evaluating from_avro(to_avro(0xE5246491F58D92320E69A17714FBD06D6FF02255AD36E05BC5986568ADEE4D1A9AE683CE1623E8EFAB4C4971D2E61020A167504EB0869C73A8C20A157C84D165AD60CA405A8AF112455F13E3C8BA02AE8CE360D1F49BFD867FEEF61D43E1AF6AA005EA8C4494DC47DD9D65B36626A298193A275F1DD704A7C4037BC663260C3D669BF832BDD10B680AC8258122AA9170E8D7F629777BC905C8C1083986D25DABE8B572C791BCA0CE739EEF18F3335A18CB6254B</li></div><div><li>Exception evaluating from_avro(to_avro([称镡蒁겅縢ᕭ馾뙽☞㓘嫮㞭珮蓽孎釛̴鎤㖝䮕퇅≹⍠燜員ᑷ㑝⻻⟨䨀线㟎ᦆ㜵㾚㥳࢑㦰ᓒ㧢鷭ꙶ忸Ղ됔曮詉㇞귶Ꭼ摳儁쒹摋캁ᣕ费㿔㹭憠뉧༳鼃ᾘ轡녜빀뒎䏀쀉哢䐪훚樋崜럷䩈聫㚖⼌ꕁ隇㜈煜拎똠쉇ɱᗄ樏ꦂ㘫喬컧鞚䐶ج凭建⤆驜渠쉓豍㺄魙螷퀛⿴飋귆緐埍듲颹挽貄ᆸᄬ踲抪뻺ढ쟎Ṳⅻ휳㐏屠瞁횵㱺訄䲮ꢭሲ⠝凿䲽禥ɦಅ᱈ꡝ苜䥠ོ㿨昇ᱝ率葒Ʊ巈媒숚毁绊ൃ摘젞挲ᵮ늅黪盖믨ઊ훷㡏⑎ಗ㍏֠㛸ᶥᎹ䏄躯ҵ煌┡춿膠濺꡵䮵⑜潮⹐ᱻ냻맙⹲㄁㈆靻䉷킯䎛䎳㾋ꖞ榋ᏼ鞮傞̀셸쵑⭺樧瘯䫁穓ࠡ鬔泍꤁ㄜ럔랛츮띥トŵ뢦㷰屮Ṟ쬭㹮췡䝘驄寤烆◣䓼魹થ⬓ౣ◬䲕䢮絠芤댵Cᷘ鄭ᜨ痸耏ʓ峻굺ꡣ㗙➫株㲞霦䜡榬䂝魔跊䆭塒銢㴶셟ꈔথ꓋駿쀦푽⿎㺐綽儝奒帹کላൎ十㵝ಽꃎ檍⬄၈崌췌搦糫બ㞥̏츹板⓳⬟ᙣ⢂░犀痹ꊫ醿옭肴ჭᖤ澔㷢䎈㦹퀡鴖콒ҥ尌墩腡፟Ղ楖ଙḫᄅ髉鲙љᢝ˘퍶䶼屏멒㾈囻ꥏওᏪ勾砙釡ޥ❯魖ͽ鄫欰월埭</li></div><div><li>Exception evaluating from_avro(to_avro([2.935247154829122E-234,-18326,2.2901227174936357E-290,false,4226010822930472]), {"type":"record","name":"topLevelRecord","fields":[{"name":"col_0","type":"double"},{"name":"col_1","type":["int","null"]},{"name":"col_2","type":"double"},{"name":"col_3","type":["boolean","null"]},{"name":"col_4","type":{"type":"fixed","name":"fixed","namespace":"topLevelRecord</li></div><div><li>Exception evaluating from_avro(to_avro([0,false,-2147483648,[B@4cd7d5e1,8864.1886]), {"type":"record","name":"topLevelRecord","fields":[{"name":"col_0","type":["int","null"]},{"name":"col_1","type":"boolean"},{"name":"col_2","type":["int","null"]},{"name":"col_3","type":["bytes","null"]},{"name":"col_4","type":[{"type":"fixed","name":"fixed","namespace":"topLevelRecord.col_4","size":4,"logicalType</li></div><div><li>Exception evaluating from_avro(to_avro([false,5966198930035477775,2258.9127,-1.4582987853185218E43,2.2075536073859405E251]), {"type":"record","name":"topLevelRecord","fields":[{"name":"col_0","type":"boolean"},{"name":"col_1","type":"long"},{"name":"col_2","type":[{"type":"fixed","name":"fixed","namespace":"topLevelRecord.col_2","size":4,"logicalType":"decimal","precision":8,"scale":4},"null"]},{"</li></div><div><li>Exception evaluating from_avro(to_avro([36787020,0,-996930183505727,1834247892471119121,9529241676532266]), {"type":"record","name":"topLevelRecord","fields":[{"name":"col_0","type":{"type":"fixed","name":"fixed","namespace":"topLevelRecord.col_0","size":4,"logicalType":"decimal","precision":8,"scale":0}},{"name":"col_1","type":["int","null"]},{"name":"col_2","type":{"type":"fixed","name":"fixed",</li></div><div><li>Exception evaluating from_avro(to_avro([[놙ࢫ䄱곊椷憾抧䙗㰠舉Ჰ⻙픂Ǜ鞽쮴䈉翯瑟䋹釭呤뽻䋺䡕턽뎐镗ᄣ뻵覻쐩첧৑ꪠ꽥檧ੈ䇑ꯅ혗譺奎볫ᮺ솸꜔ၭ瞌讏軤곶盶┛Ꝗ漤䗬显撱㯓◦荿灚鸙䳽⭍룥䭸㡬俰졛ӕ쌯纜㼺୛俵ব樉쀈肮⩻ù磯䄔퀘곫㵨縈ꌥᄼ抮휞↑탅ཕ燏ⵘ볥⮡怭⿟웖訃半Ⅹ縚룈綇煑獾笩壯뵣槥㞴ꪠ⋶ᄕ鎅耯௧䜤뱂䂒糰쳰ꠔ삒㙶萆㏿牬ǐ硭獜მ㥏脭뭥蘔û鑫㊀幹饒ᄴᰛ⋡읏嬜楯ꘔꤏ玎땍턅ꐄㅠ司凙㞩怒냹볰쿠䐠굎䕿猐故䧉矐걤痋 餘᱔圍⡥婃縣ꭐ리愗꽼郢㥯폏巣㛛馻櫌怒ნ酚࣮૱ꀌૅ。댜⫘❬哞垃蛸泶欼툭✋団腩㗂᫞ꨨ斗ꯌ㦪燲㴓괫꫃莠֭霪ጸ䀢償Ⱜ㤰ꭶ偬ᲈ汱탟祛Ṍդõꁮ趇涫⋴ꌁꋉ赔䩍᱘詧頻໢䃏嵣イ袸㰛퓲싘ۄ넽␗䋺꩟㋾⋴ᬩ楙顄⯦ݒℱཝ摷陌Ẫ밚鞉饷侓䤜搜忩逢䀦ꑧ蕀鬑О趂઼齊赮ꐬ朷石篗癆薿瓵吺놳㟁缓霪䰐퐱ꇶᕮà栆밯伺ঢ়隃筇殐蝩㟚덋ꐼ∾劬繸䵇ْ芾玻ὥ蔱Ⳅ洖ǅ튊ᯗ䏖⡇뺣ো뿅ᦗ魔냴孕ʝ쫖∿੎䏽Ἀ繥銡柩ᆗ腻늦㲺쒰</li></div><div><li>Exception evaluating from_avro(to_avro([false,[0.0],-32768,5482278255177208853,3882319446858866,[-28562],[[B@31204303],[-35983296],[0],6.610711E25]), {"type":"record","name":"topLevelRecord","fields":[{"name":"col_0","type":["boolean","null"]},{"name":"col_1","type":[{"type":"array","items":["float","null"]},"null"]},{"name":"col_2","type":["int","null"]},{"name":"col_3","type":{"type":"fixed","na</li></div><div><li>Exception evaluating from_avro(to_avro([2125290332910151179,[[-2711710026058830382],[[[B@6de9bba2]],[[293819151239072]]],-128,[[8367359393815727]],[89978502],[[127],[[[-7.85613218762047390E-21]]]],[6540344542330754226]]), {"type":"record","name":"topLevelRecord","fields":[{"name":"col_0","type":"long"},{"name":"col_1","type":{"type":"record","name":"col_1","namespace":"topLevelRecord","fields":[{"</li></div><div><li>Exception evaluating from_avro(to_avro([[1.4E-45,[88010.49745835038]],[5984.5580],[[[-2147483648,[null]],[-1.466879541705704586E-20],[[true]]],[[9223372036854775807]]],[[[[[[-1298047]]]]]],[[true]]]), {"type":"record","name":"topLevelRecord","fields":[{"name":"col_0","type":{"type":"record","name":"col_0","namespace":"topLevelRecord","fields":[{"name":"col_0","type":"float"},{"name":"col_1","type"</li></div><div><li>Exception evaluating from_avro(to_avro([[127],true,[0],-72,[34563843],[49387.56493673525],[2.6534004E7],[8858011293636760],[-1.2637242398957963E151],[챁蒶펋吃矻䀍춾䂐㡀㮊ꪂ椡砑썸윘綼璈篿钷펼矛谓⏪奌佾띱ᨫ䣣艆폝ฃꀋ艿⚪㌩㣦쑚桹ค᫄轟䙈弦ᴔ䢇ಸ㤟嫵繢紲磍기榕↘命獵忊ᰒꇇꖝ끠텧熘孯⸂챽ա⛃廥৭咵哻ր蛶뱀삳蠅໱되偸莬ꎋ蕴쇹뇺쟋쭳䂊쏣饶ꨵ籹㶇孋┢緑売㜳䇨篛턢宇萱퉦኉歱ꟙ됶黶鋵䇣怀ޯ뒛鈹窔颖鰇틁젬笗쇵ǳ숬䎳䦉ꝸ둭퀗̲ଅ᩿ಙ眬蠎紜Ǻ垒䫓᥍갇瑩뗟妲꒯袅᱃ݯፏ흙Ꭲᇄ臄⼨ᯯ㻺ꬴ朐滰㵀莶᤿䯈ӛ䙬⾰嫓ᜄ⚍現氀蕺얛忦讉⫻内軇薤꺒䫾鷁␸䩿郉ä콝䭼Ꜿ축迮㘼귷묂鞙往ꆵ쩠혉葋솞䷳ꋏ䆸ꗴハ翝츗즙鵔氈뒈獸ݳ꿽엒䦜⋱Ề耨犔遠馜뽗儆䂪ᷙ⨴௃땫赣嚺聓᜔嗌క릷姿⳥ꮅỔ쳣润쥮멅ꎺ㎘쥿Ғ</li></div><div><li>Exception evaluating from_avro(to_avro(1), &amp;#010;{&amp;#010;  "type": "string",&amp;#010;  "name": "my_string"&amp;#010;}&amp;#010;       , (mode,PERMISSIVE))</li></div><div><li>Exception evaluating from_avro(to_avro(abc), &amp;#010;{&amp;#010;  "type": "int",&amp;#010;  "name": "my_int"&amp;#010;}&amp;#010;       )</li></div><div><li>Exception evaluating from_avro(to_avro(1.23), &amp;#010;{&amp;#010;  "type": "double",&amp;#010;  "name": "my_double"&amp;#010;}&amp;#010;       , (mode,PERMISSIVE))</li></div><div><li>Exception evaluating from_avro(to_avro(1.23), &amp;#010;{&amp;#010;  "type": "float",&amp;#010;  "name": "my_float"&amp;#010;}&amp;#010;       )</li></div><div><li>create random record with seed 7044430769401605127 Exception evaluating from_avro(to_avro([⊁盘ទ暭樋鐑홈椣랶퇙෦ů㋤仗ꇑ셶筲瓕ꤘ䃦ᢄ⢥⑮桶逧∜㎣쥨걞鑈횧竇藆迪4인뻤㙝힑ῂ耝ʁ⠾蚶踒䱍⯨ⷺ餂ⵝ⦳쒙ㅤ誽厸ይ૱꾙뎝늍襛觭ᗟ旞녲哅緇鋉霖뱞従䪋㻹꺡湔葟鹞킨쟜甾顪춵蠚⅁ꅪ巍爬嘡魫풘髅達ꑶ䯇㐲풰Ꝕ狀ᄅ팜ྎ讆宐ᰶᔻ嵋᪳裄껙⁲厪幃斜䁄㋙⠐ℨ삐ᡅ袋Ḯ罤걯넼₠᝙檻␀牌涖콩㌉诓厁栒玭╋䝇썂⠐鐑鐑맂俒䌣냒竓㢠袒⩴뾋뽹เ㓚읥㛇㜁啷㒾臚뮍㌳㋺〲ⴝ䨓儖옱덒✳ࣷ꽙ꪵ쿻ꣀ囬㓷僻恥슯⻾䉦秡葯閳칺绶銟╔齴馽嬤㻰峙⤪䣣-氄⛯ꭁ롍൷䫵愶⼟놟૔䷐旀䁡鐌Ꝕ䂲携픰ࣂ㄁璏辨겭Ŀ堶蔭֗族镊䥶쬯㈒༻犓틟宂़諻쒬ঌ쓠倠광䐉⬭酴泊キ喱쌀穩뿧䢥馇힤뚭ြ䶓ᱢ꺟挚鳛Ď䖷䵨崩躋夙戄㗡润䃳"☥趗癐탎⫑ᎀ䈒☷笿슕㻹懊ݹ弛ᶓ䍤翊儂䳖ᓉ獊칅梜⒪斈毶啨픜ẁ壦紫■舠江六朰ᚌ䇨蔰</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.sql.avro.AvroFunctionsSuite.beforeAll(AvroFunctionsSuite.scala:27)&amp;#010;org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:212)&amp;#010;org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:210)&amp;#010;org.apache.spark.SparkFunSuite.run(SparkFunS</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.sql.avro.AvroFunctionsSuite.beforeAll(AvroFunctionsSuite.scala:27)&amp;#010;org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:212)&amp;#010;org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:210)&amp;#010;org.apache.spark.SparkFunSuite.run(SparkFunS</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>SparkContext has been shutdown</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.PageRankSuite.withSpark</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.PageRankSuite.withSpark</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.VertexRDDSuite.withSpark(Ve</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphOpsSuite.withSpark(GraphOpsSuite.scala:23)&amp;#010;org.apache.spark.graphx.GraphOpsSuite$$anonfun$9.apply(GraphO</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphOpsSuite.withSpark(Gra</li></div><div><li>SparkContext has been shutdown</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>SparkContext has been shutdown</li></div><div><li>SparkContext has been shutdown</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphOpsSuite.withSpark(Gra</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphOpsSuite.withSpark(Gra</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphOpsSuite.withSpark(Gra</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphOpsSuite.withSpark(Gra</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphOpsSuite.withSpark(Gra</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.SVDPlusPlusSuite.withSp</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.SVDPlusPlusSuite.withSp</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.SVDPlusPlusSuite.withSp</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.SVDPlusPlusSuite.withSp</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.SVDPlusPlusSuite.withSp</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.SVDPlusPlusSuite.withSp</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.SVDPlusPlusSuite.withSp</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.SVDPlusPlusSuite.withSp</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.SVDPlusPlusSuite.withSp</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.SVDPlusPlusSuite.withSp</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.SVDPlusPlusSuite.withSp</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.SVDPlusPlusSuite.withSp</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.SVDPlusPlusSuite.withSp</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.SVDPlusPlusSuite.withSp</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.SVDPlusPlusSuite.withSp</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.SVDPlusPlusSuite.withSp</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.SVDPlusPlusSuite.withSp</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.SVDPlusPlusSuite.withSp</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.SVDPlusPlusSuite.withSp</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.SVDPlusPlusSuite.withSp</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.SVDPlusPlusSuite.withSp</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.SVDPlusPlusSuite.withSp</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.PregelSuite.withSpark(PregelSuite.scala:22)&amp;#010;org.apache.spark.graphx.PregelSuite$$anonfun$4.apply(PregelSuite.</li></div><div><li>SparkContext has been shutdown</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.util.GraphGeneratorsSuite.w</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.util.GraphGeneratorsSuite.w</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.util.GraphGeneratorsSuite.w</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.VertexRDDSuite.withSpark(VertexRDDSuite.scala:25)&amp;#010;org.apache.spark.graphx.VertexRDDSuite$$anonfun$20.apply(Ve</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.VertexRDDSuite.withSpark(Ve</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.VertexRDDSuite.withSpark(Ve</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.VertexRDDSuite.withSpark(Ve</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.VertexRDDSuite.withSpark(Ve</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.VertexRDDSuite.withSpark(Ve</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.VertexRDDSuite.withSpark(Ve</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.VertexRDDSuite.withSpark(Ve</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.VertexRDDSuite.withSpark(Ve</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.VertexRDDSuite.withSpark(Ve</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.VertexRDDSuite.withSpark(Ve</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.VertexRDDSuite.withSpark(Ve</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.PageRankSuite.withSpark(PageRankSuite.scala:60)&amp;#010;org.apache.spark.graphx.lib.PageRankSuite$$anonfun$19.app</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.PageRankSuite.withSpark(PageRankSuite.scala:60)&amp;#010;org.apache.spark.graphx.lib.PageRankSuite$$anonfun$23.app</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.PageRankSuite.withSpark</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.SVDPlusPlusSuite.withSpark(SVDPlusPlusSuite.scala:24)&amp;#010;org.apache.spark.graphx.lib.SVDPlusPlusSuite$$anonf</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.VertexRDDSuite.withSpark(Ve</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.VertexRDDSuite.withSpark(Ve</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.VertexRDDSuite.withSpark(Ve</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.VertexRDDSuite.withSpark(Ve</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.VertexRDDSuite.withSpark(Ve</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.VertexRDDSuite.withSpark(Ve</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.VertexRDDSuite.withSpark(Ve</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.util.GraphGeneratorsSuite.withSpark(GraphGeneratorsSuite.scala:23)&amp;#010;org.apache.spark.graphx.util.GraphGenerato</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.util.GraphGeneratorsSuite.w</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.util.PeriodicGraphCheckpointerSuite.withSpark(PeriodicGraphCheckpointerSuite.scala:28)&amp;#010;org.apache.spark.graph</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.util.PeriodicGraphCheckpoin</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Error while instantiating 'org.apache.spark.sql.internal.SessionStateBuilder':</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Error while instantiating 'org.apache.spark.sql.internal.SessionStateBuilder':</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Error while instantiating 'org.apache.spark.sql.internal.SessionStateBuilder':</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Error while instantiating 'org.apache.spark.sql.internal.SessionStateBuilder':</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflec</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Error while instantiating 'org.apache.spark.sql.internal.SessionStateBuilder':</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.tuning.JavaCrossValidatorSuite.setUp(JavaCrossValidatorSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.util.JavaDefaultReadWriteSuite.setUp(JavaDefaultReadWriteSuite.java:34)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflec</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:4</li></div><div><li>java.lang.NullPointerException
	at org.apache.spark.mllib.classification.JavaStreamingLogisticRegressionSuite.tearDown(JavaStreamingLogisticRegressionSuite.java:55)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(StreamingContext.scala:85)
org.apache.spark.streaming.api.java.JavaStreamingContext.&lt;init&gt;(JavaStreaming</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:4</li></div><div><li>java.lang.NullPointerException
	at org.apache.spark.mllib.regression.JavaStreamingLinearRegressionSuite.tearDown(JavaStreamingLinearRegressionSuite.java:54)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
	at org.apache.spark.mllib.stat.JavaStatisticsSuite.tearDown(JavaStatisticsSuite.java:65)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
	at org.apache.spark.mllib.stat.JavaStatisticsSuite.tearDown(JavaStatisticsSuite.java:65)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
	at org.apache.spark.mllib.stat.JavaStatisticsSuite.tearDown(JavaStatisticsSuite.java:65)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
	at org.apache.spark.mllib.stat.JavaStatisticsSuite.tearDown(JavaStatisticsSuite.java:65)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)&amp;#010;org.apache.spark.repl.Main$.createSparkSession(Main.scala:112)&amp;#010;&lt;init&gt;(&lt;console&gt;:15)&amp;#010;&lt;init&gt;(&lt;console&gt;:43)&amp;#</li></div><div><li>isContain was true Interpreter output contained 'Exception':&amp;#010;Welcome to&amp;#010;      ____              __&amp;#010;     / __/__  ___ _____/ /__&amp;#010;    _\ \/ _ \/ _ `/ __/  '_/&amp;#010;   /___/ .__/\_,_/_/ /_/\_\   version 3.0.0-SNAPSHOT&amp;#010;      /_/&amp;#010;         &amp;#010;Using Scala version 2.11.12 (OpenJDK 64-Bit Server VM, Java 1.8.0_151)&amp;#010;Type in expressions to have them evaluated.&amp;#010;Type </li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;Welcome to&amp;#010;      ____              __&amp;#010;     / __/__  ___ _____/ /__&amp;#010;    _\ \/ _ \/ _ `/ __/  '_/&amp;#010;   /___/ .__/\_,_/_/ /_/\_\   version 3.0.0-SNAPSHOT&amp;#010;      /_/&amp;#010;         &amp;#010;Using Scala version 2.11.12 (OpenJDK 64-Bit Server VM, Java 1.8.0_151)&amp;#010;Type in expressions to have them evaluated.&amp;#010;Type :he</li></div><div><li>isContain was true Interpreter output contained 'Exception':&amp;#010;Welcome to&amp;#010;      ____              __&amp;#010;     / __/__  ___ _____/ /__&amp;#010;    _\ \/ _ \/ _ `/ __/  '_/&amp;#010;   /___/ .__/\_,_/_/ /_/\_\   version 3.0.0-SNAPSHOT&amp;#010;      /_/&amp;#010;         &amp;#010;Using Scala version 2.11.12 (OpenJDK 64-Bit Server VM, Java 1.8.0_151)&amp;#010;Type in expressions to have them evaluated.&amp;#010;Type </li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;java.lang.NoClassDefFoundError: org/spark_project/guava/cache/Weigher&amp;#010;  at org.apache.spark.network.shuffle.ExternalShuffleBlockHandler.&lt;init&gt;(ExternalShuffleBlockHandler.java:64)&amp;#010;  at org.apache.spark.deploy.ExternalShuffleService.newShuffleBlockHandler(ExternalShuffleService.scala:63)&amp;#010;  at org.apache.spark.deploy.Exter</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:17: error: not found: value sc&amp;#010;       val accum = sc.longAccumulator&amp;#010;                   ^&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:18: error: not found: value sc&amp;#010;       sc.parallelize(1 to 10).foreach(x =&gt; accum.add(x))&amp;#010;       ^&amp;#010;&lt;console&gt;:18: error: not found: value accum&amp;#010;       sc.parallelize(1 </li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; v: Int = 7&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:19: error: not found: value sc&amp;#010;       val res1 = sc.parallelize(1 to 10).map(x =&gt; v).collect().reduceLeft(_+_)&amp;#010;                  ^&amp;#010;&amp;#010;scala&gt; v: Int = 10&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:19: error: not found: value sc&amp;#010;       val res2 = sc.parallelize(1 to 10).map(x =&gt;</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt;      |      | defined class C&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:18: error: not found: value sc&amp;#010;       val res = sc.parallelize(1 to 10).map(x =&gt; (new C).foo).collect().reduceLeft(_+_)&amp;#010;                 ^&amp;#010;&amp;#010;scala&gt;      | _result_1542234421575: Int = 1&amp;#010;&amp;#010;scala&gt; </li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; double: (x: Int)Int&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:19: error: not found: value sc&amp;#010;       val res = sc.parallelize(1 to 10).map(x =&gt; double(x)).collect().reduceLeft(_+_)&amp;#010;                 ^&amp;#010;&amp;#010;scala&gt;      | _result_1542234421929: Int = 1&amp;#010;&amp;#010;scala&gt; </li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; v: Int = 7&amp;#010;&amp;#010;scala&gt; getV: ()Int&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:19: error: not found: value sc&amp;#010;       val res1 = sc.parallelize(1 to 10).map(x =&gt; getV()).collect().reduceLeft(_+_)&amp;#010;                  ^&amp;#010;&amp;#010;scala&gt; v: Int = 10&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:19: error: not found: value sc&amp;#010;       val res2</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; array: Array[Int] = Array(0, 0, 0, 0, 0)&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:19: error: not found: value sc&amp;#010;       val broadcastArray = sc.broadcast(array)&amp;#010;                            ^&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:17: error: not found: value sc&amp;#010;       val res1 = sc.parallelize(0 to 4).map(x =&gt; broadcastArray.value(x</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:17: error: not found: value sc&amp;#010;       var file = sc.textFile("/var/lib/jenkins/workspace/spark/repl/target/tmp/spark-86d481eb-b7bf-41ea-a338-b8588c431eca/input").cache()&amp;#010;                  ^&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:17: error: not found: value file&amp;#010;       val res1 = file.count()&amp;#010;            </li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; v: Int = 7&amp;#010;&amp;#010;scala&gt; getV: ()Int&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:19: error: not found: value sc&amp;#010;       val res1 = sc.parallelize(1 to 10).map(x =&gt; getV()).collect().reduceLeft(_+_)&amp;#010;                  ^&amp;#010;&amp;#010;scala&gt; v: Int = 10&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:19: error: not found: value sc&amp;#010;       val res2</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:17: error: not found: value spark&amp;#010;       import spark.implicits._&amp;#010;              ^&amp;#010;&amp;#010;scala&gt; defined class TestCaseClass&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:22: error: not found: value sc&amp;#010;       sc.parallelize(1 to 10).map(x =&gt; TestCaseClass(x)).toDF().collect()&amp;#010;       ^&amp;#010;&amp;#010;scala&gt; &amp;#010</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; import org.apache.spark.sql.functions._&amp;#010;&amp;#010;scala&gt; import org.apache.spark.sql.{Encoder, Encoders}&amp;#010;&amp;#010;scala&gt; import org.apache.spark.sql.expressions.Aggregator&amp;#010;&amp;#010;scala&gt; import org.apache.spark.sql.TypedColumn&amp;#010;&amp;#010;scala&gt;      |      |      |      |      |      |      | simpleSum: org.apache.sp</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; defined class TestClass&amp;#010;&amp;#010;scala&gt; t: TestClass = TestClass@73caa3dd&amp;#010;&amp;#010;scala&gt; import t.testMethod&amp;#010;&amp;#010;scala&gt; defined class TestCaseClass&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:31: error: not found: value sc&amp;#010;       sc.parallelize(1 to 10).map(x =&gt; TestCaseClass(x)).collect()&amp;#010;       ^&amp;#010;&amp;#010;scala&gt; </li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; defined class Foo&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:28: error: not found: value sc&amp;#010;       val res = sc.parallelize((1 to 100).map(Foo), 10).collect()&amp;#010;                 ^&amp;#010;&amp;#010;scala&gt;      | _result_1542234432565: Int = 1&amp;#010;&amp;#010;scala&gt; </li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; defined class Foo&amp;#010;&amp;#010;scala&gt; list: List[(Int, Foo)] = List((1,Foo(1)), (1,Foo(2)))&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:28: error: not found: value sc&amp;#010;       val res = sc.parallelize(list).groupByKey().collect()&amp;#010;                 ^&amp;#010;&amp;#010;scala&gt;      | _result_1542234433021: Int = 1&amp;#010;&amp;#010;scala&gt; </li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; timeout: Int = 60000&amp;#010;&amp;#010;scala&gt; start: Long = 1542234434309&amp;#010;&amp;#010;scala&gt;      |      |      | &lt;console&gt;:31: error: not found: value sc&amp;#010;       while(sc.statusTracker.getExecutorInfos.size != 3 &amp;&amp;&amp;#010;             ^&amp;#010;&amp;#010;scala&gt;      |      | &amp;#010;scala&gt; import org.apache.spark.storage.StorageLevel._&amp;</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; defined class Click&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:33: error: not found: value spark&amp;#010;       spark.implicits.newProductSeqEncoder[Click]&amp;#010;       ^&amp;#010;&amp;#010;scala&gt;      | _result_1542234436050: Int = 1&amp;#010;&amp;#010;scala&gt; </li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetAggregatorSuiteBase.setUp(JavaDatasetAggregatorSuiteBase.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.jav</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaApplySchemaSuite.tearDown(JavaApplySchemaSuite.java:60)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetAggregatorSuiteBase.setUp(JavaDatasetAggregatorSuiteBase.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.jav</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaApplySchemaSuite.tearDown(JavaApplySchemaSuite.java:60)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetAggregatorSuiteBase.setUp(JavaDatasetAggregatorSuiteBase.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.jav</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaApplySchemaSuite.tearDown(JavaApplySchemaSuite.java:60)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaBeanDeserializationSuite.tearDown(JavaBeanDeserializationSuite.java:41)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaBeanDeserializationSuite.tearDown(JavaBeanDeserializationSuite.java:41)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaColumnExpressionSuite.tearDown(JavaColumnExpressionSuite.java:46)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaColumnExpressionSuite.tearDown(JavaColumnExpressionSuite.java:46)
</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetAggregatorSuiteBase.setUp(JavaDatasetAggregatorSuiteBase.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.jav</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetAggregatorSuiteBase.setUp(JavaDatasetAggregatorSuiteBase.java:45)
sun.reflect.NativeMethodAccessorImp</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetAggregatorSuiteBase.setUp(JavaDatasetAggregatorSuiteBase.java:45)
sun.reflect.NativeMethodAccessorImp</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetAggregatorSuiteBase.setUp(JavaDatasetAggregatorSuiteBase.java:45)
sun.reflect.NativeMethodAccessorImp</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetAggregatorSuiteBase.setUp(JavaDatasetAggregatorSuiteBase.java:45)
sun.reflect.NativeMethodAccessorImp</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.Delegating</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.Delegating</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDatasetSuite.setUp(JavaDatasetSuite.java:57)
sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
sun</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeC</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaSaveLoadSuite.tearDown(JavaSaveLoadSuite.java:76)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeC</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaSaveLoadSuite.tearDown(JavaSaveLoadSuite.java:76)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeC</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaUDAFSuite.tearDown(JavaUDAFSuite.java:42)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMetho</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaUDFSuite.tearDown(JavaUDFSuite.java:50)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMetho</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaUDFSuite.tearDown(JavaUDFSuite.java:50)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMetho</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaUDFSuite.tearDown(JavaUDFSuite.java:50)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMetho</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaUDFSuite.tearDown(JavaUDFSuite.java:50)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMetho</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaUDFSuite.tearDown(JavaUDFSuite.java:50)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaUDFSuite.setUp(JavaUDFSuite.java:45)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMetho</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaUDFSuite.tearDown(JavaUDFSuite.java:50)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.tearDown(JavaDataStreamReaderWriterSuite.java:49)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.tearDown(JavaDataStreamReaderWriterSuite.java:49)
</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.sql.hive.HiveMetastoreLazyInitializationSuite$$anonfun$1.apply(HiveMetastoreLazyInitializationSuite.scala:32)&amp;#010;org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)&amp;#010;org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)&amp;#010;org.scalatest.Transformer.apply(Trans</li></div><div><li>java.lang.ExceptionInInitializerError
	at org.apache.spark.sql.hive.JavaDataFrameSuite.setUp(JavaDataFrameSuite.java:50)
Caused by: java.lang.IllegalStateException: 
Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.hive.JavaDataFrameSuite.setUp(JavaDataFrameSuite.java:50)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>Could not initialize class org.apache.spark.sql.hive.test.TestHive$</li></div><div><li>Could not initialize class org.apache.spark.sql.hive.test.TestHive$</li></div><div><li>Could not initialize class org.apache.spark.sql.hive.test.TestHive$</li></div><div><li>Could not initialize class org.apache.spark.sql.hive.test.TestHive$</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(StreamingContext.scala:85)
org.apache.spark.streaming.api.java.JavaStreamingContext.&lt;init&gt;(JavaStreaming</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(StreamingContext.scala:85)
org.apache.spark.streaming.api.java.JavaStreamingContext.&lt;init&gt;(JavaStreaming</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(StreamingContext.scala:85)
org.apache.spark.streaming.api.java.JavaStreamingContext.&lt;init&gt;(JavaStreaming</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(StreamingContext.scala:85)
org.apache.spark.streaming.api.java.JavaStreamingContext.&lt;init&gt;(JavaStreaming</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(StreamingContext.scala:85)
org.apache.spark.streaming.api.java.JavaStreamingContext.&lt;init&gt;(JavaStreaming</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(StreamingContext.scala:85)
org.apache.spark.streaming.api.java.JavaStreamingContext.&lt;init&gt;(JavaStreaming</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Could not load library. Reasons: [no leveldbjni64-1.8 in java.library.path, no leveldbjni-1.8 in java.library.path, no leveldbjni in java.library.path, /var/lib/jenkins/workspace/spark/common/kvstore/target/tmp/libleveldbjni-64-1-7684364522220548016.8: /var/lib/jenkins/workspace/spark/common/kvstore/target/tmp/libleveldbjni-64-1-7684364522220548016.8: cannot open shared object file: No such file o</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Timeout after waiting for 10000 ms.</li></div><div><li>&amp;#010;Assert on query failed: Add partitions: The code passed to eventually never returned normally. Attempted 3958 times over 1.0000657575833334 minutes. Last failure message: assertion failed: Partition [topic-17-suffix, 6] metadata not propagated after timeout.&amp;#010;org.scalatest.concurrent.Eventually$class.tryTryAgain$1(Eventually.scala:421)&amp;#010; org.scalatest.concurrent.Eventually$class.even</li></div><div><li>Timeout after waiting for 10000 ms.</li></div><div><li>0 was not greater than 0</li></div><div><li>&amp;#010;Timed out waiting for stream: The code passed to failAfter did not complete within 10 seconds.&amp;#010;java.lang.Thread.getStackTrace(Thread.java:1559)&amp;#010; org.scalatest.concurrent.TimeLimits$class.failAfterImpl(TimeLimits.scala:234)&amp;#010; org.apache.spark.sql.streaming.EventTimeWatermarkSuite.failAfterImpl(EventTimeWatermarkSuite.scala:38)&amp;#010; org.scalatest.concurrent.TimeLimits$class.fail</li></div><div><li>&amp;#010;Timed out waiting for stream: The code passed to failAfter did not complete within 10 seconds.&amp;#010;java.lang.Thread.getStackTrace(Thread.java:1559)&amp;#010; org.scalatest.concurrent.TimeLimits$class.failAfterImpl(TimeLimits.scala:234)&amp;#010; org.apache.spark.sql.streaming.StreamingOuterJoinSuite.failAfterImpl(StreamingJoinSuite.scala:424)&amp;#010; org.scalatest.concurrent.TimeLimits$class.failAfte</li></div><div><li>Timeout of './bin/spark-submit' '--class' 'org.apache.spark.sql.hive.SparkSQLConfTest' '--name' 'SparkSQLConfTest' '--master' 'local-cluster[2,1,1024]' '--conf' 'spark.ui.enabled=false' '--conf' 'spark.master.rest.enabled=false' '--conf' 'spark.sql.hive.metastore.version=0.12' '--conf' 'spark.sql.hive.metastore.jars=maven' '--driver-java-options' '-Dderby.system.durability=test' 'file:/var/lib/jen</li></div><div><li>Timeout of './bin/spark-submit' '--class' 'org.apache.spark.sql.hive.SPARK_9757' '--name' 'SparkSQLConfTest' '--master' 'local-cluster[2,1,1024]' '--conf' 'spark.ui.enabled=false' '--conf' 'spark.master.rest.enabled=false' '--driver-java-options' '-Dderby.system.durability=test' 'file:/var/lib/jenkins/workspace/spark/sql/hive/target/tmp/spark-5b12473b-a197-4efe-860f-92e663dd1083/testJar-1542246955</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Timeout getting response from the server</li></div><div><li>expected:&lt;true&gt; but was:&lt;false&gt;</li></div><div><li>expected:&lt;2&gt; but was:&lt;0&gt;</li></div><div><li>java.lang.AssertionError
	at org.apache.spark.ExecutorPluginSuite.testPluginClassDoesNotExist(ExecutorPluginSuite.java:72)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.ExecutorPluginSuite.testAddMultiplePlugins(ExecutorPluginSuite.java:91)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessor</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.ExecutorPluginSuite.testAddMultiplePlugins(ExecutorPluginSuite.java:91)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessor</li></div><div><li>SparkContext has been shutdown</li></div><div><li>SparkContext has been shutdown</li></div><div><li>Job aborted due to stage failure: Task serialization failed: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.refl</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.Java8RDDAPISuite.setUp(Java8RDDAPISuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.Java8RDDAPISuite.tearDown(Java8RDDAPISuite.java:59)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:57)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:57)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:57)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:57)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:57)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:57)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:57)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:57)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:57)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:57)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:57)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:57)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:57)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:57)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:57)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:57)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:57)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:57)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:57)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:57)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:57)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:57)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:57)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:57)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:57)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:57)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:57)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:57)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:57)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:57)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:57)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:57)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:57)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:57)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:57)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:57)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:57)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:57)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Metho</li></div><div><li>SparkContext has been shutdown</li></div><div><li>SparkContext has been shutdown</li></div><div><li>SparkContext has been shutdown</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Job aborted.</li></div><div><li>SparkContext has been shutdown</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:63)
test.org.apache.spark.JavaAPISuite.setUp(JavaAPISuite.java:88)
sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.JavaAPISuite.tearDown(JavaAPISuite.java:95)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:57)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:57)
test.org.apache.spark.JavaSparkContextSuite.javaSparkContext(JavaSparkContextSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>Exception evaluating from_avro(to_avro(76), "int")</li></div><div><li>Exception evaluating from_avro(to_avro(-11589), "int")</li></div><div><li>Exception evaluating from_avro(to_avro(2147483647), "int")</li></div><div><li>Exception evaluating from_avro(to_avro(704744889333226528), "long")</li></div><div><li>Exception evaluating from_avro(to_avro(-23.867805), "float")</li></div><div><li>Exception evaluating from_avro(to_avro(1.7976931348623157E308), "double")</li></div><div><li>Exception evaluating from_avro(to_avro(21192938), {"type":"fixed","name":"fixed","namespace":"topLevelRecord","size":4,"logicalType":"decimal","precision":8,"scale":0})</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>Exception evaluating from_avro(to_avro(-572137571863271), {"type":"fixed","name":"fixed","namespace":"topLevelRecord","size":7,"logicalType":"decimal","precision":16,"scale":0})</li></div><div><li>Exception evaluating from_avro(to_avro(55581.00198802774), {"type":"fixed","name":"fixed","namespace":"topLevelRecord","size":7,"logicalType":"decimal","precision":16,"scale":11})</li></div><div><li>Exception evaluating from_avro(to_avro(5865405902210970733), {"type":"fixed","name":"fixed","namespace":"topLevelRecord","size":16,"logicalType":"decimal","precision":38,"scale":0})</li></div><div><li>Exception evaluating from_avro(to_avro(5.096119993768149695E-20), {"type":"fixed","name":"fixed","namespace":"topLevelRecord","size":16,"logicalType":"decimal","precision":38,"scale":38})</li></div><div><li>Exception evaluating from_avro(to_avro(텎ⳛ韞㌲Ὧ毂쭆쵗㛧疨䚵ꐋ쪞蜔뷵祌ꇅ薎榢誐脂꘸될됴鹹屑哱ꤘ휺푆嗖躖⌞俄黫覤쀑શ씤ᴉ糳瘜與寫䊀➨ᱯ묖ꁽ⬑䚕䕬㥨̄Ɦ展림톾뿨荭笩輇镱䃻홣宑턾쌬㢃େ辑੃톷绘묄뫄ᯓ薅㺕ᴖ꟞哰敐곩ꞌ쪩嵮ꤟ糶猵迳婯樎葯徒⡉Ḛ냖㣃蚛믢ᮮ봑虔௛硸勳뜢뺁흞失쾇⁂㊁過쿎ㆤ䜥콇狉貋婿乗⻔鏸펀㺗褷ొ᳢ܲ좧땊ࡑ쏐좨⠕ᶗ꫆孂잍ᶺ薲橐緯鋨⢤絘虓킭⁏額⪏蚙鄐资㗺㚥饉鷬⢳嗌ꘐ꽨뎌끴㚖嵁靡襜辯䪻蔺㬁扬제ገ⌴蝽웣䗃뉟嬧ዱ旛䐅ს罆黲䣓晌ᗛ䕗陰䉅὇芔ަ폶葭螑Ⲉ䡕ǿ秆㇙꿎થ༁嶝꾛ʋ㰰羋ꗪ详舅㡞錿戣⦰ၑ爁뢅꽸휼跙粮氬᪻쀲Ữ碕汑톴໸〲{虌餉鴇Ꮠ럽㛟琯ඹ展㭤ʛ㘹Ꙙ䡭Ū鼙戄萮䍍趿獄耝鳯傦궏䔅㳄귮至퀼᪄藘栊蟷瞤΂췷੅䨡ޑ⻲䄷⸍ⷄ棫䉡䭃遈弤⁔嘫썊ꢷ혌ƀ耎䫴ퟶ鸣䵹ب怑Ĺ蘣巘ꔉ웇俐碾㡵趏ᢨ肇獈㯡㮖땧齫晚ꋢ肞הꕵ⇜ᛲ鮙齒솝촯깵貛룟頠棪ᅂ鋪䕘䢴溹릹쯌䓌钡츎ᨁ䈇᭾叿骨ࣧ뵋肸꿠栾郐₽墮첣깘㕊ך糤막</li></div><div><li>Exception evaluating from_avro(to_avro(0xD5E1964474578450B9665AE9D17114B7137D0C8F27360B7EDAC983BDD361535BDFF4F4CFBBE82EEE1603F6708EF20DEE50E120E2FC64ADA6EDDD45E4187DEF6E103A5C857EB77C325B0FB660F56580ED3E957E1FACA266205CE9D54F3F24B14295F00561219BD6CEB24259882308B263CD6832F236B59ACFE59620798F74DCFC41831B3CDB574DC18BA25465298089DED1ED9D8DAAC82641C9FFE8188231FDF209D98AC039E5521C19870DA08BCA4C63825070F</li></div><div><li>Exception evaluating from_avro(to_avro([戛ና엺㧩뜩淖纾ⵗ䫽뎍뤔붪灹驒榋긏䅩噕葈䗬剞掫Ɤퟮ합樶族嗫ᧂ〺輱轌౨⧗≟퇑쳝趴醻楢⟮廗⮚浏템쥱煀ⷚ羹莒隹ꂨ滈伫츢镗ꕓ┟᢫띿Ӹ꡼汉旃堞蒦䪛㚏ߣ鬚졛픓찵⥙袝撮靲Ѹ甖뗼᱆浵忾⪵퀿繧鉬徉䫈指䌿ᎉ잵鵑馅➒䀘팹닡ӹㄑ髖┾岇秓㒞셭冀ࡧޚ䇴唒堚⤝ۣ꣱轳⬭ԧ̮넷뛉쒼懲嚐蹞䶘䁩睼玻턁䡁便簩䍈㐝딇뷂欈붯맾頳걺ꅎ꣫皴ɞ켜⚣징⯐厜Ǟ▁浿ꌹ憦ⲟ㚳岽佡㥝S䗹鵪雂꺪ᓜ큊⢐媭脝枤黃蹔엫磏ႂぉ覘ᅬ妢䩾뱂謃鋆ር捤ந침鋅뼨嗟홝㖌ݤ⺠넁愜↰徻⸔㵹ꘘᐗ崃៭媣港ଭ뱗ᓕ넾碚姡臂᩺⥢詣哶櫭廀潯峹됀뚭䔆ᔫ廭꫹쵽ⴁ䓅⤽⑪y처蔶嶶⢂軄鍘飋袪쭨ꕠ圓遧⽀ㄕ뀄퓑觹쨾麰ᚌ榑鍶竅贪뤷鏍휓⺆副Ⳛ乐囒酬ᰢꦚ쫛ꝁ渭ˤ豖腣㒔枌琐꼾⺮ጼ亡嫲⻾뉫ᢔ≹橺咉⺼ᖻꦿꕺꮅ뀧苝䵒뙊ᅒꒁꉔ䈎௭蛷앃톢䑺曆棩貕ꚋ齪㾰螎䒒ࠇ両䡎贱Ӛ䓒幫Ꮬ깝H켍ᖾ⯝䅶恃Ꞥ撊쑊髾痩汴࿼἗苝븤쇯몹遰叅ᕍ蒿眜⼀䜩훠卦蠽ᠸ㤔耪ᶫ浠ꉷ</li></div><div><li>Exception evaluating from_avro(to_avro([9.5533465E32,false,[B@2ee92e7d,-4.141334343639939099E-20,[B@759a678a]), {"type":"record","name":"topLevelRecord","fields":[{"name":"col_0","type":"float"},{"name":"col_1","type":"boolean"},{"name":"col_2","type":"bytes"},{"name":"col_3","type":{"type":"fixed","name":"fixed","namespace":"topLevelRecord.col_3","size":16,"logicalType":"decimal","precision":38,"</li></div><div><li>Exception evaluating from_avro(to_avro([-44548.15412238271,true,-588.8164,좨Ք桀ఐ㬮臏䛉繺侃৅环噢㤠㣙醚땛툶譩敎胊줾뵂샤잶䔸揃フ깊貾骍믾꧙榨㉵웩驖薤璀㭘筻⹍퇖鷔쾼⪶큝ऽѤ㟦䵏㚙磫⃂奨쵚༭뭆ቸ囦ॸ깈鳟ㄿ㴼숿轤椱嬋䉱䑙㧆붘ǣ즎쫑뽕⾔⠚淃۫〿럮ꀷ쾱㛉横쿽夝⧞봀綮䷱鼒揣쐞쉀换猼棏噊๳쥲쇾觿๑佁䰢歁鿦ⷖ턤䵝侏筬꿜硏쪼湊甑ᢝ휥෈䔇ཌ構驕杼쿾떺䍕玴奈䣘⎙㶓⠵丽䅳⮑ꑵ胹㻵ɨ엍ꏸ肞幊㙖櫲䪍ꆇ诈ꥲ瓝젩퉐䴋蜪稝邤ꖸ풽⡪ꢙ潾롃❬ힶ㕏৳゚㼞ꌹ삣롳劍稈쒔恲鍄霃熆㩤텒劐ଜ舮멗ㆣᛳ첵⌍谅䮨㽲继훥ꮖ⾠켇Ⰿ限櫙轥ӿ蛤ं똩⒟朧ꑤ僁믤謑먎ꚾ姗ޢ㮼礋룮鄣䥶䰖䧒⵼覸삮妏몁贴ᖿ䍹婟䇌郧蒲௭뒜䣝葽娎픓`瞯⃛綩㴊鷾ệ풖跙럥좕ѸᏀЁῴꓮ勲ඍ㼂Ꮑ篧彣砑濟頻뗲᨟엘⹏壂䖞̡䧦敶Ꝙ檦锣ᙈ從긊僱겇鹲핚蜒굝䬵踝ᴷ⣧▄ދቁ쐣㼊뉱姈萊핵䒾떮῱䭽㣮ꩃ釬骕ゐ鯼䑘鞧觋咅墢圦滑埲奶걶ἲ슔㧠䛂</li></div><div><li>Exception evaluating from_avro(to_avro([[B@708769b7,-2.618376E-5,54411.24556871358,-60,-5462168073670317023]), {"type":"record","name":"topLevelRecord","fields":[{"name":"col_0","type":["bytes","null"]},{"name":"col_1","type":"float"},{"name":"col_2","type":{"type":"fixed","name":"fixed","namespace":"topLevelRecord.col_2","size":7,"logicalType":"decimal","precision":16,"scale":11}},{"name":"col_3"</li></div><div><li>Exception evaluating from_avro(to_avro([Ɬஆ뽜毵赝ꮜ헤䍔ퟱᯱ؂ᴞ绒穑⶝ᑂ䢢嶨룳콊螫⏝믊르릥ἣǢ酷䠻佈身필ǅ什뗎狓曛膰䇇訓갑ᖮꭉ⒎襁珀볭긕캻湺ƌ큃ᮊ䂉踗ꭡ靕䭐ᑓ媉䨽࣮㉂䳮͓賓䪼Ṏ䆼滢⿡뫠㔸跘扢뀠蝳嫁⮿⫆윮顬闸༝ꊑ䒤ฅ拧勓ಶᲔ奻䈸䒉扉夗쮏쑍怨綏ㇵ徦利⇉徸뿳∴財炳荳쓽믠륛慏댏䛬捾硈攘ꋧ妺葌蛶鯐茿ᗚ롎뺦ꆔ瞤읓ࢸ꽾ᜥ䮵䳙雧ᥪ쌪껡ฌ똓毞疱ᄈǿ叢돼膶,0.0045673037,-18060121207589,true,-128]), {"type":"record","name":"topLevelRecord","fields":[{"name":"col_0","type":"string"},{"name":"col_1","type":"float"},{"name":"col_2","type":{"type":"fixed","name":"fixed</li></div><div><li>Exception evaluating from_avro(to_avro([[0],0,10295,ધꡎᠼ⧊∱㹟䁯屇꘿蒠⡶ي滅윧結ᵄ踭賑ᾙ䳗ᯋ⇀䢡亚㱖闧ꋲ۸䞚ᰚ蝁墪㇄쮥큱ꪓ籹ህ韗㔖㰭絊썄踏춛瀠뭏꙱哧ꐷ억퍁㐎뢺駣齮ႂݷ嬏疯쎿島갮鏱눿ẃ鎠㳉쿾宄⸎貒壉ꨙ℮ꕬ咸ꥪ䝒ʀ쇌嬯㑴㒶㔊ꊥ슳ꛘ挎䡺猻␆䨄ᙸ蔨ᘁ丿쎡妨瑓쨫魪䀾示颊脲怖ᵱ퐓涮씞燖ꆽ㣈ꍬ唋镳ཀྵᲓ빷姗ꕗ㠊統뭪㪤둽◴㸲욗ዖ똆凞䪪嚰玮玻噠艜䓅ᶯ뭐镽敏㺲꧖첶㝠뽣㞉웸㔪᭖ⰾ왷谑齠৮龱넴ᇔ亸劶쟅䛸쁁앆嬊냇쳳쁘鉃牒溲䫃溈䅈磥ℵ鋹긟ꤷ晕鹥뜫峩甖ቲຶཿ䡲黄焢췒ᝐ唒貢⊲齅ꍎ窺ꇫ缡欮࠙ཡ걬걞唉癭㍟넃䔚蚈跃鍇毾皧䓭⩒说䯂ࣉ驅⚍伿魝ࠋ猹Ჿ확縥⪦칥幻괺䴥ὗ튵萫놰昛ힴ噱䡧닍䇩ꓔϣ⒐陼譫⋂ꛤ쯄쮮䦸㺙鿦墤窒킵⺘䙝‰䉜鹳쮎⽗꥞泫硓ܞ溺寶陼櫕ㇹ趜ⴓᕹ옰蓼첛ਊ錓荶쐝嗌ꂕ畈굣갶舴诟뭒觕疟댵⠵얂⫿授쑳팋琷둓㋂ⱕ깆乀⾃โὖ싐ᵓ꼷唺䌑Ð繐䐟⏰፷戗梄擡⨋욟䇾靬ႋ緻雿⌹䚠䯛㍭Ⱙ苇䐵΅鏻鑈읏崩࢚戄</li></div><div><li>Exception evaluating from_avro(to_avro([[[2.473157984851636526E-20],[[設酦盦⽣᥹㉯蛟専戡្铴莖흦덖窛舩΀䪓ꉁ‾ϝⱓ쵶㟹ଞ纓䅑ⶱ뾶땱烄獻䮵ꨃၯ뷣ে皃聧훧╬ᴾ⇢릾㱉ૠ椖넔⼍京ힶꞷ꧗逨詎钥굈⤷僬銡즹蠖귲橼藜쨐짙᳷䀜眜྾⠓嫬㣭浼旒䵌ᜐ퐳鯤ᜈ⡪租ꑩꇧ縋䶚鑖扮혍衄뱋邛울뿗싆䂸藹힛㹘ୂ呗㪰ꆇ珺꒙됦뺑儃轈⓼ೢ겝뷷쳇ퟚ趿᧐⍅朳嬞緶꼣榈摽锩輑甴햹䵴뚢囼㳌揚誅ᵲ䥚䅮躓겨ᄕ폓圎廰ặ甭걟ᄛ⃳梃䢨䂬蟝됾ꃘྎ籱Ķ邴鋕Ĭ䁂盝ᐪ腿훛늹돴ꈛ斑༜ꆶ巺瀹雥犎䐁䤶옗煌謔葛ķ災탽ꍀ轴땸홤㹹싓ʑ晅저笕諮ự骄஘檪䊐㌙睤捶윰䐐澚䟯툅ྜྷ둕ꋃꯥ깊쇾ᘕ㬥邷煮她퓪퍕廴ߴす쐅皺ꆮ⃫됮霯㿂❞첮⺑蜶뾋职昬؊䬌ୋⴘ羧芁铬ꇖᢺ䢟吝퇰퐋닼ܥ塕駷쓙挮ꠙӭܜ滙碱ꐶ儫衠场ᒾ퓐ⴙ⧪蠺⦠笒۝럦睶㥙᛽喓Ồ쭑ᤥ䋒꓈붲珁㒀됻㚑堷Ḭ蕥὞ޚ猇뵑禙Ồ羬돃뭉з㴑뽛垥鳰㨬琷ᛴ㹢붽䂶䭔色叀ࠥᖺ渟ᖫ劀⊱儴둌華࿖春꟧㿅싄</li></div><div><li>Exception evaluating from_avro(to_avro([[null,[[522207337]],[[1.7288239],[祹⿋ေ䄯嚢맰毳㎚扔㺪の琌較猾ᬡ뼜扟娆猑ⓘ栬좊㴆ࣚ㕭ꋙ⁣䟬ഷ⇆犆懧व拝쮰┱兹ো퐞㩚箄蒝儶떸氩ឭ⒞ᚈ죕파큏洚咊䈫笩涖俆婞삵낐ච屹축ở漌괈펎쪷톅곽끛৪췉笡⹀떘゙≮똵㣴꽍鳢ᑸ뀜묶㶝蝺頯嶶㚎㮉騝朿㑠蛋劫洫䂔핲諩縂ꡈ壺媤ຠ耋塟벓죝뻐毕․씄丐蝪鰟₎斮祗힘탱飉⹅鄈騽돭砹ⅉ잯⟸Wⲡṑ掴鎨⸩憶鋍䩓㤍䃆떀㣦⋼뜂유◹䃳㚼쏩ᎀ堝쑨햜ᵮᓼ妰䊦ꙍ螮科緅倘⠱뜹姬曆ᗟ愻┑甝哟繢렲੊鳇螛珷硲ꂃ퐢븱⩭䮩酋資㠔妀鎟̝ọ邯ሪ糎ె✈貎㰔꼠钸龘诮첏늿鰥Ⅻ霱꘷㿅滵钲栻孃䯿팮띤㾙藻璵ꑇÆ㛭髦ẙ츍츾쳢ᑰ㸟䒨ꪨ庙Ѵ㕃該̒傤魮യ䝒せ誢좄쪭➨㓡멓꫍魭壈ᐅ聵⭁笸኿퉈雑ᇏ鞘⥦댂ܒ铀䃌썍낷㉲㿋䡕䍦痉攺ݵ袿Ꙙ腚⑈텐䅾쥨漈䀉犛냮驧ꨡ쐠ꋪ抍⢲꘡羠㡘쿔濝暪온锶뤳⣛쵦䜸㜳۔绸ꇞ켅㑼쩙븁酝湞裱敼ƒ⸻켮맮᭤늄♨吞ᬧ嚛ꙻ᫹</li></div><div><li>Exception evaluating from_avro(to_avro([[null],-Infinity,[3753341251664996148],[[[B@28e8dee7]],[-99970727],[null],5948212568513576,[5970128751657472339],[[[4927799701294757]]],[[-3.588131E-21]]]), {"type":"record","name":"topLevelRecord","fields":[{"name":"col_0","type":{"type":"array","items":[{"type":"fixed","name":"fixed","namespace":"topLevelRecord.col_0","size":7,"logicalType":"decimal","prec</li></div><div><li>Exception evaluating from_avro(to_avro([[1903481688],-4.456595672318548606E-20,[1391535077286503306],[[[[-95862.81836643823]]],[[[[[[null]]]]]]],-1409724806,[-76211.79496793744],[127],堽荩퇵䐠⛊缞쭅쯨佭姚䖤窿古퍨늄웖妮㟼ح匦潉珊鱃ꇚၾ礣㋁α긦儺叻밻䉾坔熥扺ẳ든밖ᖮ煵ꠊ桡ᆗ␹䨊섥汆ଙ덟೮즩爑煯鎅վ敇讗ꨟ⤐꿽뼆ᏽ绕箣ꐿ舽ᣋ⧖ꨑ利સ脊誅䛂磾酛㋀嶀澋죾屯ʠ颱峛藭ᐊ蛻䌲縬ⱻᘌᣁफꚣ倛姘鱻厎廟䯒㘤껚뷇෽絊ῳ⢱벉钕쪞Ⱌ읉䅽摆튓묖ن缋聶㆙擠܉璽紁蕕㻸挒赾ᾴ⎇泴㗿쑇㙀㽋ⱏ੟ᘒ嬅䪓І糾깼ҝ벤ꍼ誑뉨厛ᜳ艆準⬦멲붑諕磽턨⩏쯚῱嘖쓴㰇鮇亹疓쮕겏챔㹂⎕߮序႑пۻ뮤염⇋뗟Ꝝ㢆ʼ붠翑谝ꒉ桒锓䉕㍗ѵఽ淅阳椆뗖⦯姺㥗遧똔挐둠㶋Ṣ㗯卍샷껽礵橰굗䒏</li></div><div><li>Exception evaluating from_avro(to_avro(1), &amp;#010;{&amp;#010;  "type": "string",&amp;#010;  "name": "my_string"&amp;#010;}&amp;#010;       , (mode,PERMISSIVE))</li></div><div><li>Exception evaluating from_avro(to_avro(abc), &amp;#010;{&amp;#010;  "type": "int",&amp;#010;  "name": "my_int"&amp;#010;}&amp;#010;       )</li></div><div><li>Exception evaluating from_avro(to_avro(1.23), &amp;#010;{&amp;#010;  "type": "double",&amp;#010;  "name": "my_double"&amp;#010;}&amp;#010;       , (mode,PERMISSIVE))</li></div><div><li>Exception evaluating from_avro(to_avro(1.23), &amp;#010;{&amp;#010;  "type": "float",&amp;#010;  "name": "my_float"&amp;#010;}&amp;#010;       )</li></div><div><li>create random record with seed -3545246842155068840 Exception evaluating from_avro(to_avro([䥇碦傕覮銒诏⠒퉟쒳솰梽᱁䃔晳缣䈐厙痁᙭硳餔륑䜾뵆똛뉊⒁蚥㽧뵻웵斟叁ㆌ㫓䡸쫰嶽㖬ʀ쒢뒱ᙫ㱏䓣࠼쌠ᏽ뾸팻ൻꦗ萧㘉諓⨑麯䴨䬷빗妢斥ⶾ瘭之ꎈ퀬鶛묦꧋Ꙡࢽ爍뎠ꣂ旆ᙏ넆鳥颪뛄둆鼵⫑仰綉覽탒蚧ꔲ㵗皵攛磨桀傣ꪫ鉠웣旕궧⋾ᡝ뿞嶧Զኜ鿭Ẳ署僡诀疴⊦݌쵟᫸෽巍話㨌᎔ିꕽ巇쏚䗄涸㩩댈덛籕䍕쁰尊쏫唓皯㔏휣诛皴«석ᥝ☳⃬㷜硄鮳ଫ㶑툪內뜑䤅鯋ްꮠ싉鋻ᵂ尊넃恓᭡曌翜䖰 絩奩삋棴쨇戂㪁㘇闂쒻걧輊⵬緷哎벁됈獂贡뢱户黴瑐힯ΖⱢѢ⽱ጟ뭈겼᜾娑殍첅笫쭸㠅䒃炞ᚱ疕᭝⌢㾤㊥㢃뤩龱චᢐ숥듕⭖䫻놛ਰ뺆Ꝺ勸䃦㺷ꆐ洃꾤Ḟ牾쏴㾞遊ᦁ緞뙶诣傉菒⊰陿䙼践鳄먡ቖ뻑휙䳍ุ삥઴测䪱ꋡ磮뭌ꮴዷ渷৔끰菡輺苭ꜹ蠉蜈攼ꞧ䖭豩姮냷勘鱵ꥑⰯ藦窏ꑕ櫘瀅벢瓬텸⌹愲鱸蠟ⳅ።ვ聕Ặ跾橿컳ꊯ∿朘</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.sql.avro.AvroFunctionsSuite.beforeAll(AvroFunctionsSuite.scala:27)&amp;#010;org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:212)&amp;#010;org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:210)&amp;#010;org.apache.spark.SparkFunSuite.run(SparkFunS</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.sql.avro.AvroFunctionsSuite.beforeAll(AvroFunctionsSuite.scala:27)&amp;#010;org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:212)&amp;#010;org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:210)&amp;#010;org.apache.spark.SparkFunSuite.run(SparkFunS</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>SparkContext has been shutdown</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.util.PeriodicGraphCheckpoin</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.util.PeriodicGraphCheckpoin</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphLoaderSuite.withSpark(GraphLoaderSuite.scala:28)&amp;#010;org.apache.spark.graphx.GraphLoaderSuite$$anonfun$2.app</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphOpsSuite.withSpark(GraphOpsSuite.scala:23)&amp;#010;org.apache.spark.graphx.GraphOpsSuite$$anonfun$9.apply(GraphO</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphOpsSuite.withSpark(Gra</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphOpsSuite.withSpark(Gra</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphOpsSuite.withSpark(Gra</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphOpsSuite.withSpark(Gra</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphOpsSuite.withSpark(Gra</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphOpsSuite.withSpark(Gra</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphOpsSuite.withSpark(Gra</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphOpsSuite.withSpark(Gra</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphOpsSuite.withSpark(Gra</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphOpsSuite.withSpark(Gra</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.SVDPlusPlusSuite.withSp</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.SVDPlusPlusSuite.withSp</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.SVDPlusPlusSuite.withSp</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.SVDPlusPlusSuite.withSp</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.SVDPlusPlusSuite.withSp</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.SVDPlusPlusSuite.withSp</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.SVDPlusPlusSuite.withSp</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.SVDPlusPlusSuite.withSp</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.SVDPlusPlusSuite.withSp</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.SVDPlusPlusSuite.withSp</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.SVDPlusPlusSuite.withSp</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.SVDPlusPlusSuite.withSp</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.SVDPlusPlusSuite.withSp</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.SVDPlusPlusSuite.withSp</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.SVDPlusPlusSuite.withSp</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.SVDPlusPlusSuite.withSp</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.SVDPlusPlusSuite.withSp</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.SVDPlusPlusSuite.withSp</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.SVDPlusPlusSuite.withSp</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.SVDPlusPlusSuite.withSp</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.SVDPlusPlusSuite.withSp</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.SVDPlusPlusSuite.withSp</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.PregelSuite.withSpark(PregelSuite.scala:22)&amp;#010;org.apache.spark.graphx.PregelSuite$$anonfun$4.apply(PregelSuite.</li></div><div><li>SparkContext has been shutdown</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.util.GraphGeneratorsSuite.w</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.util.GraphGeneratorsSuite.w</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.util.GraphGeneratorsSuite.w</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.util.GraphGeneratorsSuite.w</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.util.GraphGeneratorsSuite.w</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.util.GraphGeneratorsSuite.w</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.util.GraphGeneratorsSuite.w</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.util.GraphGeneratorsSuite.w</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.util.GraphGeneratorsSuite.w</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.util.GraphGeneratorsSuite.w</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.util.GraphGeneratorsSuite.w</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.util.GraphGeneratorsSuite.w</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.util.GraphGeneratorsSuite.w</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.util.GraphGeneratorsSuite.w</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.util.GraphGeneratorsSuite.w</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.util.GraphGeneratorsSuite.w</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.LabelPropagationSuite.withSpark(LabelPropagationSuite.scala:23)&amp;#010;org.apache.spark.graphx.lib.LabelPropagat</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.util.PeriodicGraphCheckpoin</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.util.PeriodicGraphCheckpoin</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.util.PeriodicGraphCheckpoin</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.util.PeriodicGraphCheckpoin</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.util.PeriodicGraphCheckpoin</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.util.PeriodicGraphCheckpoin</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.util.PeriodicGraphCheckpoin</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.SVDPlusPlusSuite.withSpark(SVDPlusPlusSuite.scala:24)&amp;#010;org.apache.spark.graphx.lib.SVDPlusPlusSuite$$anonf</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphLoaderSuite.withSpark(</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphLoaderSuite.withSpark(</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.GraphLoaderSuite.withSpark(</li></div><div><li>java.lang.NullPointerException was thrown.</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.TriangleCountSuite.withSpark(TriangleCountSuite.scala:25)&amp;#010;org.apache.spark.graphx.lib.TriangleCountSuite$</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.TriangleCountSuite.with</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.lib.TriangleCountSuite.with</li></div><div><li>SparkContext has been shutdown</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.util.GraphGeneratorsSuite.w</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.util.PeriodicGraphCheckpointerSuite.withSpark(PeriodicGraphCheckpointerSuite.scala:28)&amp;#010;org.apache.spark.graph</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:&amp;#010;org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:127)&amp;#010;org.apache.spark.graphx.LocalSparkContext$class.withSpark(LocalSparkContext.scala:32)&amp;#010;org.apache.spark.graphx.util.PeriodicGraphCheckpoin</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.tuning.JavaCrossValidatorSuite.setUp(JavaCrossValidatorSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.tuning.JavaCrossValidatorSuite.setUp(JavaCrossValidatorSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.tuning.JavaCrossValidatorSuite.setUp(JavaCrossValidatorSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.tuning.JavaCrossValidatorSuite.setUp(JavaCrossValidatorSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.tuning.JavaCrossValidatorSuite.setUp(JavaCrossValidatorSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.tuning.JavaCrossValidatorSuite.setUp(JavaCrossValidatorSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.tuning.JavaCrossValidatorSuite.setUp(JavaCrossValidatorSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.tuning.JavaCrossValidatorSuite.setUp(JavaCrossValidatorSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.tuning.JavaCrossValidatorSuite.setUp(JavaCrossValidatorSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.tuning.JavaCrossValidatorSuite.setUp(JavaCrossValidatorSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.tuning.JavaCrossValidatorSuite.setUp(JavaCrossValidatorSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.tuning.JavaCrossValidatorSuite.setUp(JavaCrossValidatorSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.tuning.JavaCrossValidatorSuite.setUp(JavaCrossValidatorSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.tuning.JavaCrossValidatorSuite.setUp(JavaCrossValidatorSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.tuning.JavaCrossValidatorSuite.setUp(JavaCrossValidatorSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.tuning.JavaCrossValidatorSuite.setUp(JavaCrossValidatorSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.tuning.JavaCrossValidatorSuite.setUp(JavaCrossValidatorSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.tuning.JavaCrossValidatorSuite.setUp(JavaCrossValidatorSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.tuning.JavaCrossValidatorSuite.setUp(JavaCrossValidatorSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.tuning.JavaCrossValidatorSuite.setUp(JavaCrossValidatorSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.tuning.JavaCrossValidatorSuite.setUp(JavaCrossValidatorSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.tuning.JavaCrossValidatorSuite.setUp(JavaCrossValidatorSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.tuning.JavaCrossValidatorSuite.setUp(JavaCrossValidatorSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.tuning.JavaCrossValidatorSuite.setUp(JavaCrossValidatorSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.tuning.JavaCrossValidatorSuite.setUp(JavaCrossValidatorSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.tuning.JavaCrossValidatorSuite.setUp(JavaCrossValidatorSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.tuning.JavaCrossValidatorSuite.setUp(JavaCrossValidatorSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.tuning.JavaCrossValidatorSuite.setUp(JavaCrossValidatorSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.tuning.JavaCrossValidatorSuite.setUp(JavaCrossValidatorSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.tuning.JavaCrossValidatorSuite.setUp(JavaCrossValidatorSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.tuning.JavaCrossValidatorSuite.setUp(JavaCrossValidatorSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.tuning.JavaCrossValidatorSuite.setUp(JavaCrossValidatorSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.tuning.JavaCrossValidatorSuite.setUp(JavaCrossValidatorSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at org.apache.spark.ml.source.libsvm.JavaLibSVMRelationSuite.tearDown(JavaLibSVMRelationSuite.java:57)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.stat.JavaSummarizerSuite.setUp(JavaSummarizerSuite.java:42)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.refl</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.stat.JavaSummarizerSuite.setUp(JavaSummarizerSuite.java:42)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.refl</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.stat.JavaSummarizerSuite.setUp(JavaSummarizerSuite.java:42)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.refl</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.tuning.JavaCrossValidatorSuite.setUp(JavaCrossValidatorSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.tuning.JavaCrossValidatorSuite.setUp(JavaCrossValidatorSuite.java:42)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.</li></div><div><li>java.lang.NullPointerException
	at org.apache.spark.ml.util.JavaDefaultReadWriteSuite.tearDown(JavaDefaultReadWriteSuite.java:41)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:4</li></div><div><li>java.lang.NullPointerException
	at org.apache.spark.mllib.classification.JavaStreamingLogisticRegressionSuite.tearDown(JavaStreamingLogisticRegressionSuite.java:55)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.stat.JavaSummarizerSuite.setUp(JavaSummarizerSuite.java:42)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.refl</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>SparkContext has been shutdown</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.stat.JavaSummarizerSuite.setUp(JavaSummarizerSuite.java:42)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.refl</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.stat.JavaSummarizerSuite.setUp(JavaSummarizerSuite.java:42)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.refl</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>SparkContext has been shutdown</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.stat.JavaSummarizerSuite.setUp(JavaSummarizerSuite.java:42)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.refl</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.stat.JavaSummarizerSuite.setUp(JavaSummarizerSuite.java:42)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.refl</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.stat.JavaSummarizerSuite.setUp(JavaSummarizerSuite.java:42)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.refl</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.stat.JavaSummarizerSuite.setUp(JavaSummarizerSuite.java:42)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.refl</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.stat.JavaSummarizerSuite.setUp(JavaSummarizerSuite.java:42)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun</li></div><div><li>java.lang.NullPointerException
	at org.apache.spark.mllib.clustering.JavaStreamingKMeansSuite.tearDown(JavaStreamingKMeansSuite.java:54)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.stat.JavaSummarizerSuite.setUp(JavaSummarizerSuite.java:42)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.refl</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>SparkContext has been shutdown</li></div><div><li>Job aborted due to stage failure: Task serialization failed: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>SparkContext has been shutdown</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.stat.JavaSummarizerSuite.setUp(JavaSummarizerSuite.java:42)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.refl</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.stat.JavaSummarizerSuite.setUp(JavaSummarizerSuite.java:42)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.refl</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.stat.JavaSummarizerSuite.setUp(JavaSummarizerSuite.java:42)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.refl</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.stat.JavaSummarizerSuite.setUp(JavaSummarizerSuite.java:42)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.refl</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.stat.JavaSummarizerSuite.setUp(JavaSummarizerSuite.java:42)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.refl</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
org.apache.spark.ml.stat.JavaSummarizerSuite.setUp(JavaSummarizerSuite.java:42)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.refl</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:4</li></div><div><li>java.lang.NullPointerException
	at org.apache.spark.mllib.regression.JavaStreamingLinearRegressionSuite.tearDown(JavaStreamingLinearRegressionSuite.java:54)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
	at org.apache.spark.mllib.stat.JavaStatisticsSuite.tearDown(JavaStatisticsSuite.java:65)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
	at org.apache.spark.mllib.stat.JavaStatisticsSuite.tearDown(JavaStatisticsSuite.java:65)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
	at org.apache.spark.mllib.stat.JavaStatisticsSuite.tearDown(JavaStatisticsSuite.java:65)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
	at org.apache.spark.mllib.stat.JavaStatisticsSuite.tearDown(JavaStatisticsSuite.java:65)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SharedSparkSession.setUp(SharedSparkSession.java:39)
sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.run</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)&amp;#010;org.apache.spark.repl.Main$.createSparkSession(Main.scala:112)&amp;#010;&lt;init&gt;(&lt;console&gt;:15)&amp;#010;&lt;init&gt;(&lt;console&gt;:43)&amp;#</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)&amp;#010;org.apache.spark.repl.Main$.createSparkSession(Main.scala:118)&amp;#010;&lt;init&gt;(&lt;console&gt;:15)&amp;#010;&lt;init&gt;(&lt;console&gt;:43)&amp;#</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)&amp;#010;org.apache.spark.repl.Main$.createSparkSession(Main.scala:118)&amp;#010;&lt;init&gt;(&lt;console&gt;:15)&amp;#010;&lt;init&gt;(&lt;console&gt;:43)&amp;#</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)&amp;#010;org.apache.spark.repl.Main$.createSparkSession(Main.scala:118)&amp;#010;&lt;init&gt;(&lt;console&gt;:15)&amp;#010;&lt;init&gt;(&lt;console&gt;:43)&amp;#</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;java.lang.NoClassDefFoundError: org/spark_project/guava/cache/Weigher&amp;#010;  at org.apache.spark.network.shuffle.ExternalShuffleBlockHandler.&lt;init&gt;(ExternalShuffleBlockHandler.java:64)&amp;#010;  at org.apache.spark.deploy.ExternalShuffleService.newShuffleBlockHandler(ExternalShuffleService.scala:63)&amp;#010;  at org.apache.spark.deploy.Exter</li></div><div><li>isContain was true Interpreter output contained 'error: not found: value sc':&amp;#010;java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)&amp;#010;org.apache.spark.repl.Main$.createSparkSession(Main.scala:118)&amp;#010;&lt;init&gt;(&lt;console&gt;:15)&amp;#010;&lt;i</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:17: error: not found: value sc&amp;#010;       val accum = sc.longAccumulator&amp;#010;                   ^&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:18: error: not found: value sc&amp;#010;       sc.parallelize(1 to 10).foreach(x =&gt; accum.add(x))&amp;#010;       ^&amp;#010;&lt;console&gt;:18: error: not found: value accum&amp;#010;       sc.parallelize(1 </li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; v: Int = 7&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:19: error: not found: value sc&amp;#010;       val res1 = sc.parallelize(1 to 10).map(x =&gt; v).collect().reduceLeft(_+_)&amp;#010;                  ^&amp;#010;&amp;#010;scala&gt; v: Int = 10&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:19: error: not found: value sc&amp;#010;       val res2 = sc.parallelize(1 to 10).map(x =&gt;</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt;      |      | defined class C&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:18: error: not found: value sc&amp;#010;       val res = sc.parallelize(1 to 10).map(x =&gt; (new C).foo).collect().reduceLeft(_+_)&amp;#010;                 ^&amp;#010;&amp;#010;scala&gt;      | _result_1542234546790: Int = 1&amp;#010;&amp;#010;scala&gt; </li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; double: (x: Int)Int&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:19: error: not found: value sc&amp;#010;       val res = sc.parallelize(1 to 10).map(x =&gt; double(x)).collect().reduceLeft(_+_)&amp;#010;                 ^&amp;#010;&amp;#010;scala&gt;      | _result_1542234547201: Int = 1&amp;#010;&amp;#010;scala&gt; </li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; v: Int = 7&amp;#010;&amp;#010;scala&gt; getV: ()Int&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:19: error: not found: value sc&amp;#010;       val res1 = sc.parallelize(1 to 10).map(x =&gt; getV()).collect().reduceLeft(_+_)&amp;#010;                  ^&amp;#010;&amp;#010;scala&gt; v: Int = 10&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:19: error: not found: value sc&amp;#010;       val res2</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; array: Array[Int] = Array(0, 0, 0, 0, 0)&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:19: error: not found: value sc&amp;#010;       val broadcastArray = sc.broadcast(array)&amp;#010;                            ^&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:17: error: not found: value sc&amp;#010;       val res1 = sc.parallelize(0 to 4).map(x =&gt; broadcastArray.value(x</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:17: error: not found: value sc&amp;#010;       var file = sc.textFile("/var/lib/jenkins/workspace/spark/repl/target/tmp/spark-66dd6353-6dde-4b1a-b803-b5c31dae9e79/input").cache()&amp;#010;                  ^&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:17: error: not found: value file&amp;#010;       val res1 = file.count()&amp;#010;            </li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; v: Int = 7&amp;#010;&amp;#010;scala&gt; getV: ()Int&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:19: error: not found: value sc&amp;#010;       val res1 = sc.parallelize(1 to 10).map(x =&gt; getV()).collect().reduceLeft(_+_)&amp;#010;                  ^&amp;#010;&amp;#010;scala&gt; v: Int = 10&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:19: error: not found: value sc&amp;#010;       val res2</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:17: error: not found: value spark&amp;#010;       import spark.implicits._&amp;#010;              ^&amp;#010;&amp;#010;scala&gt; defined class TestCaseClass&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:22: error: not found: value sc&amp;#010;       sc.parallelize(1 to 10).map(x =&gt; TestCaseClass(x)).toDF().collect()&amp;#010;       ^&amp;#010;&amp;#010;scala&gt; &amp;#010</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; import org.apache.spark.sql.functions._&amp;#010;&amp;#010;scala&gt; import org.apache.spark.sql.{Encoder, Encoders}&amp;#010;&amp;#010;scala&gt; import org.apache.spark.sql.expressions.Aggregator&amp;#010;&amp;#010;scala&gt; import org.apache.spark.sql.TypedColumn&amp;#010;&amp;#010;scala&gt;      |      |      |      |      |      |      | simpleSum: org.apache.sp</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; defined class TestClass&amp;#010;&amp;#010;scala&gt; t: TestClass = TestClass@49b60fcd&amp;#010;&amp;#010;scala&gt; import t.testMethod&amp;#010;&amp;#010;scala&gt; defined class TestCaseClass&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:31: error: not found: value sc&amp;#010;       sc.parallelize(1 to 10).map(x =&gt; TestCaseClass(x)).collect()&amp;#010;       ^&amp;#010;&amp;#010;scala&gt; </li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; defined class Foo&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:28: error: not found: value sc&amp;#010;       val res = sc.parallelize((1 to 100).map(Foo), 10).collect()&amp;#010;                 ^&amp;#010;&amp;#010;scala&gt;      | _result_1542234556468: Int = 1&amp;#010;&amp;#010;scala&gt; </li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; defined class Foo&amp;#010;&amp;#010;scala&gt; list: List[(Int, Foo)] = List((1,Foo(1)), (1,Foo(2)))&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:28: error: not found: value sc&amp;#010;       val res = sc.parallelize(list).groupByKey().collect()&amp;#010;                 ^&amp;#010;&amp;#010;scala&gt;      | _result_1542234556836: Int = 1&amp;#010;&amp;#010;scala&gt; </li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; timeout: Int = 60000&amp;#010;&amp;#010;scala&gt; start: Long = 1542234558124&amp;#010;&amp;#010;scala&gt;      |      |      | &lt;console&gt;:31: error: not found: value sc&amp;#010;       while(sc.statusTracker.getExecutorInfos.size != 3 &amp;&amp;&amp;#010;             ^&amp;#010;&amp;#010;scala&gt;      |      | &amp;#010;scala&gt; import org.apache.spark.storage.StorageLevel._&amp;</li></div><div><li>isContain was true Interpreter output contained 'error:':&amp;#010;&amp;#010;scala&gt; defined class Click&amp;#010;&amp;#010;scala&gt; &lt;console&gt;:33: error: not found: value spark&amp;#010;       spark.implicits.newProductSeqEncoder[Click]&amp;#010;       ^&amp;#010;&amp;#010;scala&gt;      | _result_1542234559881: Int = 1&amp;#010;&amp;#010;scala&gt; </li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)
test.org.apache.spark.sql.JavaApplySchemaSuite.setUp(JavaApplySchemaSuite.java:54)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect</li></div><div><li>Error while instantiating 'org.apache.spark.sql.internal.SessionStateBuilder':</li></div><div><li>Error while instantiating 'org.apache.spark.sql.internal.SessionStateBuilder':</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaBeanDeserializationSuite.tearDown(JavaBeanDeserializationSuite.java:41)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaBeanDeserializationSuite.tearDown(JavaBeanDeserializationSuite.java:41)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaColumnExpressionSuite.tearDown(JavaColumnExpressionSuite.java:46)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaColumnExpressionSuite.tearDown(JavaColumnExpressionSuite.java:46)
</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>RpcEnv has been stopped</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.&lt;init&gt;(JavaDataFrameReaderWriterSuite.java:33)
sun.reflect.NativeConstructorAcces</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDataFrameSuite.tearDown(JavaDataFrameSuite.java:61)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaDatasetSuite.tearDown(JavaDatasetSuite.java:64)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaSaveLoadSuite.tearDown(JavaSaveLoadSuite.java:76)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaSaveLoadSuite.tearDown(JavaSaveLoadSuite.java:76)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaUDAFSuite.tearDown(JavaUDAFSuite.java:42)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaUDFSuite.tearDown(JavaUDFSuite.java:50)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaUDFSuite.tearDown(JavaUDFSuite.java:50)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaUDFSuite.tearDown(JavaUDFSuite.java:50)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaUDFSuite.tearDown(JavaUDFSuite.java:50)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaUDFSuite.tearDown(JavaUDFSuite.java:50)
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.JavaUDFSuite.tearDown(JavaUDFSuite.java:50)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.tearDown(JavaDataStreamReaderWriterSuite.java:49)
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.test.TestSparkSession.&lt;init&gt;(TestSQLContext.scala:34)
test.org.apache.spark.sql.JavaBeanDeserializationSuite.setUp(JavaBeanDeserializationSuite.java:36)
sun.reflect.NativeMethodAccessorImpl.in</li></div><div><li>java.lang.NullPointerException
	at test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.tearDown(JavaDataStreamReaderWriterSuite.java:49)
</li></div><div><li>Cannot call methods on a stopped SparkContext.&amp;#010;This stopped SparkContext was created at:&amp;#010;&amp;#010;org.apache.spark.sql.hive.HiveMetastoreLazyInitializationSuite$$anonfun$1.apply(HiveMetastoreLazyInitializationSuite.scala:32)&amp;#010;org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)&amp;#010;org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)&amp;#010;org.scalatest.Transformer.apply(Trans</li></div><div><li>java.lang.ExceptionInInitializerError
	at org.apache.spark.sql.hive.JavaDataFrameSuite.setUp(JavaDataFrameSuite.java:50)
Caused by: java.lang.IllegalStateException: 
Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.hive.JavaDataFrameSuite.setUp(JavaDataFrameSuite.java:50)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.r</li></div><div><li>Could not initialize class org.apache.spark.sql.hive.test.TestHive$</li></div><div><li>Could not initialize class org.apache.spark.sql.hive.test.TestHive$</li></div><div><li>Could not initialize class org.apache.spark.sql.hive.test.TestHive$</li></div><div><li>Could not initialize class org.apache.spark.sql.hive.test.TestHive$</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(StreamingContext.scala:85)
org.apache.spark.streaming.api.java.JavaStreamingContext.&lt;init&gt;(JavaStreaming</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>LiveListenerBus is stopped.</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(StreamingContext.scala:85)
org.apache.spark.streaming.api.java.JavaStreamingContext.&lt;init&gt;(JavaStreaming</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(StreamingContext.scala:85)
org.apache.spark.streaming.api.java.JavaStreamingContext.&lt;init&gt;(JavaStreaming</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div><div><li>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76)
org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:838)
org.apache.spark.streaming.StreamingContext.&lt;init&gt;(Streami</li></div><div><li>java.lang.NullPointerException
</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Could not load library. Reasons: [no leveldbjni64-1.8 in java.library.path, no leveldbjni-1.8 in java.library.path, no leveldbjni in java.library.path, /var/lib/jenkins/workspace/spark/common/kvstore/target/tmp/libleveldbjni-64-1-7668472912866049532.8: /var/lib/jenkins/workspace/spark/common/kvstore/target/tmp/libleveldbjni-64-1-7668472912866049532.8: cannot open shared object file: No such file o</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>Query memory [id = 2b81c0ff-1126-4454-9664-932d3f2f5960, runId = ae050588-6895-4310-a81e-6d16b0b512c3] terminated with exception: null</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.spark.sql.execution.ui.SQLAppStatusListenerSuite.driver side SQL metrics</div></li><li><div>org.apache.spark.sql.streaming.StreamingQueryListenerSuite.single listener, check trigger events are generated correctly</div></li><li><div>org.apache.spark.sql.hive.HiveSparkSubmitSuite.SPARK-8020: set sql conf in spark conf</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.spark.network.ChunkFetchIntegrationSuite.fetchFileChunk</div></li><li><div>org.apache.spark.JavaJdbcRDDSuite.testJavaJdbcRDD</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.leftOuterJoin</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.leftOuterJoin</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.foldReduce</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.foldReduce</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.mapsFromPairsToPairs</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.mapsFromPairsToPairs</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.flatMap</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.flatMap</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.foreach</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.foreach</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.map</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.map</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.zip</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.zip</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.keyBy</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.keyBy</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.groupBy</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.mapPartitions</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.foldByKey</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.mapOnPairRDD</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.sequenceFile</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.sequenceFile</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.collectPartitions</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.collectPartitions</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.reduceByKey</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.reduceByKey</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.foreachWithAnonymousClass</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.foreachWithAnonymousClass</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.collectAsMapWithIntArrayValues</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.collectAsMapWithIntArrayValues</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.zipPartitions</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.zipPartitions</div></li><li><div>test.org.apache.spark.JavaAPISuite.groupByOnPairRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.groupByOnPairRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.binaryFilesCaching</div></li><li><div>test.org.apache.spark.JavaAPISuite.binaryFilesCaching</div></li><li><div>test.org.apache.spark.JavaAPISuite.sparkContextUnion</div></li><li><div>test.org.apache.spark.JavaAPISuite.sparkContextUnion</div></li><li><div>test.org.apache.spark.JavaAPISuite.checkpointAndComputation</div></li><li><div>test.org.apache.spark.JavaAPISuite.checkpointAndComputation</div></li><li><div>test.org.apache.spark.JavaAPISuite.leftOuterJoin</div></li><li><div>test.org.apache.spark.JavaAPISuite.leftOuterJoin</div></li><li><div>test.org.apache.spark.JavaAPISuite.keyByOnPairRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.keyByOnPairRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.getNumPartitions</div></li><li><div>test.org.apache.spark.JavaAPISuite.wholeTextFiles</div></li><li><div>test.org.apache.spark.JavaAPISuite.wholeTextFiles</div></li><li><div>test.org.apache.spark.JavaAPISuite.binaryFiles</div></li><li><div>test.org.apache.spark.JavaAPISuite.binaryFiles</div></li><li><div>test.org.apache.spark.JavaAPISuite.foldReduce</div></li><li><div>test.org.apache.spark.JavaAPISuite.foldReduce</div></li><li><div>test.org.apache.spark.JavaAPISuite.writeWithNewAPIHadoopFile</div></li><li><div>test.org.apache.spark.JavaAPISuite.writeWithNewAPIHadoopFile</div></li><li><div>test.org.apache.spark.JavaAPISuite.hadoopFile</div></li><li><div>test.org.apache.spark.JavaAPISuite.hadoopFile</div></li><li><div>test.org.apache.spark.JavaAPISuite.lookup</div></li><li><div>test.org.apache.spark.JavaAPISuite.lookup</div></li><li><div>test.org.apache.spark.JavaAPISuite.countAsync</div></li><li><div>test.org.apache.spark.JavaAPISuite.countAsync</div></li><li><div>test.org.apache.spark.JavaAPISuite.textFiles</div></li><li><div>test.org.apache.spark.JavaAPISuite.textFiles</div></li><li><div>test.org.apache.spark.JavaAPISuite.binaryRecords</div></li><li><div>test.org.apache.spark.JavaAPISuite.binaryRecords</div></li><li><div>test.org.apache.spark.JavaAPISuite.toLocalIterator</div></li><li><div>test.org.apache.spark.JavaAPISuite.toLocalIterator</div></li><li><div>test.org.apache.spark.JavaAPISuite.repartitionAndSortWithinPartitions</div></li><li><div>test.org.apache.spark.JavaAPISuite.repartitionAndSortWithinPartitions</div></li><li><div>test.org.apache.spark.JavaAPISuite.reduce</div></li><li><div>test.org.apache.spark.JavaAPISuite.reduce</div></li><li><div>test.org.apache.spark.JavaAPISuite.sample</div></li><li><div>test.org.apache.spark.JavaAPISuite.sample</div></li><li><div>test.org.apache.spark.JavaAPISuite.sortBy</div></li><li><div>test.org.apache.spark.JavaAPISuite.sortBy</div></li><li><div>test.org.apache.spark.JavaAPISuite.mapsFromPairsToPairs</div></li><li><div>test.org.apache.spark.JavaAPISuite.mapsFromPairsToPairs</div></li><li><div>test.org.apache.spark.JavaAPISuite.flatMap</div></li><li><div>test.org.apache.spark.JavaAPISuite.flatMap</div></li><li><div>test.org.apache.spark.JavaAPISuite.cogroup3</div></li><li><div>test.org.apache.spark.JavaAPISuite.cogroup3</div></li><li><div>test.org.apache.spark.JavaAPISuite.cogroup4</div></li><li><div>test.org.apache.spark.JavaAPISuite.cogroup4</div></li><li><div>test.org.apache.spark.JavaAPISuite.randomSplit</div></li><li><div>test.org.apache.spark.JavaAPISuite.randomSplit</div></li><li><div>test.org.apache.spark.JavaAPISuite.persist</div></li><li><div>test.org.apache.spark.JavaAPISuite.foreach</div></li><li><div>test.org.apache.spark.JavaAPISuite.foreach</div></li><li><div>test.org.apache.spark.JavaAPISuite.hadoopFileCompressed</div></li><li><div>test.org.apache.spark.JavaAPISuite.hadoopFileCompressed</div></li><li><div>test.org.apache.spark.JavaAPISuite.textFilesCompressed</div></li><li><div>test.org.apache.spark.JavaAPISuite.textFilesCompressed</div></li><li><div>test.org.apache.spark.JavaAPISuite.testAsyncActionCancellation</div></li><li><div>test.org.apache.spark.JavaAPISuite.testAsyncActionCancellation</div></li><li><div>test.org.apache.spark.JavaAPISuite.checkpointAndRestore</div></li><li><div>test.org.apache.spark.JavaAPISuite.sortByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.aggregateByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.aggregateByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.map</div></li><li><div>test.org.apache.spark.JavaAPISuite.max</div></li><li><div>test.org.apache.spark.JavaAPISuite.max</div></li><li><div>test.org.apache.spark.JavaAPISuite.min</div></li><li><div>test.org.apache.spark.JavaAPISuite.min</div></li><li><div>test.org.apache.spark.JavaAPISuite.top</div></li><li><div>test.org.apache.spark.JavaAPISuite.top</div></li><li><div>test.org.apache.spark.JavaAPISuite.zip</div></li><li><div>test.org.apache.spark.JavaAPISuite.zip</div></li><li><div>test.org.apache.spark.JavaAPISuite.fold</div></li><li><div>test.org.apache.spark.JavaAPISuite.fold</div></li><li><div>test.org.apache.spark.JavaAPISuite.glom</div></li><li><div>test.org.apache.spark.JavaAPISuite.glom</div></li><li><div>test.org.apache.spark.JavaAPISuite.take</div></li><li><div>test.org.apache.spark.JavaAPISuite.take</div></li><li><div>test.org.apache.spark.JavaAPISuite.javaDoubleRDDHistoGram</div></li><li><div>test.org.apache.spark.JavaAPISuite.javaDoubleRDDHistoGram</div></li><li><div>test.org.apache.spark.JavaAPISuite.collectUnderlyingScalaRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.collectUnderlyingScalaRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.keyBy</div></li><li><div>test.org.apache.spark.JavaAPISuite.keyBy</div></li><li><div>test.org.apache.spark.JavaAPISuite.mapPartitionsWithIndex</div></li><li><div>test.org.apache.spark.JavaAPISuite.mapPartitionsWithIndex</div></li><li><div>test.org.apache.spark.JavaAPISuite.sampleByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.sampleByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.intersection</div></li><li><div>test.org.apache.spark.JavaAPISuite.intersection</div></li><li><div>test.org.apache.spark.JavaAPISuite.aggregate</div></li><li><div>test.org.apache.spark.JavaAPISuite.aggregate</div></li><li><div>test.org.apache.spark.JavaAPISuite.cartesian</div></li><li><div>test.org.apache.spark.JavaAPISuite.cartesian</div></li><li><div>test.org.apache.spark.JavaAPISuite.countApproxDistinctByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.countApproxDistinctByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.readWithNewAPIHadoopFile</div></li><li><div>test.org.apache.spark.JavaAPISuite.readWithNewAPIHadoopFile</div></li><li><div>test.org.apache.spark.JavaAPISuite.testRegisterKryoClasses</div></li><li><div>test.org.apache.spark.JavaAPISuite.testRegisterKryoClasses</div></li><li><div>test.org.apache.spark.JavaAPISuite.groupBy</div></li><li><div>test.org.apache.spark.JavaAPISuite.groupBy</div></li><li><div>test.org.apache.spark.JavaAPISuite.sampleByKeyExact</div></li><li><div>test.org.apache.spark.JavaAPISuite.mapPartitions</div></li><li><div>test.org.apache.spark.JavaAPISuite.takeOrdered</div></li><li><div>test.org.apache.spark.JavaAPISuite.takeOrdered</div></li><li><div>test.org.apache.spark.JavaAPISuite.foldByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.foldByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.objectFilesOfInts</div></li><li><div>test.org.apache.spark.JavaAPISuite.objectFilesOfInts</div></li><li><div>test.org.apache.spark.JavaAPISuite.treeAggregate</div></li><li><div>test.org.apache.spark.JavaAPISuite.treeAggregate</div></li><li><div>test.org.apache.spark.JavaAPISuite.testGetPersistentRDDs</div></li><li><div>test.org.apache.spark.JavaAPISuite.testGetPersistentRDDs</div></li><li><div>test.org.apache.spark.JavaAPISuite.approximateResults</div></li><li><div>test.org.apache.spark.JavaAPISuite.approximateResults</div></li><li><div>test.org.apache.spark.JavaAPISuite.treeReduce</div></li><li><div>test.org.apache.spark.JavaAPISuite.treeReduce</div></li><li><div>test.org.apache.spark.JavaAPISuite.collectAsMapAndSerialize</div></li><li><div>test.org.apache.spark.JavaAPISuite.collectAsMapAndSerialize</div></li><li><div>test.org.apache.spark.JavaAPISuite.countApproxDistinct</div></li><li><div>test.org.apache.spark.JavaAPISuite.countApproxDistinct</div></li><li><div>test.org.apache.spark.JavaAPISuite.javaDoubleRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.javaDoubleRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.mapOnPairRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.mapOnPairRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.testAsyncActionErrorWrapping</div></li><li><div>test.org.apache.spark.JavaAPISuite.testAsyncActionErrorWrapping</div></li><li><div>test.org.apache.spark.JavaAPISuite.naturalMax</div></li><li><div>test.org.apache.spark.JavaAPISuite.naturalMax</div></li><li><div>test.org.apache.spark.JavaAPISuite.naturalMin</div></li><li><div>test.org.apache.spark.JavaAPISuite.naturalMin</div></li><li><div>test.org.apache.spark.JavaAPISuite.sequenceFile</div></li><li><div>test.org.apache.spark.JavaAPISuite.sequenceFile</div></li><li><div>test.org.apache.spark.JavaAPISuite.collectPartitions</div></li><li><div>test.org.apache.spark.JavaAPISuite.collectPartitions</div></li><li><div>test.org.apache.spark.JavaAPISuite.cogroup</div></li><li><div>test.org.apache.spark.JavaAPISuite.cogroup</div></li><li><div>test.org.apache.spark.JavaAPISuite.reduceByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.reduceByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.repartition</div></li><li><div>test.org.apache.spark.JavaAPISuite.repartition</div></li><li><div>test.org.apache.spark.JavaAPISuite.iterator</div></li><li><div>test.org.apache.spark.JavaAPISuite.iterator</div></li><li><div>test.org.apache.spark.JavaAPISuite.emptyRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.emptyRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.zipWithIndex</div></li><li><div>test.org.apache.spark.JavaAPISuite.zipWithIndex</div></li><li><div>test.org.apache.spark.JavaAPISuite.foreachPartition</div></li><li><div>test.org.apache.spark.JavaAPISuite.foreachPartition</div></li><li><div>test.org.apache.spark.JavaAPISuite.combineByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.combineByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.takeAsync</div></li><li><div>test.org.apache.spark.JavaAPISuite.takeAsync</div></li><li><div>test.org.apache.spark.JavaAPISuite.collectAsMapWithIntArrayValues</div></li><li><div>test.org.apache.spark.JavaAPISuite.collectAsMapWithIntArrayValues</div></li><li><div>test.org.apache.spark.JavaAPISuite.objectFilesOfComplexTypes</div></li><li><div>test.org.apache.spark.JavaAPISuite.objectFilesOfComplexTypes</div></li><li><div>test.org.apache.spark.JavaAPISuite.zipWithUniqueId</div></li><li><div>test.org.apache.spark.JavaAPISuite.zipWithUniqueId</div></li><li><div>test.org.apache.spark.JavaAPISuite.collectAsync</div></li><li><div>test.org.apache.spark.JavaAPISuite.collectAsync</div></li><li><div>test.org.apache.spark.JavaAPISuite.foreachAsync</div></li><li><div>test.org.apache.spark.JavaAPISuite.foreachAsync</div></li><li><div>test.org.apache.spark.JavaAPISuite.zipPartitions</div></li><li><div>test.org.apache.spark.JavaAPISuite.zipPartitions</div></li><li><div>test.org.apache.spark.JavaAPISuite.reduceOnJavaDoubleRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.reduceOnJavaDoubleRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.isEmpty</div></li><li><div>test.org.apache.spark.JavaAPISuite.isEmpty</div></li><li><div>test.org.apache.spark.JavaSparkContextSuite.javaSparkContext</div></li><li><div>test.org.apache.spark.JavaSparkContextSuite.scalaSparkContext</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single BooleanType with seed 7308411183748884782</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single ByteType with seed 4446789926156407554</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single ShortType with seed -7338248970079421220</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single IntegerType with seed 1888812313568949446</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single LongType with seed 7853185275645657472</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single FloatType with seed 7319090749879310724</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single DoubleType with seed 6958769278960946522</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single DecimalType(8,0) with seed 5618052675792332838</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single DecimalType(8,4) with seed 7947691028072466321</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single DecimalType(16,0) with seed -7612334778196347769</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single DecimalType(16,11) with seed -3121050904531359657</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single DecimalType(38,0) with seed 5099428991776716343</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single DecimalType(38,38) with seed -2751330306731763895</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single StringType with seed 7066602185030133631</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single BinaryType with seed -8343309833225958373</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.flat schema struct&lt;col_0:string,col_1:decimal(38,38),col_2:binary,col_3:boolean,col_4:binary&gt; with seed 7389130032968028322</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.flat schema struct&lt;col_0:double,col_1:smallint,col_2:double,col_3:boolean,col_4:decimal(16,0)&gt; with seed 8224834773586957684</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.flat schema struct&lt;col_0:int,col_1:boolean,col_2:int,col_3:binary,col_4:decimal(8,4)&gt; with seed 3547253314093384550</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.flat schema struct&lt;col_0:boolean,col_1:bigint,col_2:decimal(8,4),col_3:double,col_4:double&gt; with seed -4492483418035148266</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.flat schema struct&lt;col_0:decimal(8,0),col_1:smallint,col_2:decimal(16,0),col_3:decimal(38,0),col_4:decimal(16,0)&gt; with seed -7421572993677096238</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.nested schema struct&lt;col_0:array&lt;string&gt;,col_1:struct&lt;col_0:array&lt;int&gt;,col_1:array&lt;boolean&gt;,col_2:decimal(38,38)&gt;,col_2:array&lt;smallint&gt;,col_3:array&lt;decimal(8,0)&gt;,col_4:struct&lt;col_0:decimal(16,11)&gt;,col_5:struct&lt;col_0:array&lt;decimal(8,4)&gt;&gt;,col_6:array&lt;smallint&gt;,col_7:array&lt;decimal(16,0)&gt;&gt; with seed -3844835813531326399</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.nested schema struct&lt;col_0:boolean,col_1:array&lt;float&gt;,col_2:smallint,col_3:decimal(38,0),col_4:decimal(16,0),col_5:struct&lt;col_0:smallint&gt;,col_6:array&lt;binary&gt;,col_7:array&lt;decimal(8,0)&gt;,col_8:array&lt;smallint&gt;,col_9:float&gt; with seed -8130169798711733448</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.nested schema struct&lt;col_0:bigint,col_1:struct&lt;col_0:array&lt;bigint&gt;,col_1:struct&lt;col_0:array&lt;binary&gt;&gt;,col_2:struct&lt;col_0:array&lt;decimal(16,0)&gt;&gt;&gt;,col_2:tinyint,col_3:struct&lt;col_0:array&lt;decimal(16,0)&gt;&gt;,col_4:array&lt;decimal(8,0)&gt;,col_5:struct&lt;col_0:array&lt;tinyint&gt;,col_1:struct&lt;col_0:struct&lt;col_0:array&lt;decimal(38,38)&gt;&gt;&gt;&gt;,col_6:array&lt;decimal(38,0)&gt;&gt; with seed 206193538942822233</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.nested schema struct&lt;col_0:struct&lt;col_0:float,col_1:array&lt;decimal(16,11)&gt;&gt;,col_1:array&lt;decimal(8,4)&gt;,col_2:struct&lt;col_0:struct&lt;col_0:struct&lt;col_0:int,col_1:array&lt;smallint&gt;&gt;,col_1:array&lt;decimal(38,38)&gt;,col_2:struct&lt;col_0:struct&lt;col_0:boolean&gt;&gt;&gt;,col_1:struct&lt;col_0:array&lt;bigint&gt;&gt;&gt;,col_3:struct&lt;col_0:struct&lt;col_0:struct&lt;col_0:struct&lt;col_0:struct&lt;col_0:array&lt;decimal(8,0)&gt;&gt;&gt;&gt;&gt;&gt;,col_4:struct&lt;col_0:array&lt;boolean&gt;&gt;&gt; with seed 7821504959510597237</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.nested schema struct&lt;col_0:array&lt;tinyint&gt;,col_1:boolean,col_2:array&lt;smallint&gt;,col_3:tinyint,col_4:array&lt;decimal(8,0)&gt;,col_5:struct&lt;col_0:decimal(16,11)&gt;,col_6:array&lt;float&gt;,col_7:array&lt;decimal(16,0)&gt;,col_8:array&lt;double&gt;,col_9:array&lt;string&gt;&gt; with seed -1627446941656039482</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.read int as string</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.read string as int</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.read float as double</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.read double as float</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.Handle unsupported input of record type</div></li><li><div>org.apache.spark.sql.avro.AvroFunctionsSuite.roundtrip in to_avro and from_avro - int and string</div></li><li><div>org.apache.spark.sql.avro.AvroFunctionsSuite.roundtrip in to_avro and from_avro - struct</div></li><li><div>org.apache.spark.sql.avro.AvroFunctionsSuite.handle invalid input in from_avro</div></li><li><div>org.apache.spark.sql.avro.AvroFunctionsSuite.roundtrip in to_avro and from_avro - array with null</div></li><li><div>org.apache.spark.streaming.kafka010.JavaDirectKafkaStreamSuite.testKafkaStream</div></li><li><div>org.apache.spark.streaming.kafka010.JavaKafkaRDDSuite.testKafkaRDD</div></li><li><div>org.apache.spark.graphx.EdgeRDDSuite.cache, getStorageLevel</div></li><li><div>org.apache.spark.graphx.EdgeRDDSuite.checkpointing</div></li><li><div>org.apache.spark.graphx.GraphLoaderSuite.GraphLoader.edgeListFile</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.joinVertices</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.collectNeighborIds</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.removeSelfEdges</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.filter</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.convertToCanonicalEdges</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.collectEdgesCycleDirectionOut</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.collectEdgesCycleDirectionIn</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.collectEdgesCycleDirectionEither</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.collectEdgesChainDirectionOut</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.collectEdgesChainDirectionIn</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.collectEdgesChainDirectionEither</div></li><li><div>org.apache.spark.graphx.GraphSuite.Graph.fromEdgeTuples</div></li><li><div>org.apache.spark.graphx.GraphSuite.Graph.fromEdges</div></li><li><div>org.apache.spark.graphx.GraphSuite.Graph.apply</div></li><li><div>org.apache.spark.graphx.GraphSuite.triplets</div></li><li><div>org.apache.spark.graphx.GraphSuite.partitionBy</div></li><li><div>org.apache.spark.graphx.GraphSuite.mapVertices</div></li><li><div>org.apache.spark.graphx.GraphSuite.mapVertices changing type with same erased type</div></li><li><div>org.apache.spark.graphx.GraphSuite.mapEdges</div></li><li><div>org.apache.spark.graphx.GraphSuite.mapTriplets</div></li><li><div>org.apache.spark.graphx.GraphSuite.reverse</div></li><li><div>org.apache.spark.graphx.GraphSuite.reverse with join elimination</div></li><li><div>org.apache.spark.graphx.GraphSuite.subgraph</div></li><li><div>org.apache.spark.graphx.GraphSuite.mask</div></li><li><div>org.apache.spark.graphx.GraphSuite.groupEdges</div></li><li><div>org.apache.spark.graphx.GraphSuite.aggregateMessages</div></li><li><div>org.apache.spark.graphx.GraphSuite.outerJoinVertices</div></li><li><div>org.apache.spark.graphx.GraphSuite.more edge partitions than vertex partitions</div></li><li><div>org.apache.spark.graphx.GraphSuite.checkpoint</div></li><li><div>org.apache.spark.graphx.GraphSuite.cache, getStorageLevel</div></li><li><div>org.apache.spark.graphx.GraphSuite.non-default number of edge partitions</div></li><li><div>org.apache.spark.graphx.GraphSuite.unpersist graph RDD</div></li><li><div>org.apache.spark.graphx.GraphSuite.SPARK-14219: pickRandomVertex</div></li><li><div>org.apache.spark.graphx.PregelSuite.1 iteration</div></li><li><div>org.apache.spark.graphx.PregelSuite.chain propagation</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.filter</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.mapValues</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.minus</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.minus with RDD[(VertexId, VD)]</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.minus with non-equal number of partitions</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.diff</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.diff with RDD[(VertexId, VD)]</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.diff vertices with non-equal number of partitions</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.leftJoin</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.leftJoin vertices with non-equal number of partitions</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.innerJoin</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.innerJoin vertices with the non-equal number of partitions</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.aggregateUsingIndex</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.mergeFunc</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.cache, getStorageLevel</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.checkpoint</div></li><li><div>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Grid Connected Components</div></li><li><div>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Reverse Grid Connected Components</div></li><li><div>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Chain Connected Components</div></li><li><div>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Reverse Chain Connected Components</div></li><li><div>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Connected Components on a Toy Connected Graph</div></li><li><div>org.apache.spark.graphx.lib.LabelPropagationSuite.Label Propagation</div></li><li><div>org.apache.spark.graphx.lib.PageRankSuite.Star PageRank</div></li><li><div>org.apache.spark.graphx.lib.PageRankSuite.Star PersonalPageRank</div></li><li><div>org.apache.spark.graphx.lib.PageRankSuite.Grid PageRank</div></li><li><div>org.apache.spark.graphx.lib.PageRankSuite.Chain PageRank</div></li><li><div>org.apache.spark.graphx.lib.PageRankSuite.Chain PersonalizedPageRank</div></li><li><div>org.apache.spark.graphx.lib.PageRankSuite.Loop with source PageRank</div></li><li><div>org.apache.spark.graphx.lib.PageRankSuite.Loop with sink PageRank</div></li><li><div>org.apache.spark.graphx.lib.SVDPlusPlusSuite.Test SVD++ with mean square error on training set</div></li><li><div>org.apache.spark.graphx.lib.ShortestPathsSuite.Shortest Path Computations</div></li><li><div>org.apache.spark.graphx.lib.StronglyConnectedComponentsSuite.Island Strongly Connected Components</div></li><li><div>org.apache.spark.graphx.lib.StronglyConnectedComponentsSuite.Cycle Strongly Connected Components</div></li><li><div>org.apache.spark.graphx.lib.StronglyConnectedComponentsSuite.2 Cycle Strongly Connected Components</div></li><li><div>org.apache.spark.graphx.lib.TriangleCountSuite.Count a single triangle</div></li><li><div>org.apache.spark.graphx.lib.TriangleCountSuite.Count two triangles</div></li><li><div>org.apache.spark.graphx.lib.TriangleCountSuite.Count two triangles with bi-directed edges</div></li><li><div>org.apache.spark.graphx.lib.TriangleCountSuite.Count a single triangle with duplicate edges</div></li><li><div>org.apache.spark.graphx.util.GraphGeneratorsSuite.GraphGenerators.logNormalGraph</div></li><li><div>org.apache.spark.graphx.util.GraphGeneratorsSuite.SPARK-5064 GraphGenerators.rmatGraph numEdges upper bound</div></li><li><div>org.apache.spark.graphx.util.PeriodicGraphCheckpointerSuite.Persisting</div></li><li><div>org.apache.spark.graphx.util.PeriodicGraphCheckpointerSuite.Checkpointing</div></li><li><div>org.apache.spark.ml.JavaPipelineSuite.pipeline</div></li><li><div>org.apache.spark.ml.JavaPipelineSuite.pipeline</div></li><li><div>org.apache.spark.ml.classification.JavaDecisionTreeClassifierSuite.runDT</div></li><li><div>org.apache.spark.ml.classification.JavaDecisionTreeClassifierSuite.runDT</div></li><li><div>org.apache.spark.ml.classification.JavaGBTClassifierSuite.runDT</div></li><li><div>org.apache.spark.ml.classification.JavaGBTClassifierSuite.runDT</div></li><li><div>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionWithSetters</div></li><li><div>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionWithSetters</div></li><li><div>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionTrainingSummary</div></li><li><div>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionTrainingSummary</div></li><li><div>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionPredictorClassifierMethods</div></li><li><div>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionPredictorClassifierMethods</div></li><li><div>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionDefaultParams</div></li><li><div>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionDefaultParams</div></li><li><div>org.apache.spark.ml.classification.JavaMultilayerPerceptronClassifierSuite.testMLPC</div></li><li><div>org.apache.spark.ml.classification.JavaMultilayerPerceptronClassifierSuite.testMLPC</div></li><li><div>org.apache.spark.ml.classification.JavaNaiveBayesSuite.testNaiveBayes</div></li><li><div>org.apache.spark.ml.classification.JavaNaiveBayesSuite.naiveBayesDefaultParams</div></li><li><div>org.apache.spark.ml.classification.JavaNaiveBayesSuite.naiveBayesDefaultParams</div></li><li><div>org.apache.spark.ml.classification.JavaOneVsRestSuite.oneVsRestDefaultParams</div></li><li><div>org.apache.spark.ml.classification.JavaOneVsRestSuite.oneVsRestDefaultParams</div></li><li><div>org.apache.spark.ml.classification.JavaRandomForestClassifierSuite.runDT</div></li><li><div>org.apache.spark.ml.classification.JavaRandomForestClassifierSuite.runDT</div></li><li><div>org.apache.spark.ml.clustering.JavaKMeansSuite.fitAndTransform</div></li><li><div>org.apache.spark.ml.feature.JavaBucketizerSuite.bucketizerTest</div></li><li><div>org.apache.spark.ml.feature.JavaBucketizerSuite.bucketizerTest</div></li><li><div>org.apache.spark.ml.feature.JavaBucketizerSuite.bucketizerMultipleColumnsTest</div></li><li><div>org.apache.spark.ml.feature.JavaBucketizerSuite.bucketizerMultipleColumnsTest</div></li><li><div>org.apache.spark.ml.feature.JavaDCTSuite.javaCompatibilityTest</div></li><li><div>org.apache.spark.ml.feature.JavaHashingTFSuite.hashingTF</div></li><li><div>org.apache.spark.ml.feature.JavaHashingTFSuite.hashingTF</div></li><li><div>org.apache.spark.ml.feature.JavaNormalizerSuite.normalizer</div></li><li><div>org.apache.spark.ml.feature.JavaNormalizerSuite.normalizer</div></li><li><div>org.apache.spark.ml.feature.JavaPCASuite.testPCA</div></li><li><div>org.apache.spark.ml.feature.JavaPCASuite.testPCA</div></li><li><div>org.apache.spark.ml.feature.JavaPolynomialExpansionSuite.polynomialExpansionTest</div></li><li><div>org.apache.spark.ml.feature.JavaPolynomialExpansionSuite.polynomialExpansionTest</div></li><li><div>org.apache.spark.ml.feature.JavaStandardScalerSuite.standardScaler</div></li><li><div>org.apache.spark.ml.feature.JavaStandardScalerSuite.standardScaler</div></li><li><div>org.apache.spark.ml.feature.JavaStopWordsRemoverSuite.javaCompatibilityTest</div></li><li><div>org.apache.spark.ml.feature.JavaStopWordsRemoverSuite.javaCompatibilityTest</div></li><li><div>org.apache.spark.ml.feature.JavaStringIndexerSuite.testStringIndexer</div></li><li><div>org.apache.spark.ml.feature.JavaStringIndexerSuite.testStringIndexer</div></li><li><div>org.apache.spark.ml.feature.JavaTokenizerSuite.regexTokenizer</div></li><li><div>org.apache.spark.ml.feature.JavaTokenizerSuite.regexTokenizer</div></li><li><div>org.apache.spark.ml.feature.JavaVectorAssemblerSuite.testVectorAssembler</div></li><li><div>org.apache.spark.ml.feature.JavaVectorAssemblerSuite.testVectorAssembler</div></li><li><div>org.apache.spark.ml.feature.JavaVectorIndexerSuite.vectorIndexerAPI</div></li><li><div>org.apache.spark.ml.feature.JavaVectorIndexerSuite.vectorIndexerAPI</div></li><li><div>org.apache.spark.ml.feature.JavaVectorSlicerSuite.vectorSlice</div></li><li><div>org.apache.spark.ml.feature.JavaVectorSlicerSuite.vectorSlice</div></li><li><div>org.apache.spark.ml.feature.JavaWord2VecSuite.testJavaWord2Vec</div></li><li><div>org.apache.spark.ml.regression.JavaDecisionTreeRegressorSuite.runDT</div></li><li><div>org.apache.spark.ml.regression.JavaDecisionTreeRegressorSuite.runDT</div></li><li><div>org.apache.spark.ml.regression.JavaGBTRegressorSuite.runDT</div></li><li><div>org.apache.spark.ml.regression.JavaGBTRegressorSuite.runDT</div></li><li><div>org.apache.spark.ml.regression.JavaLinearRegressionSuite.linearRegressionDefaultParams</div></li><li><div>org.apache.spark.ml.regression.JavaLinearRegressionSuite.linearRegressionDefaultParams</div></li><li><div>org.apache.spark.ml.regression.JavaLinearRegressionSuite.linearRegressionWithSetters</div></li><li><div>org.apache.spark.ml.regression.JavaLinearRegressionSuite.linearRegressionWithSetters</div></li><li><div>org.apache.spark.ml.regression.JavaRandomForestRegressorSuite.runDT</div></li><li><div>org.apache.spark.ml.regression.JavaRandomForestRegressorSuite.runDT</div></li><li><div>org.apache.spark.ml.source.libsvm.JavaLibSVMRelationSuite.verifyLibSVMDF</div></li><li><div>org.apache.spark.ml.stat.JavaKolmogorovSmirnovTestSuite.testKSTestNamedDistribution</div></li><li><div>org.apache.spark.ml.stat.JavaKolmogorovSmirnovTestSuite.testKSTestNamedDistribution</div></li><li><div>org.apache.spark.ml.stat.JavaKolmogorovSmirnovTestSuite.testKSTestCDF</div></li><li><div>org.apache.spark.ml.stat.JavaKolmogorovSmirnovTestSuite.testKSTestCDF</div></li><li><div>org.apache.spark.ml.stat.JavaSummarizerSuite.testSummarizer</div></li><li><div>org.apache.spark.ml.stat.JavaSummarizerSuite.testSummarizer</div></li><li><div>org.apache.spark.ml.tuning.JavaCrossValidatorSuite.crossValidationWithLogisticRegression</div></li><li><div>org.apache.spark.ml.util.JavaDefaultReadWriteSuite.testDefaultReadWrite</div></li><li><div>org.apache.spark.mllib.classification.JavaLogisticRegressionSuite.runLRUsingConstructor</div></li><li><div>org.apache.spark.mllib.classification.JavaLogisticRegressionSuite.runLRUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.classification.JavaLogisticRegressionSuite.runLRUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.testPredictJavaRDD</div></li><li><div>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.testPredictJavaRDD</div></li><li><div>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.runUsingConstructor</div></li><li><div>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.runUsingConstructor</div></li><li><div>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.runUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.runUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.testModelTypeSetters</div></li><li><div>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.testModelTypeSetters</div></li><li><div>org.apache.spark.mllib.classification.JavaSVMSuite.runSVMUsingConstructor</div></li><li><div>org.apache.spark.mllib.classification.JavaSVMSuite.runSVMUsingConstructor</div></li><li><div>org.apache.spark.mllib.classification.JavaSVMSuite.runSVMUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.classification.JavaSVMSuite.runSVMUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.classification.JavaStreamingLogisticRegressionSuite.javaAPI</div></li><li><div>org.apache.spark.mllib.classification.JavaStreamingLogisticRegressionSuite.javaAPI</div></li><li><div>org.apache.spark.mllib.clustering.JavaBisectingKMeansSuite.twoDimensionalData</div></li><li><div>org.apache.spark.mllib.clustering.JavaBisectingKMeansSuite.twoDimensionalData</div></li><li><div>org.apache.spark.mllib.clustering.JavaGaussianMixtureSuite.runGaussianMixture</div></li><li><div>org.apache.spark.mllib.clustering.JavaGaussianMixtureSuite.runGaussianMixture</div></li><li><div>org.apache.spark.mllib.clustering.JavaKMeansSuite.testPredictJavaRDD</div></li><li><div>org.apache.spark.mllib.clustering.JavaKMeansSuite.runKMeansUsingConstructor</div></li><li><div>org.apache.spark.mllib.clustering.JavaKMeansSuite.runKMeansUsingConstructor</div></li><li><div>org.apache.spark.mllib.clustering.JavaKMeansSuite.runKMeansUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.clustering.JavaKMeansSuite.runKMeansUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.clustering.JavaLDASuite.onlineOptimizerCompatibility</div></li><li><div>org.apache.spark.mllib.clustering.JavaLDASuite.onlineOptimizerCompatibility</div></li><li><div>org.apache.spark.mllib.clustering.JavaLDASuite.distributedLDAModel</div></li><li><div>org.apache.spark.mllib.clustering.JavaLDASuite.distributedLDAModel</div></li><li><div>org.apache.spark.mllib.clustering.JavaLDASuite.localLDAModel</div></li><li><div>org.apache.spark.mllib.clustering.JavaLDASuite.localLDAModel</div></li><li><div>org.apache.spark.mllib.clustering.JavaLDASuite.localLdaMethods</div></li><li><div>org.apache.spark.mllib.clustering.JavaLDASuite.localLdaMethods</div></li><li><div>org.apache.spark.mllib.clustering.JavaStreamingKMeansSuite.javaAPI</div></li><li><div>org.apache.spark.mllib.evaluation.JavaRankingMetricsSuite.rankingMetrics</div></li><li><div>org.apache.spark.mllib.evaluation.JavaRankingMetricsSuite.rankingMetrics</div></li><li><div>org.apache.spark.mllib.feature.JavaTfIdfSuite.tfIdfMinimumDocumentFrequency</div></li><li><div>org.apache.spark.mllib.feature.JavaTfIdfSuite.tfIdfMinimumDocumentFrequency</div></li><li><div>org.apache.spark.mllib.feature.JavaTfIdfSuite.tfIdf</div></li><li><div>org.apache.spark.mllib.feature.JavaTfIdfSuite.tfIdf</div></li><li><div>org.apache.spark.mllib.feature.JavaWord2VecSuite.word2Vec</div></li><li><div>org.apache.spark.mllib.feature.JavaWord2VecSuite.word2Vec</div></li><li><div>org.apache.spark.mllib.fpm.JavaAssociationRulesSuite.runAssociationRules</div></li><li><div>org.apache.spark.mllib.fpm.JavaFPGrowthSuite.runFPGrowthSaveLoad</div></li><li><div>org.apache.spark.mllib.fpm.JavaFPGrowthSuite.runFPGrowthSaveLoad</div></li><li><div>org.apache.spark.mllib.fpm.JavaFPGrowthSuite.runFPGrowth</div></li><li><div>org.apache.spark.mllib.fpm.JavaFPGrowthSuite.runFPGrowth</div></li><li><div>org.apache.spark.mllib.fpm.JavaPrefixSpanSuite.runPrefixSpan</div></li><li><div>org.apache.spark.mllib.fpm.JavaPrefixSpanSuite.runPrefixSpan</div></li><li><div>org.apache.spark.mllib.fpm.JavaPrefixSpanSuite.runPrefixSpanSaveLoad</div></li><li><div>org.apache.spark.mllib.fpm.JavaPrefixSpanSuite.runPrefixSpanSaveLoad</div></li><li><div>org.apache.spark.mllib.linalg.distributed.JavaRowMatrixSuite.rowMatrixQRDecomposition</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testNormalVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testNormalVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testArbitrary</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testArbitrary</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testLogNormalVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testLogNormalVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testExponentialVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testExponentialVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testUniformRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testUniformRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testRandomVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testRandomVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testGammaRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testGammaRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testUniformVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testUniformVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testPoissonRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testPoissonRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testNormalRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testNormalRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testPoissonVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testPoissonVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testGammaVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testGammaVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testExponentialRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testExponentialRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testLNormalRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testLNormalRDD</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runALSUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runALSUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSUsingConstructor</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSUsingConstructor</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runRecommend</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runRecommend</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSWithNegativeWeight</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSWithNegativeWeight</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runALSUsingConstructor</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runALSUsingConstructor</div></li><li><div>org.apache.spark.mllib.regression.JavaIsotonicRegressionSuite.testIsotonicRegressionJavaRDD</div></li><li><div>org.apache.spark.mllib.regression.JavaIsotonicRegressionSuite.testIsotonicRegressionJavaRDD</div></li><li><div>org.apache.spark.mllib.regression.JavaIsotonicRegressionSuite.testIsotonicRegressionPredictionsJavaRDD</div></li><li><div>org.apache.spark.mllib.regression.JavaIsotonicRegressionSuite.testIsotonicRegressionPredictionsJavaRDD</div></li><li><div>org.apache.spark.mllib.regression.JavaLassoSuite.runLassoUsingConstructor</div></li><li><div>org.apache.spark.mllib.regression.JavaLassoSuite.runLassoUsingConstructor</div></li><li><div>org.apache.spark.mllib.regression.JavaLassoSuite.runLassoUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.regression.JavaLassoSuite.runLassoUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.testPredictJavaRDD</div></li><li><div>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.testPredictJavaRDD</div></li><li><div>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.runLinearRegressionUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.runLinearRegressionUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.runLinearRegressionUsingConstructor</div></li><li><div>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.runLinearRegressionUsingConstructor</div></li><li><div>org.apache.spark.mllib.regression.JavaRidgeRegressionSuite.runRidgeRegressionUsingConstructor</div></li><li><div>org.apache.spark.mllib.regression.JavaRidgeRegressionSuite.runRidgeRegressionUsingConstructor</div></li><li><div>org.apache.spark.mllib.regression.JavaRidgeRegressionSuite.runRidgeRegressionUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.regression.JavaRidgeRegressionSuite.runRidgeRegressionUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.regression.JavaStreamingLinearRegressionSuite.javaAPI</div></li><li><div>org.apache.spark.mllib.regression.JavaStreamingLinearRegressionSuite.javaAPI</div></li><li><div>org.apache.spark.mllib.stat.JavaStatisticsSuite.testCorr</div></li><li><div>org.apache.spark.mllib.stat.JavaStatisticsSuite.testCorr</div></li><li><div>org.apache.spark.mllib.stat.JavaStatisticsSuite.chiSqTest</div></li><li><div>org.apache.spark.mllib.stat.JavaStatisticsSuite.chiSqTest</div></li><li><div>org.apache.spark.mllib.stat.JavaStatisticsSuite.streamingTest</div></li><li><div>org.apache.spark.mllib.stat.JavaStatisticsSuite.streamingTest</div></li><li><div>org.apache.spark.mllib.stat.JavaStatisticsSuite.kolmogorovSmirnovTest</div></li><li><div>org.apache.spark.mllib.stat.JavaStatisticsSuite.kolmogorovSmirnovTest</div></li><li><div>org.apache.spark.mllib.tree.JavaDecisionTreeSuite.runDTUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.tree.JavaDecisionTreeSuite.runDTUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.tree.JavaDecisionTreeSuite.runDTUsingConstructor</div></li><li><div>org.apache.spark.mllib.tree.JavaDecisionTreeSuite.runDTUsingConstructor</div></li><li><div>org.apache.spark.mllib.util.JavaMLUtilsSuite.testConvertMatrixColumnsToAndFromML</div></li><li><div>org.apache.spark.mllib.util.JavaMLUtilsSuite.testConvertMatrixColumnsToAndFromML</div></li><li><div>org.apache.spark.mllib.util.JavaMLUtilsSuite.testConvertVectorColumnsToAndFromML</div></li><li><div>org.apache.spark.mllib.util.JavaMLUtilsSuite.testConvertVectorColumnsToAndFromML</div></li><li><div>org.apache.spark.repl.ReplSuite.SPARK-15236: use Hive catalog</div></li><li><div>org.apache.spark.repl.ReplSuite.SPARK-15236: use in-memory catalog</div></li><li><div>org.apache.spark.repl.ReplSuite.broadcast vars</div></li><li><div>org.apache.spark.repl.ReplSuite.line wrapper only initialized once when used as encoder outer scope</div></li><li><div>org.apache.spark.repl.ReplSuite.define case class and create Dataset together with paste mode</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.simple foreach with accumulator</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.external vars</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.external classes</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.external functions</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.external functions that access vars</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.broadcast vars</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.interacting with files</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.local-cluster mode</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.SPARK-2576 importing implicits</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.Datasets and encoders</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.SPARK-2632 importing a method from non serializable class and not using it.</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.collecting objects of class defined in repl</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.collecting objects of class defined in repl - shuffling</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.replicating blocks of object with class defined in repl</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.newProductSeqEncoder with REPL defined class</div></li><li><div>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationCount</div></li><li><div>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationCount</div></li><li><div>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationSumDouble</div></li><li><div>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationSumDouble</div></li><li><div>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationSumLong</div></li><li><div>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationSumLong</div></li><li><div>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationAverage</div></li><li><div>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationAverage</div></li><li><div>test.org.apache.spark.sql.JavaApplySchemaSuite.applySchema</div></li><li><div>test.org.apache.spark.sql.JavaApplySchemaSuite.applySchema</div></li><li><div>test.org.apache.spark.sql.JavaApplySchemaSuite.dataFrameRDDOperations</div></li><li><div>test.org.apache.spark.sql.JavaApplySchemaSuite.dataFrameRDDOperations</div></li><li><div>test.org.apache.spark.sql.JavaApplySchemaSuite.applySchemaToJSON</div></li><li><div>test.org.apache.spark.sql.JavaApplySchemaSuite.applySchemaToJSON</div></li><li><div>test.org.apache.spark.sql.JavaBeanDeserializationSuite.testBeanWithArrayFieldDeserialization</div></li><li><div>test.org.apache.spark.sql.JavaBeanDeserializationSuite.testBeanWithArrayFieldDeserialization</div></li><li><div>test.org.apache.spark.sql.JavaBeanDeserializationSuite.testBeanWithMapFieldsDeserialization</div></li><li><div>test.org.apache.spark.sql.JavaBeanDeserializationSuite.testBeanWithMapFieldsDeserialization</div></li><li><div>test.org.apache.spark.sql.JavaColumnExpressionSuite.isInCollectionCheckExceptionMessage</div></li><li><div>test.org.apache.spark.sql.JavaColumnExpressionSuite.isInCollectionCheckExceptionMessage</div></li><li><div>test.org.apache.spark.sql.JavaColumnExpressionSuite.isInCollectionWorksCorrectlyOnJava</div></li><li><div>test.org.apache.spark.sql.JavaColumnExpressionSuite.isInCollectionWorksCorrectlyOnJava</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testFormatAPI</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testTextAPI</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testJsonAPI</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testLoadAPI</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testOptionsAPI</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testSaveModeAPI</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testCsvAPI</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testParquetAPI</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testTextFileAPI</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCollectAndTake</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCollectAndTake</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testJsonRDDToDataFrame</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testJsonRDDToDataFrame</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testVarargMethods</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testVarargMethods</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testBeanWithoutGetter</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testBeanWithoutGetter</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateStructTypeFromList</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateStructTypeFromList</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testSampleBy</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testSampleBy</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCrosstab</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCrosstab</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testUDF</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testUDF</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFromFromList</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFromFromList</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCircularReferenceBean</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCircularReferenceBean</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testFrequentItems</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testFrequentItems</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testSampleByColumn</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testSampleByColumn</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testExecution</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testExecution</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testTextLoad</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testTextLoad</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.pivot</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.pivot</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testGenericLoad</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testGenericLoad</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCountMinSketch</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCountMinSketch</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.pivotColumnValues</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.pivotColumnValues</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFrameFromJavaBeans</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFrameFromJavaBeans</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCorrelation</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCorrelation</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testBloomFilter</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testBloomFilter</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCovariance</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCovariance</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFrameFromLocalJavaBeans</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFrameFromLocalJavaBeans</div></li><li><div>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationCount</div></li><li><div>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationCount</div></li><li><div>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationSumDouble</div></li><li><div>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationSumDouble</div></li><li><div>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationSumLong</div></li><li><div>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationSumLong</div></li><li><div>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationAnonClass</div></li><li><div>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationAnonClass</div></li><li><div>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationAverage</div></li><li><div>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationAverage</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testRuntimeNullabilityCheck</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean1</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean1</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean2</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean2</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean3</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean3</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testSerializeNull</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testSerializeNull</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testRandomSplit</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testRandomSplit</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testTypedFilterPreservingSchema</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testTypedFilterPreservingSchema</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJoin</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJoin</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testTake</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testTake</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testToLocalIterator</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testToLocalIterator</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testSpecificLists</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testSpecificLists</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testForeach</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testForeach</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJavaEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJavaEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testPrimitiveEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testPrimitiveEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testEmptyBean</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testEmptyBean</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testCommonOperation</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testNullInTopLevelBean</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testGroupBy</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testSetOperation</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testSetOperation</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testBeanWithEnum</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testBeanWithEnum</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testKryoEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testKryoEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.test</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.test</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJavaBeanEncoder2</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJavaBeanEncoder2</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testCollect</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testCollect</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testKryoEncoderErrorMessageForPrivateClass</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testKryoEncoderErrorMessageForPrivateClass</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJavaBeanEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJavaBeanEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testTupleEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testTupleEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testNestedTupleEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testNestedTupleEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testTupleEncoderSchema</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testTupleEncoderSchema</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testReduce</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testReduce</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testSelect</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testSelect</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJavaEncoderErrorMessageForPrivateClass</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJavaEncoderErrorMessageForPrivateClass</div></li><li><div>test.org.apache.spark.sql.JavaSaveLoadSuite.saveAndLoadWithSchema</div></li><li><div>test.org.apache.spark.sql.JavaSaveLoadSuite.saveAndLoadWithSchema</div></li><li><div>test.org.apache.spark.sql.JavaSaveLoadSuite.saveAndLoad</div></li><li><div>test.org.apache.spark.sql.JavaSaveLoadSuite.saveAndLoad</div></li><li><div>test.org.apache.spark.sql.JavaUDAFSuite.udf1Test</div></li><li><div>test.org.apache.spark.sql.JavaUDAFSuite.udf1Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf1Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf1Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf2Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf2Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf3Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf3Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf4Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf4Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf5Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf5Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf6Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf6Test</div></li><li><div>test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.testForeachBatchAPI</div></li><li><div>test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.testForeachBatchAPI</div></li><li><div>test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.testForeachAPI</div></li><li><div>test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.testForeachAPI</div></li><li><div>org.apache.spark.sql.hive.HiveMetastoreLazyInitializationSuite.lazily initialize Hive client</div></li><li><div>org.apache.spark.sql.hive.JavaDataFrameSuite.testUDAF</div></li><li><div>org.apache.spark.sql.hive.JavaDataFrameSuite.saveTableAndQueryIt</div></li><li><div>org.apache.spark.sql.hive.JavaMetastoreDataSourcesSuite.saveExternalTableAndQueryIt</div></li><li><div>org.apache.spark.sql.hive.JavaMetastoreDataSourcesSuite.saveTableAndQueryIt</div></li><li><div>org.apache.spark.sql.hive.JavaMetastoreDataSourcesSuite.saveExternalTableWithSchemaAndQueryIt</div></li><li><div>org.apache.spark.streaming.JavaMapWithStateSuite.testBasicFunction</div></li><li><div>org.apache.spark.streaming.JavaMapWithStateSuite.testBasicFunction</div></li><li><div>org.apache.spark.streaming.JavaReceiverAPISuite.testReceiver</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testStreamingContextTransform</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testStreamingContextTransform</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testFlatMapValues</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testFlatMapValues</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testMapPartitions</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testMapPartitions</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairFilter</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairFilter</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testCombineByKey</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testCombineByKey</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testMap</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testMap</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairToNormalRDDTransform</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairToNormalRDDTransform</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairReduceByKey</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairReduceByKey</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairMap</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairMap</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testFlatMap</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testFlatMap</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testReduceByKeyAndWindowWithInverse</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testReduceByKeyAndWindowWithInverse</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testReduceByWindow</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testReduceByWindow</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairFlatMap</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairFlatMap</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairToPairFlatMapWithChangingTypes</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairToPairFlatMapWithChangingTypes</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairMapPartitions</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairMapPartitions</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testVariousTransform</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testVariousTransform</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testTransformWith</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testTransformWith</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testVariousTransformWith</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testVariousTransformWith</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testReduceByKeyAndWindow</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testReduceByKeyAndWindow</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairTransform</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairTransform</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testFilter</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testFilter</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairMap2</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairMap2</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testMapValues</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testMapValues</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testReduce</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testReduce</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testUpdateStateByKey</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testTransform</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testTransform</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testStreamingContextTransform</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testFlatMapValues</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testReduceByWindowWithInverse</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testMapPartitions</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairFilter</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testRepartitionFewerPartitions</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testRepartitionFewerPartitions</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCombineByKey</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testContextGetOrCreate</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testContextGetOrCreate</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testWindowWithSlideDuration</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testWindowWithSlideDuration</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testQueueStream</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testQueueStream</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCountByValue</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCountByValue</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testMap</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testMap</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairToNormalRDDTransform</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairToNormalRDDTransform</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairReduceByKey</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairReduceByKey</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCount</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCount</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCheckpointMasterRecovery</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCheckpointMasterRecovery</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairMap</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairMap</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testUnion</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testUnion</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testFlatMap</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testFlatMap</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testReduceByKeyAndWindowWithInverse</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testGlom</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testGlom</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testJoin</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testJoin</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairFlatMap</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairFlatMap</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairToPairFlatMapWithChangingTypes</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairToPairFlatMapWithChangingTypes</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairMapPartitions</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairMapPartitions</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testRepartitionMorePartitions</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testRepartitionMorePartitions</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testReduceByWindowWithoutInverse</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testReduceByWindowWithoutInverse</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testLeftOuterJoin</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testLeftOuterJoin</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testVariousTransform</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testVariousTransform</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testTransformWith</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testTransformWith</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testVariousTransformWith</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testVariousTransformWith</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testTextFileStream</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairGroupByKey</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCoGroup</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testInitialization</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testInitialization</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testSocketString</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testSocketString</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testGroupByKeyAndWindow</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testGroupByKeyAndWindow</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testReduceByKeyAndWindow</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testReduceByKeyAndWindow</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testForeachRDD</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testForeachRDD</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testFileStream</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testFileStream</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairTransform</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairTransform</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testFilter</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testFilter</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairMap2</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testMapValues</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testReduce</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testReduce</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testUpdateStateByKey</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testTransform</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testTransform</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testWindow</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testWindow</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCountByValueAndWindow</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testSocketTextStream</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testSocketTextStream</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testUpdateStateByKeyWithInitial</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testUpdateStateByKeyWithInitial</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testContextState</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testContextState</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexDescendingWithStart</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexWithStart</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndexDescendingWithStart</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexDescending</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexWithStart</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexWithLast</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexWithSkip</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexWithMax</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexDescending</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndexDescendingWithLast</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexDescending</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexDescendingWithLast</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndex</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndexWithLast</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexWithStart</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexDescendingWithStart</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexWithLast</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexWithSkip</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndexDescending</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.testRefWithIntNaturalKey</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexDescending</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexDescendingWithStart</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexWithMax</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndex</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexWithLast</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexWithSkip</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexWithMax</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexDescendingWithLast</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexDescendingWithLast</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexDescendingWithStart</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndex</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexWithLast</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexWithSkip</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexWithStart</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndex</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexDescendingWithLast</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndexWithStart</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndex</div></li><li><div>org.apache.spark.util.kvstore.LevelDBSuite.testMultipleTypesWriteReadDelete</div></li><li><div>org.apache.spark.util.kvstore.LevelDBSuite.testObjectWriteReadDelete</div></li><li><div>org.apache.spark.util.kvstore.LevelDBSuite.testSkip</div></li><li><div>org.apache.spark.util.kvstore.LevelDBSuite.testMultipleObjectWriteReadDelete</div></li><li><div>org.apache.spark.util.kvstore.LevelDBSuite.testReopenAndVersionCheckDb</div></li><li><div>org.apache.spark.util.kvstore.LevelDBSuite.testMetadata</div></li><li><div>org.apache.spark.util.kvstore.LevelDBSuite.testUpdate</div></li><li><div>org.apache.spark.util.kvstore.LevelDBSuite.testNegativeIndexValues</div></li><li><div>org.apache.spark.sql.kafka010.KafkaDontFailOnDataLossSuite.failOnDataLoss=false should not return duplicated records: v1</div></li><li><div>org.apache.spark.sql.kafka010.KafkaMicroBatchV1SourceSuite.subscribing topic by pattern from earliest offsets (failOnDataLoss: false)</div></li><li><div>org.apache.spark.sql.kafka010.KafkaSourceStressForDontFailOnDataLossSuite.stress test for failOnDataLoss=false</div></li><li><div>org.apache.spark.streaming.kafka010.DirectKafkaStreamSuite.offset recovery from kafka</div></li><li><div>org.apache.spark.sql.streaming.EventTimeWatermarkSuite.delay in months and years handled correctly</div></li><li><div>org.apache.spark.sql.streaming.StreamingOuterJoinSuite.windowed right outer join</div></li><li><div>org.apache.spark.sql.hive.HiveSparkSubmitSuite.SPARK-8020: set sql conf in spark conf</div></li><li><div>org.apache.spark.sql.hive.HiveSparkSubmitSuite.SPARK-9757 Persist Parquet relation with decimal column</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.spark.network.ChunkFetchIntegrationSuite.fetchBothChunks</div></li><li><div>org.apache.spark.network.sasl.SparkSaslSuite.testFileRegionEncryption</div></li><li><div>org.apache.spark.ExecutorPluginSuite.testAddMultiplePlugins</div></li><li><div>org.apache.spark.ExecutorPluginSuite.testPluginClassDoesNotExist</div></li><li><div>org.apache.spark.ExecutorPluginSuite.testAddPlugin</div></li><li><div>org.apache.spark.ExecutorPluginSuite.testPluginShutdownWithException</div></li><li><div>org.apache.spark.JavaJdbcRDDSuite.testJavaJdbcRDD</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.leftOuterJoin</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.foldReduce</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.mapsFromPairsToPairs</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.mapsFromPairsToPairs</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.flatMap</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.flatMap</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.foreach</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.foreach</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.map</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.map</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.zip</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.zip</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.keyBy</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.keyBy</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.groupBy</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.groupBy</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.mapPartitions</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.mapPartitions</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.foldByKey</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.foldByKey</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.mapOnPairRDD</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.mapOnPairRDD</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.sequenceFile</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.sequenceFile</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.collectPartitions</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.collectPartitions</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.reduceByKey</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.reduceByKey</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.foreachWithAnonymousClass</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.foreachWithAnonymousClass</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.collectAsMapWithIntArrayValues</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.collectAsMapWithIntArrayValues</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.zipPartitions</div></li><li><div>test.org.apache.spark.Java8RDDAPISuite.zipPartitions</div></li><li><div>test.org.apache.spark.JavaAPISuite.groupByOnPairRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.groupByOnPairRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.binaryFilesCaching</div></li><li><div>test.org.apache.spark.JavaAPISuite.binaryFilesCaching</div></li><li><div>test.org.apache.spark.JavaAPISuite.sparkContextUnion</div></li><li><div>test.org.apache.spark.JavaAPISuite.sparkContextUnion</div></li><li><div>test.org.apache.spark.JavaAPISuite.checkpointAndComputation</div></li><li><div>test.org.apache.spark.JavaAPISuite.checkpointAndComputation</div></li><li><div>test.org.apache.spark.JavaAPISuite.leftOuterJoin</div></li><li><div>test.org.apache.spark.JavaAPISuite.leftOuterJoin</div></li><li><div>test.org.apache.spark.JavaAPISuite.keyByOnPairRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.keyByOnPairRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.getNumPartitions</div></li><li><div>test.org.apache.spark.JavaAPISuite.getNumPartitions</div></li><li><div>test.org.apache.spark.JavaAPISuite.wholeTextFiles</div></li><li><div>test.org.apache.spark.JavaAPISuite.wholeTextFiles</div></li><li><div>test.org.apache.spark.JavaAPISuite.binaryFiles</div></li><li><div>test.org.apache.spark.JavaAPISuite.binaryFiles</div></li><li><div>test.org.apache.spark.JavaAPISuite.foldReduce</div></li><li><div>test.org.apache.spark.JavaAPISuite.foldReduce</div></li><li><div>test.org.apache.spark.JavaAPISuite.writeWithNewAPIHadoopFile</div></li><li><div>test.org.apache.spark.JavaAPISuite.writeWithNewAPIHadoopFile</div></li><li><div>test.org.apache.spark.JavaAPISuite.hadoopFile</div></li><li><div>test.org.apache.spark.JavaAPISuite.hadoopFile</div></li><li><div>test.org.apache.spark.JavaAPISuite.lookup</div></li><li><div>test.org.apache.spark.JavaAPISuite.lookup</div></li><li><div>test.org.apache.spark.JavaAPISuite.countAsync</div></li><li><div>test.org.apache.spark.JavaAPISuite.countAsync</div></li><li><div>test.org.apache.spark.JavaAPISuite.textFiles</div></li><li><div>test.org.apache.spark.JavaAPISuite.textFiles</div></li><li><div>test.org.apache.spark.JavaAPISuite.binaryRecords</div></li><li><div>test.org.apache.spark.JavaAPISuite.binaryRecords</div></li><li><div>test.org.apache.spark.JavaAPISuite.toLocalIterator</div></li><li><div>test.org.apache.spark.JavaAPISuite.toLocalIterator</div></li><li><div>test.org.apache.spark.JavaAPISuite.repartitionAndSortWithinPartitions</div></li><li><div>test.org.apache.spark.JavaAPISuite.repartitionAndSortWithinPartitions</div></li><li><div>test.org.apache.spark.JavaAPISuite.reduce</div></li><li><div>test.org.apache.spark.JavaAPISuite.reduce</div></li><li><div>test.org.apache.spark.JavaAPISuite.sample</div></li><li><div>test.org.apache.spark.JavaAPISuite.sample</div></li><li><div>test.org.apache.spark.JavaAPISuite.sortBy</div></li><li><div>test.org.apache.spark.JavaAPISuite.sortBy</div></li><li><div>test.org.apache.spark.JavaAPISuite.mapsFromPairsToPairs</div></li><li><div>test.org.apache.spark.JavaAPISuite.mapsFromPairsToPairs</div></li><li><div>test.org.apache.spark.JavaAPISuite.flatMap</div></li><li><div>test.org.apache.spark.JavaAPISuite.flatMap</div></li><li><div>test.org.apache.spark.JavaAPISuite.cogroup3</div></li><li><div>test.org.apache.spark.JavaAPISuite.cogroup3</div></li><li><div>test.org.apache.spark.JavaAPISuite.cogroup4</div></li><li><div>test.org.apache.spark.JavaAPISuite.cogroup4</div></li><li><div>test.org.apache.spark.JavaAPISuite.randomSplit</div></li><li><div>test.org.apache.spark.JavaAPISuite.randomSplit</div></li><li><div>test.org.apache.spark.JavaAPISuite.persist</div></li><li><div>test.org.apache.spark.JavaAPISuite.persist</div></li><li><div>test.org.apache.spark.JavaAPISuite.foreach</div></li><li><div>test.org.apache.spark.JavaAPISuite.foreach</div></li><li><div>test.org.apache.spark.JavaAPISuite.hadoopFileCompressed</div></li><li><div>test.org.apache.spark.JavaAPISuite.hadoopFileCompressed</div></li><li><div>test.org.apache.spark.JavaAPISuite.textFilesCompressed</div></li><li><div>test.org.apache.spark.JavaAPISuite.textFilesCompressed</div></li><li><div>test.org.apache.spark.JavaAPISuite.testAsyncActionCancellation</div></li><li><div>test.org.apache.spark.JavaAPISuite.testAsyncActionCancellation</div></li><li><div>test.org.apache.spark.JavaAPISuite.checkpointAndRestore</div></li><li><div>test.org.apache.spark.JavaAPISuite.checkpointAndRestore</div></li><li><div>test.org.apache.spark.JavaAPISuite.sortByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.sortByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.aggregateByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.aggregateByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.map</div></li><li><div>test.org.apache.spark.JavaAPISuite.map</div></li><li><div>test.org.apache.spark.JavaAPISuite.max</div></li><li><div>test.org.apache.spark.JavaAPISuite.max</div></li><li><div>test.org.apache.spark.JavaAPISuite.min</div></li><li><div>test.org.apache.spark.JavaAPISuite.min</div></li><li><div>test.org.apache.spark.JavaAPISuite.top</div></li><li><div>test.org.apache.spark.JavaAPISuite.top</div></li><li><div>test.org.apache.spark.JavaAPISuite.zip</div></li><li><div>test.org.apache.spark.JavaAPISuite.fold</div></li><li><div>test.org.apache.spark.JavaAPISuite.glom</div></li><li><div>test.org.apache.spark.JavaAPISuite.take</div></li><li><div>test.org.apache.spark.JavaAPISuite.javaDoubleRDDHistoGram</div></li><li><div>test.org.apache.spark.JavaAPISuite.javaDoubleRDDHistoGram</div></li><li><div>test.org.apache.spark.JavaAPISuite.collectUnderlyingScalaRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.collectUnderlyingScalaRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.keyBy</div></li><li><div>test.org.apache.spark.JavaAPISuite.keyBy</div></li><li><div>test.org.apache.spark.JavaAPISuite.mapPartitionsWithIndex</div></li><li><div>test.org.apache.spark.JavaAPISuite.mapPartitionsWithIndex</div></li><li><div>test.org.apache.spark.JavaAPISuite.sampleByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.sampleByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.intersection</div></li><li><div>test.org.apache.spark.JavaAPISuite.intersection</div></li><li><div>test.org.apache.spark.JavaAPISuite.aggregate</div></li><li><div>test.org.apache.spark.JavaAPISuite.aggregate</div></li><li><div>test.org.apache.spark.JavaAPISuite.cartesian</div></li><li><div>test.org.apache.spark.JavaAPISuite.cartesian</div></li><li><div>test.org.apache.spark.JavaAPISuite.countApproxDistinctByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.countApproxDistinctByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.readWithNewAPIHadoopFile</div></li><li><div>test.org.apache.spark.JavaAPISuite.readWithNewAPIHadoopFile</div></li><li><div>test.org.apache.spark.JavaAPISuite.testRegisterKryoClasses</div></li><li><div>test.org.apache.spark.JavaAPISuite.testRegisterKryoClasses</div></li><li><div>test.org.apache.spark.JavaAPISuite.groupBy</div></li><li><div>test.org.apache.spark.JavaAPISuite.groupBy</div></li><li><div>test.org.apache.spark.JavaAPISuite.sampleByKeyExact</div></li><li><div>test.org.apache.spark.JavaAPISuite.sampleByKeyExact</div></li><li><div>test.org.apache.spark.JavaAPISuite.mapPartitions</div></li><li><div>test.org.apache.spark.JavaAPISuite.mapPartitions</div></li><li><div>test.org.apache.spark.JavaAPISuite.takeOrdered</div></li><li><div>test.org.apache.spark.JavaAPISuite.takeOrdered</div></li><li><div>test.org.apache.spark.JavaAPISuite.foldByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.foldByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.objectFilesOfInts</div></li><li><div>test.org.apache.spark.JavaAPISuite.objectFilesOfInts</div></li><li><div>test.org.apache.spark.JavaAPISuite.treeAggregate</div></li><li><div>test.org.apache.spark.JavaAPISuite.treeAggregate</div></li><li><div>test.org.apache.spark.JavaAPISuite.testGetPersistentRDDs</div></li><li><div>test.org.apache.spark.JavaAPISuite.testGetPersistentRDDs</div></li><li><div>test.org.apache.spark.JavaAPISuite.approximateResults</div></li><li><div>test.org.apache.spark.JavaAPISuite.approximateResults</div></li><li><div>test.org.apache.spark.JavaAPISuite.treeReduce</div></li><li><div>test.org.apache.spark.JavaAPISuite.treeReduce</div></li><li><div>test.org.apache.spark.JavaAPISuite.collectAsMapAndSerialize</div></li><li><div>test.org.apache.spark.JavaAPISuite.collectAsMapAndSerialize</div></li><li><div>test.org.apache.spark.JavaAPISuite.countApproxDistinct</div></li><li><div>test.org.apache.spark.JavaAPISuite.countApproxDistinct</div></li><li><div>test.org.apache.spark.JavaAPISuite.javaDoubleRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.javaDoubleRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.mapOnPairRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.mapOnPairRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.testAsyncActionErrorWrapping</div></li><li><div>test.org.apache.spark.JavaAPISuite.testAsyncActionErrorWrapping</div></li><li><div>test.org.apache.spark.JavaAPISuite.naturalMax</div></li><li><div>test.org.apache.spark.JavaAPISuite.naturalMax</div></li><li><div>test.org.apache.spark.JavaAPISuite.naturalMin</div></li><li><div>test.org.apache.spark.JavaAPISuite.naturalMin</div></li><li><div>test.org.apache.spark.JavaAPISuite.sequenceFile</div></li><li><div>test.org.apache.spark.JavaAPISuite.collectPartitions</div></li><li><div>test.org.apache.spark.JavaAPISuite.cogroup</div></li><li><div>test.org.apache.spark.JavaAPISuite.cogroup</div></li><li><div>test.org.apache.spark.JavaAPISuite.reduceByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.reduceByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.repartition</div></li><li><div>test.org.apache.spark.JavaAPISuite.repartition</div></li><li><div>test.org.apache.spark.JavaAPISuite.iterator</div></li><li><div>test.org.apache.spark.JavaAPISuite.iterator</div></li><li><div>test.org.apache.spark.JavaAPISuite.emptyRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.emptyRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.zipWithIndex</div></li><li><div>test.org.apache.spark.JavaAPISuite.zipWithIndex</div></li><li><div>test.org.apache.spark.JavaAPISuite.foreachPartition</div></li><li><div>test.org.apache.spark.JavaAPISuite.foreachPartition</div></li><li><div>test.org.apache.spark.JavaAPISuite.combineByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.combineByKey</div></li><li><div>test.org.apache.spark.JavaAPISuite.takeAsync</div></li><li><div>test.org.apache.spark.JavaAPISuite.takeAsync</div></li><li><div>test.org.apache.spark.JavaAPISuite.collectAsMapWithIntArrayValues</div></li><li><div>test.org.apache.spark.JavaAPISuite.collectAsMapWithIntArrayValues</div></li><li><div>test.org.apache.spark.JavaAPISuite.objectFilesOfComplexTypes</div></li><li><div>test.org.apache.spark.JavaAPISuite.objectFilesOfComplexTypes</div></li><li><div>test.org.apache.spark.JavaAPISuite.zipWithUniqueId</div></li><li><div>test.org.apache.spark.JavaAPISuite.zipWithUniqueId</div></li><li><div>test.org.apache.spark.JavaAPISuite.collectAsync</div></li><li><div>test.org.apache.spark.JavaAPISuite.collectAsync</div></li><li><div>test.org.apache.spark.JavaAPISuite.foreachAsync</div></li><li><div>test.org.apache.spark.JavaAPISuite.foreachAsync</div></li><li><div>test.org.apache.spark.JavaAPISuite.zipPartitions</div></li><li><div>test.org.apache.spark.JavaAPISuite.zipPartitions</div></li><li><div>test.org.apache.spark.JavaAPISuite.reduceOnJavaDoubleRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.reduceOnJavaDoubleRDD</div></li><li><div>test.org.apache.spark.JavaAPISuite.isEmpty</div></li><li><div>test.org.apache.spark.JavaAPISuite.isEmpty</div></li><li><div>test.org.apache.spark.JavaSparkContextSuite.javaSparkContext</div></li><li><div>test.org.apache.spark.JavaSparkContextSuite.scalaSparkContext</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single BooleanType with seed 841981079902811434</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single ByteType with seed 2243310682325962444</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single ShortType with seed 9028587868712380514</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single IntegerType with seed 3259778497745115189</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single LongType with seed 3951291623248444947</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single FloatType with seed 48979942793015499</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single DoubleType with seed -9114732779414028070</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single DecimalType(8,0) with seed 9065409153204310824</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single DecimalType(8,4) with seed 756481727812831179</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single DecimalType(16,0) with seed 5406300250631918627</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single DecimalType(16,11) with seed -678323241171125044</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single DecimalType(38,0) with seed -8422934417194024128</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single DecimalType(38,38) with seed -1905569850793528289</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single StringType with seed 3309456092726877802</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.single BinaryType with seed 1174879911608330560</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.flat schema struct&lt;col_0:string,col_1:int,col_2:bigint,col_3:tinyint,col_4:bigint&gt; with seed 2038646069863484639</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.flat schema struct&lt;col_0:float,col_1:boolean,col_2:binary,col_3:decimal(38,38),col_4:binary&gt; with seed -7053316220995128080</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.flat schema struct&lt;col_0:decimal(16,11),col_1:boolean,col_2:decimal(8,4),col_3:string,col_4:float&gt; with seed -3311834729306860181</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.flat schema struct&lt;col_0:binary,col_1:float,col_2:decimal(16,11),col_3:tinyint,col_4:bigint&gt; with seed 2965881151385800849</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.flat schema struct&lt;col_0:string,col_1:float,col_2:decimal(16,0),col_3:boolean,col_4:tinyint&gt; with seed -5118190953296608296</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.nested schema struct&lt;col_0:array&lt;tinyint&gt;,col_1:smallint,col_2:smallint,col_3:string,col_4:struct&lt;col_0:array&lt;bigint&gt;,col_1:struct&lt;col_0:array&lt;decimal(8,0)&gt;&gt;,col_2:decimal(16,0)&gt;,col_5:tinyint,col_6:struct&lt;col_0:struct&lt;col_0:decimal(8,0)&gt;&gt;,col_7:struct&lt;col_0:struct&lt;col_0:struct&lt;col_0:boolean&gt;&gt;&gt;&gt; with seed 3345718503295646388</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.nested schema struct&lt;col_0:struct&lt;col_0:array&lt;decimal(38,38)&gt;,col_1:struct&lt;col_0:array&lt;string&gt;&gt;,col_2:struct&lt;col_0:array&lt;decimal(38,0)&gt;&gt;,col_3:decimal(8,0),col_4:array&lt;tinyint&gt;,col_5:array&lt;tinyint&gt;,col_6:decimal(16,0)&gt;,col_1:struct&lt;col_0:struct&lt;col_0:struct&lt;col_0:struct&lt;col_0:string&gt;&gt;&gt;&gt;,col_2:double,col_3:array&lt;binary&gt;&gt; with seed -8107408962085047900</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.nested schema struct&lt;col_0:struct&lt;col_0:array&lt;boolean&gt;,col_1:struct&lt;col_0:struct&lt;col_0:int&gt;&gt;,col_2:struct&lt;col_0:array&lt;float&gt;,col_1:struct&lt;col_0:string&gt;&gt;,col_3:decimal(8,0),col_4:struct&lt;col_0:struct&lt;col_0:array&lt;decimal(16,11)&gt;&gt;&gt;,col_5:decimal(16,0)&gt;,col_1:decimal(8,0),col_2:bigint,col_3:smallint&gt; with seed 8354719024368488688</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.nested schema struct&lt;col_0:array&lt;decimal(16,11)&gt;,col_1:float,col_2:array&lt;bigint&gt;,col_3:struct&lt;col_0:array&lt;binary&gt;&gt;,col_4:struct&lt;col_0:decimal(8,0)&gt;,col_5:array&lt;tinyint&gt;,col_6:decimal(16,0),col_7:array&lt;decimal(38,0)&gt;,col_8:struct&lt;col_0:struct&lt;col_0:array&lt;decimal(16,0)&gt;&gt;&gt;,col_9:struct&lt;col_0:array&lt;float&gt;&gt;&gt; with seed 7164111805930087924</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.nested schema struct&lt;col_0:array&lt;int&gt;,col_1:decimal(38,38),col_2:array&lt;decimal(38,0)&gt;,col_3:struct&lt;col_0:struct&lt;col_0:struct&lt;col_0:struct&lt;col_0:decimal(16,11)&gt;&gt;&gt;,col_1:struct&lt;col_0:struct&lt;col_0:struct&lt;col_0:struct&lt;col_0:struct&lt;col_0:array&lt;decimal(16,0)&gt;&gt;&gt;&gt;&gt;&gt;&gt;,col_4:int,col_5:array&lt;decimal(16,11)&gt;,col_6:array&lt;tinyint&gt;,col_7:string,col_8:array&lt;smallint&gt;&gt; with seed -3483466748189527307</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.read int as string</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.read string as int</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.read float as double</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.read double as float</div></li><li><div>org.apache.spark.sql.avro.AvroCatalystDataConversionSuite.Handle unsupported input of record type</div></li><li><div>org.apache.spark.sql.avro.AvroFunctionsSuite.roundtrip in to_avro and from_avro - int and string</div></li><li><div>org.apache.spark.sql.avro.AvroFunctionsSuite.roundtrip in to_avro and from_avro - struct</div></li><li><div>org.apache.spark.sql.avro.AvroFunctionsSuite.handle invalid input in from_avro</div></li><li><div>org.apache.spark.sql.avro.AvroFunctionsSuite.roundtrip in to_avro and from_avro - array with null</div></li><li><div>org.apache.spark.streaming.kafka010.JavaDirectKafkaStreamSuite.testKafkaStream</div></li><li><div>org.apache.spark.streaming.kafka010.JavaKafkaRDDSuite.testKafkaRDD</div></li><li><div>org.apache.spark.graphx.EdgeRDDSuite.cache, getStorageLevel</div></li><li><div>org.apache.spark.graphx.EdgeRDDSuite.checkpointing</div></li><li><div>org.apache.spark.graphx.GraphLoaderSuite.GraphLoader.edgeListFile</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.joinVertices</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.collectNeighborIds</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.removeSelfEdges</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.filter</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.convertToCanonicalEdges</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.collectEdgesCycleDirectionOut</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.collectEdgesCycleDirectionIn</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.collectEdgesCycleDirectionEither</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.collectEdgesChainDirectionOut</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.collectEdgesChainDirectionIn</div></li><li><div>org.apache.spark.graphx.GraphOpsSuite.collectEdgesChainDirectionEither</div></li><li><div>org.apache.spark.graphx.GraphSuite.Graph.fromEdgeTuples</div></li><li><div>org.apache.spark.graphx.GraphSuite.Graph.fromEdges</div></li><li><div>org.apache.spark.graphx.GraphSuite.Graph.apply</div></li><li><div>org.apache.spark.graphx.GraphSuite.triplets</div></li><li><div>org.apache.spark.graphx.GraphSuite.partitionBy</div></li><li><div>org.apache.spark.graphx.GraphSuite.mapVertices</div></li><li><div>org.apache.spark.graphx.GraphSuite.mapVertices changing type with same erased type</div></li><li><div>org.apache.spark.graphx.GraphSuite.mapEdges</div></li><li><div>org.apache.spark.graphx.GraphSuite.mapTriplets</div></li><li><div>org.apache.spark.graphx.GraphSuite.reverse</div></li><li><div>org.apache.spark.graphx.GraphSuite.reverse with join elimination</div></li><li><div>org.apache.spark.graphx.GraphSuite.subgraph</div></li><li><div>org.apache.spark.graphx.GraphSuite.mask</div></li><li><div>org.apache.spark.graphx.GraphSuite.groupEdges</div></li><li><div>org.apache.spark.graphx.GraphSuite.aggregateMessages</div></li><li><div>org.apache.spark.graphx.GraphSuite.outerJoinVertices</div></li><li><div>org.apache.spark.graphx.GraphSuite.more edge partitions than vertex partitions</div></li><li><div>org.apache.spark.graphx.GraphSuite.checkpoint</div></li><li><div>org.apache.spark.graphx.GraphSuite.cache, getStorageLevel</div></li><li><div>org.apache.spark.graphx.GraphSuite.non-default number of edge partitions</div></li><li><div>org.apache.spark.graphx.GraphSuite.unpersist graph RDD</div></li><li><div>org.apache.spark.graphx.GraphSuite.SPARK-14219: pickRandomVertex</div></li><li><div>org.apache.spark.graphx.PregelSuite.1 iteration</div></li><li><div>org.apache.spark.graphx.PregelSuite.chain propagation</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.filter</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.mapValues</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.minus</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.minus with RDD[(VertexId, VD)]</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.minus with non-equal number of partitions</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.diff</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.diff with RDD[(VertexId, VD)]</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.diff vertices with non-equal number of partitions</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.leftJoin</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.leftJoin vertices with non-equal number of partitions</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.innerJoin</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.innerJoin vertices with the non-equal number of partitions</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.aggregateUsingIndex</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.mergeFunc</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.cache, getStorageLevel</div></li><li><div>org.apache.spark.graphx.VertexRDDSuite.checkpoint</div></li><li><div>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Grid Connected Components</div></li><li><div>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Reverse Grid Connected Components</div></li><li><div>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Chain Connected Components</div></li><li><div>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Reverse Chain Connected Components</div></li><li><div>org.apache.spark.graphx.lib.ConnectedComponentsSuite.Connected Components on a Toy Connected Graph</div></li><li><div>org.apache.spark.graphx.lib.LabelPropagationSuite.Label Propagation</div></li><li><div>org.apache.spark.graphx.lib.PageRankSuite.Star PageRank</div></li><li><div>org.apache.spark.graphx.lib.PageRankSuite.Star PersonalPageRank</div></li><li><div>org.apache.spark.graphx.lib.PageRankSuite.Grid PageRank</div></li><li><div>org.apache.spark.graphx.lib.PageRankSuite.Chain PageRank</div></li><li><div>org.apache.spark.graphx.lib.PageRankSuite.Chain PersonalizedPageRank</div></li><li><div>org.apache.spark.graphx.lib.PageRankSuite.Loop with source PageRank</div></li><li><div>org.apache.spark.graphx.lib.PageRankSuite.Loop with sink PageRank</div></li><li><div>org.apache.spark.graphx.lib.SVDPlusPlusSuite.Test SVD++ with mean square error on training set</div></li><li><div>org.apache.spark.graphx.lib.ShortestPathsSuite.Shortest Path Computations</div></li><li><div>org.apache.spark.graphx.lib.StronglyConnectedComponentsSuite.Island Strongly Connected Components</div></li><li><div>org.apache.spark.graphx.lib.StronglyConnectedComponentsSuite.Cycle Strongly Connected Components</div></li><li><div>org.apache.spark.graphx.lib.StronglyConnectedComponentsSuite.2 Cycle Strongly Connected Components</div></li><li><div>org.apache.spark.graphx.lib.TriangleCountSuite.Count a single triangle</div></li><li><div>org.apache.spark.graphx.lib.TriangleCountSuite.Count two triangles</div></li><li><div>org.apache.spark.graphx.lib.TriangleCountSuite.Count two triangles with bi-directed edges</div></li><li><div>org.apache.spark.graphx.lib.TriangleCountSuite.Count a single triangle with duplicate edges</div></li><li><div>org.apache.spark.graphx.util.GraphGeneratorsSuite.GraphGenerators.logNormalGraph</div></li><li><div>org.apache.spark.graphx.util.GraphGeneratorsSuite.SPARK-5064 GraphGenerators.rmatGraph numEdges upper bound</div></li><li><div>org.apache.spark.graphx.util.PeriodicGraphCheckpointerSuite.Persisting</div></li><li><div>org.apache.spark.graphx.util.PeriodicGraphCheckpointerSuite.Checkpointing</div></li><li><div>org.apache.spark.ml.JavaPipelineSuite.pipeline</div></li><li><div>org.apache.spark.ml.JavaPipelineSuite.pipeline</div></li><li><div>org.apache.spark.ml.classification.JavaDecisionTreeClassifierSuite.runDT</div></li><li><div>org.apache.spark.ml.classification.JavaDecisionTreeClassifierSuite.runDT</div></li><li><div>org.apache.spark.ml.classification.JavaGBTClassifierSuite.runDT</div></li><li><div>org.apache.spark.ml.classification.JavaGBTClassifierSuite.runDT</div></li><li><div>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionWithSetters</div></li><li><div>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionWithSetters</div></li><li><div>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionTrainingSummary</div></li><li><div>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionTrainingSummary</div></li><li><div>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionPredictorClassifierMethods</div></li><li><div>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionPredictorClassifierMethods</div></li><li><div>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionDefaultParams</div></li><li><div>org.apache.spark.ml.classification.JavaLogisticRegressionSuite.logisticRegressionDefaultParams</div></li><li><div>org.apache.spark.ml.classification.JavaMultilayerPerceptronClassifierSuite.testMLPC</div></li><li><div>org.apache.spark.ml.classification.JavaMultilayerPerceptronClassifierSuite.testMLPC</div></li><li><div>org.apache.spark.ml.classification.JavaNaiveBayesSuite.testNaiveBayes</div></li><li><div>org.apache.spark.ml.classification.JavaNaiveBayesSuite.testNaiveBayes</div></li><li><div>org.apache.spark.ml.classification.JavaNaiveBayesSuite.naiveBayesDefaultParams</div></li><li><div>org.apache.spark.ml.classification.JavaNaiveBayesSuite.naiveBayesDefaultParams</div></li><li><div>org.apache.spark.ml.classification.JavaOneVsRestSuite.oneVsRestDefaultParams</div></li><li><div>org.apache.spark.ml.classification.JavaOneVsRestSuite.oneVsRestDefaultParams</div></li><li><div>org.apache.spark.ml.classification.JavaRandomForestClassifierSuite.runDT</div></li><li><div>org.apache.spark.ml.classification.JavaRandomForestClassifierSuite.runDT</div></li><li><div>org.apache.spark.ml.clustering.JavaKMeansSuite.fitAndTransform</div></li><li><div>org.apache.spark.ml.clustering.JavaKMeansSuite.fitAndTransform</div></li><li><div>org.apache.spark.ml.feature.JavaBucketizerSuite.bucketizerTest</div></li><li><div>org.apache.spark.ml.feature.JavaBucketizerSuite.bucketizerTest</div></li><li><div>org.apache.spark.ml.feature.JavaBucketizerSuite.bucketizerMultipleColumnsTest</div></li><li><div>org.apache.spark.ml.feature.JavaBucketizerSuite.bucketizerMultipleColumnsTest</div></li><li><div>org.apache.spark.ml.feature.JavaDCTSuite.javaCompatibilityTest</div></li><li><div>org.apache.spark.ml.feature.JavaDCTSuite.javaCompatibilityTest</div></li><li><div>org.apache.spark.ml.feature.JavaHashingTFSuite.hashingTF</div></li><li><div>org.apache.spark.ml.feature.JavaHashingTFSuite.hashingTF</div></li><li><div>org.apache.spark.ml.feature.JavaNormalizerSuite.normalizer</div></li><li><div>org.apache.spark.ml.feature.JavaNormalizerSuite.normalizer</div></li><li><div>org.apache.spark.ml.feature.JavaPCASuite.testPCA</div></li><li><div>org.apache.spark.ml.feature.JavaPCASuite.testPCA</div></li><li><div>org.apache.spark.ml.feature.JavaPolynomialExpansionSuite.polynomialExpansionTest</div></li><li><div>org.apache.spark.ml.feature.JavaPolynomialExpansionSuite.polynomialExpansionTest</div></li><li><div>org.apache.spark.ml.feature.JavaStandardScalerSuite.standardScaler</div></li><li><div>org.apache.spark.ml.feature.JavaStandardScalerSuite.standardScaler</div></li><li><div>org.apache.spark.ml.feature.JavaStopWordsRemoverSuite.javaCompatibilityTest</div></li><li><div>org.apache.spark.ml.feature.JavaStopWordsRemoverSuite.javaCompatibilityTest</div></li><li><div>org.apache.spark.ml.feature.JavaStringIndexerSuite.testStringIndexer</div></li><li><div>org.apache.spark.ml.feature.JavaStringIndexerSuite.testStringIndexer</div></li><li><div>org.apache.spark.ml.feature.JavaTokenizerSuite.regexTokenizer</div></li><li><div>org.apache.spark.ml.feature.JavaTokenizerSuite.regexTokenizer</div></li><li><div>org.apache.spark.ml.feature.JavaVectorAssemblerSuite.testVectorAssembler</div></li><li><div>org.apache.spark.ml.feature.JavaVectorAssemblerSuite.testVectorAssembler</div></li><li><div>org.apache.spark.ml.feature.JavaVectorIndexerSuite.vectorIndexerAPI</div></li><li><div>org.apache.spark.ml.feature.JavaVectorIndexerSuite.vectorIndexerAPI</div></li><li><div>org.apache.spark.ml.feature.JavaVectorSlicerSuite.vectorSlice</div></li><li><div>org.apache.spark.ml.feature.JavaVectorSlicerSuite.vectorSlice</div></li><li><div>org.apache.spark.ml.feature.JavaWord2VecSuite.testJavaWord2Vec</div></li><li><div>org.apache.spark.ml.feature.JavaWord2VecSuite.testJavaWord2Vec</div></li><li><div>org.apache.spark.ml.regression.JavaDecisionTreeRegressorSuite.runDT</div></li><li><div>org.apache.spark.ml.regression.JavaDecisionTreeRegressorSuite.runDT</div></li><li><div>org.apache.spark.ml.regression.JavaGBTRegressorSuite.runDT</div></li><li><div>org.apache.spark.ml.regression.JavaGBTRegressorSuite.runDT</div></li><li><div>org.apache.spark.ml.regression.JavaLinearRegressionSuite.linearRegressionDefaultParams</div></li><li><div>org.apache.spark.ml.regression.JavaLinearRegressionSuite.linearRegressionDefaultParams</div></li><li><div>org.apache.spark.ml.regression.JavaLinearRegressionSuite.linearRegressionWithSetters</div></li><li><div>org.apache.spark.ml.regression.JavaLinearRegressionSuite.linearRegressionWithSetters</div></li><li><div>org.apache.spark.ml.regression.JavaRandomForestRegressorSuite.runDT</div></li><li><div>org.apache.spark.ml.source.libsvm.JavaLibSVMRelationSuite.verifyLibSVMDF</div></li><li><div>org.apache.spark.ml.source.libsvm.JavaLibSVMRelationSuite.verifyLibSVMDF</div></li><li><div>org.apache.spark.ml.stat.JavaKolmogorovSmirnovTestSuite.testKSTestNamedDistribution</div></li><li><div>org.apache.spark.ml.stat.JavaKolmogorovSmirnovTestSuite.testKSTestNamedDistribution</div></li><li><div>org.apache.spark.ml.stat.JavaKolmogorovSmirnovTestSuite.testKSTestCDF</div></li><li><div>org.apache.spark.ml.stat.JavaKolmogorovSmirnovTestSuite.testKSTestCDF</div></li><li><div>org.apache.spark.ml.stat.JavaSummarizerSuite.testSummarizer</div></li><li><div>org.apache.spark.ml.tuning.JavaCrossValidatorSuite.crossValidationWithLogisticRegression</div></li><li><div>org.apache.spark.ml.tuning.JavaCrossValidatorSuite.crossValidationWithLogisticRegression</div></li><li><div>org.apache.spark.ml.util.JavaDefaultReadWriteSuite.testDefaultReadWrite</div></li><li><div>org.apache.spark.ml.util.JavaDefaultReadWriteSuite.testDefaultReadWrite</div></li><li><div>org.apache.spark.mllib.classification.JavaLogisticRegressionSuite.runLRUsingConstructor</div></li><li><div>org.apache.spark.mllib.classification.JavaLogisticRegressionSuite.runLRUsingConstructor</div></li><li><div>org.apache.spark.mllib.classification.JavaLogisticRegressionSuite.runLRUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.classification.JavaLogisticRegressionSuite.runLRUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.testPredictJavaRDD</div></li><li><div>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.testPredictJavaRDD</div></li><li><div>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.runUsingConstructor</div></li><li><div>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.runUsingConstructor</div></li><li><div>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.runUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.runUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.testModelTypeSetters</div></li><li><div>org.apache.spark.mllib.classification.JavaNaiveBayesSuite.testModelTypeSetters</div></li><li><div>org.apache.spark.mllib.classification.JavaSVMSuite.runSVMUsingConstructor</div></li><li><div>org.apache.spark.mllib.classification.JavaSVMSuite.runSVMUsingConstructor</div></li><li><div>org.apache.spark.mllib.classification.JavaSVMSuite.runSVMUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.classification.JavaSVMSuite.runSVMUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.classification.JavaStreamingLogisticRegressionSuite.javaAPI</div></li><li><div>org.apache.spark.mllib.classification.JavaStreamingLogisticRegressionSuite.javaAPI</div></li><li><div>org.apache.spark.mllib.clustering.JavaBisectingKMeansSuite.twoDimensionalData</div></li><li><div>org.apache.spark.mllib.clustering.JavaBisectingKMeansSuite.twoDimensionalData</div></li><li><div>org.apache.spark.mllib.clustering.JavaGaussianMixtureSuite.runGaussianMixture</div></li><li><div>org.apache.spark.mllib.clustering.JavaKMeansSuite.testPredictJavaRDD</div></li><li><div>org.apache.spark.mllib.clustering.JavaKMeansSuite.testPredictJavaRDD</div></li><li><div>org.apache.spark.mllib.clustering.JavaKMeansSuite.runKMeansUsingConstructor</div></li><li><div>org.apache.spark.mllib.clustering.JavaKMeansSuite.runKMeansUsingConstructor</div></li><li><div>org.apache.spark.mllib.clustering.JavaKMeansSuite.runKMeansUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.clustering.JavaLDASuite.onlineOptimizerCompatibility</div></li><li><div>org.apache.spark.mllib.clustering.JavaLDASuite.onlineOptimizerCompatibility</div></li><li><div>org.apache.spark.mllib.clustering.JavaLDASuite.distributedLDAModel</div></li><li><div>org.apache.spark.mllib.clustering.JavaLDASuite.distributedLDAModel</div></li><li><div>org.apache.spark.mllib.clustering.JavaLDASuite.localLDAModel</div></li><li><div>org.apache.spark.mllib.clustering.JavaLDASuite.localLDAModel</div></li><li><div>org.apache.spark.mllib.clustering.JavaLDASuite.localLdaMethods</div></li><li><div>org.apache.spark.mllib.clustering.JavaLDASuite.localLdaMethods</div></li><li><div>org.apache.spark.mllib.clustering.JavaStreamingKMeansSuite.javaAPI</div></li><li><div>org.apache.spark.mllib.clustering.JavaStreamingKMeansSuite.javaAPI</div></li><li><div>org.apache.spark.mllib.evaluation.JavaRankingMetricsSuite.rankingMetrics</div></li><li><div>org.apache.spark.mllib.evaluation.JavaRankingMetricsSuite.rankingMetrics</div></li><li><div>org.apache.spark.mllib.feature.JavaTfIdfSuite.tfIdfMinimumDocumentFrequency</div></li><li><div>org.apache.spark.mllib.feature.JavaTfIdfSuite.tfIdf</div></li><li><div>org.apache.spark.mllib.feature.JavaWord2VecSuite.word2Vec</div></li><li><div>org.apache.spark.mllib.feature.JavaWord2VecSuite.word2Vec</div></li><li><div>org.apache.spark.mllib.fpm.JavaFPGrowthSuite.runFPGrowthSaveLoad</div></li><li><div>org.apache.spark.mllib.fpm.JavaFPGrowthSuite.runFPGrowthSaveLoad</div></li><li><div>org.apache.spark.mllib.fpm.JavaFPGrowthSuite.runFPGrowth</div></li><li><div>org.apache.spark.mllib.fpm.JavaFPGrowthSuite.runFPGrowth</div></li><li><div>org.apache.spark.mllib.fpm.JavaPrefixSpanSuite.runPrefixSpan</div></li><li><div>org.apache.spark.mllib.fpm.JavaPrefixSpanSuite.runPrefixSpanSaveLoad</div></li><li><div>org.apache.spark.mllib.fpm.JavaPrefixSpanSuite.runPrefixSpanSaveLoad</div></li><li><div>org.apache.spark.mllib.linalg.distributed.JavaRowMatrixSuite.rowMatrixQRDecomposition</div></li><li><div>org.apache.spark.mllib.linalg.distributed.JavaRowMatrixSuite.rowMatrixQRDecomposition</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testNormalVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testNormalVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testArbitrary</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testArbitrary</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testLogNormalVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testLogNormalVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testExponentialVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testExponentialVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testUniformRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testUniformRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testRandomVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testRandomVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testGammaRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testGammaRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testUniformVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testUniformVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testPoissonRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testPoissonRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testNormalRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testNormalRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testPoissonVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testPoissonVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testGammaVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testGammaVectorRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testExponentialRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testExponentialRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testLNormalRDD</div></li><li><div>org.apache.spark.mllib.random.JavaRandomRDDsSuite.testLNormalRDD</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runALSUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runALSUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSUsingConstructor</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSUsingConstructor</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runRecommend</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runRecommend</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSWithNegativeWeight</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSWithNegativeWeight</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runImplicitALSUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runALSUsingConstructor</div></li><li><div>org.apache.spark.mllib.recommendation.JavaALSSuite.runALSUsingConstructor</div></li><li><div>org.apache.spark.mllib.regression.JavaIsotonicRegressionSuite.testIsotonicRegressionJavaRDD</div></li><li><div>org.apache.spark.mllib.regression.JavaIsotonicRegressionSuite.testIsotonicRegressionJavaRDD</div></li><li><div>org.apache.spark.mllib.regression.JavaIsotonicRegressionSuite.testIsotonicRegressionPredictionsJavaRDD</div></li><li><div>org.apache.spark.mllib.regression.JavaIsotonicRegressionSuite.testIsotonicRegressionPredictionsJavaRDD</div></li><li><div>org.apache.spark.mllib.regression.JavaLassoSuite.runLassoUsingConstructor</div></li><li><div>org.apache.spark.mllib.regression.JavaLassoSuite.runLassoUsingConstructor</div></li><li><div>org.apache.spark.mllib.regression.JavaLassoSuite.runLassoUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.regression.JavaLassoSuite.runLassoUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.testPredictJavaRDD</div></li><li><div>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.testPredictJavaRDD</div></li><li><div>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.runLinearRegressionUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.runLinearRegressionUsingConstructor</div></li><li><div>org.apache.spark.mllib.regression.JavaLinearRegressionSuite.runLinearRegressionUsingConstructor</div></li><li><div>org.apache.spark.mllib.regression.JavaRidgeRegressionSuite.runRidgeRegressionUsingConstructor</div></li><li><div>org.apache.spark.mllib.regression.JavaRidgeRegressionSuite.runRidgeRegressionUsingConstructor</div></li><li><div>org.apache.spark.mllib.regression.JavaRidgeRegressionSuite.runRidgeRegressionUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.regression.JavaRidgeRegressionSuite.runRidgeRegressionUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.regression.JavaStreamingLinearRegressionSuite.javaAPI</div></li><li><div>org.apache.spark.mllib.regression.JavaStreamingLinearRegressionSuite.javaAPI</div></li><li><div>org.apache.spark.mllib.stat.JavaStatisticsSuite.testCorr</div></li><li><div>org.apache.spark.mllib.stat.JavaStatisticsSuite.testCorr</div></li><li><div>org.apache.spark.mllib.stat.JavaStatisticsSuite.chiSqTest</div></li><li><div>org.apache.spark.mllib.stat.JavaStatisticsSuite.chiSqTest</div></li><li><div>org.apache.spark.mllib.stat.JavaStatisticsSuite.streamingTest</div></li><li><div>org.apache.spark.mllib.stat.JavaStatisticsSuite.streamingTest</div></li><li><div>org.apache.spark.mllib.stat.JavaStatisticsSuite.kolmogorovSmirnovTest</div></li><li><div>org.apache.spark.mllib.stat.JavaStatisticsSuite.kolmogorovSmirnovTest</div></li><li><div>org.apache.spark.mllib.tree.JavaDecisionTreeSuite.runDTUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.tree.JavaDecisionTreeSuite.runDTUsingStaticMethods</div></li><li><div>org.apache.spark.mllib.tree.JavaDecisionTreeSuite.runDTUsingConstructor</div></li><li><div>org.apache.spark.mllib.tree.JavaDecisionTreeSuite.runDTUsingConstructor</div></li><li><div>org.apache.spark.mllib.util.JavaMLUtilsSuite.testConvertMatrixColumnsToAndFromML</div></li><li><div>org.apache.spark.mllib.util.JavaMLUtilsSuite.testConvertMatrixColumnsToAndFromML</div></li><li><div>org.apache.spark.mllib.util.JavaMLUtilsSuite.testConvertVectorColumnsToAndFromML</div></li><li><div>org.apache.spark.mllib.util.JavaMLUtilsSuite.testConvertVectorColumnsToAndFromML</div></li><li><div>org.apache.spark.repl.ReplSuite.SPARK-15236: use Hive catalog</div></li><li><div>org.apache.spark.repl.ReplSuite.SPARK-15236: use in-memory catalog</div></li><li><div>org.apache.spark.repl.ReplSuite.broadcast vars</div></li><li><div>org.apache.spark.repl.ReplSuite.line wrapper only initialized once when used as encoder outer scope</div></li><li><div>org.apache.spark.repl.ReplSuite.define case class and create Dataset together with paste mode</div></li><li><div>org.apache.spark.repl.ReplSuite.:replay should work correctly</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.simple foreach with accumulator</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.external vars</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.external classes</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.external functions</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.external functions that access vars</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.broadcast vars</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.interacting with files</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.local-cluster mode</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.SPARK-2576 importing implicits</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.Datasets and encoders</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.SPARK-2632 importing a method from non serializable class and not using it.</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.collecting objects of class defined in repl</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.collecting objects of class defined in repl - shuffling</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.replicating blocks of object with class defined in repl</div></li><li><div>org.apache.spark.repl.SingletonReplSuite.newProductSeqEncoder with REPL defined class</div></li><li><div>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationCount</div></li><li><div>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationCount</div></li><li><div>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationSumDouble</div></li><li><div>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationSumDouble</div></li><li><div>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationSumLong</div></li><li><div>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationSumLong</div></li><li><div>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationAverage</div></li><li><div>test.org.apache.spark.sql.Java8DatasetAggregatorSuite.testTypedAggregationAverage</div></li><li><div>test.org.apache.spark.sql.JavaApplySchemaSuite.applySchema</div></li><li><div>test.org.apache.spark.sql.JavaApplySchemaSuite.dataFrameRDDOperations</div></li><li><div>test.org.apache.spark.sql.JavaApplySchemaSuite.applySchemaToJSON</div></li><li><div>test.org.apache.spark.sql.JavaBeanDeserializationSuite.testBeanWithArrayFieldDeserialization</div></li><li><div>test.org.apache.spark.sql.JavaBeanDeserializationSuite.testBeanWithArrayFieldDeserialization</div></li><li><div>test.org.apache.spark.sql.JavaBeanDeserializationSuite.testBeanWithMapFieldsDeserialization</div></li><li><div>test.org.apache.spark.sql.JavaBeanDeserializationSuite.testBeanWithMapFieldsDeserialization</div></li><li><div>test.org.apache.spark.sql.JavaColumnExpressionSuite.isInCollectionCheckExceptionMessage</div></li><li><div>test.org.apache.spark.sql.JavaColumnExpressionSuite.isInCollectionCheckExceptionMessage</div></li><li><div>test.org.apache.spark.sql.JavaColumnExpressionSuite.isInCollectionWorksCorrectlyOnJava</div></li><li><div>test.org.apache.spark.sql.JavaColumnExpressionSuite.isInCollectionWorksCorrectlyOnJava</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testFormatAPI</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testTextAPI</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testJsonAPI</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testLoadAPI</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testOptionsAPI</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testSaveModeAPI</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testCsvAPI</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testParquetAPI</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.testTextFileAPI</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCollectAndTake</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCollectAndTake</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testJsonRDDToDataFrame</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testJsonRDDToDataFrame</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testVarargMethods</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testVarargMethods</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testBeanWithoutGetter</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testBeanWithoutGetter</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateStructTypeFromList</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateStructTypeFromList</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testSampleBy</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testSampleBy</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCrosstab</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCrosstab</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testUDF</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testUDF</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFromFromList</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFromFromList</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCircularReferenceBean</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCircularReferenceBean</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testFrequentItems</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testFrequentItems</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testSampleByColumn</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testSampleByColumn</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testExecution</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testExecution</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testTextLoad</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testTextLoad</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.pivot</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.pivot</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testGenericLoad</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testGenericLoad</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCountMinSketch</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCountMinSketch</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.pivotColumnValues</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.pivotColumnValues</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFrameFromJavaBeans</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFrameFromJavaBeans</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCorrelation</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCorrelation</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testBloomFilter</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testBloomFilter</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCovariance</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCovariance</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFrameFromLocalJavaBeans</div></li><li><div>test.org.apache.spark.sql.JavaDataFrameSuite.testCreateDataFrameFromLocalJavaBeans</div></li><li><div>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationCount</div></li><li><div>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationCount</div></li><li><div>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationSumDouble</div></li><li><div>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationSumDouble</div></li><li><div>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationSumLong</div></li><li><div>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationSumLong</div></li><li><div>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationAnonClass</div></li><li><div>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationAnonClass</div></li><li><div>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationAverage</div></li><li><div>test.org.apache.spark.sql.JavaDatasetAggregatorSuite.testTypedAggregationAverage</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testRuntimeNullabilityCheck</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testRuntimeNullabilityCheck</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean1</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean1</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean2</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean2</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean3</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testCircularReferenceBean3</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testSerializeNull</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testSerializeNull</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testRandomSplit</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testRandomSplit</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testTypedFilterPreservingSchema</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testTypedFilterPreservingSchema</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJoin</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJoin</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testTake</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testTake</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testToLocalIterator</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testToLocalIterator</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testSpecificLists</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testSpecificLists</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testForeach</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testForeach</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJavaEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJavaEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testPrimitiveEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testPrimitiveEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testEmptyBean</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testEmptyBean</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testCommonOperation</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testCommonOperation</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testNullInTopLevelBean</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testNullInTopLevelBean</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testGroupBy</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testGroupBy</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testSetOperation</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testSetOperation</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testBeanWithEnum</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testBeanWithEnum</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testKryoEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testKryoEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.test</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.test</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJavaBeanEncoder2</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJavaBeanEncoder2</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testCollect</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testCollect</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testKryoEncoderErrorMessageForPrivateClass</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testKryoEncoderErrorMessageForPrivateClass</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJavaBeanEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJavaBeanEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testTupleEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testTupleEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testNestedTupleEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testNestedTupleEncoder</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testTupleEncoderSchema</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testTupleEncoderSchema</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testReduce</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testReduce</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testSelect</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testSelect</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJavaEncoderErrorMessageForPrivateClass</div></li><li><div>test.org.apache.spark.sql.JavaDatasetSuite.testJavaEncoderErrorMessageForPrivateClass</div></li><li><div>test.org.apache.spark.sql.JavaSaveLoadSuite.saveAndLoadWithSchema</div></li><li><div>test.org.apache.spark.sql.JavaSaveLoadSuite.saveAndLoadWithSchema</div></li><li><div>test.org.apache.spark.sql.JavaSaveLoadSuite.saveAndLoad</div></li><li><div>test.org.apache.spark.sql.JavaSaveLoadSuite.saveAndLoad</div></li><li><div>test.org.apache.spark.sql.JavaUDAFSuite.udf1Test</div></li><li><div>test.org.apache.spark.sql.JavaUDAFSuite.udf1Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf1Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf1Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf2Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf2Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf3Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf3Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf4Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf4Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf5Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf5Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf6Test</div></li><li><div>test.org.apache.spark.sql.JavaUDFSuite.udf6Test</div></li><li><div>test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.testForeachBatchAPI</div></li><li><div>test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.testForeachBatchAPI</div></li><li><div>test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.testForeachAPI</div></li><li><div>test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite.testForeachAPI</div></li><li><div>org.apache.spark.sql.hive.HiveMetastoreLazyInitializationSuite.lazily initialize Hive client</div></li><li><div>org.apache.spark.sql.hive.JavaDataFrameSuite.testUDAF</div></li><li><div>org.apache.spark.sql.hive.JavaDataFrameSuite.saveTableAndQueryIt</div></li><li><div>org.apache.spark.sql.hive.JavaMetastoreDataSourcesSuite.saveExternalTableAndQueryIt</div></li><li><div>org.apache.spark.sql.hive.JavaMetastoreDataSourcesSuite.saveTableAndQueryIt</div></li><li><div>org.apache.spark.sql.hive.JavaMetastoreDataSourcesSuite.saveExternalTableWithSchemaAndQueryIt</div></li><li><div>org.apache.spark.streaming.JavaMapWithStateSuite.testBasicFunction</div></li><li><div>org.apache.spark.streaming.JavaReceiverAPISuite.testReceiver</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testStreamingContextTransform</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testStreamingContextTransform</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testFlatMapValues</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testMapPartitions</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testMapPartitions</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairFilter</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairFilter</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testCombineByKey</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testCombineByKey</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testMap</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testMap</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairToNormalRDDTransform</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairToNormalRDDTransform</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairReduceByKey</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairReduceByKey</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairMap</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairMap</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testFlatMap</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testFlatMap</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testReduceByKeyAndWindowWithInverse</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testReduceByKeyAndWindowWithInverse</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testReduceByWindow</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testReduceByWindow</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairFlatMap</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairFlatMap</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairToPairFlatMapWithChangingTypes</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairToPairFlatMapWithChangingTypes</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairMapPartitions</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairMapPartitions</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testVariousTransform</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testVariousTransform</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testTransformWith</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testTransformWith</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testVariousTransformWith</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testVariousTransformWith</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testReduceByKeyAndWindow</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testReduceByKeyAndWindow</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairTransform</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairTransform</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testFilter</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testFilter</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairMap2</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testPairMap2</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testMapValues</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testMapValues</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testReduce</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testReduce</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testUpdateStateByKey</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testUpdateStateByKey</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testTransform</div></li><li><div>test.org.apache.spark.streaming.Java8APISuite.testTransform</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testStreamingContextTransform</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testFlatMapValues</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testReduceByWindowWithInverse</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testReduceByWindowWithInverse</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testMapPartitions</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testMapPartitions</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairFilter</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairFilter</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testRepartitionFewerPartitions</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testRepartitionFewerPartitions</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCombineByKey</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCombineByKey</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testContextGetOrCreate</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testContextGetOrCreate</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testWindowWithSlideDuration</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testWindowWithSlideDuration</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testQueueStream</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testQueueStream</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCountByValue</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCountByValue</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testMap</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testMap</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairToNormalRDDTransform</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairToNormalRDDTransform</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairReduceByKey</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairReduceByKey</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCount</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCount</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCheckpointMasterRecovery</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCheckpointMasterRecovery</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairMap</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairMap</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testUnion</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testUnion</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testFlatMap</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testFlatMap</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testReduceByKeyAndWindowWithInverse</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testReduceByKeyAndWindowWithInverse</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testGlom</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testGlom</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testJoin</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testJoin</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairFlatMap</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairFlatMap</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairToPairFlatMapWithChangingTypes</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairToPairFlatMapWithChangingTypes</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairMapPartitions</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairMapPartitions</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testRepartitionMorePartitions</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testRepartitionMorePartitions</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testReduceByWindowWithoutInverse</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testReduceByWindowWithoutInverse</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testLeftOuterJoin</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testVariousTransform</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testVariousTransform</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testTransformWith</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testTransformWith</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testVariousTransformWith</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testVariousTransformWith</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testTextFileStream</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testTextFileStream</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairGroupByKey</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairGroupByKey</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCoGroup</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCoGroup</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testInitialization</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testInitialization</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testSocketString</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testSocketString</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testGroupByKeyAndWindow</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testGroupByKeyAndWindow</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testReduceByKeyAndWindow</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testReduceByKeyAndWindow</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testForeachRDD</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testFileStream</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testFileStream</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairTransform</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairTransform</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testFilter</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testFilter</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairMap2</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testPairMap2</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testMapValues</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testMapValues</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testReduce</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testReduce</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testUpdateStateByKey</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testUpdateStateByKey</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testTransform</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testTransform</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testWindow</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testWindow</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCountByValueAndWindow</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testCountByValueAndWindow</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testRawSocketStream</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testRawSocketStream</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testSocketTextStream</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testSocketTextStream</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testUpdateStateByKeyWithInitial</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testUpdateStateByKeyWithInitial</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testContextState</div></li><li><div>test.org.apache.spark.streaming.JavaAPISuite.testContextState</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexDescendingWithStart</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexWithStart</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndexDescendingWithStart</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexDescending</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexWithStart</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexWithLast</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexWithSkip</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexWithMax</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexDescending</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndexDescendingWithLast</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexDescending</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexDescendingWithLast</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndex</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndexWithLast</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexWithStart</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexDescendingWithStart</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexWithLast</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexWithSkip</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndexDescending</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.testRefWithIntNaturalKey</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexDescending</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexDescendingWithStart</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexWithMax</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndex</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexWithLast</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexWithSkip</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndexWithMax</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndexDescendingWithLast</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.refIndexDescendingWithLast</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexDescendingWithStart</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.childIndex</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexWithLast</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexWithSkip</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexWithStart</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.copyIndex</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.naturalIndexDescendingWithLast</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndexWithStart</div></li><li><div>org.apache.spark.util.kvstore.LevelDBIteratorSuite.numericIndex</div></li><li><div>org.apache.spark.util.kvstore.LevelDBSuite.testMultipleTypesWriteReadDelete</div></li><li><div>org.apache.spark.util.kvstore.LevelDBSuite.testObjectWriteReadDelete</div></li><li><div>org.apache.spark.util.kvstore.LevelDBSuite.testSkip</div></li><li><div>org.apache.spark.util.kvstore.LevelDBSuite.testMultipleObjectWriteReadDelete</div></li><li><div>org.apache.spark.util.kvstore.LevelDBSuite.testReopenAndVersionCheckDb</div></li><li><div>org.apache.spark.util.kvstore.LevelDBSuite.testMetadata</div></li><li><div>org.apache.spark.util.kvstore.LevelDBSuite.testUpdate</div></li><li><div>org.apache.spark.util.kvstore.LevelDBSuite.testNegativeIndexValues</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.spark.sql.kafka010.KafkaContinuousSourceStressForDontFailOnDataLossSuite.stress test for failOnDataLoss=false</div></li></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="sqoop"><div style="font-weight:bold;" class="panel-heading">SQOOP<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Yussuf)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/trunk</div><div><b>Last Revision: </b>d58e5f106ba932879112d7b69997301e442335f1</div><div><b>Last Run: </b>16-11-2018 01:55 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC UBUNTU18</th><th>X86 UBUNTU18</th><th>PPC RHEL72</th><th>X86 RHEL72</th><th>PPC RHEL75</th><th>X86 RHEL75</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 887</div><div>Failed Count : 1</div><div>Skipped Count : 44</div></td><td><div>Total Count : 888</div><div>Failed Count : 0</div><div>Skipped Count : 44</div></td><td><div>Total Count : 887</div><div>Failed Count : 1</div><div>Skipped Count : 44</div></td><td><div>Total Count : 888</div><div>Failed Count : 0</div><div>Skipped Count : 44</div></td><td><div>Total Count : 887</div><div>Failed Count : 1</div><div>Skipped Count : 44</div></td><td><div>Total Count : 888</div><div>Failed Count : 0</div><div>Skipped Count : 44</div></td><td><div>Total Count : 888</div><div>Failed Count : 0</div><div>Skipped Count : 44</div></td><td><div>Total Count : 888</div><div>Failed Count : 0</div><div>Skipped Count : 44</div></td></tr><tr><td>Result</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"><div><li>org.apache.sqoop.mapreduce.db.netezza.TestNetezzaExternalTableExportMapper.testPassingJDBC</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.sqoop.mapreduce.db.netezza.TestNetezzaExternalTableExportMapper.testPassingJDBC</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.sqoop.mapreduce.db.netezza.TestNetezzaExternalTableExportMapper.testPassingJDBC</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"><div><li>Timeout occurred. Please note the time in the report does not reflect the time until the timeout.</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>Timeout occurred. Please note the time in the report does not reflect the time until the timeout.</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"><div><li>Timeout occurred. Please note the time in the report does not reflect the time until the timeout.</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.sqoop.mapreduce.db.netezza.TestNetezzaExternalTableExportMapper.testPassingJDBC</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.sqoop.mapreduce.db.netezza.TestNetezzaExternalTableExportMapper.testPassingJDBC</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.sqoop.mapreduce.db.netezza.TestNetezzaExternalTableExportMapper.testPassingJDBC</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="storm"><div style="font-weight:bold;" class="panel-heading">STORM<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Alisha)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>f17b3dad8572f96217a8bf058022b26fe147a0b5</div><div><b>Last Run: </b>15-11-2018 01:18 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC UBUNTU18</th><th>X86 UBUNTU18</th><th>PPC RHEL72</th><th>X86 RHEL72</th><th>PPC RHEL75</th><th>X86 RHEL75</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 1162</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 1162</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 1162</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 1162</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 1162</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 1162</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 1162</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td><td><div>Total Count : 1162</div><div>Failed Count : 0</div><div>Skipped Count : 5</div></td></tr><tr><td>Result</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="tez"><div style="font-weight:bold;" class="panel-heading">TEZ<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Prajyot)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>efc73318342e40dac30ec321119c6536b67c0a64</div><div><b>Last Run: </b>15-11-2018 03:53 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC UBUNTU18</th><th>X86 UBUNTU18</th><th>PPC RHEL72</th><th>X86 RHEL72</th><th>PPC RHEL75</th><th>X86 RHEL75</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 1836</div><div>Failed Count : 3</div><div>Skipped Count : 14</div></td><td><div>Total Count : 1836</div><div>Failed Count : 1</div><div>Skipped Count : 14</div></td><td><div>Total Count : 1833</div><div>Failed Count : 3</div><div>Skipped Count : 14</div></td><td><div>Total Count : 1836</div><div>Failed Count : 1</div><div>Skipped Count : 14</div></td><td><div>Total Count : 1833</div><div>Failed Count : 3</div><div>Skipped Count : 14</div></td><td><div>Total Count : 1836</div><div>Failed Count : 1</div><div>Skipped Count : 14</div></td><td><div>Total Count : 1836</div><div>Failed Count : 1</div><div>Skipped Count : 14</div></td><td><div>Total Count : 1836</div><div>Failed Count : 2</div><div>Skipped Count : 14</div></td></tr><tr><td>Result</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"><div><li>org.apache.tez.dag.app.TestMockDAGAppMaster.testInternalPreemption</li></div><div><li>org.apache.tez.dag.app.TestPreemption.testPreemptionWithoutSession</li></div><div><li>org.apache.tez.tests.TestExtServicesWithLocalMode.test1</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.tez.tests.TestExtServicesWithLocalMode.test1</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.tez.tests.TestExtServicesWithLocalMode.test1</li></div><div><li>org.apache.tez.auxservices.TestShuffleHandler.testRecoveryFromOtherVersions</li></div><div><li>org.apache.tez.auxservices.TestShuffleHandler.testRecovery</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.tez.tests.TestExtServicesWithLocalMode.test1</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.tez.tests.TestExtServicesWithLocalMode.test1</li></div><div><li>org.apache.tez.auxservices.TestShuffleHandler.testRecoveryFromOtherVersions</li></div><div><li>org.apache.tez.auxservices.TestShuffleHandler.testRecovery</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.tez.tests.TestExtServicesWithLocalMode.test1</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.tez.tests.TestExtServicesWithLocalMode.test1</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.tez.tests.TestExtServicesWithLocalMode.test1</li></div><div><li>org.apache.tez.test.TestRecovery.testRecovery_HashJoin</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;KILLED&gt; but was:&lt;FAILED&gt;</li></div><div><li>test timed out after 5000 milliseconds</li></div><div><li>org.apache.tez.dag.api.TezUncheckedException: java.lang.reflect.InvocationTargetException</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.tez.dag.api.TezUncheckedException: java.lang.reflect.InvocationTargetException</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.tez.dag.api.TezUncheckedException: java.lang.reflect.InvocationTargetException</li></div><div><li>Could not load library. Reasons: [no leveldbjni64-1.8 in java.library.path, no leveldbjni-1.8 in java.library.path, no leveldbjni in java.library.path, /tmp/libleveldbjni-64-1-931229913805246822.8: /tmp/libleveldbjni-64-1-931229913805246822.8: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64-bit platform)]</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.tez.dag.api.TezUncheckedException: java.lang.reflect.InvocationTargetException</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.tez.dag.api.TezUncheckedException: java.lang.reflect.InvocationTargetException</li></div><div><li>Could not load library. Reasons: [no leveldbjni64-1.8 in java.library.path, no leveldbjni-1.8 in java.library.path, no leveldbjni in java.library.path, /tmp/libleveldbjni-64-1-430432351942585312.8: /tmp/libleveldbjni-64-1-430432351942585312.8: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64-bit platform)]</li></div><div><li>Could not initialize class org.fusesource.leveldbjni.JniDBFactory</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.tez.dag.api.TezUncheckedException: java.lang.reflect.InvocationTargetException</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.tez.dag.api.TezUncheckedException: java.lang.reflect.InvocationTargetException</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.tez.dag.api.TezUncheckedException: java.lang.reflect.InvocationTargetException</li></div><div><li>While parsing a protocol message, the input ended unexpectedly in the middle of a field.  This could mean either than the input has been truncated or that an embedded message misreported its own length.</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.tez.dag.app.TestMockDAGAppMaster.testInternalPreemption</div></li><li><div>org.apache.tez.dag.app.TestPreemption.testPreemptionWithoutSession</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.tez.auxservices.TestShuffleHandler.testRecoveryFromOtherVersions</div></li><li><div>org.apache.tez.auxservices.TestShuffleHandler.testRecovery</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.tez.auxservices.TestShuffleHandler.testRecoveryFromOtherVersions</div></li><li><div>org.apache.tez.auxservices.TestShuffleHandler.testRecovery</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.tez.test.TestRecovery.testRecovery_HashJoin</div></li></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="zeppelin"><div style="font-weight:bold;" class="panel-heading">ZEPPELIN<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Alisha)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>d37cc1b611dc761b1ec69c29d6b8ceaa84f301f4</div><div><b>Last Run: </b>14-09-2018 03:06 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC UBUNTU18</th><th>X86 UBUNTU18</th><th>PPC RHEL72</th><th>X86 RHEL72</th><th>PPC RHEL75</th><th>X86 RHEL75</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 868</div><div>Failed Count : 1</div><div>Skipped Count : 5</div></td><td><div>Total Count : 869</div><div>Failed Count : 3</div><div>Skipped Count : 5</div></td><td><div>Total Count : 869</div><div>Failed Count : 3</div><div>Skipped Count : 5</div></td><td><div>Total Count : 869</div><div>Failed Count : 2</div><div>Skipped Count : 5</div></td><td><div>Total Count : 872</div><div>Failed Count : 8</div><div>Skipped Count : 5</div></td><td><div>Total Count : 858</div><div>Failed Count : 3</div><div>Skipped Count : 5</div></td><td><div>Total Count : 859</div><div>Failed Count : 17</div><div>Skipped Count : 5</div></td><td><div>Total Count : 869</div><div>Failed Count : 3</div><div>Skipped Count : 5</div></td></tr><tr><td>Result</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"><div><li>org.apache.zeppelin.rest.NotebookRestApiTest.testRunWithServerRestart</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.zeppelin.rest.InterpreterRestApiTest.testRestartInterpreterPerNote</li></div><div><li>org.apache.zeppelin.rest.NotebookRestApiTest.testRunWithServerRestart</li></div><div><li>org.apache.zeppelin.rest.ZeppelinRestApiTest.org.apache.zeppelin.rest.ZeppelinRestApiTest</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.zeppelin.ignite.IgniteInterpreterTest.testInterpretInvalidInput</li></div><div><li>org.apache.zeppelin.ignite.IgniteInterpreterTest.testInterpretInvalidInput</li></div><div><li>org.apache.zeppelin.rest.NotebookRestApiTest.testRunWithServerRestart</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.zeppelin.rest.NotebookRestApiTest.testRunWithServerRestart</li></div><div><li>org.apache.zeppelin.rest.ZeppelinRestApiTest.org.apache.zeppelin.rest.ZeppelinRestApiTest</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.zeppelin.ignite.IgniteInterpreterTest.testInterpret</li></div><div><li>org.apache.zeppelin.ignite.IgniteInterpreterTest.testInterpret</li></div><div><li>org.apache.zeppelin.ignite.IgniteInterpreterTest.testInterpretInvalidInput</li></div><div><li>org.apache.zeppelin.ignite.IgniteInterpreterTest.testInterpretInvalidInput</li></div><div><li>org.apache.zeppelin.ignite.IgniteSqlInterpreterTest.testSql</li></div><div><li>org.apache.zeppelin.ignite.IgniteSqlInterpreterTest.testSql</li></div><div><li>org.apache.zeppelin.rest.NotebookRestApiTest.testRunWithServerRestart</li></div><div><li>org.apache.zeppelin.rest.ZeppelinRestApiTest.org.apache.zeppelin.rest.ZeppelinRestApiTest</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.zeppelin.spark.IPySparkInterpreterTest.testRedefinitionZeppelinContext</li></div><div><li>org.apache.zeppelin.rest.NotebookRestApiTest.testRunWithServerRestart</li></div><div><li>org.apache.zeppelin.helium.HeliumBundleFactoryTest.bundlePackage</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.zeppelin.python.IPythonInterpreterTest.testIPythonPlotting</li></div><div><li>org.apache.zeppelin.python.IPythonInterpreterTest.testZeppelinContext</li></div><div><li>org.apache.zeppelin.python.PythonInterpreterMatplotlibTest.testClose</li></div><div><li>org.apache.zeppelin.python.PythonInterpreterMatplotlibTest.showPlot</li></div><div><li>org.apache.zeppelin.python.PythonInterpreterMatplotlibTest.dependenciesAreInstalled</li></div><div><li>org.apache.zeppelin.python.PythonInterpreterMatplotlibTest.testNoClose</li></div><div><li>org.apache.zeppelin.python.PythonInterpreterTest.testZeppelinContext</li></div><div><li>org.apache.zeppelin.spark.IPySparkInterpreterTest.testIPySpark</li></div><div><li>org.apache.zeppelin.spark.IPySparkInterpreterTest.testIPythonPlotting</li></div><div><li>org.apache.zeppelin.spark.IPySparkInterpreterTest.testZeppelinContext</li></div><div><li>org.apache.zeppelin.spark.PySparkInterpreterTest.testPySpark</li></div><div><li>org.apache.zeppelin.spark.PySparkInterpreterTest.testZeppelinContext</li></div><div><li>org.apache.zeppelin.rest.InterpreterRestApiTest.testRestartInterpreterPerNote</li></div><div><li>org.apache.zeppelin.rest.NotebookRestApiTest.testRunWithServerRestart</li></div><div><li>org.apache.zeppelin.rest.ZeppelinRestApiTest.org.apache.zeppelin.rest.ZeppelinRestApiTest</li></div><div><li>org.apache.zeppelin.rest.ZeppelinSparkClusterTest.pySparkDepLoaderTest[3]</li></div><div><li>org.apache.zeppelin.rest.ZeppelinSparkClusterTest.testZeppelinContextResource[3]</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.zeppelin.rest.InterpreterRestApiTest.testRestartInterpreterPerNote</li></div><div><li>org.apache.zeppelin.rest.NotebookRestApiTest.testRunWithServerRestart</li></div><div><li>org.apache.zeppelin.rest.ZeppelinRestApiTest.org.apache.zeppelin.rest.ZeppelinRestApiTest</li></div></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"><div><li>java.lang.NullPointerException: null
	at org.apache.zeppelin.service.JobManagerService$NoteJobInfo.&lt;init&gt;(JobManagerService.java:124)
	at org.apache.zeppelin.service.JobManagerService.getNoteJobInfo(JobManagerService.java:52)
	at org.apache.zeppelin.socket.NotebookServer$NotebookInformationListener.onParagraphStatusChange(NotebookServer.java:1896)
	at org.apache.zeppelin.socket.NotebookServer.onSt</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>shared interpreter restart:
Expected: HTTP response &lt;200&gt; from /api/interpreter/setting/restart/md
     but: got &lt;500&gt; Request failed.</li></div><div><li>java.lang.NullPointerException: null
	at org.apache.zeppelin.service.JobManagerService$NoteJobInfo.&lt;init&gt;(JobManagerService.java:124)
	at org.apache.zeppelin.service.JobManagerService.getNoteJobInfo(JobManagerService.java:52)
	at org.apache.zeppelin.socket.NotebookServer$NotebookInformationListener.onParagraphStatusChange(NotebookServer.java:1896)
	at org.apache.zeppelin.socket.NotebookServer.onSt</li></div><div><li>java.lang.NullPointerException: null
	at org.apache.zeppelin.service.JobManagerService$NoteJobInfo.&lt;init&gt;(JobManagerService.java:124)
	at org.apache.zeppelin.service.JobManagerService.getNoteJobInfo(JobManagerService.java:52)
	at org.apache.zeppelin.socket.NotebookServer$NotebookInformationListener.onParagraphStatusChange(NotebookServer.java:1896)
	at org.apache.zeppelin.socket.NotebookServer.onSt</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Unable to establish plain connection. Was remote cluster configured with SSL? [rmtAddr=/127.0.0.1:47507, errMsg="Failed to deserialize object with given class loader: sun.misc.Launcher$AppClassLoader@135fbaa4"]</li></div><div><li>java.lang.NullPointerException: null
	at org.apache.zeppelin.ignite.IgniteInterpreterTest.tearDown(IgniteInterpreterTest.java:75)
</li></div><div><li>java.lang.NullPointerException: null
	at org.apache.zeppelin.service.JobManagerService$NoteJobInfo.&lt;init&gt;(JobManagerService.java:124)
	at org.apache.zeppelin.service.JobManagerService.getNoteJobInfo(JobManagerService.java:52)
	at org.apache.zeppelin.socket.NotebookServer$NotebookInformationListener.onParagraphStatusChange(NotebookServer.java:1896)
	at org.apache.zeppelin.socket.NotebookServer.onSt</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>java.lang.NullPointerException: null
	at org.apache.zeppelin.service.JobManagerService$NoteJobInfo.&lt;init&gt;(JobManagerService.java:124)
	at org.apache.zeppelin.service.JobManagerService.getNoteJobInfo(JobManagerService.java:52)
	at org.apache.zeppelin.socket.NotebookServer$NotebookInformationListener.onParagraphStatusChange(NotebookServer.java:1896)
	at org.apache.zeppelin.socket.NotebookServer.onSt</li></div><div><li>java.lang.NullPointerException: null
	at org.apache.zeppelin.service.JobManagerService$NoteJobInfo.&lt;init&gt;(JobManagerService.java:124)
	at org.apache.zeppelin.service.JobManagerService.getNoteJobInfo(JobManagerService.java:52)
	at org.apache.zeppelin.socket.NotebookServer$NotebookInformationListener.onParagraphStatusChange(NotebookServer.java:1896)
	at org.apache.zeppelin.socket.NotebookServer.onSt</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Unable to establish plain connection. Was remote cluster configured with SSL? [rmtAddr=/127.0.0.1:47507, errMsg="Failed to deserialize object with given class loader: sun.misc.Launcher$AppClassLoader@75b84c92"]</li></div><div><li>java.lang.NullPointerException: null
	at org.apache.zeppelin.ignite.IgniteInterpreterTest.tearDown(IgniteInterpreterTest.java:75)
</li></div><div><li>Unable to establish plain connection. Was remote cluster configured with SSL? [rmtAddr=/127.0.0.1:47507, errMsg="Failed to deserialize object with given class loader: sun.misc.Launcher$AppClassLoader@75b84c92"]</li></div><div><li>java.lang.NullPointerException: null
	at org.apache.zeppelin.ignite.IgniteInterpreterTest.tearDown(IgniteInterpreterTest.java:75)
</li></div><div><li>Unable to establish plain connection. Was remote cluster configured with SSL? [rmtAddr=/127.0.0.1:47507, errMsg="Failed to deserialize object with given class loader: sun.misc.Launcher$AppClassLoader@75b84c92"]</li></div><div><li>java.lang.NullPointerException: null
	at org.apache.zeppelin.ignite.IgniteSqlInterpreterTest.tearDown(IgniteSqlInterpreterTest.java:87)
</li></div><div><li>java.lang.NullPointerException: null
	at org.apache.zeppelin.service.JobManagerService$NoteJobInfo.&lt;init&gt;(JobManagerService.java:124)
	at org.apache.zeppelin.service.JobManagerService.getNoteJobInfo(JobManagerService.java:52)
	at org.apache.zeppelin.socket.NotebookServer$NotebookInformationListener.onParagraphStatusChange(NotebookServer.java:1896)
	at org.apache.zeppelin.socket.NotebookServer.onSt</li></div><div><li>java.lang.NullPointerException: null
	at org.apache.zeppelin.service.JobManagerService$NoteJobInfo.&lt;init&gt;(JobManagerService.java:124)
	at org.apache.zeppelin.service.JobManagerService.getNoteJobInfo(JobManagerService.java:52)
	at org.apache.zeppelin.socket.NotebookServer$NotebookInformationListener.onParagraphStatusChange(NotebookServer.java:1896)
	at org.apache.zeppelin.socket.NotebookServer.onSt</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>Fail to open IPythonInterpreter</li></div><div><li>java.lang.NullPointerException: null
	at org.apache.zeppelin.service.JobManagerService$NoteJobInfo.&lt;init&gt;(JobManagerService.java:124)
	at org.apache.zeppelin.service.JobManagerService.getNoteJobInfo(JobManagerService.java:52)
	at org.apache.zeppelin.socket.NotebookServer$NotebookInformationListener.onParagraphStatusChange(NotebookServer.java:1896)
	at org.apache.zeppelin.socket.NotebookServer.onSt</li></div><div><li>com.github.eirslett.maven.plugins.frontend.lib.TaskRunnerException: 'yarn install --fetch-retries=2 --fetch-retry-factor=1 --fetch-retry-mintimeout=5000 --registry=http://registry.npmjs.org/' failed. (error code 1)</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>expected:&lt;1&gt; but was:&lt;2&gt;</li></div><div><li>Index: 0, Size: 0</li></div><div><li>Index: 0, Size: 0</li></div><div><li>[%text Fail to execute line 1: import matplotlib
Traceback (most recent call last):
  File "/tmp/1536900576226-0/zeppelin_python.py", line 158, in &lt;module&gt;
    exec(code, _zcUserQueryNameSpace)
  File "&lt;stdin&gt;", line 1, in &lt;module&gt;
  File "/usr/lib64/python2.7/site-packages/matplotlib/__init__.py", line 124, in &lt;module&gt;
    from . import cbook
ImportError: cannot import name cbook
] expected:&lt;SUCC</li></div><div><li>Index: 0, Size: 0</li></div><div><li>expected:&lt;1&gt; but was:&lt;2&gt;</li></div><div><li>expected:&lt;[]+---+---+
| _1| _2|
...&gt; but was:&lt;[/usr/lib64/python2.7/site-packages/pandas/core/nanops.py:39: UserWarning: The installed version of bottleneck 0.7.0 is not supported in pandas and will be not be used
The minimum supported version is 1.0.0

  ver=ver, min_ver=_MIN_BOTTLENECK_VERSION), UserWarning)
]+---+---+
| _1| _2|
...&gt;</li></div><div><li>expected:&lt;SUCCESS&gt; but was:&lt;ERROR&gt;</li></div><div><li>expected:&lt;id name
1 a
2 b
3 c
[]&gt; but was:&lt;id name
1 a
2 b
3 c
[/usr/lib64/python2.7/site-packages/pandas/core/nanops.py:39: UserWarning: The installed version of bottleneck 0.7.0 is not supported in pandas and will be not be used
The minimum supported version is 1.0.0
  ver=ver, min_ver=_MIN_BOTTLENECK_VERSION), UserWarning)
]&gt;</li></div><div><li>expected:&lt;[]+---+---+
| _1| _2|
...&gt; but was:&lt;[/usr/lib64/python2.7/site-packages/pandas/core/nanops.py:39: UserWarning: The installed version of bottleneck 0.7.0 is not supported in pandas and will be not be used
The minimum supported version is 1.0.0

  ver=ver, min_ver=_MIN_BOTTLENECK_VERSION), UserWarning)
]+---+---+
| _1| _2|
...&gt;</li></div><div><li>expected:&lt;1&gt; but was:&lt;2&gt;</li></div><div><li>shared interpreter restart:
Expected: HTTP response &lt;200&gt; from /api/interpreter/setting/restart/md
     but: got &lt;500&gt; Request failed.</li></div><div><li>java.lang.NullPointerException: null
	at org.apache.zeppelin.service.JobManagerService$NoteJobInfo.&lt;init&gt;(JobManagerService.java:124)
	at org.apache.zeppelin.service.JobManagerService.getNoteJobInfo(JobManagerService.java:52)
	at org.apache.zeppelin.socket.NotebookServer$NotebookInformationListener.onParagraphStatusChange(NotebookServer.java:1896)
	at org.apache.zeppelin.socket.NotebookServer.onSt</li></div><div><li>java.lang.NullPointerException: null
	at org.apache.zeppelin.service.JobManagerService$NoteJobInfo.&lt;init&gt;(JobManagerService.java:124)
	at org.apache.zeppelin.service.JobManagerService.getNoteJobInfo(JobManagerService.java:52)
	at org.apache.zeppelin.socket.NotebookServer$NotebookInformationListener.onParagraphStatusChange(NotebookServer.java:1896)
	at org.apache.zeppelin.socket.NotebookServer.onSt</li></div><div><li>expected:&lt;[]2
&gt; but was:&lt;[/usr/lib64/python2.7/site-packages/pandas/core/nanops.py:39: UserWarning: The installed version of bottleneck 0.7.0 is not supported in pandas and will be not be used
The minimum supported version is 1.0.0

  ver=ver, min_ver=_MIN_BOTTLENECK_VERSION), UserWarning)
]2
&gt;</li></div><div><li>expected:&lt;[]hello world
&gt; but was:&lt;[/usr/lib64/python2.7/site-packages/pandas/core/nanops.py:39: UserWarning: The installed version of bottleneck 0.7.0 is not supported in pandas and will be not be used
The minimum supported version is 1.0.0

  ver=ver, min_ver=_MIN_BOTTLENECK_VERSION), UserWarning)
]hello world
&gt;</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>shared interpreter restart:
Expected: HTTP response &lt;200&gt; from /api/interpreter/setting/restart/md
     but: got &lt;500&gt; Request failed.</li></div><div><li>java.lang.NullPointerException: null
	at org.apache.zeppelin.service.JobManagerService$NoteJobInfo.&lt;init&gt;(JobManagerService.java:124)
	at org.apache.zeppelin.service.JobManagerService.getNoteJobInfo(JobManagerService.java:52)
	at org.apache.zeppelin.socket.NotebookServer$NotebookInformationListener.onParagraphStatusChange(NotebookServer.java:1896)
	at org.apache.zeppelin.socket.NotebookServer.onSt</li></div><div><li>java.lang.NullPointerException: null
	at org.apache.zeppelin.service.JobManagerService$NoteJobInfo.&lt;init&gt;(JobManagerService.java:124)
	at org.apache.zeppelin.service.JobManagerService.getNoteJobInfo(JobManagerService.java:52)
	at org.apache.zeppelin.socket.NotebookServer$NotebookInformationListener.onParagraphStatusChange(NotebookServer.java:1896)
	at org.apache.zeppelin.socket.NotebookServer.onSt</li></div></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.zeppelin.rest.InterpreterRestApiTest.testRestartInterpreterPerNote</div></li><li><div>org.apache.zeppelin.rest.ZeppelinRestApiTest.org.apache.zeppelin.rest.ZeppelinRestApiTest</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.zeppelin.ignite.IgniteInterpreterTest.testInterpretInvalidInput</div></li><li><div>org.apache.zeppelin.ignite.IgniteInterpreterTest.testInterpretInvalidInput</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.zeppelin.rest.ZeppelinRestApiTest.org.apache.zeppelin.rest.ZeppelinRestApiTest</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.zeppelin.ignite.IgniteInterpreterTest.testInterpret</div></li><li><div>org.apache.zeppelin.ignite.IgniteInterpreterTest.testInterpret</div></li><li><div>org.apache.zeppelin.ignite.IgniteInterpreterTest.testInterpretInvalidInput</div></li><li><div>org.apache.zeppelin.ignite.IgniteInterpreterTest.testInterpretInvalidInput</div></li><li><div>org.apache.zeppelin.ignite.IgniteSqlInterpreterTest.testSql</div></li><li><div>org.apache.zeppelin.ignite.IgniteSqlInterpreterTest.testSql</div></li><li><div>org.apache.zeppelin.rest.ZeppelinRestApiTest.org.apache.zeppelin.rest.ZeppelinRestApiTest</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.zeppelin.spark.IPySparkInterpreterTest.testRedefinitionZeppelinContext</div></li><li><div>org.apache.zeppelin.helium.HeliumBundleFactoryTest.bundlePackage</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.zeppelin.python.IPythonInterpreterTest.testIPythonPlotting</div></li><li><div>org.apache.zeppelin.python.IPythonInterpreterTest.testZeppelinContext</div></li><li><div>org.apache.zeppelin.python.PythonInterpreterMatplotlibTest.testClose</div></li><li><div>org.apache.zeppelin.python.PythonInterpreterMatplotlibTest.showPlot</div></li><li><div>org.apache.zeppelin.python.PythonInterpreterMatplotlibTest.dependenciesAreInstalled</div></li><li><div>org.apache.zeppelin.python.PythonInterpreterMatplotlibTest.testNoClose</div></li><li><div>org.apache.zeppelin.python.PythonInterpreterTest.testZeppelinContext</div></li><li><div>org.apache.zeppelin.spark.IPySparkInterpreterTest.testIPySpark</div></li><li><div>org.apache.zeppelin.spark.IPySparkInterpreterTest.testIPythonPlotting</div></li><li><div>org.apache.zeppelin.spark.IPySparkInterpreterTest.testZeppelinContext</div></li><li><div>org.apache.zeppelin.spark.PySparkInterpreterTest.testPySpark</div></li><li><div>org.apache.zeppelin.spark.PySparkInterpreterTest.testZeppelinContext</div></li><li><div>org.apache.zeppelin.rest.ZeppelinSparkClusterTest.pySparkDepLoaderTest[3]</div></li><li><div>org.apache.zeppelin.rest.ZeppelinSparkClusterTest.testZeppelinContextResource[3]</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div style="font-weight:bold;display:none;" class="panel panel-info" name="data" id="zookeeper"><div style="font-weight:bold;" class="panel-heading">ZOOKEEPER<p align="right" role="presentation" style="padding-left:5px;color:grey;display:inline;font-weight:normal">(Pravin)</p></div><div class="panel-body"><div class="bs-callout bs-callout-info"><div><b>Branch Details:</b> refs/remotes/origin/master</div><div><b>Last Revision: </b>fe25fed9390a159b24f4c4fa31e3a7911f2c3b81</div><div><b>Last Run: </b>16-11-2018 03:51 UTC</div></div><table width="100%" style="font-size:13" class="table table-striped"><thead><tr><th width="10%"></th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC UBUNTU18</th><th>X86 UBUNTU18</th><th>PPC RHEL72</th><th>X86 RHEL72</th><th>PPC RHEL75</th><th>X86 RHEL75</th></tr></thead><tbody><tr><td>Summary</td><td><div>Total Count : 2217</div><div>Failed Count : 5</div><div>Skipped Count : 1</div></td><td><div>Total Count : 2231</div><div>Failed Count : 1</div><div>Skipped Count : 2</div></td><td><div>Total Count : 2231</div><div>Failed Count : 5</div><div>Skipped Count : 2</div></td><td><div>Total Count : 2231</div><div>Failed Count : 1</div><div>Skipped Count : 2</div></td><td><div>Total Count : 2220</div><div>Failed Count : 6</div><div>Skipped Count : 2</div></td><td><div>Total Count : 2231</div><div>Failed Count : 2</div><div>Skipped Count : 2</div></td><td><div>Total Count : 2231</div><div>Failed Count : 1</div><div>Skipped Count : 2</div></td><td><div>Total Count : 2231</div><div>Failed Count : 0</div><div>Skipped Count : 2</div></td></tr><tr><td>Result</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;"></img>UNSTABLE</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;"></img>SUCCESS</td></tr><tr><td>Failures</td><td><ol style="padding-left: 1.0em"><div><li>org.apache.zookeeper.server.quorum.QuorumPeerMainTest.testQuorumV6</li></div><div><li>org.apache.zookeeper.test.DisconnectedWatcherTest.testManyChildWatchersAutoReset</li></div><div><li>org.apache.zookeeper.test.ObserverQuorumHammerTest.testHammerBasic</li></div><div><li>org.apache.zookeeper.test.QuorumHammerTest.testHammerBasic</li></div><div><li>org.apache.zookeeper.test.QuorumTest.testMultipleWatcherObjs</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.zookeeper.server.quorum.QuorumPeerMainTest.testQuorumV6</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.zookeeper.server.quorum.QuorumPeerMainTest.testQuorumV6</li></div><div><li>org.apache.zookeeper.test.DisconnectedWatcherTest.testManyChildWatchersAutoReset</li></div><div><li>org.apache.zookeeper.test.ObserverQuorumHammerTest.testHammerBasic</li></div><div><li>org.apache.zookeeper.test.QuorumHammerTest.testHammerBasic</li></div><div><li>org.apache.zookeeper.test.QuorumZxidSyncTest.testBehindLeader</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.zookeeper.server.quorum.QuorumPeerMainTest.testQuorumV6</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.zookeeper.server.quorum.QuorumPeerMainTest.testQuorumV6</li></div><div><li>org.apache.zookeeper.server.quorum.Zab1_0Test.testNormalObserverRun</li></div><div><li>org.apache.zookeeper.test.DisconnectedWatcherTest.testManyChildWatchersAutoReset</li></div><div><li>org.apache.zookeeper.test.ObserverQuorumHammerTest.testHammerBasic</li></div><div><li>org.apache.zookeeper.test.QuorumHammerTest.testHammerBasic</li></div><div><li>org.apache.zookeeper.test.QuorumTest.testSequentialNodeNames</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.zookeeper.server.quorum.QuorumPeerMainTest.testLeaderOutOfView</li></div><div><li>org.apache.zookeeper.server.quorum.ReconfigRecoveryTest.testCurrentServersAreObserversInNextConfig</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>org.apache.zookeeper.server.quorum.QuorumPeerMainTest.testQuorumV6</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td>Description</td><td><ol style="padding-left: 1.0em"><div><li>waiting for server 1 being up</li></div><div><li>test timed out after 840000 milliseconds</li></div><div><li>Timeout occurred. Please note the time in the report does not reflect the time until the timeout.</li></div><div><li>Timeout occurred. Please note the time in the report does not reflect the time until the timeout.</li></div><div><li>Timeout occurred. Please note the time in the report does not reflect the time until the timeout.</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>waiting for server 1 being up</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>waiting for server 1 being up</li></div><div><li>test timed out after 840000 milliseconds</li></div><div><li>expected:&lt;1000&gt; but was:&lt;23&gt;</li></div><div><li>Timeout occurred. Please note the time in the report does not reflect the time until the timeout.</li></div><div><li>KeeperErrorCode = ConnectionLoss for /2</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>waiting for server 1 being up</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>waiting for server 1 being up</li></div><div><li>Timeout occurred. Please note the time in the report does not reflect the time until the timeout.</li></div><div><li>test timed out after 840000 milliseconds</li></div><div><li>Timeout occurred. Please note the time in the report does not reflect the time until the timeout.</li></div><div><li>expected:&lt;1000&gt; but was:&lt;22&gt;</li></div><div><li>KeeperErrorCode = ConnectionLoss for /SEQUENCE/TEST</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>junit.framework.AssertionFailedError
	at org.apache.zookeeper.server.quorum.QuorumPeerMainTest.testLeaderOutOfView(QuorumPeerMainTest.java:973)
	at org.apache.zookeeper.JUnit4ZKTestRunner$LoggedInvokeMethod.evaluate(JUnit4ZKTestRunner.java:79)
</li></div><div><li>waiting for server 4 being up</li></div></ol></td><td><ol style="padding-left: 1.0em"><div><li>waiting for server 1 being up</li></div></ol></td><td><ol style="padding-left: 1.0em"></ol></td></tr><tr><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;">Unique Failures</td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.zookeeper.test.DisconnectedWatcherTest.testManyChildWatchersAutoReset</div></li><li><div>org.apache.zookeeper.test.ObserverQuorumHammerTest.testHammerBasic</div></li><li><div>org.apache.zookeeper.test.QuorumHammerTest.testHammerBasic</div></li><li><div>org.apache.zookeeper.test.QuorumTest.testMultipleWatcherObjs</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.zookeeper.test.DisconnectedWatcherTest.testManyChildWatchersAutoReset</div></li><li><div>org.apache.zookeeper.test.ObserverQuorumHammerTest.testHammerBasic</div></li><li><div>org.apache.zookeeper.test.QuorumHammerTest.testHammerBasic</div></li><li><div>org.apache.zookeeper.test.QuorumZxidSyncTest.testBehindLeader</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.zookeeper.server.quorum.QuorumPeerMainTest.testQuorumV6</div></li><li><div>org.apache.zookeeper.server.quorum.Zab1_0Test.testNormalObserverRun</div></li><li><div>org.apache.zookeeper.test.DisconnectedWatcherTest.testManyChildWatchersAutoReset</div></li><li><div>org.apache.zookeeper.test.ObserverQuorumHammerTest.testHammerBasic</div></li><li><div>org.apache.zookeeper.test.QuorumHammerTest.testHammerBasic</div></li><li><div>org.apache.zookeeper.test.QuorumTest.testSequentialNodeNames</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.zookeeper.server.quorum.QuorumPeerMainTest.testLeaderOutOfView</div></li><li><div>org.apache.zookeeper.server.quorum.ReconfigRecoveryTest.testCurrentServersAreObserversInNextConfig</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"><li><div>org.apache.zookeeper.server.quorum.QuorumPeerMainTest.testQuorumV6</div></li></ol></td><td style="word-wrap: break-word;min-width: 160px;max-width: 220px;"><ol style="padding-left: 1.0em"></ol></td></tr></tbody></table></div></div><div id="ubuntu16" style="font-weight:bold;font-size:12;display:none" class="panel panel-info" name="summary"><div class="panel-heading"><div class="panel-title">UBUNTU16 SUMMARY</div></div><div class="bs-callout bs-callout-info"><div><b>OS: </b>UBUNTU 16.04</div></div><table style="font-size:14" class="table table-striped"><tbody><tr><th>Package Name</th><th>PPC</th><th>X86</th><th></th></tr><tr><td><a href="#" id="anchor_accumulo" onclick="showme(this.id);">ACCUMULO</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_ambari" onclick="showme(this.id);">AMBARI</a></td><td><img src="resources/red.png" align="top" style="width: 16px; height: 16px;" title="FAILURE"></img>0</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td></tr><tr><td><a href="#" id="anchor_atlas" onclick="showme(this.id);">ATLAS</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_calcite" onclick="showme(this.id);">CALCITE</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_datafu" onclick="showme(this.id);">DATAFU</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_druid" onclick="showme(this.id);">DRUID</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>6 (4)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2</td></tr><tr><td><a href="#" id="anchor_falcon" onclick="showme(this.id);">FALCON</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_flume" onclick="showme(this.id);">FLUME</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_hadoop" onclick="showme(this.id);">HADOOP</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>67 (10)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>61 (4)</td></tr><tr><td><a href="#" id="anchor_hbase" onclick="showme(this.id);">HBASE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>8 (2)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>6</td></tr><tr><td><a href="#" id="anchor_hive" onclick="showme(this.id);">HIVE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3 (2)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3 (2)</td></tr><tr><td><a href="#" id="anchor_kafka" onclick="showme(this.id);">KAFKA</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_knox" onclick="showme(this.id);">KNOX</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_metron" onclick="showme(this.id);">METRON</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>13</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>13</td></tr><tr><td><a href="#" id="anchor_oozie" onclick="showme(this.id);">OOZIE</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td></tr><tr><td><a href="#" id="anchor_phoenix" onclick="showme(this.id);">PHOENIX</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_pig" onclick="showme(this.id);">PIG</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>9</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>9</td></tr><tr><td><a href="#" id="anchor_ranger" onclick="showme(this.id);">RANGER</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_slider" onclick="showme(this.id);">SLIDER</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_spark" onclick="showme(this.id);">SPARK</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3 (3)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>862 (862)</td></tr><tr><td><a href="#" id="anchor_sqoop" onclick="showme(this.id);">SQOOP</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_storm" onclick="showme(this.id);">STORM</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_tez" onclick="showme(this.id);">TEZ</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3 (2)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td></tr><tr><td><a href="#" id="anchor_zeppelin" onclick="showme(this.id);">ZEPPELIN</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3 (2)</td></tr><tr><td><a href="#" id="anchor_zookeeper" onclick="showme(this.id);">ZOOKEEPER</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>5 (4)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td></tr></tbody></table></div><div id="ubuntu18" style="font-weight:bold;font-size:12;display:none" class="panel panel-info" name="summary"><div class="panel-heading"><div class="panel-title">UBUNTU18 SUMMARY</div></div><div class="bs-callout bs-callout-info"><div><b>OS: </b>UBUNTU 18.04</div></div><table style="font-size:14" class="table table-striped"><tbody><tr><th>Package Name</th><th>PPC</th><th>X86</th><th></th></tr><tr><td><a href="#" id="anchor_accumulo" onclick="showme(this.id);">ACCUMULO</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_ambari" onclick="showme(this.id);">AMBARI</a></td><td><img src="resources/red.png" align="top" style="width: 16px; height: 16px;" title="FAILURE"></img>0</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td></tr><tr><td><a href="#" id="anchor_atlas" onclick="showme(this.id);">ATLAS</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_calcite" onclick="showme(this.id);">CALCITE</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_datafu" onclick="showme(this.id);">DATAFU</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_druid" onclick="showme(this.id);">DRUID</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>6 (4)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2</td></tr><tr><td><a href="#" id="anchor_falcon" onclick="showme(this.id);">FALCON</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_flume" onclick="showme(this.id);">FLUME</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_hadoop" onclick="showme(this.id);">HADOOP</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>69 (6)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>71 (8)</td></tr><tr><td><a href="#" id="anchor_hbase" onclick="showme(this.id);">HBASE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>69 (65)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>6 (2)</td></tr><tr><td><a href="#" id="anchor_hive" onclick="showme(this.id);">HIVE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>15 (12)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3</td></tr><tr><td><a href="#" id="anchor_kafka" onclick="showme(this.id);">KAFKA</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td></tr><tr><td><a href="#" id="anchor_knox" onclick="showme(this.id);">KNOX</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_metron" onclick="showme(this.id);">METRON</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>13</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>13</td></tr><tr><td><a href="#" id="anchor_oozie" onclick="showme(this.id);">OOZIE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (2)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (2)</td></tr><tr><td><a href="#" id="anchor_phoenix" onclick="showme(this.id);">PHOENIX</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_pig" onclick="showme(this.id);">PIG</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>9</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>9</td></tr><tr><td><a href="#" id="anchor_ranger" onclick="showme(this.id);">RANGER</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_slider" onclick="showme(this.id);">SLIDER</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td></tr><tr><td><a href="#" id="anchor_spark" onclick="showme(this.id);">SPARK</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>54 (54)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>885 (885)</td></tr><tr><td><a href="#" id="anchor_sqoop" onclick="showme(this.id);">SQOOP</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_storm" onclick="showme(this.id);">STORM</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_tez" onclick="showme(this.id);">TEZ</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3 (2)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td></tr><tr><td><a href="#" id="anchor_zeppelin" onclick="showme(this.id);">ZEPPELIN</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3 (2)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (1)</td></tr><tr><td><a href="#" id="anchor_zookeeper" onclick="showme(this.id);">ZOOKEEPER</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>5 (4)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td></tr></tbody></table></div><div id="rhel72" style="font-weight:bold;font-size:12;display:none" class="panel panel-info" name="summary"><div class="panel-heading"><div class="panel-title">RHEL72 SUMMARY</div></div><div class="bs-callout bs-callout-info"><div><b>OS: </b>RHEL 7.2</div></div><table style="font-size:14" class="table table-striped"><tbody><tr><th>Package Name</th><th>PPC</th><th>X86</th><th></th></tr><tr><td><a href="#" id="anchor_accumulo" onclick="showme(this.id);">ACCUMULO</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_ambari" onclick="showme(this.id);">AMBARI</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_atlas" onclick="showme(this.id);">ATLAS</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_calcite" onclick="showme(this.id);">CALCITE</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_datafu" onclick="showme(this.id);">DATAFU</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_druid" onclick="showme(this.id);">DRUID</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>6 (4)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2</td></tr><tr><td><a href="#" id="anchor_falcon" onclick="showme(this.id);">FALCON</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_flume" onclick="showme(this.id);">FLUME</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_hadoop" onclick="showme(this.id);">HADOOP</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>212 (158)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>63 (9)</td></tr><tr><td><a href="#" id="anchor_hbase" onclick="showme(this.id);">HBASE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3 (3)</td></tr><tr><td><a href="#" id="anchor_hive" onclick="showme(this.id);">HIVE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3</td></tr><tr><td><a href="#" id="anchor_kafka" onclick="showme(this.id);">KAFKA</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_knox" onclick="showme(this.id);">KNOX</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_metron" onclick="showme(this.id);">METRON</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>13</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>13</td></tr><tr><td><a href="#" id="anchor_oozie" onclick="showme(this.id);">OOZIE</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td></tr><tr><td><a href="#" id="anchor_phoenix" onclick="showme(this.id);">PHOENIX</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_pig" onclick="showme(this.id);">PIG</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>9</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>9</td></tr><tr><td><a href="#" id="anchor_ranger" onclick="showme(this.id);">RANGER</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_slider" onclick="showme(this.id);">SLIDER</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_spark" onclick="showme(this.id);">SPARK</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>46 (46)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_sqoop" onclick="showme(this.id);">SQOOP</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_storm" onclick="showme(this.id);">STORM</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_tez" onclick="showme(this.id);">TEZ</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3 (2)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td></tr><tr><td><a href="#" id="anchor_zeppelin" onclick="showme(this.id);">ZEPPELIN</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>8 (7)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3 (2)</td></tr><tr><td><a href="#" id="anchor_zookeeper" onclick="showme(this.id);">ZOOKEEPER</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>6 (6)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (2)</td></tr></tbody></table></div><div id="rhel75" style="font-weight:bold;font-size:12;display:none" class="panel panel-info" name="summary"><div class="panel-heading"><div class="panel-title">RHEL75 SUMMARY</div></div><div class="bs-callout bs-callout-info"><div><b>OS: </b>RHEL 7.5</div></div><table style="font-size:14" class="table table-striped"><tbody><tr><th>Package Name</th><th>PPC</th><th>X86</th><th></th></tr><tr><td><a href="#" id="anchor_accumulo" onclick="showme(this.id);">ACCUMULO</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_ambari" onclick="showme(this.id);">AMBARI</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (2)</td></tr><tr><td><a href="#" id="anchor_atlas" onclick="showme(this.id);">ATLAS</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_calcite" onclick="showme(this.id);">CALCITE</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_datafu" onclick="showme(this.id);">DATAFU</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_druid" onclick="showme(this.id);">DRUID</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>6 (4)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2</td></tr><tr><td><a href="#" id="anchor_falcon" onclick="showme(this.id);">FALCON</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_flume" onclick="showme(this.id);">FLUME</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_hadoop" onclick="showme(this.id);">HADOOP</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>60 (8)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>68 (16)</td></tr><tr><td><a href="#" id="anchor_hbase" onclick="showme(this.id);">HBASE</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td></tr><tr><td><a href="#" id="anchor_hive" onclick="showme(this.id);">HIVE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>8 (7)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td></tr><tr><td><a href="#" id="anchor_kafka" onclick="showme(this.id);">KAFKA</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td></tr><tr><td><a href="#" id="anchor_knox" onclick="showme(this.id);">KNOX</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_metron" onclick="showme(this.id);">METRON</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>13</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>13</td></tr><tr><td><a href="#" id="anchor_oozie" onclick="showme(this.id);">OOZIE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_phoenix" onclick="showme(this.id);">PHOENIX</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_pig" onclick="showme(this.id);">PIG</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>9</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>10 (1)</td></tr><tr><td><a href="#" id="anchor_ranger" onclick="showme(this.id);">RANGER</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_slider" onclick="showme(this.id);">SLIDER</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_spark" onclick="showme(this.id);">SPARK</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td></tr><tr><td><a href="#" id="anchor_sqoop" onclick="showme(this.id);">SQOOP</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_storm" onclick="showme(this.id);">STORM</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_tez" onclick="showme(this.id);">TEZ</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (1)</td></tr><tr><td><a href="#" id="anchor_zeppelin" onclick="showme(this.id);">ZEPPELIN</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>17 (14)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3</td></tr><tr><td><a href="#" id="anchor_zookeeper" onclick="showme(this.id);">ZOOKEEPER</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr></tbody></table></div><div id="ppcx86" style="display:block;font-weight:bold" class="panel panel-info" name="summary"><div class="panel-heading"><div class="panel-title">FULL SUMMARY</div></div><table style="font-size:14" id="summarytable" class="table table-striped"><tbody><tr><th></th></tr><tr><th>Package Name</th><th>PPC UBUNTU16</th><th>X86 UBUNTU16</th><th>PPC UBUNTU18</th><th>X86 UBUNTU18</th><th>PPC RHEL72</th><th>X86 RHEL72</th><th>PPC RHEL75</th><th>X86 RHEL75</th></tr><tr><td><a href="#" id="anchor_accumulo" onclick="showme(this.id);">ACCUMULO</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_ambari" onclick="showme(this.id);">AMBARI</a></td><td>N/A</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td>N/A</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (2)</td></tr><tr><td><a href="#" id="anchor_atlas" onclick="showme(this.id);">ATLAS</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_calcite" onclick="showme(this.id);">CALCITE</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_datafu" onclick="showme(this.id);">DATAFU</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_druid" onclick="showme(this.id);">DRUID</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>6 (4)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>6 (4)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>6 (4)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>6 (4)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2</td></tr><tr><td><a href="#" id="anchor_falcon" onclick="showme(this.id);">FALCON</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_flume" onclick="showme(this.id);">FLUME</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_hadoop" onclick="showme(this.id);">HADOOP</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>67 (10)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>61 (4)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>69 (6)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>71 (8)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>212 (158)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>63 (9)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>60 (8)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>68 (16)</td></tr><tr><td><a href="#" id="anchor_hbase" onclick="showme(this.id);">HBASE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>8 (2)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>6</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>69 (65)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>6 (2)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3 (3)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td></tr><tr><td><a href="#" id="anchor_hive" onclick="showme(this.id);">HIVE</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3 (2)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3 (2)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>15 (12)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>8 (7)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td></tr><tr><td><a href="#" id="anchor_kafka" onclick="showme(this.id);">KAFKA</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td></tr><tr><td><a href="#" id="anchor_knox" onclick="showme(this.id);">KNOX</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_metron" onclick="showme(this.id);">METRON</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>13</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>13</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>13</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>13</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>13</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>13</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>13</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>13</td></tr><tr><td><a href="#" id="anchor_oozie" onclick="showme(this.id);">OOZIE</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (2)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (2)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_phoenix" onclick="showme(this.id);">PHOENIX</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_pig" onclick="showme(this.id);">PIG</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>9</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>9</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>9</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>9</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>9</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>9</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>9</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>10 (1)</td></tr><tr><td><a href="#" id="anchor_ranger" onclick="showme(this.id);">RANGER</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_slider" onclick="showme(this.id);">SLIDER</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_spark" onclick="showme(this.id);">SPARK</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3 (3)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>862 (862)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>54 (54)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>885 (885)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>46 (46)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td></tr><tr><td><a href="#" id="anchor_sqoop" onclick="showme(this.id);">SQOOP</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_storm" onclick="showme(this.id);">STORM</a></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr><tr><td><a href="#" id="anchor_tez" onclick="showme(this.id);">TEZ</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3 (2)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3 (2)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3 (2)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (1)</td></tr><tr><td><a href="#" id="anchor_zeppelin" onclick="showme(this.id);">ZEPPELIN</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3 (2)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3 (2)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (1)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>8 (7)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3 (2)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>17 (14)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>3</td></tr><tr><td><a href="#" id="anchor_zookeeper" onclick="showme(this.id);">ZOOKEEPER</a></td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>5 (4)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>5 (4)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>6 (6)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>2 (2)</td><td><img src="resources/yellow.png" align="top" style="width: 16px; height: 16px;" title="UNSTABLE"></img>1 (1)</td><td><img src="resources/blue.png" align="top" style="width: 16px; height: 16px;" title="SUCCESS"></img></td></tr></tbody></table></div></div></body></html>